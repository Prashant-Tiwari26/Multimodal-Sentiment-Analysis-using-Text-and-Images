{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import load_npz\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torch.utils.data import random_split, DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_tfidf = load_npz(\"../Data/Text/TF-IDF/captions.npz\")\n",
    "tweets_bert = np.load(\"../Data/Text/BERT/captions.npy\")\n",
    "EfficientNet = np.load(\"../Data/Images/Image Embeddings/EfficientNet.npy\")\n",
    "DenseNet = np.load(\"../Data/Images/Image Embeddings/densenet.npy\")\n",
    "DenseNet = np.squeeze(DenseNet, axis=1)\n",
    "y = np.load(\"../Data/Text/TF-IDF/labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_efficientnet = np.hstack([tweets_bert, EfficientNet])\n",
    "tfidf_efficientnet = np.hstack([tweets_tfidf.toarray(), EfficientNet])\n",
    "bert_densenet = np.hstack([tweets_bert, DenseNet])\n",
    "tfidf_densenet = np.hstack([tweets_tfidf.toarray(), DenseNet])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4869, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = OneHotEncoder(sparse_output=False).fit_transform(y.reshape(-1,1))\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model01_bert_eff(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2048, 1024),\n",
    "            torch.nn.SELU(),\n",
    "            torch.nn.Linear(1024, 512),\n",
    "            torch.nn.SELU(),\n",
    "            torch.nn.Linear(512, 256),\n",
    "            torch.nn.SELU(),\n",
    "            torch.nn.Linear(256, 3)\n",
    "        )\n",
    "\n",
    "        for layer in self.model:\n",
    "            if isinstance(layer, torch.nn.Linear):\n",
    "                torch.nn.init.kaiming_uniform_(layer.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.from_numpy(bert_efficientnet).type(torch.float)\n",
    "y = torch.from_numpy(y).type(torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_bert_eff = TensorDataset(x,y)\n",
    "train_size = int(0.7 * len(dataset_bert_eff))\n",
    "test_size = int(0.2*len(dataset_bert_eff))\n",
    "val_size = len(dataset_bert_eff) - train_size - test_size\n",
    "train_set, test_set, val_set = random_split(dataset_bert_eff, [train_size, test_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_set, 32, True)\n",
    "test_loader = DataLoader(test_set, 32, True)\n",
    "val_loader = DataLoader(val_set, 32, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model01_bert_eff()\n",
    "optimizer = torch.optim.NAdam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1              [-1, 1, 1024]       2,098,176\n",
      "              SELU-2              [-1, 1, 1024]               0\n",
      "            Linear-3               [-1, 1, 512]         524,800\n",
      "              SELU-4               [-1, 1, 512]               0\n",
      "            Linear-5               [-1, 1, 256]         131,328\n",
      "              SELU-6               [-1, 1, 256]               0\n",
      "            Linear-7                 [-1, 1, 3]             771\n",
      "================================================================\n",
      "Total params: 2,755,075\n",
      "Trainable params: 2,755,075\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.03\n",
      "Params size (MB): 10.51\n",
      "Estimated Total Size (MB): 10.54\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "model.to(\"cuda\")\n",
    "summary(model, (1, 2048))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0\n",
      "----------------------\n",
      "Loss for batch: 0 = 0.16510362923145294\n",
      "Loss for batch: 1 = 0.1815323531627655\n",
      "Loss for batch: 2 = 0.25004953145980835\n",
      "Loss for batch: 3 = 0.24825891852378845\n",
      "Loss for batch: 4 = 0.35715755820274353\n",
      "Loss for batch: 5 = 0.29252269864082336\n",
      "Loss for batch: 6 = 0.12436291575431824\n",
      "Loss for batch: 7 = 0.21199136972427368\n",
      "Loss for batch: 8 = 0.2951031029224396\n",
      "Loss for batch: 9 = 0.2718105912208557\n",
      "Loss for batch: 10 = 0.2768493890762329\n",
      "Loss for batch: 11 = 0.1324886679649353\n",
      "Loss for batch: 12 = 0.16855637729167938\n",
      "Loss for batch: 13 = 0.34600186347961426\n",
      "Loss for batch: 14 = 0.14948579668998718\n",
      "Loss for batch: 15 = 0.1653413623571396\n",
      "Loss for batch: 16 = 0.27836960554122925\n",
      "Loss for batch: 17 = 0.16349713504314423\n",
      "Loss for batch: 18 = 0.15971356630325317\n",
      "Loss for batch: 19 = 0.29361850023269653\n",
      "Loss for batch: 20 = 0.1472408026456833\n",
      "Loss for batch: 21 = 0.34642142057418823\n",
      "Loss for batch: 22 = 0.21201108396053314\n",
      "Loss for batch: 23 = 0.2578083872795105\n",
      "Loss for batch: 24 = 0.17373421788215637\n",
      "Loss for batch: 25 = 0.1067134365439415\n",
      "Loss for batch: 26 = 0.21379810571670532\n",
      "Loss for batch: 27 = 0.1186816543340683\n",
      "Loss for batch: 28 = 0.27601122856140137\n",
      "Loss for batch: 29 = 0.17177240550518036\n",
      "Loss for batch: 30 = 0.2483648955821991\n",
      "Loss for batch: 31 = 0.3121281564235687\n",
      "Loss for batch: 32 = 0.41762232780456543\n",
      "Loss for batch: 33 = 0.23884177207946777\n",
      "Loss for batch: 34 = 0.18288104236125946\n",
      "Loss for batch: 35 = 0.4434894323348999\n",
      "Loss for batch: 36 = 0.29474639892578125\n",
      "Loss for batch: 37 = 0.20376162230968475\n",
      "Loss for batch: 38 = 0.2230471819639206\n",
      "Loss for batch: 39 = 0.1977311074733734\n",
      "Loss for batch: 40 = 0.3122851848602295\n",
      "Loss for batch: 41 = 0.348774790763855\n",
      "Loss for batch: 42 = 0.25524306297302246\n",
      "Loss for batch: 43 = 0.3988732695579529\n",
      "Loss for batch: 44 = 0.2587921917438507\n",
      "Loss for batch: 45 = 0.2518835961818695\n",
      "Loss for batch: 46 = 0.21803176403045654\n",
      "Loss for batch: 47 = 0.25341498851776123\n",
      "Loss for batch: 48 = 0.1694917380809784\n",
      "Loss for batch: 49 = 0.2584247291088104\n",
      "Loss for batch: 50 = 0.3154396712779999\n",
      "Loss for batch: 51 = 0.4218977391719818\n",
      "Loss for batch: 52 = 0.11921781301498413\n",
      "Loss for batch: 53 = 0.13983309268951416\n",
      "Loss for batch: 54 = 0.24004289507865906\n",
      "Loss for batch: 55 = 0.46691787242889404\n",
      "Loss for batch: 56 = 0.31489214301109314\n",
      "Loss for batch: 57 = 0.3525276780128479\n",
      "Loss for batch: 58 = 0.1379268914461136\n",
      "Loss for batch: 59 = 0.29283607006073\n",
      "Loss for batch: 60 = 0.13399532437324524\n",
      "Loss for batch: 61 = 0.17233288288116455\n",
      "Loss for batch: 62 = 0.23545241355895996\n",
      "Loss for batch: 63 = 0.2379726618528366\n",
      "Loss for batch: 64 = 0.20989327132701874\n",
      "Loss for batch: 65 = 0.15070360898971558\n",
      "Loss for batch: 66 = 0.12782834470272064\n",
      "Loss for batch: 67 = 0.37555962800979614\n",
      "Loss for batch: 68 = 0.5055205821990967\n",
      "Loss for batch: 69 = 0.19012737274169922\n",
      "Loss for batch: 70 = 0.27869904041290283\n",
      "Loss for batch: 71 = 0.1213960275053978\n",
      "Loss for batch: 72 = 0.1854667365550995\n",
      "Loss for batch: 73 = 0.3800508379936218\n",
      "Loss for batch: 74 = 0.31431061029434204\n",
      "Loss for batch: 75 = 0.21311329305171967\n",
      "Loss for batch: 76 = 0.2074558436870575\n",
      "Loss for batch: 77 = 0.24524430930614471\n",
      "Loss for batch: 78 = 0.27288687229156494\n",
      "Loss for batch: 79 = 0.2721145749092102\n",
      "Loss for batch: 80 = 0.14492836594581604\n",
      "Loss for batch: 81 = 0.21277296543121338\n",
      "Loss for batch: 82 = 0.1939348429441452\n",
      "Loss for batch: 83 = 0.11706283688545227\n",
      "Loss for batch: 84 = 0.17899028956890106\n",
      "Loss for batch: 85 = 0.43093788623809814\n",
      "Loss for batch: 86 = 0.4715064764022827\n",
      "Loss for batch: 87 = 0.12198051065206528\n",
      "Loss for batch: 88 = 0.2811243236064911\n",
      "Loss for batch: 89 = 0.22367310523986816\n",
      "Loss for batch: 90 = 0.31331339478492737\n",
      "Loss for batch: 91 = 0.1665063053369522\n",
      "Loss for batch: 92 = 0.15861955285072327\n",
      "Loss for batch: 93 = 0.16286221146583557\n",
      "Loss for batch: 94 = 0.36220991611480713\n",
      "Loss for batch: 95 = 0.2655540704727173\n",
      "Loss for batch: 96 = 0.3599584102630615\n",
      "Loss for batch: 97 = 0.25684642791748047\n",
      "Loss for batch: 98 = 0.2708819508552551\n",
      "Loss for batch: 99 = 0.1521218866109848\n",
      "Loss for batch: 100 = 0.206537127494812\n",
      "Loss for batch: 101 = 0.23677115142345428\n",
      "Loss for batch: 102 = 0.15294748544692993\n",
      "Loss for batch: 103 = 0.2303837239742279\n",
      "Loss for batch: 104 = 0.27731454372406006\n",
      "Loss for batch: 105 = 0.2388538122177124\n",
      "Loss for batch: 106 = 0.37074384093284607\n",
      "\n",
      "Training Loss for epoch 0 = 26.234827041625977\n",
      "\n",
      "Current Validation Loss = 20.710865020751953\n",
      "Best Validation Loss = 20.710865020751953\n",
      "Epochs without Improvement = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [00:00<00:42,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1\n",
      "----------------------\n",
      "Loss for batch: 0 = 0.43679746985435486\n",
      "Loss for batch: 1 = 0.33996373414993286\n",
      "Loss for batch: 2 = 0.32471832633018494\n",
      "Loss for batch: 3 = 0.5458394289016724\n",
      "Loss for batch: 4 = 0.9429407119750977\n",
      "Loss for batch: 5 = 0.25562068819999695\n",
      "Loss for batch: 6 = 0.25737106800079346\n",
      "Loss for batch: 7 = 0.2225121706724167\n",
      "Loss for batch: 8 = 0.28715047240257263\n",
      "Loss for batch: 9 = 0.11325522512197495\n",
      "Loss for batch: 10 = 0.18216297030448914\n",
      "Loss for batch: 11 = 0.12377022206783295\n",
      "Loss for batch: 12 = 0.21989071369171143\n",
      "Loss for batch: 13 = 0.17333540320396423\n",
      "Loss for batch: 14 = 0.13166514039039612\n",
      "Loss for batch: 15 = 0.2821546196937561\n",
      "Loss for batch: 16 = 0.2561570107936859\n",
      "Loss for batch: 17 = 0.20859217643737793\n",
      "Loss for batch: 18 = 0.1429685652256012\n",
      "Loss for batch: 19 = 0.1838344782590866\n",
      "Loss for batch: 20 = 0.15666206181049347\n",
      "Loss for batch: 21 = 0.11295008659362793\n",
      "Loss for batch: 22 = 0.148330420255661\n",
      "Loss for batch: 23 = 0.09682852029800415\n",
      "Loss for batch: 24 = 0.20329934358596802\n",
      "Loss for batch: 25 = 0.1427551805973053\n",
      "Loss for batch: 26 = 0.13367360830307007\n",
      "Loss for batch: 27 = 0.15298666059970856\n",
      "Loss for batch: 28 = 0.0745115578174591\n",
      "Loss for batch: 29 = 0.23976825177669525\n",
      "Loss for batch: 30 = 0.09914565086364746\n",
      "Loss for batch: 31 = 0.08824577927589417\n",
      "Loss for batch: 32 = 0.15228775143623352\n",
      "Loss for batch: 33 = 0.19715569913387299\n",
      "Loss for batch: 34 = 0.326657772064209\n",
      "Loss for batch: 35 = 0.15046122670173645\n",
      "Loss for batch: 36 = 0.22477887570858002\n",
      "Loss for batch: 37 = 0.22330805659294128\n",
      "Loss for batch: 38 = 0.17921030521392822\n",
      "Loss for batch: 39 = 0.15390680730342865\n",
      "Loss for batch: 40 = 0.1603367179632187\n",
      "Loss for batch: 41 = 0.21765610575675964\n",
      "Loss for batch: 42 = 0.23867718875408173\n",
      "Loss for batch: 43 = 0.1256578117609024\n",
      "Loss for batch: 44 = 0.14446398615837097\n",
      "Loss for batch: 45 = 0.16367816925048828\n",
      "Loss for batch: 46 = 0.23884263634681702\n",
      "Loss for batch: 47 = 0.12371062487363815\n",
      "Loss for batch: 48 = 0.27831321954727173\n",
      "Loss for batch: 49 = 0.2947753965854645\n",
      "Loss for batch: 50 = 0.1290985643863678\n",
      "Loss for batch: 51 = 0.17623791098594666\n",
      "Loss for batch: 52 = 0.2006593644618988\n",
      "Loss for batch: 53 = 0.2646860182285309\n",
      "Loss for batch: 54 = 0.09867188334465027\n",
      "Loss for batch: 55 = 0.24541941285133362\n",
      "Loss for batch: 56 = 0.25602537393569946\n",
      "Loss for batch: 57 = 0.17874473333358765\n",
      "Loss for batch: 58 = 0.2680099904537201\n",
      "Loss for batch: 59 = 0.20948517322540283\n",
      "Loss for batch: 60 = 0.34173548221588135\n",
      "Loss for batch: 61 = 0.19173729419708252\n",
      "Loss for batch: 62 = 0.22310611605644226\n",
      "Loss for batch: 63 = 0.19813036918640137\n",
      "Loss for batch: 64 = 0.2379559725522995\n",
      "Loss for batch: 65 = 0.11719898879528046\n",
      "Loss for batch: 66 = 0.2685636281967163\n",
      "Loss for batch: 67 = 0.1946568489074707\n",
      "Loss for batch: 68 = 0.2601436376571655\n",
      "Loss for batch: 69 = 0.45772671699523926\n",
      "Loss for batch: 70 = 0.29393088817596436\n",
      "Loss for batch: 71 = 0.13281899690628052\n",
      "Loss for batch: 72 = 0.11801942437887192\n",
      "Loss for batch: 73 = 0.2771393060684204\n",
      "Loss for batch: 74 = 0.28159791231155396\n",
      "Loss for batch: 75 = 0.4261629581451416\n",
      "Loss for batch: 76 = 0.5271780490875244\n",
      "Loss for batch: 77 = 0.3154561221599579\n",
      "Loss for batch: 78 = 0.22363732755184174\n",
      "Loss for batch: 79 = 0.22543257474899292\n",
      "Loss for batch: 80 = 0.1718388795852661\n",
      "Loss for batch: 81 = 0.2054964303970337\n",
      "Loss for batch: 82 = 0.12455301731824875\n",
      "Loss for batch: 83 = 0.1911589354276657\n",
      "Loss for batch: 84 = 0.2610956132411957\n",
      "Loss for batch: 85 = 0.16028201580047607\n",
      "Loss for batch: 86 = 0.23975138366222382\n",
      "Loss for batch: 87 = 0.12435738742351532\n",
      "Loss for batch: 88 = 0.12747757136821747\n",
      "Loss for batch: 89 = 0.17112666368484497\n",
      "Loss for batch: 90 = 0.21348357200622559\n",
      "Loss for batch: 91 = 0.3044898211956024\n",
      "Loss for batch: 92 = 0.3399432599544525\n",
      "Loss for batch: 93 = 0.18174783885478973\n",
      "Loss for batch: 94 = 0.1659080535173416\n",
      "Loss for batch: 95 = 0.20892399549484253\n",
      "Loss for batch: 96 = 0.20192137360572815\n",
      "Loss for batch: 97 = 0.14338915050029755\n",
      "Loss for batch: 98 = 0.21612165868282318\n",
      "Loss for batch: 99 = 0.2944384813308716\n",
      "Loss for batch: 100 = 0.34973394870758057\n",
      "Loss for batch: 101 = 0.35820361971855164\n",
      "Loss for batch: 102 = 0.07056024670600891\n",
      "Loss for batch: 103 = 0.24906107783317566\n",
      "Loss for batch: 104 = 0.1516527533531189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/50 [00:01<00:36,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch: 105 = 0.2905783951282501\n",
      "Loss for batch: 106 = 0.32832321524620056\n",
      "\n",
      "Training Loss for epoch 1 = 24.0587215423584\n",
      "\n",
      "Current Validation Loss = 15.77910041809082\n",
      "Best Validation Loss = 15.77910041809082\n",
      "Epochs without Improvement = 0\n",
      "Epoch : 2\n",
      "----------------------\n",
      "Loss for batch: 0 = 0.08063609898090363\n",
      "Loss for batch: 1 = 0.09495377540588379\n",
      "Loss for batch: 2 = 0.15742838382720947\n",
      "Loss for batch: 3 = 0.11323980987071991\n",
      "Loss for batch: 4 = 0.09009574353694916\n",
      "Loss for batch: 5 = 0.13801951706409454\n",
      "Loss for batch: 6 = 0.07868731766939163\n",
      "Loss for batch: 7 = 0.10021860897541046\n",
      "Loss for batch: 8 = 0.12226293981075287\n",
      "Loss for batch: 9 = 0.13276687264442444\n",
      "Loss for batch: 10 = 0.09520523250102997\n",
      "Loss for batch: 11 = 0.1266527771949768\n",
      "Loss for batch: 12 = 0.2730567455291748\n",
      "Loss for batch: 13 = 0.40013188123703003\n",
      "Loss for batch: 14 = 0.1914074867963791\n",
      "Loss for batch: 15 = 0.1970391720533371\n",
      "Loss for batch: 16 = 0.11657094955444336\n",
      "Loss for batch: 17 = 0.2310301512479782\n",
      "Loss for batch: 18 = 0.0982414036989212\n",
      "Loss for batch: 19 = 0.057862959802150726\n",
      "Loss for batch: 20 = 0.08652544021606445\n",
      "Loss for batch: 21 = 0.15552155673503876\n",
      "Loss for batch: 22 = 0.21137657761573792\n",
      "Loss for batch: 23 = 0.19536122679710388\n",
      "Loss for batch: 24 = 0.11025097221136093\n",
      "Loss for batch: 25 = 0.15915042161941528\n",
      "Loss for batch: 26 = 0.2292000949382782\n",
      "Loss for batch: 27 = 0.13953454792499542\n",
      "Loss for batch: 28 = 0.12919101119041443\n",
      "Loss for batch: 29 = 0.17106497287750244\n",
      "Loss for batch: 30 = 0.23918110132217407\n",
      "Loss for batch: 31 = 0.1223052591085434\n",
      "Loss for batch: 32 = 0.06043010205030441\n",
      "Loss for batch: 33 = 0.08351299166679382\n",
      "Loss for batch: 34 = 0.1617012321949005\n",
      "Loss for batch: 35 = 0.11857013404369354\n",
      "Loss for batch: 36 = 0.08148671686649323\n",
      "Loss for batch: 37 = 0.11063738167285919\n",
      "Loss for batch: 38 = 0.23891830444335938\n",
      "Loss for batch: 39 = 0.5374130010604858\n",
      "Loss for batch: 40 = 0.9305111765861511\n",
      "Loss for batch: 41 = 0.2596329152584076\n",
      "Loss for batch: 42 = 0.1472817361354828\n",
      "Loss for batch: 43 = 0.18481704592704773\n",
      "Loss for batch: 44 = 0.20909202098846436\n",
      "Loss for batch: 45 = 0.22448347508907318\n",
      "Loss for batch: 46 = 0.17156866192817688\n",
      "Loss for batch: 47 = 0.11365765333175659\n",
      "Loss for batch: 48 = 0.25349897146224976\n",
      "Loss for batch: 49 = 0.12515413761138916\n",
      "Loss for batch: 50 = 0.17593072354793549\n",
      "Loss for batch: 51 = 0.1996566653251648\n",
      "Loss for batch: 52 = 0.16747115552425385\n",
      "Loss for batch: 53 = 0.21928346157073975\n",
      "Loss for batch: 54 = 0.12116356939077377\n",
      "Loss for batch: 55 = 0.0679766833782196\n",
      "Loss for batch: 56 = 0.04772630333900452\n",
      "Loss for batch: 57 = 0.33664822578430176\n",
      "Loss for batch: 58 = 0.2576697766780853\n",
      "Loss for batch: 59 = 0.23370234668254852\n",
      "Loss for batch: 60 = 0.1527286320924759\n",
      "Loss for batch: 61 = 0.06817946583032608\n",
      "Loss for batch: 62 = 0.17570504546165466\n",
      "Loss for batch: 63 = 0.17933277785778046\n",
      "Loss for batch: 64 = 0.31877362728118896\n",
      "Loss for batch: 65 = 0.16176486015319824\n",
      "Loss for batch: 66 = 0.279010146856308\n",
      "Loss for batch: 67 = 0.16146200895309448\n",
      "Loss for batch: 68 = 0.10758926719427109\n",
      "Loss for batch: 69 = 0.155027374625206\n",
      "Loss for batch: 70 = 0.32028353214263916\n",
      "Loss for batch: 71 = 0.28028303384780884\n",
      "Loss for batch: 72 = 0.138492614030838\n",
      "Loss for batch: 73 = 0.04377881810069084\n",
      "Loss for batch: 74 = 0.11619704961776733\n",
      "Loss for batch: 75 = 0.19342273473739624\n",
      "Loss for batch: 76 = 0.08029049634933472\n",
      "Loss for batch: 77 = 0.1000942811369896\n",
      "Loss for batch: 78 = 0.09097704291343689\n",
      "Loss for batch: 79 = 0.2689187228679657\n",
      "Loss for batch: 80 = 0.38373860716819763\n",
      "Loss for batch: 81 = 0.31741589307785034\n",
      "Loss for batch: 82 = 0.16400013864040375\n",
      "Loss for batch: 83 = 0.22571353614330292\n",
      "Loss for batch: 84 = 0.11281593888998032\n",
      "Loss for batch: 85 = 0.11083140224218369\n",
      "Loss for batch: 86 = 0.1667643040418625\n",
      "Loss for batch: 87 = 0.20380181074142456\n",
      "Loss for batch: 88 = 0.1821763515472412\n",
      "Loss for batch: 89 = 0.20320960879325867\n",
      "Loss for batch: 90 = 0.14417067170143127\n",
      "Loss for batch: 91 = 0.1662517488002777\n",
      "Loss for batch: 92 = 0.26374566555023193\n",
      "Loss for batch: 93 = 0.29802948236465454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/50 [00:02<00:33,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch: 94 = 0.21434542536735535\n",
      "Loss for batch: 95 = 0.044514529407024384\n",
      "Loss for batch: 96 = 0.09577655792236328\n",
      "Loss for batch: 97 = 0.1606481969356537\n",
      "Loss for batch: 98 = 0.1874389350414276\n",
      "Loss for batch: 99 = 0.21058712899684906\n",
      "Loss for batch: 100 = 0.44434303045272827\n",
      "Loss for batch: 101 = 0.7268588542938232\n",
      "Loss for batch: 102 = 0.29041752219200134\n",
      "Loss for batch: 103 = 0.14556364715099335\n",
      "Loss for batch: 104 = 0.091647207736969\n",
      "Loss for batch: 105 = 0.1960747241973877\n",
      "Loss for batch: 106 = 0.025669675320386887\n",
      "\n",
      "Training Loss for epoch 2 = 19.85262680053711\n",
      "\n",
      "Current Validation Loss = 16.479522705078125\n",
      "Best Validation Loss = 15.77910041809082\n",
      "Epochs without Improvement = 1\n",
      "Epoch : 3\n",
      "----------------------\n",
      "Loss for batch: 0 = 0.1283120959997177\n",
      "Loss for batch: 1 = 0.09565150737762451\n",
      "Loss for batch: 2 = 0.14328795671463013\n",
      "Loss for batch: 3 = 0.034172654151916504\n",
      "Loss for batch: 4 = 0.18578529357910156\n",
      "Loss for batch: 5 = 0.1703205108642578\n",
      "Loss for batch: 6 = 0.07611048221588135\n",
      "Loss for batch: 7 = 0.04724074527621269\n",
      "Loss for batch: 8 = 0.083812415599823\n",
      "Loss for batch: 9 = 0.06463740766048431\n",
      "Loss for batch: 10 = 0.13921619951725006\n",
      "Loss for batch: 11 = 0.12185259163379669\n",
      "Loss for batch: 12 = 0.04608246311545372\n",
      "Loss for batch: 13 = 0.199294775724411\n",
      "Loss for batch: 14 = 0.2310139238834381\n",
      "Loss for batch: 15 = 0.06746922433376312\n",
      "Loss for batch: 16 = 0.058343224227428436\n",
      "Loss for batch: 17 = 0.12942127883434296\n",
      "Loss for batch: 18 = 0.07272671908140182\n",
      "Loss for batch: 19 = 0.16942250728607178\n",
      "Loss for batch: 20 = 0.12127983570098877\n",
      "Loss for batch: 21 = 0.12937799096107483\n",
      "Loss for batch: 22 = 0.13649101555347443\n",
      "Loss for batch: 23 = 0.10879974067211151\n",
      "Loss for batch: 24 = 0.23012694716453552\n",
      "Loss for batch: 25 = 0.06922931224107742\n",
      "Loss for batch: 26 = 0.0464341901242733\n",
      "Loss for batch: 27 = 0.08582042157649994\n",
      "Loss for batch: 28 = 0.07245199382305145\n",
      "Loss for batch: 29 = 0.09604968875646591\n",
      "Loss for batch: 30 = 0.2253788560628891\n",
      "Loss for batch: 31 = 0.39993080496788025\n",
      "Loss for batch: 32 = 0.15466925501823425\n",
      "Loss for batch: 33 = 0.2455366849899292\n",
      "Loss for batch: 34 = 0.06651468575000763\n",
      "Loss for batch: 35 = 0.11719869077205658\n",
      "Loss for batch: 36 = 0.1959756314754486\n",
      "Loss for batch: 37 = 0.15808725357055664\n",
      "Loss for batch: 38 = 0.09833855926990509\n",
      "Loss for batch: 39 = 0.094403937458992\n",
      "Loss for batch: 40 = 0.1324721723794937\n",
      "Loss for batch: 41 = 0.15879788994789124\n",
      "Loss for batch: 42 = 0.25886061787605286\n",
      "Loss for batch: 43 = 0.08897936344146729\n",
      "Loss for batch: 44 = 0.03441193699836731\n",
      "Loss for batch: 45 = 0.17052647471427917\n",
      "Loss for batch: 46 = 0.15531933307647705\n",
      "Loss for batch: 47 = 0.06006723642349243\n",
      "Loss for batch: 48 = 0.166072815656662\n",
      "Loss for batch: 49 = 0.36645668745040894\n",
      "Loss for batch: 50 = 0.24326947331428528\n",
      "Loss for batch: 51 = 0.19382290542125702\n",
      "Loss for batch: 52 = 0.08583268523216248\n",
      "Loss for batch: 53 = 0.15264716744422913\n",
      "Loss for batch: 54 = 0.04011085256934166\n",
      "Loss for batch: 55 = 0.14680799841880798\n",
      "Loss for batch: 56 = 0.22264264523983002\n",
      "Loss for batch: 57 = 0.125200092792511\n",
      "Loss for batch: 58 = 0.1469642072916031\n",
      "Loss for batch: 59 = 0.07271954417228699\n",
      "Loss for batch: 60 = 0.18108026683330536\n",
      "Loss for batch: 61 = 0.10730499029159546\n",
      "Loss for batch: 62 = 0.06587423384189606\n",
      "Loss for batch: 63 = 0.1951153576374054\n",
      "Loss for batch: 64 = 0.05571331828832626\n",
      "Loss for batch: 65 = 0.10887996852397919\n",
      "Loss for batch: 66 = 0.17090241611003876\n",
      "Loss for batch: 67 = 0.09092170000076294\n",
      "Loss for batch: 68 = 0.11823590844869614\n",
      "Loss for batch: 69 = 0.10190171003341675\n",
      "Loss for batch: 70 = 0.2268817126750946\n",
      "Loss for batch: 71 = 0.17787908017635345\n",
      "Loss for batch: 72 = 0.059975218027830124\n",
      "Loss for batch: 73 = 0.0723181813955307\n",
      "Loss for batch: 74 = 0.1394958645105362\n",
      "Loss for batch: 75 = 0.08661385625600815\n",
      "Loss for batch: 76 = 0.0919337123632431\n",
      "Loss for batch: 77 = 0.06277697533369064\n",
      "Loss for batch: 78 = 0.03586939722299576\n",
      "Loss for batch: 79 = 0.16191069781780243\n",
      "Loss for batch: 80 = 0.12261645495891571\n",
      "Loss for batch: 81 = 0.19406995177268982\n",
      "Loss for batch: 82 = 0.22383567690849304\n",
      "Loss for batch: 83 = 0.11889956891536713\n",
      "Loss for batch: 84 = 0.14564833045005798\n",
      "Loss for batch: 85 = 0.24685299396514893\n",
      "Loss for batch: 86 = 0.1541103571653366\n",
      "Loss for batch: 87 = 0.1797516644001007\n",
      "Loss for batch: 88 = 0.24221965670585632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/50 [00:02<00:32,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch: 89 = 0.1854497194290161\n",
      "Loss for batch: 90 = 0.2706308364868164\n",
      "Loss for batch: 91 = 0.09032183885574341\n",
      "Loss for batch: 92 = 0.14713987708091736\n",
      "Loss for batch: 93 = 0.08365463465452194\n",
      "Loss for batch: 94 = 0.09052613377571106\n",
      "Loss for batch: 95 = 0.11844419687986374\n",
      "Loss for batch: 96 = 0.17908482253551483\n",
      "Loss for batch: 97 = 0.07003109902143478\n",
      "Loss for batch: 98 = 0.10591845959424973\n",
      "Loss for batch: 99 = 0.048052676022052765\n",
      "Loss for batch: 100 = 0.037276025861501694\n",
      "Loss for batch: 101 = 0.060804933309555054\n",
      "Loss for batch: 102 = 0.22055888175964355\n",
      "Loss for batch: 103 = 0.1966637372970581\n",
      "Loss for batch: 104 = 0.16912393271923065\n",
      "Loss for batch: 105 = 0.18120837211608887\n",
      "Loss for batch: 106 = 0.09776651859283447\n",
      "\n",
      "Training Loss for epoch 3 = 14.430496215820312\n",
      "\n",
      "Current Validation Loss = 21.48734474182129\n",
      "Best Validation Loss = 15.77910041809082\n",
      "Epochs without Improvement = 2\n",
      "Epoch : 4\n",
      "----------------------\n",
      "Loss for batch: 0 = 0.03813524544239044\n",
      "Loss for batch: 1 = 0.22765813767910004\n",
      "Loss for batch: 2 = 0.04186336696147919\n",
      "Loss for batch: 3 = 0.1775280088186264\n",
      "Loss for batch: 4 = 0.07262642681598663\n",
      "Loss for batch: 5 = 0.09739749878644943\n",
      "Loss for batch: 6 = 0.09986381977796555\n",
      "Loss for batch: 7 = 0.14720149338245392\n",
      "Loss for batch: 8 = 0.10628636926412582\n",
      "Loss for batch: 9 = 0.10310995578765869\n",
      "Loss for batch: 10 = 0.11121523380279541\n",
      "Loss for batch: 11 = 0.12264424562454224\n",
      "Loss for batch: 12 = 0.1523154228925705\n",
      "Loss for batch: 13 = 0.175737202167511\n",
      "Loss for batch: 14 = 0.12296165525913239\n",
      "Loss for batch: 15 = 0.142965629696846\n",
      "Loss for batch: 16 = 0.07593877613544464\n",
      "Loss for batch: 17 = 0.03822767734527588\n",
      "Loss for batch: 18 = 0.04107528179883957\n",
      "Loss for batch: 19 = 0.1922779083251953\n",
      "Loss for batch: 20 = 0.08654048293828964\n",
      "Loss for batch: 21 = 0.04946769401431084\n",
      "Loss for batch: 22 = 0.07673083245754242\n",
      "Loss for batch: 23 = 0.08631262183189392\n",
      "Loss for batch: 24 = 0.2708016633987427\n",
      "Loss for batch: 25 = 0.5639940500259399\n",
      "Loss for batch: 26 = 0.23229478299617767\n",
      "Loss for batch: 27 = 0.2916739583015442\n",
      "Loss for batch: 28 = 0.07400879263877869\n",
      "Loss for batch: 29 = 0.08138009160757065\n",
      "Loss for batch: 30 = 0.3341537117958069\n",
      "Loss for batch: 31 = 0.08796368539333344\n",
      "Loss for batch: 32 = 0.10605078190565109\n",
      "Loss for batch: 33 = 0.06133820861577988\n",
      "Loss for batch: 34 = 0.11312015354633331\n",
      "Loss for batch: 35 = 0.03675874322652817\n",
      "Loss for batch: 36 = 0.17840513586997986\n",
      "Loss for batch: 37 = 0.1694059669971466\n",
      "Loss for batch: 38 = 0.15306925773620605\n",
      "Loss for batch: 39 = 0.05402778834104538\n",
      "Loss for batch: 40 = 0.07235658168792725\n",
      "Loss for batch: 41 = 0.0858202800154686\n",
      "Loss for batch: 42 = 0.09502089023590088\n",
      "Loss for batch: 43 = 0.03219534456729889\n",
      "Loss for batch: 44 = 0.14878451824188232\n",
      "Loss for batch: 45 = 0.19857680797576904\n",
      "Loss for batch: 46 = 0.4350653290748596\n",
      "Loss for batch: 47 = 0.7009205222129822\n",
      "Loss for batch: 48 = 0.17980782687664032\n",
      "Loss for batch: 49 = 0.17686471343040466\n",
      "Loss for batch: 50 = 0.11422872543334961\n",
      "Loss for batch: 51 = 0.060311850160360336\n",
      "Loss for batch: 52 = 0.10416062921285629\n",
      "Loss for batch: 53 = 0.10487570613622665\n",
      "Loss for batch: 54 = 0.08642663061618805\n",
      "Loss for batch: 55 = 0.04087730497121811\n",
      "Loss for batch: 56 = 0.3551357686519623\n",
      "Loss for batch: 57 = 0.26110565662384033\n",
      "Loss for batch: 58 = 0.24508778750896454\n",
      "Loss for batch: 59 = 0.18426179885864258\n",
      "Loss for batch: 60 = 0.17829620838165283\n",
      "Loss for batch: 61 = 0.36803776025772095\n",
      "Loss for batch: 62 = 0.1778574436903\n",
      "Loss for batch: 63 = 0.14056017994880676\n",
      "Loss for batch: 64 = 0.04560168832540512\n",
      "Loss for batch: 65 = 0.07704053819179535\n",
      "Loss for batch: 66 = 0.194564089179039\n",
      "Loss for batch: 67 = 0.09086471796035767\n",
      "Loss for batch: 68 = 0.0960158258676529\n",
      "Loss for batch: 69 = 0.060015201568603516\n",
      "Loss for batch: 70 = 0.06535284221172333\n",
      "Loss for batch: 71 = 0.23774933815002441\n",
      "Loss for batch: 72 = 0.2032400667667389\n",
      "Loss for batch: 73 = 0.058030687272548676\n",
      "Loss for batch: 74 = 0.19667908549308777\n",
      "Loss for batch: 75 = 0.15443751215934753\n",
      "Loss for batch: 76 = 0.06343942880630493\n",
      "Loss for batch: 77 = 0.28384310007095337\n",
      "Loss for batch: 78 = 0.1545705795288086\n",
      "Loss for batch: 79 = 0.11536902189254761\n",
      "Loss for batch: 80 = 0.30401116609573364\n",
      "Loss for batch: 81 = 0.16768233478069305\n",
      "Loss for batch: 82 = 0.15096557140350342\n",
      "Loss for batch: 83 = 0.04760420322418213\n",
      "Loss for batch: 84 = 0.10910382121801376\n",
      "Loss for batch: 85 = 0.03546217083930969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/50 [00:03<00:30,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch: 86 = 0.03999333083629608\n",
      "Loss for batch: 87 = 0.15676003694534302\n",
      "Loss for batch: 88 = 0.13397574424743652\n",
      "Loss for batch: 89 = 0.07885976880788803\n",
      "Loss for batch: 90 = 0.12170401215553284\n",
      "Loss for batch: 91 = 0.07206914573907852\n",
      "Loss for batch: 92 = 0.11849038302898407\n",
      "Loss for batch: 93 = 0.3461650311946869\n",
      "Loss for batch: 94 = 0.20113776624202728\n",
      "Loss for batch: 95 = 0.09905613958835602\n",
      "Loss for batch: 96 = 0.21386754512786865\n",
      "Loss for batch: 97 = 0.174710214138031\n",
      "Loss for batch: 98 = 0.274278461933136\n",
      "Loss for batch: 99 = 0.18814346194267273\n",
      "Loss for batch: 100 = 0.12245812267065048\n",
      "Loss for batch: 101 = 0.05789395421743393\n",
      "Loss for batch: 102 = 0.026436910033226013\n",
      "Loss for batch: 103 = 0.0639922022819519\n",
      "Loss for batch: 104 = 0.055550359189510345\n",
      "Loss for batch: 105 = 0.14493918418884277\n",
      "Loss for batch: 106 = 0.029643550515174866\n",
      "\n",
      "Training Loss for epoch 4 = 15.5729341506958\n",
      "\n",
      "Current Validation Loss = 19.696487426757812\n",
      "Best Validation Loss = 15.77910041809082\n",
      "Epochs without Improvement = 3\n",
      "Epoch : 5\n",
      "----------------------\n",
      "Loss for batch: 0 = 0.08166255801916122\n",
      "Loss for batch: 1 = 0.10404562950134277\n",
      "Loss for batch: 2 = 0.20786306262016296\n",
      "Loss for batch: 3 = 0.11585277318954468\n",
      "Loss for batch: 4 = 0.20974694192409515\n",
      "Loss for batch: 5 = 0.07906906306743622\n",
      "Loss for batch: 6 = 0.08901813626289368\n",
      "Loss for batch: 7 = 0.09932880848646164\n",
      "Loss for batch: 8 = 0.19516222178936005\n",
      "Loss for batch: 9 = 0.12252449244260788\n",
      "Loss for batch: 10 = 0.11843691766262054\n",
      "Loss for batch: 11 = 0.16031968593597412\n",
      "Loss for batch: 12 = 0.11106650531291962\n",
      "Loss for batch: 13 = 0.06130014359951019\n",
      "Loss for batch: 14 = 0.11878819763660431\n",
      "Loss for batch: 15 = 0.1933443546295166\n",
      "Loss for batch: 16 = 0.02739647589623928\n",
      "Loss for batch: 17 = 0.06523057818412781\n",
      "Loss for batch: 18 = 0.031801965087652206\n",
      "Loss for batch: 19 = 0.18201236426830292\n",
      "Loss for batch: 20 = 0.1113949567079544\n",
      "Loss for batch: 21 = 0.06465432047843933\n",
      "Loss for batch: 22 = 0.1461944580078125\n",
      "Loss for batch: 23 = 0.08355164527893066\n",
      "Loss for batch: 24 = 0.10594251751899719\n",
      "Loss for batch: 25 = 0.10820001363754272\n",
      "Loss for batch: 26 = 0.2509083151817322\n",
      "Loss for batch: 27 = 0.27797871828079224\n",
      "Loss for batch: 28 = 0.03597046434879303\n",
      "Loss for batch: 29 = 0.011884590610861778\n",
      "Loss for batch: 30 = 0.09274496883153915\n",
      "Loss for batch: 31 = 0.04943923279643059\n",
      "Loss for batch: 32 = 0.06972198188304901\n",
      "Loss for batch: 33 = 0.20145122706890106\n",
      "Loss for batch: 34 = 0.052152033895254135\n",
      "Loss for batch: 35 = 0.1752462089061737\n",
      "Loss for batch: 36 = 0.12016718089580536\n",
      "Loss for batch: 37 = 0.1292320042848587\n",
      "Loss for batch: 38 = 0.09240196645259857\n",
      "Loss for batch: 39 = 0.028423935174942017\n",
      "Loss for batch: 40 = 0.07173886895179749\n",
      "Loss for batch: 41 = 0.06585299968719482\n",
      "Loss for batch: 42 = 0.04262596368789673\n",
      "Loss for batch: 43 = 0.0585365891456604\n",
      "Loss for batch: 44 = 0.07020192593336105\n",
      "Loss for batch: 45 = 0.3058359622955322\n",
      "Loss for batch: 46 = 0.7498108148574829\n",
      "Loss for batch: 47 = 0.567931056022644\n",
      "Loss for batch: 48 = 0.11660462617874146\n",
      "Loss for batch: 49 = 0.0795813798904419\n",
      "Loss for batch: 50 = 0.052653536200523376\n",
      "Loss for batch: 51 = 0.057455241680145264\n",
      "Loss for batch: 52 = 0.032857682555913925\n",
      "Loss for batch: 53 = 0.12218668311834335\n",
      "Loss for batch: 54 = 0.06569108366966248\n",
      "Loss for batch: 55 = 0.1753610372543335\n",
      "Loss for batch: 56 = 0.10912393033504486\n",
      "Loss for batch: 57 = 0.10638050734996796\n",
      "Loss for batch: 58 = 0.05976074934005737\n",
      "Loss for batch: 59 = 0.09860469400882721\n",
      "Loss for batch: 60 = 0.11939119547605515\n",
      "Loss for batch: 61 = 0.03986078128218651\n",
      "Loss for batch: 62 = 0.053582411259412766\n",
      "Loss for batch: 63 = 0.07219056040048599\n",
      "Loss for batch: 64 = 0.13088098168373108\n",
      "Loss for batch: 65 = 0.13546708226203918\n",
      "Loss for batch: 66 = 0.09053380787372589\n",
      "Loss for batch: 67 = 0.17867927253246307\n",
      "Loss for batch: 68 = 0.06964552402496338\n",
      "Loss for batch: 69 = 0.06678615510463715\n",
      "Loss for batch: 70 = 0.13906288146972656\n",
      "Loss for batch: 71 = 0.03962069749832153\n",
      "Loss for batch: 72 = 0.11501458287239075\n",
      "Loss for batch: 73 = 0.07507706433534622\n",
      "Loss for batch: 74 = 0.17520037293434143\n",
      "Loss for batch: 75 = 0.0315915048122406\n",
      "Loss for batch: 76 = 0.21846255660057068\n",
      "Loss for batch: 77 = 0.1396220624446869\n",
      "Loss for batch: 78 = 0.1310223937034607\n",
      "Loss for batch: 79 = 0.04415157437324524\n",
      "Loss for batch: 80 = 0.07868246734142303\n",
      "Loss for batch: 81 = 0.12359023094177246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6/50 [00:04<00:29,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch: 82 = 0.17901670932769775\n",
      "Loss for batch: 83 = 0.12905848026275635\n",
      "Loss for batch: 84 = 0.10307067632675171\n",
      "Loss for batch: 85 = 0.10299666970968246\n",
      "Loss for batch: 86 = 0.12243771553039551\n",
      "Loss for batch: 87 = 0.06134437769651413\n",
      "Loss for batch: 88 = 0.07576120644807816\n",
      "Loss for batch: 89 = 0.14296561479568481\n",
      "Loss for batch: 90 = 0.07902342081069946\n",
      "Loss for batch: 91 = 0.2565266788005829\n",
      "Loss for batch: 92 = 0.41095656156539917\n",
      "Loss for batch: 93 = 0.3212074637413025\n",
      "Loss for batch: 94 = 0.44206738471984863\n",
      "Loss for batch: 95 = 0.19339267909526825\n",
      "Loss for batch: 96 = 0.11088995635509491\n",
      "Loss for batch: 97 = 0.07679824531078339\n",
      "Loss for batch: 98 = 0.19947358965873718\n",
      "Loss for batch: 99 = 0.1831795871257782\n",
      "Loss for batch: 100 = 0.23917941749095917\n",
      "Loss for batch: 101 = 0.08859628438949585\n",
      "Loss for batch: 102 = 0.34390968084335327\n",
      "Loss for batch: 103 = 0.1312209665775299\n",
      "Loss for batch: 104 = 0.12752851843833923\n",
      "Loss for batch: 105 = 0.09780947864055634\n",
      "Loss for batch: 106 = 0.1216483935713768\n",
      "\n",
      "Training Loss for epoch 5 = 14.198602676391602\n",
      "\n",
      "Current Validation Loss = 21.08816146850586\n",
      "Best Validation Loss = 15.77910041809082\n",
      "Epochs without Improvement = 4\n",
      "Epoch : 6\n",
      "----------------------\n",
      "Loss for batch: 0 = 0.053376153111457825\n",
      "Loss for batch: 1 = 0.07927891612052917\n",
      "Loss for batch: 2 = 0.14783747494220734\n",
      "Loss for batch: 3 = 0.02349793165922165\n",
      "Loss for batch: 4 = 0.1158963292837143\n",
      "Loss for batch: 5 = 0.016892004758119583\n",
      "Loss for batch: 6 = 0.06054460629820824\n",
      "Loss for batch: 7 = 0.019396649673581123\n",
      "Loss for batch: 8 = 0.11066700518131256\n",
      "Loss for batch: 9 = 0.0413798987865448\n",
      "Loss for batch: 10 = 0.08268818259239197\n",
      "Loss for batch: 11 = 0.07661399245262146\n",
      "Loss for batch: 12 = 0.17245221138000488\n",
      "Loss for batch: 13 = 0.025612836703658104\n",
      "Loss for batch: 14 = 0.015398254618048668\n",
      "Loss for batch: 15 = 0.08021552860736847\n",
      "Loss for batch: 16 = 0.1156497523188591\n",
      "Loss for batch: 17 = 0.11604464054107666\n",
      "Loss for batch: 18 = 0.08291079849004745\n",
      "Loss for batch: 19 = 0.07632414996623993\n",
      "Loss for batch: 20 = 0.1767142117023468\n",
      "Loss for batch: 21 = 0.05289218947291374\n",
      "Loss for batch: 22 = 0.0492226779460907\n",
      "Loss for batch: 23 = 0.009286592714488506\n",
      "Loss for batch: 24 = 0.010631274431943893\n",
      "Loss for batch: 25 = 0.05996052548289299\n",
      "Loss for batch: 26 = 0.023253194987773895\n",
      "Loss for batch: 27 = 0.03445696458220482\n",
      "Loss for batch: 28 = 0.026677414774894714\n",
      "Loss for batch: 29 = 0.04754466935992241\n",
      "Loss for batch: 30 = 0.022791381925344467\n",
      "Loss for batch: 31 = 0.020703045651316643\n",
      "Loss for batch: 32 = 0.08754684776067734\n",
      "Loss for batch: 33 = 0.02722666971385479\n",
      "Loss for batch: 34 = 0.13081960380077362\n",
      "Loss for batch: 35 = 0.11529628932476044\n",
      "Loss for batch: 36 = 0.07050465047359467\n",
      "Loss for batch: 37 = 0.025089919567108154\n",
      "Loss for batch: 38 = 0.011768357828259468\n",
      "Loss for batch: 39 = 0.10053649544715881\n",
      "Loss for batch: 40 = 0.2229825109243393\n",
      "Loss for batch: 41 = 0.03804773464798927\n",
      "Loss for batch: 42 = 0.014700187370181084\n",
      "Loss for batch: 43 = 0.24741363525390625\n",
      "Loss for batch: 44 = 0.04872128367424011\n",
      "Loss for batch: 45 = 0.024522578343749046\n",
      "Loss for batch: 46 = 0.035038456320762634\n",
      "Loss for batch: 47 = 0.031146004796028137\n",
      "Loss for batch: 48 = 0.0576104037463665\n",
      "Loss for batch: 49 = 0.03386303409934044\n",
      "Loss for batch: 50 = 0.09749127924442291\n",
      "Loss for batch: 51 = 0.15599007904529572\n",
      "Loss for batch: 52 = 0.04014429450035095\n",
      "Loss for batch: 53 = 0.05338131636381149\n",
      "Loss for batch: 54 = 0.03047924116253853\n",
      "Loss for batch: 55 = 0.09623125195503235\n",
      "Loss for batch: 56 = 0.14454394578933716\n",
      "Loss for batch: 57 = 0.1133226603269577\n",
      "Loss for batch: 58 = 0.024222955107688904\n",
      "Loss for batch: 59 = 0.05112912505865097\n",
      "Loss for batch: 60 = 0.02337881177663803\n",
      "Loss for batch: 61 = 0.06748317927122116\n",
      "Loss for batch: 62 = 0.07227522134780884\n",
      "Loss for batch: 63 = 0.06382003426551819\n",
      "Loss for batch: 64 = 0.05185199901461601\n",
      "Loss for batch: 65 = 0.0602007731795311\n",
      "Loss for batch: 66 = 0.09388186782598495\n",
      "Loss for batch: 67 = 0.11449563503265381\n",
      "Loss for batch: 68 = 0.015626661479473114\n",
      "Loss for batch: 69 = 0.056695275008678436\n",
      "Loss for batch: 70 = 0.03436747565865517\n",
      "Loss for batch: 71 = 0.013678306713700294\n",
      "Loss for batch: 72 = 0.0494568906724453\n",
      "Loss for batch: 73 = 0.17306606471538544\n",
      "Loss for batch: 74 = 0.29703575372695923\n",
      "Loss for batch: 75 = 0.34187978506088257\n",
      "Loss for batch: 76 = 0.13956478238105774\n",
      "Loss for batch: 77 = 0.08155416697263718\n",
      "Loss for batch: 78 = 0.13396523892879486\n",
      "Loss for batch: 79 = 0.011519741266965866\n",
      "Loss for batch: 80 = 0.030889227986335754\n",
      "Loss for batch: 81 = 0.05665118247270584\n",
      "Loss for batch: 82 = 0.049118489027023315\n",
      "Loss for batch: 83 = 0.11434777826070786\n",
      "Loss for batch: 84 = 0.1328670084476471\n",
      "Loss for batch: 85 = 0.2671050429344177\n",
      "Loss for batch: 86 = 0.09851102530956268\n",
      "Loss for batch: 87 = 0.11040090024471283\n",
      "Loss for batch: 88 = 0.03777235746383667\n",
      "Loss for batch: 89 = 0.09365370869636536\n",
      "Loss for batch: 90 = 0.055752336978912354\n",
      "Loss for batch: 91 = 0.05078046768903732\n",
      "Loss for batch: 92 = 0.04406268149614334\n",
      "Loss for batch: 93 = 0.05378654599189758\n",
      "Loss for batch: 94 = 0.007771848700940609\n",
      "Loss for batch: 95 = 0.07066421210765839\n",
      "Loss for batch: 96 = 0.08442547172307968\n",
      "Loss for batch: 97 = 0.06434305012226105\n",
      "Loss for batch: 98 = 0.1569090634584427\n",
      "Loss for batch: 99 = 0.10037069022655487\n",
      "Loss for batch: 100 = 0.05446269363164902\n",
      "Loss for batch: 101 = 0.08373977988958359\n",
      "Loss for batch: 102 = 0.03653046488761902\n",
      "Loss for batch: 103 = 0.11056143045425415\n",
      "Loss for batch: 104 = 0.36826062202453613\n",
      "Loss for batch: 105 = 0.7112614512443542\n",
      "Loss for batch: 106 = 0.5708582401275635\n",
      "\n",
      "Training Loss for epoch 6 = 9.688238143920898\n",
      "\n",
      "Current Validation Loss = 22.009462356567383\n",
      "Best Validation Loss = 15.77910041809082\n",
      "Epochs without Improvement = 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 7/50 [00:04<00:29,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 7\n",
      "----------------------\n",
      "Loss for batch: 0 = 0.16250011324882507\n",
      "Loss for batch: 1 = 0.06626918166875839\n",
      "Loss for batch: 2 = 0.023615708574652672\n",
      "Loss for batch: 3 = 0.11458858102560043\n",
      "Loss for batch: 4 = 0.12645679712295532\n",
      "Loss for batch: 5 = 0.0462816022336483\n",
      "Loss for batch: 6 = 0.0669855922460556\n",
      "Loss for batch: 7 = 0.05859294533729553\n",
      "Loss for batch: 8 = 0.045108404010534286\n",
      "Loss for batch: 9 = 0.11311481893062592\n",
      "Loss for batch: 10 = 0.021454088389873505\n",
      "Loss for batch: 11 = 0.2536334693431854\n",
      "Loss for batch: 12 = 0.044521182775497437\n",
      "Loss for batch: 13 = 0.06983977556228638\n",
      "Loss for batch: 14 = 0.044312864542007446\n",
      "Loss for batch: 15 = 0.05674610659480095\n",
      "Loss for batch: 16 = 0.03245308995246887\n",
      "Loss for batch: 17 = 0.03629111871123314\n",
      "Loss for batch: 18 = 0.026814958080649376\n",
      "Loss for batch: 19 = 0.04673457890748978\n",
      "Loss for batch: 20 = 0.06054137274622917\n",
      "Loss for batch: 21 = 0.035101067274808884\n",
      "Loss for batch: 22 = 0.07081745564937592\n",
      "Loss for batch: 23 = 0.029393678531050682\n",
      "Loss for batch: 24 = 0.07192444056272507\n",
      "Loss for batch: 25 = 0.12680785357952118\n",
      "Loss for batch: 26 = 0.09189649671316147\n",
      "Loss for batch: 27 = 0.053219135850667953\n",
      "Loss for batch: 28 = 0.11420296132564545\n",
      "Loss for batch: 29 = 0.0354725606739521\n",
      "Loss for batch: 30 = 0.15057659149169922\n",
      "Loss for batch: 31 = 0.018756121397018433\n",
      "Loss for batch: 32 = 0.16157934069633484\n",
      "Loss for batch: 33 = 0.054390907287597656\n",
      "Loss for batch: 34 = 0.06670528650283813\n",
      "Loss for batch: 35 = 0.0732913538813591\n",
      "Loss for batch: 36 = 0.023104678839445114\n",
      "Loss for batch: 37 = 0.1332356333732605\n",
      "Loss for batch: 38 = 0.04746710881590843\n",
      "Loss for batch: 39 = 0.030507683753967285\n",
      "Loss for batch: 40 = 0.07067327946424484\n",
      "Loss for batch: 41 = 0.027572613209486008\n",
      "Loss for batch: 42 = 0.03400244563817978\n",
      "Loss for batch: 43 = 0.05303025245666504\n",
      "Loss for batch: 44 = 0.08762107789516449\n",
      "Loss for batch: 45 = 0.06303410232067108\n",
      "Loss for batch: 46 = 0.0873124748468399\n",
      "Loss for batch: 47 = 0.010920602828264236\n",
      "Loss for batch: 48 = 0.1824079006910324\n",
      "Loss for batch: 49 = 0.03496183454990387\n",
      "Loss for batch: 50 = 0.011140098795294762\n",
      "Loss for batch: 51 = 0.0723622739315033\n",
      "Loss for batch: 52 = 0.034653034061193466\n",
      "Loss for batch: 53 = 0.04563356563448906\n",
      "Loss for batch: 54 = 0.0459769070148468\n",
      "Loss for batch: 55 = 0.1556096374988556\n",
      "Loss for batch: 56 = 0.132346510887146\n",
      "Loss for batch: 57 = 0.08081997185945511\n",
      "Loss for batch: 58 = 0.03957868739962578\n",
      "Loss for batch: 59 = 0.034075528383255005\n",
      "Loss for batch: 60 = 0.08970846980810165\n",
      "Loss for batch: 61 = 0.04645818471908569\n",
      "Loss for batch: 62 = 0.08537879586219788\n",
      "Loss for batch: 63 = 0.01493728905916214\n",
      "Loss for batch: 64 = 0.04925938695669174\n",
      "Loss for batch: 65 = 0.04225532338023186\n",
      "Loss for batch: 66 = 0.026404395699501038\n",
      "Loss for batch: 67 = 0.1605910360813141\n",
      "Loss for batch: 68 = 0.05568891763687134\n",
      "Loss for batch: 69 = 0.007225927896797657\n",
      "Loss for batch: 70 = 0.08135539293289185\n",
      "Loss for batch: 71 = 0.029375722631812096\n",
      "Loss for batch: 72 = 0.020397435873746872\n",
      "Loss for batch: 73 = 0.04689031094312668\n",
      "Loss for batch: 74 = 0.14439734816551208\n",
      "Loss for batch: 75 = 0.04425954073667526\n",
      "Loss for batch: 76 = 0.015029219910502434\n",
      "Loss for batch: 77 = 0.059074290096759796\n",
      "Loss for batch: 78 = 0.18415826559066772\n",
      "Loss for batch: 79 = 0.09468096494674683\n",
      "Loss for batch: 80 = 0.08549894392490387\n",
      "Loss for batch: 81 = 0.02917207032442093\n",
      "Loss for batch: 82 = 0.023279011249542236\n",
      "Loss for batch: 83 = 0.05597060173749924\n",
      "Loss for batch: 84 = 0.21404020488262177\n",
      "Loss for batch: 85 = 0.09140965342521667\n",
      "Loss for batch: 86 = 0.03189786523580551\n",
      "Loss for batch: 87 = 0.1755981147289276\n",
      "Loss for batch: 88 = 0.2458125650882721\n",
      "Loss for batch: 89 = 0.15284714102745056\n",
      "Loss for batch: 90 = 0.04872540757060051\n",
      "Loss for batch: 91 = 0.021108169108629227\n",
      "Loss for batch: 92 = 0.031833186745643616\n",
      "Loss for batch: 93 = 0.05346788093447685\n",
      "Loss for batch: 94 = 0.19312399625778198\n",
      "Loss for batch: 95 = 0.03502477705478668\n",
      "Loss for batch: 96 = 0.03789959102869034\n",
      "Loss for batch: 97 = 0.009774243459105492\n",
      "Loss for batch: 98 = 0.026012567803263664\n",
      "Loss for batch: 99 = 0.04197610914707184\n",
      "Loss for batch: 100 = 0.08609895408153534\n",
      "Loss for batch: 101 = 0.1389854997396469\n",
      "Loss for batch: 102 = 0.03967602178454399\n",
      "Loss for batch: 103 = 0.14394739270210266\n",
      "Loss for batch: 104 = 0.21008360385894775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 8/50 [00:05<00:28,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch: 105 = 0.15595096349716187\n",
      "Loss for batch: 106 = 0.07889905571937561\n",
      "\n",
      "Training Loss for epoch 7 = 7.964679718017578\n",
      "\n",
      "Current Validation Loss = 24.39595603942871\n",
      "Best Validation Loss = 15.77910041809082\n",
      "Epochs without Improvement = 6\n",
      "Epoch : 8\n",
      "----------------------\n",
      "Loss for batch: 0 = 0.03323990851640701\n",
      "Loss for batch: 1 = 0.07577000558376312\n",
      "Loss for batch: 2 = 0.08577436208724976\n",
      "Loss for batch: 3 = 0.05366399511694908\n",
      "Loss for batch: 4 = 0.06097172945737839\n",
      "Loss for batch: 5 = 0.04034486413002014\n",
      "Loss for batch: 6 = 0.0023456790950149298\n",
      "Loss for batch: 7 = 0.017205309122800827\n",
      "Loss for batch: 8 = 0.23152169585227966\n",
      "Loss for batch: 9 = 0.08329544961452484\n",
      "Loss for batch: 10 = 0.08371048420667648\n",
      "Loss for batch: 11 = 0.02858499065041542\n",
      "Loss for batch: 12 = 0.04029606282711029\n",
      "Loss for batch: 13 = 0.04076554253697395\n",
      "Loss for batch: 14 = 0.012213698588311672\n",
      "Loss for batch: 15 = 0.013024982064962387\n",
      "Loss for batch: 16 = 0.055250994861125946\n",
      "Loss for batch: 17 = 0.024648215621709824\n",
      "Loss for batch: 18 = 0.02011016383767128\n",
      "Loss for batch: 19 = 0.05662974715232849\n",
      "Loss for batch: 20 = 0.1523580253124237\n",
      "Loss for batch: 21 = 0.010076366364955902\n",
      "Loss for batch: 22 = 0.024557432159781456\n",
      "Loss for batch: 23 = 0.022592071443796158\n",
      "Loss for batch: 24 = 0.01428439375013113\n",
      "Loss for batch: 25 = 0.035283077508211136\n",
      "Loss for batch: 26 = 0.046637602150440216\n",
      "Loss for batch: 27 = 0.021343355998396873\n",
      "Loss for batch: 28 = 0.06675559282302856\n",
      "Loss for batch: 29 = 0.06952054798603058\n",
      "Loss for batch: 30 = 0.005020031705498695\n",
      "Loss for batch: 31 = 0.10703128576278687\n",
      "Loss for batch: 32 = 0.3243867754936218\n",
      "Loss for batch: 33 = 0.3357320725917816\n",
      "Loss for batch: 34 = 0.23753798007965088\n",
      "Loss for batch: 35 = 0.10709985345602036\n",
      "Loss for batch: 36 = 0.0609528124332428\n",
      "Loss for batch: 37 = 0.16813403367996216\n",
      "Loss for batch: 38 = 0.19422471523284912\n",
      "Loss for batch: 39 = 0.4416350722312927\n",
      "Loss for batch: 40 = 0.47430071234703064\n",
      "Loss for batch: 41 = 0.1040729507803917\n",
      "Loss for batch: 42 = 0.02647387981414795\n",
      "Loss for batch: 43 = 0.07740508764982224\n",
      "Loss for batch: 44 = 0.08000396192073822\n",
      "Loss for batch: 45 = 0.10534679889678955\n",
      "Loss for batch: 46 = 0.04202897101640701\n",
      "Loss for batch: 47 = 0.03201901167631149\n",
      "Loss for batch: 48 = 0.11935272812843323\n",
      "Loss for batch: 49 = 0.20754298567771912\n",
      "Loss for batch: 50 = 0.10355131328105927\n",
      "Loss for batch: 51 = 0.15492738783359528\n",
      "Loss for batch: 52 = 0.10083585977554321\n",
      "Loss for batch: 53 = 0.08584727346897125\n",
      "Loss for batch: 54 = 0.06513792276382446\n",
      "Loss for batch: 55 = 0.04583170264959335\n",
      "Loss for batch: 56 = 0.029953908175230026\n",
      "Loss for batch: 57 = 0.027746926993131638\n",
      "Loss for batch: 58 = 0.024642862379550934\n",
      "Loss for batch: 59 = 0.1240827664732933\n",
      "Loss for batch: 60 = 0.15424688160419464\n",
      "Loss for batch: 61 = 0.051217496395111084\n",
      "Loss for batch: 62 = 0.10296796262264252\n",
      "Loss for batch: 63 = 0.060761839151382446\n",
      "Loss for batch: 64 = 0.07261034846305847\n",
      "Loss for batch: 65 = 0.08704941719770432\n",
      "Loss for batch: 66 = 0.05059870332479477\n",
      "Loss for batch: 67 = 0.016932006925344467\n",
      "Loss for batch: 68 = 0.05810265988111496\n",
      "Loss for batch: 69 = 0.06732915341854095\n",
      "Loss for batch: 70 = 0.2737398147583008\n",
      "Loss for batch: 71 = 0.5190151929855347\n",
      "Loss for batch: 72 = 0.4364759624004364\n",
      "Loss for batch: 73 = 0.2901255190372467\n",
      "Loss for batch: 74 = 0.267998069524765\n",
      "Loss for batch: 75 = 0.07590848207473755\n",
      "Loss for batch: 76 = 0.02396932616829872\n",
      "Loss for batch: 77 = 0.08476924151182175\n",
      "Loss for batch: 78 = 0.07229392975568771\n",
      "Loss for batch: 79 = 0.12474924325942993\n",
      "Loss for batch: 80 = 0.06825233995914459\n",
      "Loss for batch: 81 = 0.048074766993522644\n",
      "Loss for batch: 82 = 0.10450851172208786\n",
      "Loss for batch: 83 = 0.031282663345336914\n",
      "Loss for batch: 84 = 0.08864802122116089\n",
      "Loss for batch: 85 = 0.04790230840444565\n",
      "Loss for batch: 86 = 0.07171428203582764\n",
      "Loss for batch: 87 = 0.13555461168289185\n",
      "Loss for batch: 88 = 0.02648811601102352\n",
      "Loss for batch: 89 = 0.04087407886981964\n",
      "Loss for batch: 90 = 0.09974627941846848\n",
      "Loss for batch: 91 = 0.16532063484191895\n",
      "Loss for batch: 92 = 0.44288894534111023\n",
      "Loss for batch: 93 = 0.8923825025558472\n",
      "Loss for batch: 94 = 0.19413071870803833\n",
      "Loss for batch: 95 = 0.2504766583442688\n",
      "Loss for batch: 96 = 0.11269859969615936\n",
      "Loss for batch: 97 = 0.061267584562301636\n",
      "Loss for batch: 98 = 0.04756876081228256\n",
      "Loss for batch: 99 = 0.1253235936164856\n",
      "Loss for batch: 100 = 0.09198033809661865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 9/50 [00:06<00:27,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch: 101 = 0.010850060731172562\n",
      "Loss for batch: 102 = 0.13807463645935059\n",
      "Loss for batch: 103 = 0.04458640143275261\n",
      "Loss for batch: 104 = 0.13463808596134186\n",
      "Loss for batch: 105 = 0.08359362930059433\n",
      "Loss for batch: 106 = 0.2524615228176117\n",
      "\n",
      "Training Loss for epoch 8 = 12.237785339355469\n",
      "\n",
      "Current Validation Loss = 23.837173461914062\n",
      "Best Validation Loss = 15.77910041809082\n",
      "Epochs without Improvement = 7\n",
      "Epoch : 9\n",
      "----------------------\n",
      "Loss for batch: 0 = 0.13706544041633606\n",
      "Loss for batch: 1 = 0.01659512333571911\n",
      "Loss for batch: 2 = 0.03912167623639107\n",
      "Loss for batch: 3 = 0.08617568016052246\n",
      "Loss for batch: 4 = 0.12312889099121094\n",
      "Loss for batch: 5 = 0.03517026826739311\n",
      "Loss for batch: 6 = 0.09112583100795746\n",
      "Loss for batch: 7 = 0.04767683893442154\n",
      "Loss for batch: 8 = 0.06253445893526077\n",
      "Loss for batch: 9 = 0.08500417321920395\n",
      "Loss for batch: 10 = 0.07323049753904343\n",
      "Loss for batch: 11 = 0.031243961304426193\n",
      "Loss for batch: 12 = 0.1272439807653427\n",
      "Loss for batch: 13 = 0.23118585348129272\n",
      "Loss for batch: 14 = 0.28947240114212036\n",
      "Loss for batch: 15 = 0.15910884737968445\n",
      "Loss for batch: 16 = 0.030102504417300224\n",
      "Loss for batch: 17 = 0.038201820105314255\n",
      "Loss for batch: 18 = 0.046318382024765015\n",
      "Loss for batch: 19 = 0.040183767676353455\n",
      "Loss for batch: 20 = 0.0591130331158638\n",
      "Loss for batch: 21 = 0.13724994659423828\n",
      "Loss for batch: 22 = 0.14264878630638123\n",
      "Loss for batch: 23 = 0.060064636170864105\n",
      "Loss for batch: 24 = 0.08657758682966232\n",
      "Loss for batch: 25 = 0.14168891310691833\n",
      "Loss for batch: 26 = 0.07358035445213318\n",
      "Loss for batch: 27 = 0.03413557633757591\n",
      "Loss for batch: 28 = 0.02627504989504814\n",
      "Loss for batch: 29 = 0.003375291358679533\n",
      "Loss for batch: 30 = 0.03147009015083313\n",
      "Loss for batch: 31 = 0.016344528645277023\n",
      "Loss for batch: 32 = 0.038949768990278244\n",
      "Loss for batch: 33 = 0.024777919054031372\n",
      "Loss for batch: 34 = 0.07553860545158386\n",
      "Loss for batch: 35 = 0.04828692227602005\n",
      "Loss for batch: 36 = 0.07064904272556305\n",
      "Loss for batch: 37 = 0.04533103108406067\n",
      "Loss for batch: 38 = 0.03417246788740158\n",
      "Loss for batch: 39 = 0.1164562925696373\n",
      "Loss for batch: 40 = 0.02945113740861416\n",
      "Loss for batch: 41 = 0.10719794034957886\n",
      "Loss for batch: 42 = 0.01719055324792862\n",
      "Loss for batch: 43 = 0.08222588151693344\n",
      "Loss for batch: 44 = 0.07757533341646194\n",
      "Loss for batch: 45 = 0.01424461416900158\n",
      "Loss for batch: 46 = 0.014884550124406815\n",
      "Loss for batch: 47 = 0.1638844758272171\n",
      "Loss for batch: 48 = 0.11215086281299591\n",
      "Loss for batch: 49 = 0.026530001312494278\n",
      "Loss for batch: 50 = 0.020523767918348312\n",
      "Loss for batch: 51 = 0.07504422962665558\n",
      "Loss for batch: 52 = 0.02183838188648224\n",
      "Loss for batch: 53 = 0.03868221119046211\n",
      "Loss for batch: 54 = 0.0961209088563919\n",
      "Loss for batch: 55 = 0.07703860104084015\n",
      "Loss for batch: 56 = 0.11125179380178452\n",
      "Loss for batch: 57 = 0.09009501338005066\n",
      "Loss for batch: 58 = 0.015383190475404263\n",
      "Loss for batch: 59 = 0.144351527094841\n",
      "Loss for batch: 60 = 0.04547819495201111\n",
      "Loss for batch: 61 = 0.016216911375522614\n",
      "Loss for batch: 62 = 0.1362604796886444\n",
      "Loss for batch: 63 = 0.1827506124973297\n",
      "Loss for batch: 64 = 0.0988629013299942\n",
      "Loss for batch: 65 = 0.05226367712020874\n",
      "Loss for batch: 66 = 0.006887614727020264\n",
      "Loss for batch: 67 = 0.045655544847249985\n",
      "Loss for batch: 68 = 0.03965653106570244\n",
      "Loss for batch: 69 = 0.027404870837926865\n",
      "Loss for batch: 70 = 0.053325384855270386\n",
      "Loss for batch: 71 = 0.06257082521915436\n",
      "Loss for batch: 72 = 0.010585635900497437\n",
      "Loss for batch: 73 = 0.022006643936038017\n",
      "Loss for batch: 74 = 0.09970267862081528\n",
      "Loss for batch: 75 = 0.0787559449672699\n",
      "Loss for batch: 76 = 0.027225196361541748\n",
      "Loss for batch: 77 = 0.02231663092970848\n",
      "Loss for batch: 78 = 0.08182941377162933\n",
      "Loss for batch: 79 = 0.01055910438299179\n",
      "Loss for batch: 80 = 0.030443241819739342\n",
      "Loss for batch: 81 = 0.027002345770597458\n",
      "Loss for batch: 82 = 0.04572978615760803\n",
      "Loss for batch: 83 = 0.14128009974956512\n",
      "Loss for batch: 84 = 0.012517823837697506\n",
      "Loss for batch: 85 = 0.05354113504290581\n",
      "Loss for batch: 86 = 0.005241244565695524\n",
      "Loss for batch: 87 = 0.050178781151771545\n",
      "Loss for batch: 88 = 0.07140810787677765\n",
      "Loss for batch: 89 = 0.037744611501693726\n",
      "Loss for batch: 90 = 0.03320583328604698\n",
      "Loss for batch: 91 = 0.057400062680244446\n",
      "Loss for batch: 92 = 0.15528574585914612\n",
      "Loss for batch: 93 = 0.005950049497187138\n",
      "Loss for batch: 94 = 0.01674731820821762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 10/50 [00:06<00:26,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch: 95 = 0.013391698710620403\n",
      "Loss for batch: 96 = 0.024393483996391296\n",
      "Loss for batch: 97 = 0.03681925684213638\n",
      "Loss for batch: 98 = 0.04686897248029709\n",
      "Loss for batch: 99 = 0.03862439841032028\n",
      "Loss for batch: 100 = 0.0324954017996788\n",
      "Loss for batch: 101 = 0.07265356183052063\n",
      "Loss for batch: 102 = 0.01151801086962223\n",
      "Loss for batch: 103 = 0.07569149136543274\n",
      "Loss for batch: 104 = 0.053056955337524414\n",
      "Loss for batch: 105 = 0.08973926305770874\n",
      "Loss for batch: 106 = 0.13830213248729706\n",
      "\n",
      "Training Loss for epoch 9 = 6.889788627624512\n",
      "\n",
      "Current Validation Loss = 26.52515411376953\n",
      "Best Validation Loss = 15.77910041809082\n",
      "Epochs without Improvement = 8\n",
      "Epoch : 10\n",
      "----------------------\n",
      "Loss for batch: 0 = 0.0986524447798729\n",
      "Loss for batch: 1 = 0.04826497286558151\n",
      "Loss for batch: 2 = 0.012993909418582916\n",
      "Loss for batch: 3 = 0.01958363503217697\n",
      "Loss for batch: 4 = 0.023198461160063744\n",
      "Loss for batch: 5 = 0.12894117832183838\n",
      "Loss for batch: 6 = 0.030048003420233727\n",
      "Loss for batch: 7 = 0.009940767660737038\n",
      "Loss for batch: 8 = 0.05261319503188133\n",
      "Loss for batch: 9 = 0.00916734803467989\n",
      "Loss for batch: 10 = 0.00670340470969677\n",
      "Loss for batch: 11 = 0.02612645924091339\n",
      "Loss for batch: 12 = 0.03841067850589752\n",
      "Loss for batch: 13 = 0.008111681789159775\n",
      "Loss for batch: 14 = 0.08794759213924408\n",
      "Loss for batch: 15 = 0.014666585251688957\n",
      "Loss for batch: 16 = 0.1165781170129776\n",
      "Loss for batch: 17 = 0.06517760455608368\n",
      "Loss for batch: 18 = 0.06599511206150055\n",
      "Loss for batch: 19 = 0.02997009828686714\n",
      "Loss for batch: 20 = 0.01710016280412674\n",
      "Loss for batch: 21 = 0.06170925125479698\n",
      "Loss for batch: 22 = 0.00214151619002223\n",
      "Loss for batch: 23 = 0.008214372210204601\n",
      "Loss for batch: 24 = 0.01166079193353653\n",
      "Loss for batch: 25 = 0.03533848375082016\n",
      "Loss for batch: 26 = 0.007488111034035683\n",
      "Loss for batch: 27 = 0.007678610738366842\n",
      "Loss for batch: 28 = 0.025312265381217003\n",
      "Loss for batch: 29 = 0.009907442145049572\n",
      "Loss for batch: 30 = 0.039710529148578644\n",
      "Loss for batch: 31 = 0.014962738379836082\n",
      "Loss for batch: 32 = 0.09670400619506836\n",
      "Loss for batch: 33 = 0.002773132175207138\n",
      "Loss for batch: 34 = 0.09393416345119476\n",
      "Loss for batch: 35 = 0.06422185897827148\n",
      "Loss for batch: 36 = 0.08722089231014252\n",
      "Loss for batch: 37 = 0.0404973141849041\n",
      "Loss for batch: 38 = 0.02783854678273201\n",
      "Loss for batch: 39 = 0.020231077447533607\n",
      "Loss for batch: 40 = 0.06887489557266235\n",
      "Loss for batch: 41 = 0.05774106830358505\n",
      "Loss for batch: 42 = 0.014239795506000519\n",
      "Loss for batch: 43 = 0.04833938926458359\n",
      "Loss for batch: 44 = 0.017089713364839554\n",
      "Loss for batch: 45 = 0.0029788967221975327\n",
      "Loss for batch: 46 = 0.12303555011749268\n",
      "Loss for batch: 47 = 0.06906072795391083\n",
      "Loss for batch: 48 = 0.015346157364547253\n",
      "Loss for batch: 49 = 0.08953747153282166\n",
      "Loss for batch: 50 = 0.012535709887742996\n",
      "Loss for batch: 51 = 0.03281253203749657\n",
      "Loss for batch: 52 = 0.05887673795223236\n",
      "Loss for batch: 53 = 0.12259258329868317\n",
      "Loss for batch: 54 = 0.13600914180278778\n",
      "Loss for batch: 55 = 0.07730858772993088\n",
      "Loss for batch: 56 = 0.2005394548177719\n",
      "Loss for batch: 57 = 0.021161556243896484\n",
      "Loss for batch: 58 = 0.01079893484711647\n",
      "Loss for batch: 59 = 0.02704351767897606\n",
      "Loss for batch: 60 = 0.10316847264766693\n",
      "Loss for batch: 61 = 0.1447463482618332\n",
      "Loss for batch: 62 = 0.01675049215555191\n",
      "Loss for batch: 63 = 0.058522239327430725\n",
      "Loss for batch: 64 = 0.03128175064921379\n",
      "Loss for batch: 65 = 0.009132924489676952\n",
      "Loss for batch: 66 = 0.005672718398272991\n",
      "Loss for batch: 67 = 0.040142517536878586\n",
      "Loss for batch: 68 = 0.0761290043592453\n",
      "Loss for batch: 69 = 0.013638107106089592\n",
      "Loss for batch: 70 = 0.042176615446805954\n",
      "Loss for batch: 71 = 0.017941750586032867\n",
      "Loss for batch: 72 = 0.018011046573519707\n",
      "Loss for batch: 73 = 0.013354280963540077\n",
      "Loss for batch: 74 = 0.012723423540592194\n",
      "Loss for batch: 75 = 0.022120093926787376\n",
      "Loss for batch: 76 = 0.04030944034457207\n",
      "Loss for batch: 77 = 0.00492399837821722\n",
      "Loss for batch: 78 = 0.03725285455584526\n",
      "Loss for batch: 79 = 0.08114619553089142\n",
      "Loss for batch: 80 = 0.06702181696891785\n",
      "Loss for batch: 81 = 0.0944376289844513\n",
      "Loss for batch: 82 = 0.010439082980155945\n",
      "Loss for batch: 83 = 0.02093396708369255\n",
      "Loss for batch: 84 = 0.1328374445438385\n",
      "Loss for batch: 85 = 0.0259622223675251\n",
      "Loss for batch: 86 = 0.0998680517077446\n",
      "Loss for batch: 87 = 0.09070426225662231\n",
      "Loss for batch: 88 = 0.011148249730467796\n",
      "Loss for batch: 89 = 0.0570821538567543\n",
      "Loss for batch: 90 = 0.04304499551653862\n",
      "Loss for batch: 91 = 0.013434389606118202\n",
      "Loss for batch: 92 = 0.08586543053388596\n",
      "Loss for batch: 93 = 0.007598353084176779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [00:07<00:26,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch: 94 = 0.06202935799956322\n",
      "Loss for batch: 95 = 0.07709059119224548\n",
      "Loss for batch: 96 = 0.04608812928199768\n",
      "Loss for batch: 97 = 0.05765935406088829\n",
      "Loss for batch: 98 = 0.04111674427986145\n",
      "Loss for batch: 99 = 0.049062579870224\n",
      "Loss for batch: 100 = 0.0026500881649553776\n",
      "Loss for batch: 101 = 0.003879975061863661\n",
      "Loss for batch: 102 = 0.014771517366170883\n",
      "Loss for batch: 103 = 0.020314618945121765\n",
      "Loss for batch: 104 = 0.05576393008232117\n",
      "Loss for batch: 105 = 0.01518828235566616\n",
      "Loss for batch: 106 = 0.015299986116588116\n",
      "\n",
      "Training Loss for epoch 10 = 4.784097194671631\n",
      "\n",
      "Current Validation Loss = 24.941267013549805\n",
      "Best Validation Loss = 15.77910041809082\n",
      "Epochs without Improvement = 9\n",
      "Epoch : 11\n",
      "----------------------\n",
      "Loss for batch: 0 = 0.012010614387691021\n",
      "Loss for batch: 1 = 0.09333281964063644\n",
      "Loss for batch: 2 = 0.1904304325580597\n",
      "Loss for batch: 3 = 0.02130088210105896\n",
      "Loss for batch: 4 = 0.04855386167764664\n",
      "Loss for batch: 5 = 0.03576979041099548\n",
      "Loss for batch: 6 = 0.04408981651067734\n",
      "Loss for batch: 7 = 0.009450672194361687\n",
      "Loss for batch: 8 = 0.041166242212057114\n",
      "Loss for batch: 9 = 0.0071332152001559734\n",
      "Loss for batch: 10 = 0.0012322639813646674\n",
      "Loss for batch: 11 = 0.059393372386693954\n",
      "Loss for batch: 12 = 0.015117034316062927\n",
      "Loss for batch: 13 = 0.16119621694087982\n",
      "Loss for batch: 14 = 0.013271430507302284\n",
      "Loss for batch: 15 = 0.09608688950538635\n",
      "Loss for batch: 16 = 0.11452016234397888\n",
      "Loss for batch: 17 = 0.0439179465174675\n",
      "Loss for batch: 18 = 0.04047395661473274\n",
      "Loss for batch: 19 = 0.01353471726179123\n",
      "Loss for batch: 20 = 0.05515393242239952\n",
      "Loss for batch: 21 = 0.0025971285067498684\n",
      "Loss for batch: 22 = 0.09413720667362213\n",
      "Loss for batch: 23 = 0.003193868091329932\n",
      "Loss for batch: 24 = 0.20750582218170166\n",
      "Loss for batch: 25 = 0.03443935886025429\n",
      "Loss for batch: 26 = 0.023893609642982483\n",
      "Loss for batch: 27 = 0.0030832369811832905\n",
      "Loss for batch: 28 = 0.05096179246902466\n",
      "Loss for batch: 29 = 0.03687719628214836\n",
      "Loss for batch: 30 = 0.026793677359819412\n",
      "Loss for batch: 31 = 0.05040371045470238\n",
      "Loss for batch: 32 = 0.03324108570814133\n",
      "Loss for batch: 33 = 0.014947260729968548\n",
      "Loss for batch: 34 = 0.011692870408296585\n",
      "Loss for batch: 35 = 0.020395271480083466\n",
      "Loss for batch: 36 = 0.04881289601325989\n",
      "Loss for batch: 37 = 0.008536781184375286\n",
      "Loss for batch: 38 = 0.029391925781965256\n",
      "Loss for batch: 39 = 0.009571174159646034\n",
      "Loss for batch: 40 = 0.022781148552894592\n",
      "Loss for batch: 41 = 0.00827166996896267\n",
      "Loss for batch: 42 = 0.05940268561244011\n",
      "Loss for batch: 43 = 0.011005516164004803\n",
      "Loss for batch: 44 = 0.0038691048976033926\n",
      "Loss for batch: 45 = 0.0018969596130773425\n",
      "Loss for batch: 46 = 0.04952332004904747\n",
      "Loss for batch: 47 = 0.08138569444417953\n",
      "Loss for batch: 48 = 0.018953679129481316\n",
      "Loss for batch: 49 = 0.06740233302116394\n",
      "Loss for batch: 50 = 0.008312761783599854\n",
      "Loss for batch: 51 = 0.023020630702376366\n",
      "Loss for batch: 52 = 0.007139559369534254\n",
      "Loss for batch: 53 = 0.01063523255288601\n",
      "Loss for batch: 54 = 0.003514460753649473\n",
      "Loss for batch: 55 = 0.03780324012041092\n",
      "Loss for batch: 56 = 0.019997701048851013\n",
      "Loss for batch: 57 = 0.004989740438759327\n",
      "Loss for batch: 58 = 0.08120695501565933\n",
      "Loss for batch: 59 = 0.07633837312459946\n",
      "Loss for batch: 60 = 0.03727664425969124\n",
      "Loss for batch: 61 = 0.07121684402227402\n",
      "Loss for batch: 62 = 0.039460837841033936\n",
      "Loss for batch: 63 = 0.1158134788274765\n",
      "Loss for batch: 64 = 0.018408332020044327\n",
      "Loss for batch: 65 = 0.07070125639438629\n",
      "Loss for batch: 66 = 0.03461933135986328\n",
      "Loss for batch: 67 = 0.1863497495651245\n",
      "Loss for batch: 68 = 0.09644962847232819\n",
      "Loss for batch: 69 = 0.34956392645835876\n",
      "Loss for batch: 70 = 0.0818631500005722\n",
      "Loss for batch: 71 = 0.027859050780534744\n",
      "Loss for batch: 72 = 0.029556233435869217\n",
      "Loss for batch: 73 = 0.012401675805449486\n",
      "Loss for batch: 74 = 0.040610432624816895\n",
      "Loss for batch: 75 = 0.00194123899564147\n",
      "Loss for batch: 76 = 0.06598242372274399\n",
      "Loss for batch: 77 = 0.03775125741958618\n",
      "Loss for batch: 78 = 0.0165867879986763\n",
      "Loss for batch: 79 = 0.042050495743751526\n",
      "Loss for batch: 80 = 0.019482729956507683\n",
      "Loss for batch: 81 = 0.02155437134206295\n",
      "Loss for batch: 82 = 0.06149255484342575\n",
      "Loss for batch: 83 = 0.04011138528585434\n",
      "Loss for batch: 84 = 0.030110172927379608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [00:08<00:29,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch: 85 = 0.002214370295405388\n",
      "Loss for batch: 86 = 0.038589369505643845\n",
      "Loss for batch: 87 = 0.025360098108649254\n",
      "Loss for batch: 88 = 0.029356643557548523\n",
      "Loss for batch: 89 = 0.057436566799879074\n",
      "Loss for batch: 90 = 0.01017427071928978\n",
      "Loss for batch: 91 = 0.006005163304507732\n",
      "Loss for batch: 92 = 0.04359331354498863\n",
      "Loss for batch: 93 = 0.006033105310052633\n",
      "Loss for batch: 94 = 0.050317443907260895\n",
      "Loss for batch: 95 = 0.02899501845240593\n",
      "Loss for batch: 96 = 0.021943524479866028\n",
      "Loss for batch: 97 = 0.04002695530653\n",
      "Loss for batch: 98 = 0.007274298928678036\n",
      "Loss for batch: 99 = 0.043344080448150635\n",
      "Loss for batch: 100 = 0.014854892157018185\n",
      "Loss for batch: 101 = 0.07208295166492462\n",
      "Loss for batch: 102 = 0.05634059011936188\n",
      "Loss for batch: 103 = 0.033085472881793976\n",
      "Loss for batch: 104 = 0.04262159392237663\n",
      "Loss for batch: 105 = 0.008665496483445168\n",
      "Loss for batch: 106 = 0.017255373299121857\n",
      "\n",
      "Training Loss for epoch 11 = 4.629846096038818\n",
      "\n",
      "Current Validation Loss = 29.644752502441406\n",
      "Best Validation Loss = 15.77910041809082\n",
      "Epochs without Improvement = 10\n",
      "Early Stoppping Triggered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "epochs_without_improvement = 0\n",
    "total_train_loss = []\n",
    "total_val_loss = []\n",
    "best_model_weights = model.state_dict()\n",
    "model.to(\"cuda\")\n",
    "for epoch in tqdm(range(50)):\n",
    "    model.train()\n",
    "    print(f\"Epoch : {epoch}\\n----------------------\")\n",
    "    train_loss = 0\n",
    "    for batch, (x,y) in enumerate(train_loader):\n",
    "        x, y = x.to(\"cuda\"), y.to(\"cuda\")\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Loss for batch: {batch} = {loss}\")\n",
    "        train_loss += loss\n",
    "\n",
    "    print(\"\\nTraining Loss for epoch {} = {}\\n\".format(epoch, train_loss))\n",
    "    total_train_loss.append(train_loss/len(train_loader.dataset))\n",
    "\n",
    "    model.eval()\n",
    "    validation_loss = 0\n",
    "    with torch.inference_mode():\n",
    "        for x,y in val_loader:\n",
    "            x,y = x.to(\"cuda\"), y.to(\"cuda\")\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            validation_loss+=loss\n",
    "\n",
    "        if validation_loss < best_val_loss:\n",
    "            best_val_loss = validation_loss\n",
    "            epochs_without_improvement = 0\n",
    "            best_model_weights = model.state_dict()\n",
    "        else:\n",
    "            epochs_without_improvement+=1\n",
    "\n",
    "        print(f\"Current Validation Loss = {validation_loss}\")\n",
    "        print(f\"Best Validation Loss = {best_val_loss}\")\n",
    "        print(f\"Epochs without Improvement = {epochs_without_improvement}\")\n",
    "\n",
    "    total_val_loss.append(validation_loss/len(val_loader.dataset))\n",
    "    if epochs_without_improvement >= 10:\n",
    "        print(\"Early Stoppping Triggered\")\n",
    "        break\n",
    "\n",
    "# model.load_state_dict(best_model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "def TrainLoopv2(\n",
    "    model,\n",
    "    optimizer:torch.optim.Optimizer,\n",
    "    criterion:torch.nn.Module,\n",
    "    train_dataloader:torch.utils.data.DataLoader,\n",
    "    val_dataloader:torch.utils.data.DataLoader,\n",
    "    test_dataloader:torch.utils.data.DataLoader=None,\n",
    "    num_epochs:int=20,\n",
    "    early_stopping_rounds:int=5,\n",
    "    return_best_model:bool=True,\n",
    "    device:str='cpu'\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a PyTorch model using the provided data loaders and monitor training progress.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The PyTorch model to train.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer for updating model parameters.\n",
    "        criterion (torch.nn.Module): The loss function to optimize.\n",
    "        train_dataloader (torch.utils.data.DataLoader): DataLoader for the training dataset.\n",
    "        val_dataloader (torch.utils.data.DataLoader): DataLoader for the validation dataset.\n",
    "        test_dataloader (torch.utils.data.DataLoader, optional): DataLoader for the test dataset (default: None).\n",
    "        num_epochs (int, optional): Number of training epochs (default: 20).\n",
    "        early_stopping_rounds (int, optional): Number of epochs to wait for improvement in validation loss\n",
    "            before early stopping (default: 5).\n",
    "        return_best_model (bool, optional): Whether to return the model with the best validation loss (default: True).\n",
    "        device (str, optional): Device to use for training ('cpu' or 'cuda') (default: 'cpu').\n",
    "\n",
    "    Returns:\n",
    "        None or torch.nn.Module: If return_best_model is True, returns the trained model with the best validation loss;\n",
    "        otherwise, returns None.\n",
    "\n",
    "    Note:\n",
    "        This function monitors training and validation loss and accuracy over epochs and can optionally\n",
    "        plot the loss and accuracy curves.\n",
    "\n",
    "    Example:\n",
    "        See the code example provided for usage.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    total_train_loss = []\n",
    "    total_val_loss = []\n",
    "    total_test_loss = []\n",
    "    best_model_weights = model.state_dict()\n",
    "\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        model.train()\n",
    "        print(\"\\nEpoch {}\\n----------\".format(epoch))\n",
    "        train_loss = 0\n",
    "        for i, (batch, label) in enumerate(train_dataloader):\n",
    "            batch, label = batch.to(device), label.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch)\n",
    "            loss = criterion(outputs, label.float())\n",
    "            train_loss += loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(\"Loss for batch {} = {}\".format(i, loss))\n",
    "\n",
    "        print(\"\\nTraining Loss for epoch {} = {}\\n\".format(epoch, train_loss))\n",
    "        total_train_loss.append(train_loss/len(train_dataloader.dataset))\n",
    "\n",
    "        model.eval()\n",
    "        validation_loss = 0\n",
    "        with torch.inference_mode():\n",
    "            val_true_labels = []\n",
    "            train_true_labels = []\n",
    "            val_pred_labels = []\n",
    "            train_pred_labels = []\n",
    "            for batch, label in val_dataloader:\n",
    "                batch, label = batch.to(device), label.to(device)\n",
    "                outputs = model(batch)\n",
    "                loss = criterion(outputs, label.float())\n",
    "                validation_loss += loss\n",
    "\n",
    "                outputs = torch.round(torch.sigmoid(outputs))\n",
    "                val_true_labels.extend(label.cpu().numpy())\n",
    "                val_pred_labels.extend(outputs.cpu().numpy())\n",
    "\n",
    "            for batch, label in train_dataloader:\n",
    "                batch, label = batch.to(device), label.to(device)\n",
    "                outputs = model(batch)\n",
    "\n",
    "                outputs = torch.round(torch.sigmoid(outputs))\n",
    "                train_true_labels.extend(label.cpu().numpy())\n",
    "                train_pred_labels.extend(outputs.cpu().numpy())\n",
    "\n",
    "            if validation_loss < best_val_loss:\n",
    "                best_val_loss = validation_loss\n",
    "                epochs_without_improvement = 0\n",
    "                best_model_weights = model.state_dict()\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "\n",
    "            val_true_labels = np.array(val_true_labels)\n",
    "            train_true_labels = np.array(train_true_labels)\n",
    "            val_pred_labels = np.array(val_pred_labels)\n",
    "            train_pred_labels = np.array(train_pred_labels)\n",
    "\n",
    "            train_accuracy = accuracy_score(train_true_labels, train_pred_labels)\n",
    "            val_accuracy = accuracy_score(val_true_labels, val_pred_labels)\n",
    "\n",
    "            train_accuracies.append(train_accuracy)\n",
    "            val_accuracies.append(val_accuracy)\n",
    "\n",
    "            print(f\"Current Validation Loss = {validation_loss}\")\n",
    "            print(f\"Best Validation Loss = {best_val_loss}\")\n",
    "            print(f\"Epochs without Improvement = {epochs_without_improvement}\")\n",
    "\n",
    "            print(f\"Train Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "            print(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
    "\n",
    "        total_val_loss.append(validation_loss/len(val_dataloader.dataset))\n",
    "\n",
    "        if test_dataloader is not None:\n",
    "            test_loss = 0\n",
    "            with torch.inference_mode():\n",
    "                for batch, label in test_dataloader:\n",
    "                    batch, label = batch.to(device), label.to(device)\n",
    "                    outputs = model(batch)\n",
    "                    loss = criterion(outputs, label.float())\n",
    "                    test_loss += loss\n",
    "\n",
    "                print(\"\\nTest Loss for epoch {} = {}\\n\".format(epoch, test_loss))\n",
    "            total_test_loss.append(test_loss/len(test_dataloader.dataset))\n",
    "\n",
    "        if epochs_without_improvement >= early_stopping_rounds:\n",
    "            print(\"Early Stoppping Triggered\")\n",
    "            break\n",
    "\n",
    "    if return_best_model == True:\n",
    "        model.load_state_dict(best_model_weights)\n",
    "    total_train_loss = [item.cpu().detach().numpy() for item in total_train_loss]\n",
    "    total_val_loss = [item.cpu().detach().numpy() for item in total_val_loss]\n",
    "\n",
    "    total_train_loss = np.array(total_train_loss)\n",
    "    total_val_loss = np.array(total_val_loss)\n",
    "\n",
    "    train_accuracies = np.array(train_accuracies)\n",
    "    val_accuracies = np.array(val_accuracies)\n",
    "\n",
    "    x_train = np.arange(len(total_train_loss))\n",
    "    x_val = np.arange(len(total_val_loss))\n",
    "    \n",
    "    sns.set_style('whitegrid')\n",
    "    plt.figure(figsize=(14,5))\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    try:\n",
    "        total_test_loss = [item.cpu().detach().numpy() for item in total_test_loss]\n",
    "        total_test_loss = np.array(total_test_loss)\n",
    "        x_test = np.arange(len(total_test_loss))\n",
    "        sns.lineplot(x=x_test, y=total_test_loss, label='Testing Loss')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    sns.lineplot(x=x_train, y=total_train_loss, label='Training Loss')\n",
    "    sns.lineplot(x=x_val, y=total_val_loss, label='Validation Loss')\n",
    "    plt.title(\"Loss over {} Epochs\".format(len(total_train_loss)))\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xticks(np.arange(len(total_train_loss)))\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    sns.lineplot(x=x_train, y=train_accuracies, label='Training Accuracy')\n",
    "    sns.lineplot(x=x_val, y=val_accuracies, label='Validation Accuracy')\n",
    "    plt.title(\"Accuracy over {} Epochs\".format(len(total_train_loss)))\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xticks(np.arange(len(total_train_loss)))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0\n",
      "----------\n",
      "Loss for batch 0 = 1.014578104019165\n",
      "Loss for batch 1 = 5.19977331161499\n",
      "Loss for batch 2 = 1.5508254766464233\n",
      "Loss for batch 3 = 3.372941017150879\n",
      "Loss for batch 4 = 4.161283493041992\n",
      "Loss for batch 5 = 1.3018360137939453\n",
      "Loss for batch 6 = 1.601271152496338\n",
      "Loss for batch 7 = 1.0607154369354248\n",
      "Loss for batch 8 = 1.0760916471481323\n",
      "Loss for batch 9 = 1.5593236684799194\n",
      "Loss for batch 10 = 1.9911893606185913\n",
      "Loss for batch 11 = 1.4221689701080322\n",
      "Loss for batch 12 = 0.6035967469215393\n",
      "Loss for batch 13 = 0.6954449415206909\n",
      "Loss for batch 14 = 0.719988226890564\n",
      "Loss for batch 15 = 0.7601801753044128\n",
      "Loss for batch 16 = 0.5828694701194763\n",
      "Loss for batch 17 = 0.6983851194381714\n",
      "Loss for batch 18 = 0.9198417067527771\n",
      "Loss for batch 19 = 0.7893872261047363\n",
      "Loss for batch 20 = 0.8139294385910034\n",
      "Loss for batch 21 = 0.9181838035583496\n",
      "Loss for batch 22 = 0.9654979705810547\n",
      "Loss for batch 23 = 0.8743981719017029\n",
      "Loss for batch 24 = 0.5406288504600525\n",
      "Loss for batch 25 = 0.6463393568992615\n",
      "Loss for batch 26 = 0.5838353037834167\n",
      "Loss for batch 27 = 0.6691142916679382\n",
      "Loss for batch 28 = 0.6715995669364929\n",
      "Loss for batch 29 = 0.7068371772766113\n",
      "Loss for batch 30 = 0.6494026780128479\n",
      "Loss for batch 31 = 0.683491587638855\n",
      "Loss for batch 32 = 0.7378929853439331\n",
      "Loss for batch 33 = 0.8340051174163818\n",
      "Loss for batch 34 = 1.3512781858444214\n",
      "Loss for batch 35 = 1.2013241052627563\n",
      "Loss for batch 36 = 0.9633253812789917\n",
      "Loss for batch 37 = 0.5696349143981934\n",
      "Loss for batch 38 = 0.6643600463867188\n",
      "Loss for batch 39 = 0.7890620231628418\n",
      "Loss for batch 40 = 0.8063260912895203\n",
      "Loss for batch 41 = 0.5413909554481506\n",
      "Loss for batch 42 = 0.7152899503707886\n",
      "Loss for batch 43 = 0.6817318797111511\n",
      "Loss for batch 44 = 0.6681042909622192\n",
      "Loss for batch 45 = 0.7844070792198181\n",
      "Loss for batch 46 = 0.7919373512268066\n",
      "Loss for batch 47 = 0.5910276770591736\n",
      "Loss for batch 48 = 0.6281454563140869\n",
      "Loss for batch 49 = 0.8212289214134216\n",
      "Loss for batch 50 = 0.6737083792686462\n",
      "Loss for batch 51 = 0.7151787281036377\n",
      "Loss for batch 52 = 0.55173259973526\n",
      "Loss for batch 53 = 0.7179104089736938\n",
      "Loss for batch 54 = 0.6341390609741211\n",
      "Loss for batch 55 = 0.6994854211807251\n",
      "Loss for batch 56 = 0.6477508544921875\n",
      "Loss for batch 57 = 0.6276707649230957\n",
      "Loss for batch 58 = 0.8421095609664917\n",
      "Loss for batch 59 = 0.8056859374046326\n",
      "Loss for batch 60 = 0.8169295191764832\n",
      "Loss for batch 61 = 0.6580591201782227\n",
      "Loss for batch 62 = 0.5688959956169128\n",
      "Loss for batch 63 = 0.6197896003723145\n",
      "Loss for batch 64 = 0.6001483798027039\n",
      "Loss for batch 65 = 0.6453882455825806\n",
      "Loss for batch 66 = 0.6058367490768433\n",
      "Loss for batch 67 = 0.5333544611930847\n",
      "Loss for batch 68 = 0.7825216054916382\n",
      "Loss for batch 69 = 0.8282926082611084\n",
      "Loss for batch 70 = 0.5260195136070251\n",
      "Loss for batch 71 = 0.7959369421005249\n",
      "Loss for batch 72 = 0.7327059507369995\n",
      "Loss for batch 73 = 0.6398744583129883\n",
      "Loss for batch 74 = 0.7199383974075317\n",
      "Loss for batch 75 = 0.5803489685058594\n",
      "Loss for batch 76 = 0.6024199724197388\n",
      "Loss for batch 77 = 0.5530114769935608\n",
      "Loss for batch 78 = 0.6844634413719177\n",
      "Loss for batch 79 = 0.6234883666038513\n",
      "Loss for batch 80 = 0.6024397611618042\n",
      "Loss for batch 81 = 0.5574532747268677\n",
      "Loss for batch 82 = 0.7080468535423279\n",
      "Loss for batch 83 = 0.6586136817932129\n",
      "Loss for batch 84 = 0.611261785030365\n",
      "Loss for batch 85 = 0.633955717086792\n",
      "Loss for batch 86 = 0.6018700003623962\n",
      "Loss for batch 87 = 0.6529017090797424\n",
      "Loss for batch 88 = 0.6936758756637573\n",
      "Loss for batch 89 = 0.5145175457000732\n",
      "Loss for batch 90 = 0.6830024123191833\n",
      "Loss for batch 91 = 0.5005866289138794\n",
      "Loss for batch 92 = 0.4958636462688446\n",
      "Loss for batch 93 = 0.6619300246238708\n",
      "Loss for batch 94 = 0.47848278284072876\n",
      "Loss for batch 95 = 0.6861359477043152\n",
      "Loss for batch 96 = 0.6284510493278503\n",
      "Loss for batch 97 = 0.6407117247581482\n",
      "Loss for batch 98 = 0.7101705074310303\n",
      "Loss for batch 99 = 0.6918294429779053\n",
      "Loss for batch 100 = 0.529706597328186\n",
      "Loss for batch 101 = 0.5109184384346008\n",
      "Loss for batch 102 = 0.5443305969238281\n",
      "Loss for batch 103 = 0.6311678886413574\n",
      "Loss for batch 104 = 0.5880602598190308\n",
      "Loss for batch 105 = 0.6121606826782227\n",
      "Loss for batch 106 = 0.7776455283164978\n",
      "\n",
      "Training Loss for epoch 0 = 90.5840835571289\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [00:00<00:48,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Validation Loss = 13.568245887756348\n",
      "Best Validation Loss = 13.568245887756348\n",
      "Epochs without Improvement = 0\n",
      "Train Accuracy: 39.00%\n",
      "Validation Accuracy: 32.17%\n",
      "\n",
      "Epoch 1\n",
      "----------\n",
      "Loss for batch 0 = 0.6224752068519592\n",
      "Loss for batch 1 = 0.5158944129943848\n",
      "Loss for batch 2 = 0.511666476726532\n",
      "Loss for batch 3 = 0.4529153108596802\n",
      "Loss for batch 4 = 0.471196711063385\n",
      "Loss for batch 5 = 0.49165672063827515\n",
      "Loss for batch 6 = 0.5233616232872009\n",
      "Loss for batch 7 = 0.6138163805007935\n",
      "Loss for batch 8 = 0.5104269981384277\n",
      "Loss for batch 9 = 0.410862535238266\n",
      "Loss for batch 10 = 0.6461741924285889\n",
      "Loss for batch 11 = 0.6865035891532898\n",
      "Loss for batch 12 = 0.5346152782440186\n",
      "Loss for batch 13 = 0.48199009895324707\n",
      "Loss for batch 14 = 0.4166538715362549\n",
      "Loss for batch 15 = 0.4026637077331543\n",
      "Loss for batch 16 = 0.39624613523483276\n",
      "Loss for batch 17 = 0.4803256392478943\n",
      "Loss for batch 18 = 0.4373602271080017\n",
      "Loss for batch 19 = 0.5597971081733704\n",
      "Loss for batch 20 = 0.6041118502616882\n",
      "Loss for batch 21 = 0.5139990448951721\n",
      "Loss for batch 22 = 0.47992846369743347\n",
      "Loss for batch 23 = 0.5244743824005127\n",
      "Loss for batch 24 = 0.5054954290390015\n",
      "Loss for batch 25 = 0.47671887278556824\n",
      "Loss for batch 26 = 0.44189053773880005\n",
      "Loss for batch 27 = 0.4909094274044037\n",
      "Loss for batch 28 = 0.5317872762680054\n",
      "Loss for batch 29 = 0.3813256025314331\n",
      "Loss for batch 30 = 0.5059574842453003\n",
      "Loss for batch 31 = 0.6042556762695312\n",
      "Loss for batch 32 = 0.39280855655670166\n",
      "Loss for batch 33 = 0.45519065856933594\n",
      "Loss for batch 34 = 0.5075797438621521\n",
      "Loss for batch 35 = 0.43282437324523926\n",
      "Loss for batch 36 = 0.5570003986358643\n",
      "Loss for batch 37 = 0.33569973707199097\n",
      "Loss for batch 38 = 0.40634167194366455\n",
      "Loss for batch 39 = 0.46097010374069214\n",
      "Loss for batch 40 = 0.42750319838523865\n",
      "Loss for batch 41 = 0.6446451544761658\n",
      "Loss for batch 42 = 0.5575306415557861\n",
      "Loss for batch 43 = 0.454568088054657\n",
      "Loss for batch 44 = 0.6028479337692261\n",
      "Loss for batch 45 = 0.6652176976203918\n",
      "Loss for batch 46 = 0.570196270942688\n",
      "Loss for batch 47 = 0.4330558180809021\n",
      "Loss for batch 48 = 0.47410887479782104\n",
      "Loss for batch 49 = 0.4528610408306122\n",
      "Loss for batch 50 = 0.44922107458114624\n",
      "Loss for batch 51 = 0.5145812630653381\n",
      "Loss for batch 52 = 0.5227178335189819\n",
      "Loss for batch 53 = 0.7824504375457764\n",
      "Loss for batch 54 = 0.5669326186180115\n",
      "Loss for batch 55 = 0.48202595114707947\n",
      "Loss for batch 56 = 0.4256705045700073\n",
      "Loss for batch 57 = 0.4761258065700531\n",
      "Loss for batch 58 = 0.4467623233795166\n",
      "Loss for batch 59 = 0.4581778645515442\n",
      "Loss for batch 60 = 0.4536283016204834\n",
      "Loss for batch 61 = 0.5409556031227112\n",
      "Loss for batch 62 = 0.658014714717865\n",
      "Loss for batch 63 = 0.557628333568573\n",
      "Loss for batch 64 = 0.41232818365097046\n",
      "Loss for batch 65 = 0.5138856768608093\n",
      "Loss for batch 66 = 0.5824562907218933\n",
      "Loss for batch 67 = 0.5603172779083252\n",
      "Loss for batch 68 = 0.4885375499725342\n",
      "Loss for batch 69 = 0.52147376537323\n",
      "Loss for batch 70 = 0.4470894932746887\n",
      "Loss for batch 71 = 0.5016923546791077\n",
      "Loss for batch 72 = 0.5853288173675537\n",
      "Loss for batch 73 = 0.48688647150993347\n",
      "Loss for batch 74 = 0.5661351680755615\n",
      "Loss for batch 75 = 0.652206540107727\n",
      "Loss for batch 76 = 0.5030745267868042\n",
      "Loss for batch 77 = 0.48826003074645996\n",
      "Loss for batch 78 = 0.6314976215362549\n",
      "Loss for batch 79 = 0.506994366645813\n",
      "Loss for batch 80 = 0.5132342576980591\n",
      "Loss for batch 81 = 0.6046910285949707\n",
      "Loss for batch 82 = 0.5781737565994263\n",
      "Loss for batch 83 = 0.4598843455314636\n",
      "Loss for batch 84 = 0.44575250148773193\n",
      "Loss for batch 85 = 0.5740071535110474\n",
      "Loss for batch 86 = 0.767626941204071\n",
      "Loss for batch 87 = 0.8648139834403992\n",
      "Loss for batch 88 = 0.6685218811035156\n",
      "Loss for batch 89 = 0.45084381103515625\n",
      "Loss for batch 90 = 0.5774766206741333\n",
      "Loss for batch 91 = 0.39957737922668457\n",
      "Loss for batch 92 = 0.3908865451812744\n",
      "Loss for batch 93 = 0.4739413857460022\n",
      "Loss for batch 94 = 0.38283178210258484\n",
      "Loss for batch 95 = 0.5772295594215393\n",
      "Loss for batch 96 = 0.459076464176178\n",
      "Loss for batch 97 = 0.5064408183097839\n",
      "Loss for batch 98 = 0.627543568611145\n",
      "Loss for batch 99 = 0.7888997793197632\n",
      "Loss for batch 100 = 0.858717679977417\n",
      "Loss for batch 101 = 0.6366150975227356\n",
      "Loss for batch 102 = 0.4751225411891937\n",
      "Loss for batch 103 = 0.6234628558158875\n",
      "Loss for batch 104 = 0.5176979899406433\n",
      "Loss for batch 105 = 0.5738697052001953\n",
      "Loss for batch 106 = 0.35813814401626587\n",
      "\n",
      "Training Loss for epoch 1 = 55.996543884277344\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/50 [00:01<00:41,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Validation Loss = 9.587371826171875\n",
      "Best Validation Loss = 9.587371826171875\n",
      "Epochs without Improvement = 0\n",
      "Train Accuracy: 61.24%\n",
      "Validation Accuracy: 41.60%\n",
      "\n",
      "Epoch 2\n",
      "----------\n",
      "Loss for batch 0 = 0.42861220240592957\n",
      "Loss for batch 1 = 0.33986586332321167\n",
      "Loss for batch 2 = 0.3351646661758423\n",
      "Loss for batch 3 = 0.3220907747745514\n",
      "Loss for batch 4 = 0.3239196538925171\n",
      "Loss for batch 5 = 0.3844642639160156\n",
      "Loss for batch 6 = 0.3552582263946533\n",
      "Loss for batch 7 = 0.31061673164367676\n",
      "Loss for batch 8 = 0.2647508978843689\n",
      "Loss for batch 9 = 0.30426493287086487\n",
      "Loss for batch 10 = 0.3822447955608368\n",
      "Loss for batch 11 = 0.30273327231407166\n",
      "Loss for batch 12 = 0.3198814392089844\n",
      "Loss for batch 13 = 0.3870933949947357\n",
      "Loss for batch 14 = 0.29657649993896484\n",
      "Loss for batch 15 = 0.2368837594985962\n",
      "Loss for batch 16 = 0.3963087797164917\n",
      "Loss for batch 17 = 0.3874983489513397\n",
      "Loss for batch 18 = 0.4231611490249634\n",
      "Loss for batch 19 = 0.4956999719142914\n",
      "Loss for batch 20 = 0.29560524225234985\n",
      "Loss for batch 21 = 0.2974921464920044\n",
      "Loss for batch 22 = 0.45678743720054626\n",
      "Loss for batch 23 = 0.2829514741897583\n",
      "Loss for batch 24 = 0.35751232504844666\n",
      "Loss for batch 25 = 0.5019066333770752\n",
      "Loss for batch 26 = 0.34108495712280273\n",
      "Loss for batch 27 = 0.3334422707557678\n",
      "Loss for batch 28 = 0.3056933879852295\n",
      "Loss for batch 29 = 0.4041171669960022\n",
      "Loss for batch 30 = 0.3306538462638855\n",
      "Loss for batch 31 = 0.37818998098373413\n",
      "Loss for batch 32 = 0.3235173225402832\n",
      "Loss for batch 33 = 0.4022429883480072\n",
      "Loss for batch 34 = 0.34525343775749207\n",
      "Loss for batch 35 = 0.4627840518951416\n",
      "Loss for batch 36 = 0.4324692487716675\n",
      "Loss for batch 37 = 0.3827362060546875\n",
      "Loss for batch 38 = 0.3273771405220032\n",
      "Loss for batch 39 = 0.30014723539352417\n",
      "Loss for batch 40 = 0.42589831352233887\n",
      "Loss for batch 41 = 0.23445241153240204\n",
      "Loss for batch 42 = 0.39200764894485474\n",
      "Loss for batch 43 = 0.39032408595085144\n",
      "Loss for batch 44 = 0.3996407389640808\n",
      "Loss for batch 45 = 0.4318937659263611\n",
      "Loss for batch 46 = 0.4504455029964447\n",
      "Loss for batch 47 = 0.3422258496284485\n",
      "Loss for batch 48 = 0.39243245124816895\n",
      "Loss for batch 49 = 0.407632052898407\n",
      "Loss for batch 50 = 0.29710856080055237\n",
      "Loss for batch 51 = 0.3437647819519043\n",
      "Loss for batch 52 = 0.3887856900691986\n",
      "Loss for batch 53 = 0.3847891092300415\n",
      "Loss for batch 54 = 0.31132346391677856\n",
      "Loss for batch 55 = 0.2670803964138031\n",
      "Loss for batch 56 = 0.38456904888153076\n",
      "Loss for batch 57 = 0.35945677757263184\n",
      "Loss for batch 58 = 0.4371114671230316\n",
      "Loss for batch 59 = 0.3666251599788666\n",
      "Loss for batch 60 = 0.40506553649902344\n",
      "Loss for batch 61 = 0.4273006319999695\n",
      "Loss for batch 62 = 0.4063088297843933\n",
      "Loss for batch 63 = 0.35952767729759216\n",
      "Loss for batch 64 = 0.5110529661178589\n",
      "Loss for batch 65 = 0.3382381200790405\n",
      "Loss for batch 66 = 0.42380112409591675\n",
      "Loss for batch 67 = 0.4548507630825043\n",
      "Loss for batch 68 = 0.35898861289024353\n",
      "Loss for batch 69 = 0.38502371311187744\n",
      "Loss for batch 70 = 0.42986583709716797\n",
      "Loss for batch 71 = 0.344578355550766\n",
      "Loss for batch 72 = 0.329035222530365\n",
      "Loss for batch 73 = 0.36019787192344666\n",
      "Loss for batch 74 = 0.3986121118068695\n",
      "Loss for batch 75 = 0.34290045499801636\n",
      "Loss for batch 76 = 0.35453152656555176\n",
      "Loss for batch 77 = 0.42863184213638306\n",
      "Loss for batch 78 = 0.42228007316589355\n",
      "Loss for batch 79 = 0.43268507719039917\n",
      "Loss for batch 80 = 0.3979818820953369\n",
      "Loss for batch 81 = 0.47526392340660095\n",
      "Loss for batch 82 = 0.4640769362449646\n",
      "Loss for batch 83 = 0.4656991958618164\n",
      "Loss for batch 84 = 0.39124712347984314\n",
      "Loss for batch 85 = 0.3203775882720947\n",
      "Loss for batch 86 = 0.41973209381103516\n",
      "Loss for batch 87 = 0.4014108180999756\n",
      "Loss for batch 88 = 0.3133690059185028\n",
      "Loss for batch 89 = 0.546815812587738\n",
      "Loss for batch 90 = 0.4481852054595947\n",
      "Loss for batch 91 = 0.33116865158081055\n",
      "Loss for batch 92 = 0.4210980534553528\n",
      "Loss for batch 93 = 0.37909960746765137\n",
      "Loss for batch 94 = 0.34940028190612793\n",
      "Loss for batch 95 = 0.5783987045288086\n",
      "Loss for batch 96 = 0.3726981580257416\n",
      "Loss for batch 97 = 0.4028129279613495\n",
      "Loss for batch 98 = 0.43282026052474976\n",
      "Loss for batch 99 = 0.33361753821372986\n",
      "Loss for batch 100 = 0.5945818424224854\n",
      "Loss for batch 101 = 0.46260136365890503\n",
      "Loss for batch 102 = 0.27460813522338867\n",
      "Loss for batch 103 = 0.4419872760772705\n",
      "Loss for batch 104 = 0.4253630042076111\n",
      "Loss for batch 105 = 0.6255819797515869\n",
      "Loss for batch 106 = 0.885893702507019\n",
      "\n",
      "Training Loss for epoch 2 = 41.535987854003906\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/50 [00:02<00:38,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Validation Loss = 10.752259254455566\n",
      "Best Validation Loss = 9.587371826171875\n",
      "Epochs without Improvement = 1\n",
      "Train Accuracy: 58.63%\n",
      "Validation Accuracy: 31.35%\n",
      "\n",
      "Epoch 3\n",
      "----------\n",
      "Loss for batch 0 = 0.46916234493255615\n",
      "Loss for batch 1 = 0.3424702286720276\n",
      "Loss for batch 2 = 0.2592969536781311\n",
      "Loss for batch 3 = 0.3235846757888794\n",
      "Loss for batch 4 = 0.2569960057735443\n",
      "Loss for batch 5 = 0.26069194078445435\n",
      "Loss for batch 6 = 0.21523723006248474\n",
      "Loss for batch 7 = 0.21477487683296204\n",
      "Loss for batch 8 = 0.2032860517501831\n",
      "Loss for batch 9 = 0.22792157530784607\n",
      "Loss for batch 10 = 0.2966669201850891\n",
      "Loss for batch 11 = 0.2626592814922333\n",
      "Loss for batch 12 = 0.29624801874160767\n",
      "Loss for batch 13 = 0.22360646724700928\n",
      "Loss for batch 14 = 0.13794010877609253\n",
      "Loss for batch 15 = 0.1758652776479721\n",
      "Loss for batch 16 = 0.216884583234787\n",
      "Loss for batch 17 = 0.16287583112716675\n",
      "Loss for batch 18 = 0.24679110944271088\n",
      "Loss for batch 19 = 0.2852206230163574\n",
      "Loss for batch 20 = 0.24875640869140625\n",
      "Loss for batch 21 = 0.2557979226112366\n",
      "Loss for batch 22 = 0.33178019523620605\n",
      "Loss for batch 23 = 0.4701453149318695\n",
      "Loss for batch 24 = 0.35784587264060974\n",
      "Loss for batch 25 = 0.3318229615688324\n",
      "Loss for batch 26 = 0.23361718654632568\n",
      "Loss for batch 27 = 0.29466551542282104\n",
      "Loss for batch 28 = 0.2644631862640381\n",
      "Loss for batch 29 = 0.2070099115371704\n",
      "Loss for batch 30 = 0.20090417563915253\n",
      "Loss for batch 31 = 0.2812417149543762\n",
      "Loss for batch 32 = 0.2340453565120697\n",
      "Loss for batch 33 = 0.3394923210144043\n",
      "Loss for batch 34 = 0.12783969938755035\n",
      "Loss for batch 35 = 0.19082467257976532\n",
      "Loss for batch 36 = 0.32419246435165405\n",
      "Loss for batch 37 = 0.2006368339061737\n",
      "Loss for batch 38 = 0.2821977138519287\n",
      "Loss for batch 39 = 0.3775765597820282\n",
      "Loss for batch 40 = 0.32528284192085266\n",
      "Loss for batch 41 = 0.17534971237182617\n",
      "Loss for batch 42 = 0.3525084853172302\n",
      "Loss for batch 43 = 0.25837787985801697\n",
      "Loss for batch 44 = 0.22858628630638123\n",
      "Loss for batch 45 = 0.21521703898906708\n",
      "Loss for batch 46 = 0.3286999762058258\n",
      "Loss for batch 47 = 0.3807772994041443\n",
      "Loss for batch 48 = 0.2963976263999939\n",
      "Loss for batch 49 = 0.33623382449150085\n",
      "Loss for batch 50 = 0.2760353684425354\n",
      "Loss for batch 51 = 0.25061607360839844\n",
      "Loss for batch 52 = 0.22211115062236786\n",
      "Loss for batch 53 = 0.23611268401145935\n",
      "Loss for batch 54 = 0.2703424096107483\n",
      "Loss for batch 55 = 0.17486970126628876\n",
      "Loss for batch 56 = 0.3629702031612396\n",
      "Loss for batch 57 = 0.26931166648864746\n",
      "Loss for batch 58 = 0.23920390009880066\n",
      "Loss for batch 59 = 0.3023909330368042\n",
      "Loss for batch 60 = 0.29168444871902466\n",
      "Loss for batch 61 = 0.3625809848308563\n",
      "Loss for batch 62 = 0.39473479986190796\n",
      "Loss for batch 63 = 0.35884663462638855\n",
      "Loss for batch 64 = 0.22207824885845184\n",
      "Loss for batch 65 = 0.22561204433441162\n",
      "Loss for batch 66 = 0.16412270069122314\n",
      "Loss for batch 67 = 0.28314706683158875\n",
      "Loss for batch 68 = 0.3891502022743225\n",
      "Loss for batch 69 = 0.21070443093776703\n",
      "Loss for batch 70 = 0.1669137179851532\n",
      "Loss for batch 71 = 0.32200413942337036\n",
      "Loss for batch 72 = 0.22384941577911377\n",
      "Loss for batch 73 = 0.2783333659172058\n",
      "Loss for batch 74 = 0.33657997846603394\n",
      "Loss for batch 75 = 0.251623272895813\n",
      "Loss for batch 76 = 0.3110806345939636\n",
      "Loss for batch 77 = 0.3852343261241913\n",
      "Loss for batch 78 = 0.3331001102924347\n",
      "Loss for batch 79 = 0.1817677617073059\n",
      "Loss for batch 80 = 0.33710217475891113\n",
      "Loss for batch 81 = 0.33519893884658813\n",
      "Loss for batch 82 = 0.36895328760147095\n",
      "Loss for batch 83 = 0.37804126739501953\n",
      "Loss for batch 84 = 0.33457106351852417\n",
      "Loss for batch 85 = 0.3384084701538086\n",
      "Loss for batch 86 = 0.3384723663330078\n",
      "Loss for batch 87 = 0.2685088515281677\n",
      "Loss for batch 88 = 0.30944666266441345\n",
      "Loss for batch 89 = 0.33996087312698364\n",
      "Loss for batch 90 = 0.3668055236339569\n",
      "Loss for batch 91 = 0.24855178594589233\n",
      "Loss for batch 92 = 0.3458700180053711\n",
      "Loss for batch 93 = 0.28970202803611755\n",
      "Loss for batch 94 = 0.3163943886756897\n",
      "Loss for batch 95 = 0.22821155190467834\n",
      "Loss for batch 96 = 0.5298789143562317\n",
      "Loss for batch 97 = 0.4070970416069031\n",
      "Loss for batch 98 = 0.4530322253704071\n",
      "Loss for batch 99 = 0.34323248267173767\n",
      "Loss for batch 100 = 0.3424738347530365\n",
      "Loss for batch 101 = 0.35329747200012207\n",
      "Loss for batch 102 = 0.3471876084804535\n",
      "Loss for batch 103 = 0.3223469853401184\n",
      "Loss for batch 104 = 0.24522094428539276\n",
      "Loss for batch 105 = 0.4246561527252197\n",
      "Loss for batch 106 = 0.3219875991344452\n",
      "\n",
      "Training Loss for epoch 3 = 31.062196731567383\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/50 [00:03<00:35,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Validation Loss = 13.389041900634766\n",
      "Best Validation Loss = 9.587371826171875\n",
      "Epochs without Improvement = 2\n",
      "Train Accuracy: 77.67%\n",
      "Validation Accuracy: 40.16%\n",
      "\n",
      "Epoch 4\n",
      "----------\n",
      "Loss for batch 0 = 0.24267956614494324\n",
      "Loss for batch 1 = 0.2357144057750702\n",
      "Loss for batch 2 = 0.19139985740184784\n",
      "Loss for batch 3 = 0.1368725299835205\n",
      "Loss for batch 4 = 0.13036316633224487\n",
      "Loss for batch 5 = 0.10246124863624573\n",
      "Loss for batch 6 = 0.2284102886915207\n",
      "Loss for batch 7 = 0.2678125500679016\n",
      "Loss for batch 8 = 0.15580898523330688\n",
      "Loss for batch 9 = 0.21217253804206848\n",
      "Loss for batch 10 = 0.14520016312599182\n",
      "Loss for batch 11 = 0.14417964220046997\n",
      "Loss for batch 12 = 0.1672966331243515\n",
      "Loss for batch 13 = 0.14858666062355042\n",
      "Loss for batch 14 = 0.14744077622890472\n",
      "Loss for batch 15 = 0.11280758678913116\n",
      "Loss for batch 16 = 0.13810881972312927\n",
      "Loss for batch 17 = 0.15958036482334137\n",
      "Loss for batch 18 = 0.1598878651857376\n",
      "Loss for batch 19 = 0.13986043632030487\n",
      "Loss for batch 20 = 0.17078600823879242\n",
      "Loss for batch 21 = 0.09204798191785812\n",
      "Loss for batch 22 = 0.1815086305141449\n",
      "Loss for batch 23 = 0.11879508197307587\n",
      "Loss for batch 24 = 0.23349778354167938\n",
      "Loss for batch 25 = 0.17974472045898438\n",
      "Loss for batch 26 = 0.11191228032112122\n",
      "Loss for batch 27 = 0.18479466438293457\n",
      "Loss for batch 28 = 0.13656261563301086\n",
      "Loss for batch 29 = 0.1946977972984314\n",
      "Loss for batch 30 = 0.19536058604717255\n",
      "Loss for batch 31 = 0.15408264100551605\n",
      "Loss for batch 32 = 0.2222040742635727\n",
      "Loss for batch 33 = 0.22266189754009247\n",
      "Loss for batch 34 = 0.13884860277175903\n",
      "Loss for batch 35 = 0.15574507415294647\n",
      "Loss for batch 36 = 0.20991384983062744\n",
      "Loss for batch 37 = 0.1410621851682663\n",
      "Loss for batch 38 = 0.20215532183647156\n",
      "Loss for batch 39 = 0.20635001361370087\n",
      "Loss for batch 40 = 0.16512176394462585\n",
      "Loss for batch 41 = 0.11710946261882782\n",
      "Loss for batch 42 = 0.17958681285381317\n",
      "Loss for batch 43 = 0.13722306489944458\n",
      "Loss for batch 44 = 0.13666704297065735\n",
      "Loss for batch 45 = 0.09348754584789276\n",
      "Loss for batch 46 = 0.12483444064855576\n",
      "Loss for batch 47 = 0.1752348393201828\n",
      "Loss for batch 48 = 0.1305193305015564\n",
      "Loss for batch 49 = 0.2191397249698639\n",
      "Loss for batch 50 = 0.134433776140213\n",
      "Loss for batch 51 = 0.32173025608062744\n",
      "Loss for batch 52 = 0.27513083815574646\n",
      "Loss for batch 53 = 0.2177722156047821\n",
      "Loss for batch 54 = 0.10530923306941986\n",
      "Loss for batch 55 = 0.18651090562343597\n",
      "Loss for batch 56 = 0.10095205903053284\n",
      "Loss for batch 57 = 0.12914754450321198\n",
      "Loss for batch 58 = 0.18518877029418945\n",
      "Loss for batch 59 = 0.09001630544662476\n",
      "Loss for batch 60 = 0.14604413509368896\n",
      "Loss for batch 61 = 0.10944785922765732\n",
      "Loss for batch 62 = 0.17421230673789978\n",
      "Loss for batch 63 = 0.24697843194007874\n",
      "Loss for batch 64 = 0.23734478652477264\n",
      "Loss for batch 65 = 0.12877756357192993\n",
      "Loss for batch 66 = 0.17094717919826508\n",
      "Loss for batch 67 = 0.21083268523216248\n",
      "Loss for batch 68 = 0.1896464228630066\n",
      "Loss for batch 69 = 0.1177005022764206\n",
      "Loss for batch 70 = 0.22729083895683289\n",
      "Loss for batch 71 = 0.191963791847229\n",
      "Loss for batch 72 = 0.24673959612846375\n",
      "Loss for batch 73 = 0.22543001174926758\n",
      "Loss for batch 74 = 0.12359415739774704\n",
      "Loss for batch 75 = 0.1927194595336914\n",
      "Loss for batch 76 = 0.19705072045326233\n",
      "Loss for batch 77 = 0.0790068507194519\n",
      "Loss for batch 78 = 0.06742140650749207\n",
      "Loss for batch 79 = 0.20682188868522644\n",
      "Loss for batch 80 = 0.25761234760284424\n",
      "Loss for batch 81 = 0.16552788019180298\n",
      "Loss for batch 82 = 0.13711096346378326\n",
      "Loss for batch 83 = 0.19539156556129456\n",
      "Loss for batch 84 = 0.2423037737607956\n",
      "Loss for batch 85 = 0.26588326692581177\n",
      "Loss for batch 86 = 0.2126634567975998\n",
      "Loss for batch 87 = 0.12677805125713348\n",
      "Loss for batch 88 = 0.18619517982006073\n",
      "Loss for batch 89 = 0.43846356868743896\n",
      "Loss for batch 90 = 0.48530900478363037\n",
      "Loss for batch 91 = 0.24708589911460876\n",
      "Loss for batch 92 = 0.24786363542079926\n",
      "Loss for batch 93 = 0.3145253658294678\n",
      "Loss for batch 94 = 0.2585785984992981\n",
      "Loss for batch 95 = 0.24565815925598145\n",
      "Loss for batch 96 = 0.26278725266456604\n",
      "Loss for batch 97 = 0.13271629810333252\n",
      "Loss for batch 98 = 0.14117023348808289\n",
      "Loss for batch 99 = 0.29607781767845154\n",
      "Loss for batch 100 = 0.16040626168251038\n",
      "Loss for batch 101 = 0.24803557991981506\n",
      "Loss for batch 102 = 0.1180812269449234\n",
      "Loss for batch 103 = 0.14104177057743073\n",
      "Loss for batch 104 = 0.1715991199016571\n",
      "Loss for batch 105 = 0.12488210201263428\n",
      "Loss for batch 106 = 0.18316984176635742\n",
      "\n",
      "Training Loss for epoch 4 = 19.541385650634766\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/50 [00:03<00:33,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Validation Loss = 14.527645111083984\n",
      "Best Validation Loss = 9.587371826171875\n",
      "Epochs without Improvement = 3\n",
      "Train Accuracy: 88.41%\n",
      "Validation Accuracy: 48.16%\n",
      "\n",
      "Epoch 5\n",
      "----------\n",
      "Loss for batch 0 = 0.13670669496059418\n",
      "Loss for batch 1 = 0.17781704664230347\n",
      "Loss for batch 2 = 0.09391134977340698\n",
      "Loss for batch 3 = 0.08425076305866241\n",
      "Loss for batch 4 = 0.0996173620223999\n",
      "Loss for batch 5 = 0.18789009749889374\n",
      "Loss for batch 6 = 0.1474422961473465\n",
      "Loss for batch 7 = 0.06814386695623398\n",
      "Loss for batch 8 = 0.1344604790210724\n",
      "Loss for batch 9 = 0.08982113003730774\n",
      "Loss for batch 10 = 0.1360238492488861\n",
      "Loss for batch 11 = 0.03691951185464859\n",
      "Loss for batch 12 = 0.06383445858955383\n",
      "Loss for batch 13 = 0.12552310526371002\n",
      "Loss for batch 14 = 0.06764228641986847\n",
      "Loss for batch 15 = 0.09336727857589722\n",
      "Loss for batch 16 = 0.08673456311225891\n",
      "Loss for batch 17 = 0.04992157220840454\n",
      "Loss for batch 18 = 0.05373045802116394\n",
      "Loss for batch 19 = 0.10353302955627441\n",
      "Loss for batch 20 = 0.09971337765455246\n",
      "Loss for batch 21 = 0.07513868808746338\n",
      "Loss for batch 22 = 0.10125528275966644\n",
      "Loss for batch 23 = 0.06909698247909546\n",
      "Loss for batch 24 = 0.10090955346822739\n",
      "Loss for batch 25 = 0.0831211656332016\n",
      "Loss for batch 26 = 0.05660142004489899\n",
      "Loss for batch 27 = 0.14007748663425446\n",
      "Loss for batch 28 = 0.06924562901258469\n",
      "Loss for batch 29 = 0.10258227586746216\n",
      "Loss for batch 30 = 0.09599311649799347\n",
      "Loss for batch 31 = 0.11327342689037323\n",
      "Loss for batch 32 = 0.08732245862483978\n",
      "Loss for batch 33 = 0.046653881669044495\n",
      "Loss for batch 34 = 0.11171140521764755\n",
      "Loss for batch 35 = 0.06459581106901169\n",
      "Loss for batch 36 = 0.12523223459720612\n",
      "Loss for batch 37 = 0.13155975937843323\n",
      "Loss for batch 38 = 0.1684800088405609\n",
      "Loss for batch 39 = 0.07372509688138962\n",
      "Loss for batch 40 = 0.08955161273479462\n",
      "Loss for batch 41 = 0.08620204776525497\n",
      "Loss for batch 42 = 0.09703491628170013\n",
      "Loss for batch 43 = 0.11548709124326706\n",
      "Loss for batch 44 = 0.036079809069633484\n",
      "Loss for batch 45 = 0.09702297300100327\n",
      "Loss for batch 46 = 0.1589042693376541\n",
      "Loss for batch 47 = 0.2069288194179535\n",
      "Loss for batch 48 = 0.1073748916387558\n",
      "Loss for batch 49 = 0.137878879904747\n",
      "Loss for batch 50 = 0.08263088762760162\n",
      "Loss for batch 51 = 0.07778993248939514\n",
      "Loss for batch 52 = 0.1064324826002121\n",
      "Loss for batch 53 = 0.11374544352293015\n",
      "Loss for batch 54 = 0.08294413983821869\n",
      "Loss for batch 55 = 0.0828453004360199\n",
      "Loss for batch 56 = 0.08662141859531403\n",
      "Loss for batch 57 = 0.10314750671386719\n",
      "Loss for batch 58 = 0.09651824831962585\n",
      "Loss for batch 59 = 0.0527973398566246\n",
      "Loss for batch 60 = 0.13193878531455994\n",
      "Loss for batch 61 = 0.06889663636684418\n",
      "Loss for batch 62 = 0.18043921887874603\n",
      "Loss for batch 63 = 0.08473994582891464\n",
      "Loss for batch 64 = 0.20710265636444092\n",
      "Loss for batch 65 = 0.1485309600830078\n",
      "Loss for batch 66 = 0.09465079009532928\n",
      "Loss for batch 67 = 0.08908087015151978\n",
      "Loss for batch 68 = 0.07507633417844772\n",
      "Loss for batch 69 = 0.07537998259067535\n",
      "Loss for batch 70 = 0.05331849306821823\n",
      "Loss for batch 71 = 0.05765870213508606\n",
      "Loss for batch 72 = 0.12039110064506531\n",
      "Loss for batch 73 = 0.15121406316757202\n",
      "Loss for batch 74 = 0.07259749621152878\n",
      "Loss for batch 75 = 0.08205991983413696\n",
      "Loss for batch 76 = 0.0872073620557785\n",
      "Loss for batch 77 = 0.11922173947095871\n",
      "Loss for batch 78 = 0.12744742631912231\n",
      "Loss for batch 79 = 0.06483601033687592\n",
      "Loss for batch 80 = 0.09327611327171326\n",
      "Loss for batch 81 = 0.07496657222509384\n",
      "Loss for batch 82 = 0.07295311987400055\n",
      "Loss for batch 83 = 0.09607499837875366\n",
      "Loss for batch 84 = 0.03890962153673172\n",
      "Loss for batch 85 = 0.059302717447280884\n",
      "Loss for batch 86 = 0.16739115118980408\n",
      "Loss for batch 87 = 0.07309538125991821\n",
      "Loss for batch 88 = 0.12700283527374268\n",
      "Loss for batch 89 = 0.1446273922920227\n",
      "Loss for batch 90 = 0.1718801110982895\n",
      "Loss for batch 91 = 0.12041737884283066\n",
      "Loss for batch 92 = 0.3695718050003052\n",
      "Loss for batch 93 = 0.42642733454704285\n",
      "Loss for batch 94 = 0.19932112097740173\n",
      "Loss for batch 95 = 0.39971816539764404\n",
      "Loss for batch 96 = 0.21593496203422546\n",
      "Loss for batch 97 = 0.10845725238323212\n",
      "Loss for batch 98 = 0.1579054892063141\n",
      "Loss for batch 99 = 0.10817036032676697\n",
      "Loss for batch 100 = 0.10075211524963379\n",
      "Loss for batch 101 = 0.2539494037628174\n",
      "Loss for batch 102 = 0.3351743817329407\n",
      "Loss for batch 103 = 0.18496039509773254\n",
      "Loss for batch 104 = 0.23218517005443573\n",
      "Loss for batch 105 = 0.21270249783992767\n",
      "Loss for batch 106 = 0.10023634880781174\n",
      "\n",
      "Training Loss for epoch 5 = 12.708667755126953\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6/50 [00:04<00:32,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Validation Loss = 17.673635482788086\n",
      "Best Validation Loss = 9.587371826171875\n",
      "Epochs without Improvement = 4\n",
      "Train Accuracy: 86.91%\n",
      "Validation Accuracy: 43.03%\n",
      "\n",
      "Epoch 6\n",
      "----------\n",
      "Loss for batch 0 = 0.09802838414907455\n",
      "Loss for batch 1 = 0.07895362377166748\n",
      "Loss for batch 2 = 0.08956029266119003\n",
      "Loss for batch 3 = 0.08586399257183075\n",
      "Loss for batch 4 = 0.12297847867012024\n",
      "Loss for batch 5 = 0.07513511180877686\n",
      "Loss for batch 6 = 0.052564576268196106\n",
      "Loss for batch 7 = 0.036803778260946274\n",
      "Loss for batch 8 = 0.08228451013565063\n",
      "Loss for batch 9 = 0.04354002699255943\n",
      "Loss for batch 10 = 0.08536206930875778\n",
      "Loss for batch 11 = 0.0645117461681366\n",
      "Loss for batch 12 = 0.07467204332351685\n",
      "Loss for batch 13 = 0.08523596823215485\n",
      "Loss for batch 14 = 0.08171524852514267\n",
      "Loss for batch 15 = 0.13931214809417725\n",
      "Loss for batch 16 = 0.07937741279602051\n",
      "Loss for batch 17 = 0.040376123040914536\n",
      "Loss for batch 18 = 0.05978720635175705\n",
      "Loss for batch 19 = 0.04919111728668213\n",
      "Loss for batch 20 = 0.09527140855789185\n",
      "Loss for batch 21 = 0.07929446548223495\n",
      "Loss for batch 22 = 0.04266821965575218\n",
      "Loss for batch 23 = 0.07262501865625381\n",
      "Loss for batch 24 = 0.06289035081863403\n",
      "Loss for batch 25 = 0.03420930355787277\n",
      "Loss for batch 26 = 0.12862184643745422\n",
      "Loss for batch 27 = 0.06971688568592072\n",
      "Loss for batch 28 = 0.12538790702819824\n",
      "Loss for batch 29 = 0.04969293624162674\n",
      "Loss for batch 30 = 0.03970596566796303\n",
      "Loss for batch 31 = 0.09012715518474579\n",
      "Loss for batch 32 = 0.033621013164520264\n",
      "Loss for batch 33 = 0.04377022013068199\n",
      "Loss for batch 34 = 0.029262002557516098\n",
      "Loss for batch 35 = 0.07748162746429443\n",
      "Loss for batch 36 = 0.08836646378040314\n",
      "Loss for batch 37 = 0.06687131524085999\n",
      "Loss for batch 38 = 0.05956360325217247\n",
      "Loss for batch 39 = 0.05487025901675224\n",
      "Loss for batch 40 = 0.06250344216823578\n",
      "Loss for batch 41 = 0.06675681471824646\n",
      "Loss for batch 42 = 0.08165135234594345\n",
      "Loss for batch 43 = 0.05224347114562988\n",
      "Loss for batch 44 = 0.023002922534942627\n",
      "Loss for batch 45 = 0.07148700207471848\n",
      "Loss for batch 46 = 0.03475472331047058\n",
      "Loss for batch 47 = 0.1545289009809494\n",
      "Loss for batch 48 = 0.12709274888038635\n",
      "Loss for batch 49 = 0.19582808017730713\n",
      "Loss for batch 50 = 0.205170676112175\n",
      "Loss for batch 51 = 0.14315637946128845\n",
      "Loss for batch 52 = 0.04295806586742401\n",
      "Loss for batch 53 = 0.08164626359939575\n",
      "Loss for batch 54 = 0.09529505670070648\n",
      "Loss for batch 55 = 0.0762522965669632\n",
      "Loss for batch 56 = 0.07944363355636597\n",
      "Loss for batch 57 = 0.04797085374593735\n",
      "Loss for batch 58 = 0.04355994984507561\n",
      "Loss for batch 59 = 0.03014884702861309\n",
      "Loss for batch 60 = 0.04362506419420242\n",
      "Loss for batch 61 = 0.04043390229344368\n",
      "Loss for batch 62 = 0.05719982832670212\n",
      "Loss for batch 63 = 0.14534148573875427\n",
      "Loss for batch 64 = 0.13417568802833557\n",
      "Loss for batch 65 = 0.08329728245735168\n",
      "Loss for batch 66 = 0.13562661409378052\n",
      "Loss for batch 67 = 0.11957179754972458\n",
      "Loss for batch 68 = 0.03194999322295189\n",
      "Loss for batch 69 = 0.07543736696243286\n",
      "Loss for batch 70 = 0.039611395448446274\n",
      "Loss for batch 71 = 0.17167089879512787\n",
      "Loss for batch 72 = 0.046425800770521164\n",
      "Loss for batch 73 = 0.1466764509677887\n",
      "Loss for batch 74 = 0.0440656952559948\n",
      "Loss for batch 75 = 0.04134553298354149\n",
      "Loss for batch 76 = 0.04504930227994919\n",
      "Loss for batch 77 = 0.09041143953800201\n",
      "Loss for batch 78 = 0.03456316888332367\n",
      "Loss for batch 79 = 0.05306636914610863\n",
      "Loss for batch 80 = 0.10056723654270172\n",
      "Loss for batch 81 = 0.060099970549345016\n",
      "Loss for batch 82 = 0.0878366082906723\n",
      "Loss for batch 83 = 0.06002063304185867\n",
      "Loss for batch 84 = 0.02977181226015091\n",
      "Loss for batch 85 = 0.1319921910762787\n",
      "Loss for batch 86 = 0.14436426758766174\n",
      "Loss for batch 87 = 0.20508691668510437\n",
      "Loss for batch 88 = 0.07801393419504166\n",
      "Loss for batch 89 = 0.05923640727996826\n",
      "Loss for batch 90 = 0.09389062225818634\n",
      "Loss for batch 91 = 0.0470612607896328\n",
      "Loss for batch 92 = 0.07208052277565002\n",
      "Loss for batch 93 = 0.040179427713155746\n",
      "Loss for batch 94 = 0.09128481149673462\n",
      "Loss for batch 95 = 0.12294157594442368\n",
      "Loss for batch 96 = 0.08790933340787888\n",
      "Loss for batch 97 = 0.12505942583084106\n",
      "Loss for batch 98 = 0.13815046846866608\n",
      "Loss for batch 99 = 0.12988658249378204\n",
      "Loss for batch 100 = 0.14907580614089966\n",
      "Loss for batch 101 = 0.10458526015281677\n",
      "Loss for batch 102 = 0.12225943803787231\n",
      "Loss for batch 103 = 0.06401597708463669\n",
      "Loss for batch 104 = 0.0689488872885704\n",
      "Loss for batch 105 = 0.07272083312273026\n",
      "Loss for batch 106 = 0.055492155253887177\n",
      "\n",
      "Training Loss for epoch 6 = 8.706801414489746\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 7/50 [00:05<00:31,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Validation Loss = 18.61337661743164\n",
      "Best Validation Loss = 9.587371826171875\n",
      "Epochs without Improvement = 5\n",
      "Train Accuracy: 91.96%\n",
      "Validation Accuracy: 42.62%\n",
      "\n",
      "Epoch 7\n",
      "----------\n",
      "Loss for batch 0 = 0.10784058272838593\n",
      "Loss for batch 1 = 0.028770241886377335\n",
      "Loss for batch 2 = 0.059828054159879684\n",
      "Loss for batch 3 = 0.07121032476425171\n",
      "Loss for batch 4 = 0.01866087131202221\n",
      "Loss for batch 5 = 0.014164569787681103\n",
      "Loss for batch 6 = 0.030290961265563965\n",
      "Loss for batch 7 = 0.03397337347269058\n",
      "Loss for batch 8 = 0.08931772410869598\n",
      "Loss for batch 9 = 0.03447660058736801\n",
      "Loss for batch 10 = 0.023478150367736816\n",
      "Loss for batch 11 = 0.08203911781311035\n",
      "Loss for batch 12 = 0.1253841519355774\n",
      "Loss for batch 13 = 0.03070523403584957\n",
      "Loss for batch 14 = 0.033302463591098785\n",
      "Loss for batch 15 = 0.040109336376190186\n",
      "Loss for batch 16 = 0.019336871802806854\n",
      "Loss for batch 17 = 0.05746005102992058\n",
      "Loss for batch 18 = 0.052698537707328796\n",
      "Loss for batch 19 = 0.04635091871023178\n",
      "Loss for batch 20 = 0.07305898517370224\n",
      "Loss for batch 21 = 0.027022114023566246\n",
      "Loss for batch 22 = 0.031802743673324585\n",
      "Loss for batch 23 = 0.034437309950590134\n",
      "Loss for batch 24 = 0.03164563328027725\n",
      "Loss for batch 25 = 0.08403736352920532\n",
      "Loss for batch 26 = 0.02723739668726921\n",
      "Loss for batch 27 = 0.030216075479984283\n",
      "Loss for batch 28 = 0.03916875272989273\n",
      "Loss for batch 29 = 0.021812034770846367\n",
      "Loss for batch 30 = 0.029527585953474045\n",
      "Loss for batch 31 = 0.04147748276591301\n",
      "Loss for batch 32 = 0.021457955241203308\n",
      "Loss for batch 33 = 0.0251205675303936\n",
      "Loss for batch 34 = 0.01154460571706295\n",
      "Loss for batch 35 = 0.0334324985742569\n",
      "Loss for batch 36 = 0.022023379802703857\n",
      "Loss for batch 37 = 0.02720637619495392\n",
      "Loss for batch 38 = 0.029249554499983788\n",
      "Loss for batch 39 = 0.044656164944171906\n",
      "Loss for batch 40 = 0.05381723493337631\n",
      "Loss for batch 41 = 0.024528248235583305\n",
      "Loss for batch 42 = 0.04454803466796875\n",
      "Loss for batch 43 = 0.03163114935159683\n",
      "Loss for batch 44 = 0.03711453825235367\n",
      "Loss for batch 45 = 0.013678202405571938\n",
      "Loss for batch 46 = 0.03099973499774933\n",
      "Loss for batch 47 = 0.03222956508398056\n",
      "Loss for batch 48 = 0.011342245154082775\n",
      "Loss for batch 49 = 0.021600350737571716\n",
      "Loss for batch 50 = 0.05105346068739891\n",
      "Loss for batch 51 = 0.01608787104487419\n",
      "Loss for batch 52 = 0.0651293471455574\n",
      "Loss for batch 53 = 0.017419006675481796\n",
      "Loss for batch 54 = 0.027426104992628098\n",
      "Loss for batch 55 = 0.017507940530776978\n",
      "Loss for batch 56 = 0.04365679621696472\n",
      "Loss for batch 57 = 0.037159573286771774\n",
      "Loss for batch 58 = 0.013163543306291103\n",
      "Loss for batch 59 = 0.031590595841407776\n",
      "Loss for batch 60 = 0.0650305300951004\n",
      "Loss for batch 61 = 0.023654863238334656\n",
      "Loss for batch 62 = 0.03445880860090256\n",
      "Loss for batch 63 = 0.044649820774793625\n",
      "Loss for batch 64 = 0.032902415841817856\n",
      "Loss for batch 65 = 0.12335425615310669\n",
      "Loss for batch 66 = 0.02266475185751915\n",
      "Loss for batch 67 = 0.020416414365172386\n",
      "Loss for batch 68 = 0.019044432789087296\n",
      "Loss for batch 69 = 0.027477994561195374\n",
      "Loss for batch 70 = 0.03020433336496353\n",
      "Loss for batch 71 = 0.013118823990225792\n",
      "Loss for batch 72 = 0.04520997032523155\n",
      "Loss for batch 73 = 0.04438231885433197\n",
      "Loss for batch 74 = 0.0880785807967186\n",
      "Loss for batch 75 = 0.11391477286815643\n",
      "Loss for batch 76 = 0.08914419263601303\n",
      "Loss for batch 77 = 0.2486463040113449\n",
      "Loss for batch 78 = 0.1025557667016983\n",
      "Loss for batch 79 = 0.07429125159978867\n",
      "Loss for batch 80 = 0.09380938857793808\n",
      "Loss for batch 81 = 0.0403301827609539\n",
      "Loss for batch 82 = 0.04871296137571335\n",
      "Loss for batch 83 = 0.04305976629257202\n",
      "Loss for batch 84 = 0.05297552049160004\n",
      "Loss for batch 85 = 0.06112002581357956\n",
      "Loss for batch 86 = 0.04153645038604736\n",
      "Loss for batch 87 = 0.16742543876171112\n",
      "Loss for batch 88 = 0.0694921612739563\n",
      "Loss for batch 89 = 0.03845503553748131\n",
      "Loss for batch 90 = 0.04397214576601982\n",
      "Loss for batch 91 = 0.055937401950359344\n",
      "Loss for batch 92 = 0.05720386654138565\n",
      "Loss for batch 93 = 0.07206326723098755\n",
      "Loss for batch 94 = 0.03805330768227577\n",
      "Loss for batch 95 = 0.03656943142414093\n",
      "Loss for batch 96 = 0.04285223409533501\n",
      "Loss for batch 97 = 0.038628797978162766\n",
      "Loss for batch 98 = 0.12592211365699768\n",
      "Loss for batch 99 = 0.036368198692798615\n",
      "Loss for batch 100 = 0.05211121588945389\n",
      "Loss for batch 101 = 0.022675588726997375\n",
      "Loss for batch 102 = 0.03222021460533142\n",
      "Loss for batch 103 = 0.17254003882408142\n",
      "Loss for batch 104 = 0.09587892144918442\n",
      "Loss for batch 105 = 0.014545510523021221\n",
      "Loss for batch 106 = 0.026890499517321587\n",
      "\n",
      "Training Loss for epoch 7 = 5.191767692565918\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 8/50 [00:06<00:30,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Validation Loss = 16.867645263671875\n",
      "Best Validation Loss = 9.587371826171875\n",
      "Epochs without Improvement = 6\n",
      "Train Accuracy: 98.71%\n",
      "Validation Accuracy: 44.67%\n",
      "\n",
      "Epoch 8\n",
      "----------\n",
      "Loss for batch 0 = 0.011555211618542671\n",
      "Loss for batch 1 = 0.02521650493144989\n",
      "Loss for batch 2 = 0.038217078894376755\n",
      "Loss for batch 3 = 0.019945040345191956\n",
      "Loss for batch 4 = 0.013237958773970604\n",
      "Loss for batch 5 = 0.1095828264951706\n",
      "Loss for batch 6 = 0.021031402051448822\n",
      "Loss for batch 7 = 0.008245117962360382\n",
      "Loss for batch 8 = 0.021938398480415344\n",
      "Loss for batch 9 = 0.012553000822663307\n",
      "Loss for batch 10 = 0.022712867707014084\n",
      "Loss for batch 11 = 0.01443809550255537\n",
      "Loss for batch 12 = 0.02051200345158577\n",
      "Loss for batch 13 = 0.010490944609045982\n",
      "Loss for batch 14 = 0.045860182493925095\n",
      "Loss for batch 15 = 0.046685487031936646\n",
      "Loss for batch 16 = 0.02748156152665615\n",
      "Loss for batch 17 = 0.017945747822523117\n",
      "Loss for batch 18 = 0.00736256456002593\n",
      "Loss for batch 19 = 0.02097196877002716\n",
      "Loss for batch 20 = 0.02656744234263897\n",
      "Loss for batch 21 = 0.022798260673880577\n",
      "Loss for batch 22 = 0.01968327723443508\n",
      "Loss for batch 23 = 0.028971146792173386\n",
      "Loss for batch 24 = 0.01156747154891491\n",
      "Loss for batch 25 = 0.017922891303896904\n",
      "Loss for batch 26 = 0.022036120295524597\n",
      "Loss for batch 27 = 0.03936421871185303\n",
      "Loss for batch 28 = 0.015306527726352215\n",
      "Loss for batch 29 = 0.014367520809173584\n",
      "Loss for batch 30 = 0.02499864622950554\n",
      "Loss for batch 31 = 0.03618413209915161\n",
      "Loss for batch 32 = 0.03752380609512329\n",
      "Loss for batch 33 = 0.031196942552924156\n",
      "Loss for batch 34 = 0.011898914352059364\n",
      "Loss for batch 35 = 0.02638036571443081\n",
      "Loss for batch 36 = 0.06871861964464188\n",
      "Loss for batch 37 = 0.09371472895145416\n",
      "Loss for batch 38 = 0.01354764960706234\n",
      "Loss for batch 39 = 0.03279409930109978\n",
      "Loss for batch 40 = 0.04269341751933098\n",
      "Loss for batch 41 = 0.006723492406308651\n",
      "Loss for batch 42 = 0.08598916977643967\n",
      "Loss for batch 43 = 0.027747947722673416\n",
      "Loss for batch 44 = 0.010793669149279594\n",
      "Loss for batch 45 = 0.02721230685710907\n",
      "Loss for batch 46 = 0.03109806403517723\n",
      "Loss for batch 47 = 0.015569346956908703\n",
      "Loss for batch 48 = 0.01166724693030119\n",
      "Loss for batch 49 = 0.01671404018998146\n",
      "Loss for batch 50 = 0.03933325782418251\n",
      "Loss for batch 51 = 0.016503620892763138\n",
      "Loss for batch 52 = 0.028487157076597214\n",
      "Loss for batch 53 = 0.046958230435848236\n",
      "Loss for batch 54 = 0.004872032906860113\n",
      "Loss for batch 55 = 0.0403045229613781\n",
      "Loss for batch 56 = 0.027383632957935333\n",
      "Loss for batch 57 = 0.009256547316908836\n",
      "Loss for batch 58 = 0.013028937391936779\n",
      "Loss for batch 59 = 0.01262926310300827\n",
      "Loss for batch 60 = 0.1077069640159607\n",
      "Loss for batch 61 = 0.10623259842395782\n",
      "Loss for batch 62 = 0.023420076817274094\n",
      "Loss for batch 63 = 0.052083201706409454\n",
      "Loss for batch 64 = 0.017385222017765045\n",
      "Loss for batch 65 = 0.0589565709233284\n",
      "Loss for batch 66 = 0.0575273334980011\n",
      "Loss for batch 67 = 0.06711646914482117\n",
      "Loss for batch 68 = 0.01028384454548359\n",
      "Loss for batch 69 = 0.011919928714632988\n",
      "Loss for batch 70 = 0.016195792704820633\n",
      "Loss for batch 71 = 0.034020330756902695\n",
      "Loss for batch 72 = 0.02145698294043541\n",
      "Loss for batch 73 = 0.03502807766199112\n",
      "Loss for batch 74 = 0.03809922933578491\n",
      "Loss for batch 75 = 0.02092967927455902\n",
      "Loss for batch 76 = 0.011761016212403774\n",
      "Loss for batch 77 = 0.016585584729909897\n",
      "Loss for batch 78 = 0.0688413754105568\n",
      "Loss for batch 79 = 0.014695974998176098\n",
      "Loss for batch 80 = 0.015370577573776245\n",
      "Loss for batch 81 = 0.05118584632873535\n",
      "Loss for batch 82 = 0.011155398562550545\n",
      "Loss for batch 83 = 0.02207120507955551\n",
      "Loss for batch 84 = 0.0125554408878088\n",
      "Loss for batch 85 = 0.04001203179359436\n",
      "Loss for batch 86 = 0.036159537732601166\n",
      "Loss for batch 87 = 0.029255634173750877\n",
      "Loss for batch 88 = 0.027363982051610947\n",
      "Loss for batch 89 = 0.004648535046726465\n",
      "Loss for batch 90 = 0.00677165761590004\n",
      "Loss for batch 91 = 0.01769069954752922\n",
      "Loss for batch 92 = 0.15621984004974365\n",
      "Loss for batch 93 = 0.1924370974302292\n",
      "Loss for batch 94 = 0.046978868544101715\n",
      "Loss for batch 95 = 0.03706912696361542\n",
      "Loss for batch 96 = 0.0451989471912384\n",
      "Loss for batch 97 = 0.031549740582704544\n",
      "Loss for batch 98 = 0.06454544514417648\n",
      "Loss for batch 99 = 0.034018129110336304\n",
      "Loss for batch 100 = 0.08246825635433197\n",
      "Loss for batch 101 = 0.02104317769408226\n",
      "Loss for batch 102 = 0.021826356649398804\n",
      "Loss for batch 103 = 0.019736213609576225\n",
      "Loss for batch 104 = 0.012036251835525036\n",
      "Loss for batch 105 = 0.04453420639038086\n",
      "Loss for batch 106 = 0.025960301980376244\n",
      "\n",
      "Training Loss for epoch 8 = 3.5266013145446777\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 9/50 [00:06<00:29,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Validation Loss = 18.486616134643555\n",
      "Best Validation Loss = 9.587371826171875\n",
      "Epochs without Improvement = 7\n",
      "Train Accuracy: 98.56%\n",
      "Validation Accuracy: 51.43%\n",
      "\n",
      "Epoch 9\n",
      "----------\n",
      "Loss for batch 0 = 0.010701848194003105\n",
      "Loss for batch 1 = 0.03228343278169632\n",
      "Loss for batch 2 = 0.06422166526317596\n",
      "Loss for batch 3 = 0.030193079262971878\n",
      "Loss for batch 4 = 0.040869954973459244\n",
      "Loss for batch 5 = 0.003991568461060524\n",
      "Loss for batch 6 = 0.005162773188203573\n",
      "Loss for batch 7 = 0.009395353496074677\n",
      "Loss for batch 8 = 0.023072976619005203\n",
      "Loss for batch 9 = 0.00538226030766964\n",
      "Loss for batch 10 = 0.006924952380359173\n",
      "Loss for batch 11 = 0.02124052867293358\n",
      "Loss for batch 12 = 0.00403050659224391\n",
      "Loss for batch 13 = 0.007145812269300222\n",
      "Loss for batch 14 = 0.009758402593433857\n",
      "Loss for batch 15 = 0.06911695003509521\n",
      "Loss for batch 16 = 0.025614529848098755\n",
      "Loss for batch 17 = 0.022749777883291245\n",
      "Loss for batch 18 = 0.05365164205431938\n",
      "Loss for batch 19 = 0.029620839282870293\n",
      "Loss for batch 20 = 0.023638978600502014\n",
      "Loss for batch 21 = 0.009331155568361282\n",
      "Loss for batch 22 = 0.04494194686412811\n",
      "Loss for batch 23 = 0.01583852246403694\n",
      "Loss for batch 24 = 0.010599891655147076\n",
      "Loss for batch 25 = 0.009132291190326214\n",
      "Loss for batch 26 = 0.017347104847431183\n",
      "Loss for batch 27 = 0.0023751012049615383\n",
      "Loss for batch 28 = 0.016120951622724533\n",
      "Loss for batch 29 = 0.0046943435445427895\n",
      "Loss for batch 30 = 0.019360005855560303\n",
      "Loss for batch 31 = 0.003317435970529914\n",
      "Loss for batch 32 = 0.011557705700397491\n",
      "Loss for batch 33 = 0.019155990332365036\n",
      "Loss for batch 34 = 0.009507245384156704\n",
      "Loss for batch 35 = 0.08277417719364166\n",
      "Loss for batch 36 = 0.014497501775622368\n",
      "Loss for batch 37 = 0.03404594585299492\n",
      "Loss for batch 38 = 0.009378088638186455\n",
      "Loss for batch 39 = 0.009730949066579342\n",
      "Loss for batch 40 = 0.006567530333995819\n",
      "Loss for batch 41 = 0.005536820739507675\n",
      "Loss for batch 42 = 0.017187586054205894\n",
      "Loss for batch 43 = 0.008273029699921608\n",
      "Loss for batch 44 = 0.009326836094260216\n",
      "Loss for batch 45 = 0.04994967579841614\n",
      "Loss for batch 46 = 0.01701294258236885\n",
      "Loss for batch 47 = 0.021549105644226074\n",
      "Loss for batch 48 = 0.004046319052577019\n",
      "Loss for batch 49 = 0.019765309989452362\n",
      "Loss for batch 50 = 0.010824094526469707\n",
      "Loss for batch 51 = 0.007188011892139912\n",
      "Loss for batch 52 = 0.009135980159044266\n",
      "Loss for batch 53 = 0.00728770112618804\n",
      "Loss for batch 54 = 0.04510725289583206\n",
      "Loss for batch 55 = 0.0030899974517524242\n",
      "Loss for batch 56 = 0.006737962365150452\n",
      "Loss for batch 57 = 0.008665776811540127\n",
      "Loss for batch 58 = 0.0038989949971437454\n",
      "Loss for batch 59 = 0.027196606621146202\n",
      "Loss for batch 60 = 0.008625402115285397\n",
      "Loss for batch 61 = 0.024763746187090874\n",
      "Loss for batch 62 = 0.025590328499674797\n",
      "Loss for batch 63 = 0.007047194987535477\n",
      "Loss for batch 64 = 0.00618250947445631\n",
      "Loss for batch 65 = 0.003463649656623602\n",
      "Loss for batch 66 = 0.045116495341062546\n",
      "Loss for batch 67 = 0.008537936955690384\n",
      "Loss for batch 68 = 0.017221486195921898\n",
      "Loss for batch 69 = 0.012565387412905693\n",
      "Loss for batch 70 = 0.007613734807819128\n",
      "Loss for batch 71 = 0.02648153156042099\n",
      "Loss for batch 72 = 0.009181616827845573\n",
      "Loss for batch 73 = 0.005835574120283127\n",
      "Loss for batch 74 = 0.018288880586624146\n",
      "Loss for batch 75 = 0.021145110949873924\n",
      "Loss for batch 76 = 0.009225326590240002\n",
      "Loss for batch 77 = 0.029140233993530273\n",
      "Loss for batch 78 = 0.003836501156911254\n",
      "Loss for batch 79 = 0.005681872367858887\n",
      "Loss for batch 80 = 0.045440495014190674\n",
      "Loss for batch 81 = 0.02328355610370636\n",
      "Loss for batch 82 = 0.014326488599181175\n",
      "Loss for batch 83 = 0.010078569874167442\n",
      "Loss for batch 84 = 0.00972815416753292\n",
      "Loss for batch 85 = 0.007014094851911068\n",
      "Loss for batch 86 = 0.011064249090850353\n",
      "Loss for batch 87 = 0.015458478592336178\n",
      "Loss for batch 88 = 0.009811166673898697\n",
      "Loss for batch 89 = 0.008696173317730427\n",
      "Loss for batch 90 = 0.03854179009795189\n",
      "Loss for batch 91 = 0.01167897880077362\n",
      "Loss for batch 92 = 0.02049822360277176\n",
      "Loss for batch 93 = 0.00452303234487772\n",
      "Loss for batch 94 = 0.03529074788093567\n",
      "Loss for batch 95 = 0.05397091433405876\n",
      "Loss for batch 96 = 0.008730369620025158\n",
      "Loss for batch 97 = 0.016917895525693893\n",
      "Loss for batch 98 = 0.030112190172076225\n",
      "Loss for batch 99 = 0.010702712461352348\n",
      "Loss for batch 100 = 0.007601132150739431\n",
      "Loss for batch 101 = 0.00501310033723712\n",
      "Loss for batch 102 = 0.009000643156468868\n",
      "Loss for batch 103 = 0.005800166632980108\n",
      "Loss for batch 104 = 0.006653235759586096\n",
      "Loss for batch 105 = 0.02084600180387497\n",
      "Loss for batch 106 = 0.050583790987730026\n",
      "\n",
      "Training Loss for epoch 9 = 1.9177324771881104\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 10/50 [00:07<00:29,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Validation Loss = 22.159175872802734\n",
      "Best Validation Loss = 9.587371826171875\n",
      "Epochs without Improvement = 8\n",
      "Train Accuracy: 99.03%\n",
      "Validation Accuracy: 47.75%\n",
      "\n",
      "Epoch 10\n",
      "----------\n",
      "Loss for batch 0 = 0.012637581676244736\n",
      "Loss for batch 1 = 0.0033158098813146353\n",
      "Loss for batch 2 = 0.003556862473487854\n",
      "Loss for batch 3 = 0.022370651364326477\n",
      "Loss for batch 4 = 0.014380509965121746\n",
      "Loss for batch 5 = 0.01358830463141203\n",
      "Loss for batch 6 = 0.01148921437561512\n",
      "Loss for batch 7 = 0.010124052874743938\n",
      "Loss for batch 8 = 0.007222641259431839\n",
      "Loss for batch 9 = 0.010075036436319351\n",
      "Loss for batch 10 = 0.010905600152909756\n",
      "Loss for batch 11 = 0.004230719059705734\n",
      "Loss for batch 12 = 0.0059631988406181335\n",
      "Loss for batch 13 = 0.010216455906629562\n",
      "Loss for batch 14 = 0.011212974786758423\n",
      "Loss for batch 15 = 0.00316992006264627\n",
      "Loss for batch 16 = 0.02755720727145672\n",
      "Loss for batch 17 = 0.010654671117663383\n",
      "Loss for batch 18 = 0.05614768713712692\n",
      "Loss for batch 19 = 0.06749105453491211\n",
      "Loss for batch 20 = 0.0031246542930603027\n",
      "Loss for batch 21 = 0.013069463893771172\n",
      "Loss for batch 22 = 0.008879758417606354\n",
      "Loss for batch 23 = 0.004069689195603132\n",
      "Loss for batch 24 = 0.002010087016969919\n",
      "Loss for batch 25 = 0.005889562889933586\n",
      "Loss for batch 26 = 0.007576707284897566\n",
      "Loss for batch 27 = 0.0014563107397407293\n",
      "Loss for batch 28 = 0.006555435713380575\n",
      "Loss for batch 29 = 0.0029540499672293663\n",
      "Loss for batch 30 = 0.005341022741049528\n",
      "Loss for batch 31 = 0.004274037200957537\n",
      "Loss for batch 32 = 0.006603367161005735\n",
      "Loss for batch 33 = 0.005577080883085728\n",
      "Loss for batch 34 = 0.016785994172096252\n",
      "Loss for batch 35 = 0.007887095212936401\n",
      "Loss for batch 36 = 0.012942994944751263\n",
      "Loss for batch 37 = 0.00850921031087637\n",
      "Loss for batch 38 = 0.003337364410981536\n",
      "Loss for batch 39 = 0.0739966481924057\n",
      "Loss for batch 40 = 0.029655467718839645\n",
      "Loss for batch 41 = 0.013067057356238365\n",
      "Loss for batch 42 = 0.005508324131369591\n",
      "Loss for batch 43 = 0.00373475207015872\n",
      "Loss for batch 44 = 0.006078554317355156\n",
      "Loss for batch 45 = 0.008305110037326813\n",
      "Loss for batch 46 = 0.01024581864476204\n",
      "Loss for batch 47 = 0.010074066929519176\n",
      "Loss for batch 48 = 0.034839652478694916\n",
      "Loss for batch 49 = 0.012987392023205757\n",
      "Loss for batch 50 = 0.009363945573568344\n",
      "Loss for batch 51 = 0.0043332865461707115\n",
      "Loss for batch 52 = 0.019439328461885452\n",
      "Loss for batch 53 = 0.008640585467219353\n",
      "Loss for batch 54 = 0.010351522825658321\n",
      "Loss for batch 55 = 0.007598340045660734\n",
      "Loss for batch 56 = 0.0035838510375469923\n",
      "Loss for batch 57 = 0.00399716105312109\n",
      "Loss for batch 58 = 0.01244675274938345\n",
      "Loss for batch 59 = 0.004955790005624294\n",
      "Loss for batch 60 = 0.006322745233774185\n",
      "Loss for batch 61 = 0.00883945357054472\n",
      "Loss for batch 62 = 0.00859741773456335\n",
      "Loss for batch 63 = 0.006778969895094633\n",
      "Loss for batch 64 = 0.00559636764228344\n",
      "Loss for batch 65 = 0.003611883847042918\n",
      "Loss for batch 66 = 0.008860169909894466\n",
      "Loss for batch 67 = 0.005398262292146683\n",
      "Loss for batch 68 = 0.0019681293051689863\n",
      "Loss for batch 69 = 0.006573175545781851\n",
      "Loss for batch 70 = 0.006762116216123104\n",
      "Loss for batch 71 = 0.010397439822554588\n",
      "Loss for batch 72 = 0.003327020211145282\n",
      "Loss for batch 73 = 0.004262223374098539\n",
      "Loss for batch 74 = 0.003245307132601738\n",
      "Loss for batch 75 = 0.009168360382318497\n",
      "Loss for batch 76 = 0.03065323457121849\n",
      "Loss for batch 77 = 0.06984682381153107\n",
      "Loss for batch 78 = 0.037020426243543625\n",
      "Loss for batch 79 = 0.0016974060563370585\n",
      "Loss for batch 80 = 0.019174691289663315\n",
      "Loss for batch 81 = 0.06379511207342148\n",
      "Loss for batch 82 = 0.04234100505709648\n",
      "Loss for batch 83 = 0.10226869583129883\n",
      "Loss for batch 84 = 0.021948792040348053\n",
      "Loss for batch 85 = 0.013655228540301323\n",
      "Loss for batch 86 = 0.01402159035205841\n",
      "Loss for batch 87 = 0.028991563245654106\n",
      "Loss for batch 88 = 0.03255017101764679\n",
      "Loss for batch 89 = 0.005525543354451656\n",
      "Loss for batch 90 = 0.010292451828718185\n",
      "Loss for batch 91 = 0.01297542080283165\n",
      "Loss for batch 92 = 0.004637356381863356\n",
      "Loss for batch 93 = 0.013202903792262077\n",
      "Loss for batch 94 = 0.0589008703827858\n",
      "Loss for batch 95 = 0.08499021828174591\n",
      "Loss for batch 96 = 0.019752446562051773\n",
      "Loss for batch 97 = 0.00375228188931942\n",
      "Loss for batch 98 = 0.04616018757224083\n",
      "Loss for batch 99 = 0.0514102540910244\n",
      "Loss for batch 100 = 0.02144387736916542\n",
      "Loss for batch 101 = 0.027623161673545837\n",
      "Loss for batch 102 = 0.004609947558492422\n",
      "Loss for batch 103 = 0.025845302268862724\n",
      "Loss for batch 104 = 0.015896636992692947\n",
      "Loss for batch 105 = 0.014434214681386948\n",
      "Loss for batch 106 = 0.023254474624991417\n",
      "\n",
      "Training Loss for epoch 10 = 1.7587387561798096\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [00:08<00:28,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Validation Loss = 20.723188400268555\n",
      "Best Validation Loss = 9.587371826171875\n",
      "Epochs without Improvement = 9\n",
      "Train Accuracy: 98.50%\n",
      "Validation Accuracy: 46.31%\n",
      "\n",
      "Epoch 11\n",
      "----------\n",
      "Loss for batch 0 = 0.017258651554584503\n",
      "Loss for batch 1 = 0.04583942890167236\n",
      "Loss for batch 2 = 0.05633950233459473\n",
      "Loss for batch 3 = 0.008646801114082336\n",
      "Loss for batch 4 = 0.06722313165664673\n",
      "Loss for batch 5 = 0.008037603460252285\n",
      "Loss for batch 6 = 0.01355762779712677\n",
      "Loss for batch 7 = 0.06683793663978577\n",
      "Loss for batch 8 = 0.02918875217437744\n",
      "Loss for batch 9 = 0.024912089109420776\n",
      "Loss for batch 10 = 0.015431126579642296\n",
      "Loss for batch 11 = 0.0161281730979681\n",
      "Loss for batch 12 = 0.07189933955669403\n",
      "Loss for batch 13 = 0.07341088354587555\n",
      "Loss for batch 14 = 0.030793890357017517\n",
      "Loss for batch 15 = 0.05415911599993706\n",
      "Loss for batch 16 = 0.04866362363100052\n",
      "Loss for batch 17 = 0.013193828985095024\n",
      "Loss for batch 18 = 0.016613341867923737\n",
      "Loss for batch 19 = 0.053918659687042236\n",
      "Loss for batch 20 = 0.08508741855621338\n",
      "Loss for batch 21 = 0.033254094421863556\n",
      "Loss for batch 22 = 0.005841928534209728\n",
      "Loss for batch 23 = 0.01586173288524151\n",
      "Loss for batch 24 = 0.03966737911105156\n",
      "Loss for batch 25 = 0.01460428535938263\n",
      "Loss for batch 26 = 0.039265718311071396\n",
      "Loss for batch 27 = 0.0872713029384613\n",
      "Loss for batch 28 = 0.03315281122922897\n",
      "Loss for batch 29 = 0.030749289318919182\n",
      "Loss for batch 30 = 0.10797405242919922\n",
      "Loss for batch 31 = 0.18551383912563324\n",
      "Loss for batch 32 = 0.4985756576061249\n",
      "Loss for batch 33 = 0.8699762225151062\n",
      "Loss for batch 34 = 2.2390341758728027\n",
      "Loss for batch 35 = 3.138223648071289\n",
      "Loss for batch 36 = 3.476316452026367\n",
      "Loss for batch 37 = 6.356239318847656\n",
      "Loss for batch 38 = 5.134115219116211\n",
      "Loss for batch 39 = 3.011214256286621\n",
      "Loss for batch 40 = 4.154697418212891\n",
      "Loss for batch 41 = 1.0644495487213135\n",
      "Loss for batch 42 = 3.517976760864258\n",
      "Loss for batch 43 = 2.1318442821502686\n",
      "Loss for batch 44 = 2.009976387023926\n",
      "Loss for batch 45 = 1.2773277759552002\n",
      "Loss for batch 46 = 0.8220555186271667\n",
      "Loss for batch 47 = 0.9452497363090515\n",
      "Loss for batch 48 = 0.8696327209472656\n",
      "Loss for batch 49 = 0.6053487062454224\n",
      "Loss for batch 50 = 0.8310426473617554\n",
      "Loss for batch 51 = 0.8634189367294312\n",
      "Loss for batch 52 = 0.7119988203048706\n",
      "Loss for batch 53 = 0.67680424451828\n",
      "Loss for batch 54 = 0.6770690679550171\n",
      "Loss for batch 55 = 0.6360059976577759\n",
      "Loss for batch 56 = 0.613905131816864\n",
      "Loss for batch 57 = 0.5761733651161194\n",
      "Loss for batch 58 = 0.6695129871368408\n",
      "Loss for batch 59 = 0.6162296533584595\n",
      "Loss for batch 60 = 0.5171791315078735\n",
      "Loss for batch 61 = 0.64304518699646\n",
      "Loss for batch 62 = 0.6462084054946899\n",
      "Loss for batch 63 = 0.48368963599205017\n",
      "Loss for batch 64 = 0.40116459131240845\n",
      "Loss for batch 65 = 0.4540606737136841\n",
      "Loss for batch 66 = 0.944779634475708\n",
      "Loss for batch 67 = 0.7702746391296387\n",
      "Loss for batch 68 = 0.4551853537559509\n",
      "Loss for batch 69 = 0.5769691467285156\n",
      "Loss for batch 70 = 0.47035107016563416\n",
      "Loss for batch 71 = 0.5529040098190308\n",
      "Loss for batch 72 = 0.446572870016098\n",
      "Loss for batch 73 = 0.401370644569397\n",
      "Loss for batch 74 = 0.39938825368881226\n",
      "Loss for batch 75 = 0.448550283908844\n",
      "Loss for batch 76 = 0.7189648151397705\n",
      "Loss for batch 77 = 0.4875509738922119\n",
      "Loss for batch 78 = 0.4181670844554901\n",
      "Loss for batch 79 = 0.6765465140342712\n",
      "Loss for batch 80 = 0.5636526942253113\n",
      "Loss for batch 81 = 0.4613078534603119\n",
      "Loss for batch 82 = 0.4253441095352173\n",
      "Loss for batch 83 = 0.47610658407211304\n",
      "Loss for batch 84 = 0.5062125325202942\n",
      "Loss for batch 85 = 0.5505427122116089\n",
      "Loss for batch 86 = 0.7026716470718384\n",
      "Loss for batch 87 = 0.4470564126968384\n",
      "Loss for batch 88 = 0.40648746490478516\n",
      "Loss for batch 89 = 0.5063487887382507\n",
      "Loss for batch 90 = 0.40661391615867615\n",
      "Loss for batch 91 = 0.45776960253715515\n",
      "Loss for batch 92 = 0.43448901176452637\n",
      "Loss for batch 93 = 0.44220292568206787\n",
      "Loss for batch 94 = 0.3251500129699707\n",
      "Loss for batch 95 = 0.47333645820617676\n",
      "Loss for batch 96 = 0.33880648016929626\n",
      "Loss for batch 97 = 0.429534375667572\n",
      "Loss for batch 98 = 0.3494614362716675\n",
      "Loss for batch 99 = 0.3730818033218384\n",
      "Loss for batch 100 = 0.5039030909538269\n",
      "Loss for batch 101 = 0.7133779525756836\n",
      "Loss for batch 102 = 0.464883029460907\n",
      "Loss for batch 103 = 0.4929685592651367\n",
      "Loss for batch 104 = 0.3228966295719147\n",
      "Loss for batch 105 = 0.34948623180389404\n",
      "Loss for batch 106 = 0.3640524744987488\n",
      "\n",
      "Training Loss for epoch 11 = 73.60538482666016\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [00:08<00:31,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Validation Loss = 12.435291290283203\n",
      "Best Validation Loss = 9.587371826171875\n",
      "Epochs without Improvement = 10\n",
      "Train Accuracy: 71.77%\n",
      "Validation Accuracy: 45.08%\n",
      "Early Stoppping Triggered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TrainLoopv2(model, optimizer, criterion, train_loader, val_loader, num_epochs=50, early_stopping_rounds=10, device=\"cuda\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
