{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchtext\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "from nltk import word_tokenize\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchvision.models.efficientnet import efficientnet_b1, EfficientNet_B1_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from Utils.nlp import pad_sequence, clean_caption, get_word_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalModel(torch.nn.Module):\n",
    "    def __init__(self, n_layers:int, embed_dim:int, hidden_dim:int, neurons:list, embedding:str=\"twitter.27B\", bidirectionality:bool=False, freeze:bool=False, weights=None) -> None:\n",
    "        super().__init__()\n",
    "        model = efficientnet_b1(weights=weights)\n",
    "        model.classifier = torch.nn.Sequential(torch.nn.Dropout(0.2,True))\n",
    "        self.CNN = model\n",
    "\n",
    "        glove_embeddings = torchtext.vocab.GloVe(embedding, embed_dim)\n",
    "        self.LSTM = torch.nn.Sequential(\n",
    "            torch.nn.Embedding.from_pretrained(glove_embeddings.vectors, freeze=freeze),\n",
    "            torch.nn.LSTM(embed_dim, hidden_dim, n_layers, batch_first=True, bidirectional=bidirectionality),\n",
    "        )\n",
    "\n",
    "        self.linear_layers = torch.nn.ModuleList()\n",
    "        if bidirectionality == True:\n",
    "            self.linear_layers.append(torch.nn.Linear(1280+(2*hidden_dim), neurons[0]))\n",
    "        else:\n",
    "            self.linear_layers.append(torch.nn.Linear(1280+hidden_dim, neurons[0]))\n",
    "        self.linear_layers.append(torch.nn.SELU())\n",
    "        for i in range(1, len(neurons)):\n",
    "            self.linear_layers.append(torch.nn.Linear(neurons[i-1], neurons[i]))\n",
    "            self.linear_layers.append(torch.nn.SELU())\n",
    "        self.linear_layers.append(torch.nn.Dropout(0.3))\n",
    "        self.linear_layers.append(torch.nn.Linear(neurons[-1], 3))\n",
    "\n",
    "    def forward(self, text, image):\n",
    "        image_embeddings = self.CNN(image)\n",
    "        text_embeddings = self.LSTM(text)\n",
    "        multimodal = torch.concat([image_embeddings, text_embeddings], dim=1).view(1,1,-1)\n",
    "        for layer in self.linear_layers:\n",
    "            multimodal = layer(multimodal)\n",
    "        return torch.nn.functional.log_softmax(multimodal, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = pd.read_csv(\"../Data/Images/ImageLabelsSequenced.csv\", index_col=False)\n",
    "d2 = pd.read_csv(\"../Data/Text/Engineered.csv\", index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1['Caption'] = d2['Caption']\n",
    "d1['Hashtags'] = d2['Hashtags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = d1[['File Name', 'Caption', 'Hashtags', 'LABEL']]\n",
    "d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.to_csv(\"../Data/multimodal.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1['Caption'] = d1['Caption'].str.replace('[#@!]', '', regex=True)\n",
    "\n",
    "# Additional cleanup: Remove any other non-alphanumeric characters\n",
    "d1['Caption'] = d1['Caption'].str.replace('[^a-zA-Z0-9\\s]', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenize(d1['Caption'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1['Caption'] = d1['Caption'].apply(word_tokenize)\n",
    "# # d1['Caption'] = d1['Caption'].apply(clean_caption)\n",
    "# d1['Caption']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(d1['Caption'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1['Caption'] = d1['Caption'].apply(lambda x: pad_sequence(x, max_seq_length=25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = d1['Caption'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.quantile([0.8,0.9,0.95, 0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.to_csv(\"../Data/multimodal.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "data = pd.read_csv(\"../Data/multimodal.csv\", index_col=False)\n",
    "data['Caption'] = data['Caption'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, tokens, image_dir, labels, images, words_to_index:dict, transform=None):\n",
    "        self.tokens = tokens\n",
    "        self.labels = labels\n",
    "        self.image_dir = image_dir\n",
    "        self.words_to_index = words_to_index\n",
    "        self.images = images\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.image_dir, self.images[index])\n",
    "        image = Image.open(img_path)\n",
    "        image = image.convert(\"RGB\")\n",
    "        label = self.labels[index]\n",
    "        text_indices = torch.LongTensor([self.words_to_index.get(word, 0) for word in self.tokens[index]])\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return (image, text_indices, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.2, shuffle=True, stratify=data['LABEL'], random_state=1)\n",
    "train, val = train_test_split(train, test_size=0.125, shuffle=True, stratify=train['LABEL'], random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = get_word_to_index(\".vector_cache/glove.twitter.27B.25d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EfficientNet_B1_Weights.IMAGENET1K_V2.transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(train['LABEL'])[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = MultimodalDataset(np.array(train['Caption']), \"../Data/Images\", np.array(train['LABEL']), np.array(train['File Name']), word_to_index, EfficientNet_B1_Weights.IMAGENET1K_V2.transforms())\n",
    "val_set = MultimodalDataset(np.array(val['Caption']), \"../Data/Images\", np.array(val['LABEL']), np.array(val['File Name']), word_to_index, EfficientNet_B1_Weights.IMAGENET1K_V2.transforms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, 32)\n",
    "val_loader = DataLoader(val_set, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainLoop(\n",
    "    model,\n",
    "    optimizer:torch.optim.Optimizer,\n",
    "    criterion:torch.nn.Module,\n",
    "    train_dataloader:torch.utils.data.DataLoader,\n",
    "    val_dataloader:torch.utils.data.DataLoader,\n",
    "    scheduler:torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "    num_epochs:int=20,\n",
    "    early_stopping_rounds:int=5,\n",
    "    return_best_model:bool=True,\n",
    "    device:str='cpu'\n",
    "):\n",
    "    model.to(device)\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    total_train_loss = []\n",
    "    total_val_loss = []\n",
    "    best_model_weights = model.state_dict()\n",
    "\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        model.train()\n",
    "        print(\"\\nEpoch {}\\n----------\".format(epoch))\n",
    "        train_loss = 0\n",
    "        for i, (images, texts, labels) in enumerate(train_dataloader):\n",
    "            images = images.to(device)\n",
    "            texts = texts.to(device)\n",
    "            labels = labels.to(device, dtype=torch.long)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(texts, images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            train_loss += loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(\"Loss for batch {} = {}\".format(i, loss))\n",
    "\n",
    "        print(\"\\nTraining Loss for epoch {} = {}\\n\".format(epoch, train_loss))\n",
    "        total_train_loss.append(train_loss/len(train_dataloader.dataset))\n",
    "\n",
    "        model.eval()\n",
    "        validation_loss = 0\n",
    "        with torch.inference_mode():\n",
    "            val_true_labels = []\n",
    "            train_true_labels = []\n",
    "            val_pred_labels = []\n",
    "            train_pred_labels = []\n",
    "            for (images, texts, labels) in val_dataloader:\n",
    "                images = images.to(device)\n",
    "                texts = texts.to(device)\n",
    "                labels = labels.to(device, dtype=torch.long)\n",
    "                outputs = model(texts, images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                validation_loss += loss\n",
    "\n",
    "                outputs = torch.argmax(outputs, dim=1)\n",
    "                val_true_labels.extend(labels.cpu().numpy())\n",
    "                val_pred_labels.extend(outputs.cpu().numpy())\n",
    "\n",
    "            for (images, texts, labels) in train_dataloader:\n",
    "                images = images.to(device)\n",
    "                texts = texts.to(device)\n",
    "                labels = labels.to(device, dtype=torch.long)\n",
    "                outputs = model(texts, images)\n",
    "\n",
    "                outputs = torch.argmax(outputs, dim=1)\n",
    "                train_true_labels.extend(labels.cpu().numpy())\n",
    "                train_pred_labels.extend(outputs.cpu().numpy())\n",
    "\n",
    "            if validation_loss < best_val_loss:\n",
    "                best_val_loss = validation_loss\n",
    "                epochs_without_improvement = 0\n",
    "                best_model_weights = model.state_dict()\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "\n",
    "            val_true_labels = np.array(val_true_labels)\n",
    "            train_true_labels = np.array(train_true_labels)\n",
    "            val_pred_labels = np.array(val_pred_labels)\n",
    "            train_pred_labels = np.array(train_pred_labels)\n",
    "\n",
    "            train_accuracy = accuracy_score(train_true_labels, train_pred_labels)\n",
    "            val_accuracy = accuracy_score(val_true_labels, val_pred_labels)\n",
    "\n",
    "            train_accuracies.append(train_accuracy)\n",
    "            val_accuracies.append(val_accuracy)\n",
    "\n",
    "            print(f\"Current Validation Loss = {validation_loss}\")\n",
    "            print(f\"Best Validation Loss = {best_val_loss}\")\n",
    "            print(f\"Epochs without Improvement = {epochs_without_improvement}\")\n",
    "\n",
    "            print(f\"Train Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "            print(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
    "\n",
    "        total_val_loss.append(validation_loss/len(val_dataloader.dataset))\n",
    "        scheduler.step(validation_loss)\n",
    "        if epochs_without_improvement == early_stopping_rounds:\n",
    "            break\n",
    "\n",
    "    if return_best_model == True:\n",
    "        model.load_state_dict(best_model_weights)\n",
    "    total_train_loss = [item.cpu().detach().numpy() for item in total_train_loss]\n",
    "    total_val_loss = [item.cpu().detach().numpy() for item in total_val_loss]\n",
    "\n",
    "    total_train_loss = np.array(total_train_loss)\n",
    "    total_val_loss = np.array(total_val_loss)\n",
    "\n",
    "    train_accuracies = np.array(train_accuracies)\n",
    "    val_accuracies = np.array(val_accuracies)\n",
    "\n",
    "    x_train = np.arange(len(total_train_loss))\n",
    "    x_val = np.arange(len(total_val_loss))\n",
    "    \n",
    "    sns.set_style('whitegrid')\n",
    "    plt.figure(figsize=(14,5))\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    sns.lineplot(x=x_train, y=total_train_loss, label='Training Loss')\n",
    "    sns.lineplot(x=x_val, y=total_val_loss, label='Validation Loss')\n",
    "    plt.title(\"Loss over {} Epochs\".format(len(total_train_loss)))\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xticks(np.arange(len(total_train_loss)))\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    sns.lineplot(x=x_train, y=train_accuracies, label='Training Accuracy')\n",
    "    sns.lineplot(x=x_val, y=val_accuracies, label='Validation Accuracy')\n",
    "    plt.title(\"Accuracy over {} Epochs\".format(len(total_train_loss)))\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xticks(np.arange(len(total_train_loss)))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = MultimodalModel(4, 25, 256, [512], bidirectionality=True, weights=EfficientNet_B1_Weights.IMAGENET1K_V2)\n",
    "optimizer = torch.optim.NAdam(model_1.parameters(), lr=0.001)\n",
    "loss_fun = torch.nn.NLLLoss()\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', 0.4, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.numel() for p in model_1.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainLoop(model_1, optimizer, loss_fun, train_loader, val_loader, scheduler, 100, 20, True, 'cpu')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
