{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from Utils.nlp import clean_caption, pad_sequence\n",
    "from Utils.models import LSTMmodel\n",
    "from Utils.neural_net import TrainLoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import torchtext\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "from torchtext.data.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x25c2543eb70>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../Data/Text/Engineered.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.load(\"../Data/Text/TF-IDF/labels.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Caption'] = data['Caption'].apply(clean_caption)\n",
    "data['Caption'] = data['Caption'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading glove embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Caption'] = data['Caption'].apply(lambda x: pad_sequence(x, max_seq_length=25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = torchtext.vocab.GloVe(name='twitter.27B', dim=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchtext.vocab.vectors.GloVe"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(glove.get_vecs_by_tokens(data['Caption'][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTextDataset(Dataset):\n",
    "    def __init__(self, tokens, labels, glove_embeddings:torchtext.vocab.vectors.GloVe):\n",
    "        self.tokens = tokens\n",
    "        self.labels = labels\n",
    "        self.embeddings = glove_embeddings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        caption_tokens = self.embeddings.get_vecs_by_tokens(self.tokens[index])\n",
    "        label = self.labels[index]\n",
    "\n",
    "        return {\n",
    "            'text_indices': caption_tokens,\n",
    "            'label': label\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_path = \".vector_cache/glove.twitter.27B.25d.txt\"\n",
    "embeddings_index = {}\n",
    "with open(glove_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "word_to_index = {word: i for i, word in enumerate(embeddings_index.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTextClassificationDataset(Dataset):\n",
    "    def __init__(self, tokens, labels, word_to_index:dict):\n",
    "        self.tokens = tokens\n",
    "        self.labels = labels\n",
    "        self.word_to_index = word_to_index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text_indices = torch.LongTensor([self.word_to_index.get(word, 0) for word in self.tokens[index]])\n",
    "        label = self.labels[index]\n",
    "\n",
    "        return {\n",
    "            'text_indices': text_indices,\n",
    "            'label': label\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data into train test and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(np.array(data['Caption']), y, test_size=0.2, shuffle=True, stratify=y, random_state=1)\n",
    "train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size=0.125, shuffle=True, stratify=train_y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = CustomTextClassificationDataset(train_x, train_y, word_to_index)\n",
    "val_set = CustomTextClassificationDataset(val_x, val_y, word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1193514"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text_indices': tensor([   3,  304,    0,    2,   53,  724,   80,  129,   16, 4352,   13, 2530,\n",
       "           39,   11, 1362,   26,  623, 6963, 1240,    9,  111,  258,    9,  506,\n",
       "            0]),\n",
       " 'label': 2}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = train_set[1]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, 32)\n",
    "val_loader = DataLoader(val_set, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMmodel(torch.nn.Module):\n",
    "    def __init__(self, n_layers, embed_dim, hidden_dim, embedding:str=\"twitter.27B\", bidirectionality:bool=False) -> None:\n",
    "        super().__init__()\n",
    "        glove_embeddings = torchtext.vocab.GloVe(embedding, embed_dim)\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(glove_embeddings.vectors, freeze=False)\n",
    "        self.lstm = torch.nn.LSTM(embed_dim, hidden_dim, n_layers, batch_first=True, bidirectional=bidirectionality)\n",
    "        if bidirectionality == False:\n",
    "            self.linear = torch.nn.Linear(hidden_dim, 3)\n",
    "        else:\n",
    "            self.linear = torch.nn.Linear(2*hidden_dim, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        a = self.embedding(x)\n",
    "        a, _ = self.lstm(a)\n",
    "        a =  a[:,-1,:]\n",
    "        a = self.linear(a)\n",
    "        return torch.nn.functional.log_softmax(a, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelv1 = LSTMmodel(4, 25, 256, bidirectionality=True)\n",
    "loss_fn = torch.nn.NLLLoss()\n",
    "optimizer = torch.optim.NAdam(modelv1.parameters(), 0.002)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', 0.5, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35149853"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in modelv1.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "def TrainLoop(\n",
    "    model,\n",
    "    optimizer:torch.optim.Optimizer,\n",
    "    criterion:torch.nn.Module,\n",
    "    train_dataloader:torch.utils.data.DataLoader,\n",
    "    val_dataloader:torch.utils.data.DataLoader,\n",
    "    scheduler:torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "    num_epochs:int=20,\n",
    "    early_stopping_rounds:int=5,\n",
    "    return_best_model:bool=True,\n",
    "    device:str='cpu'\n",
    "):\n",
    "    model.to(device)\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    total_train_loss = []\n",
    "    total_val_loss = []\n",
    "    best_model_weights = model.state_dict()\n",
    "\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        model.train()\n",
    "        print(\"\\nEpoch {}\\n----------\".format(epoch))\n",
    "        train_loss = 0\n",
    "        for i, batch in enumerate(train_dataloader):\n",
    "            caption_tokens = batch['text_indices'].to(device)\n",
    "            # caption_tokens = caption_tokens.to(torch.float32)\n",
    "            labels = batch['label'].to(device, dtype=torch.long)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(caption_tokens)\n",
    "            loss = criterion(outputs, labels)\n",
    "            train_loss += loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(\"Loss for batch {} = {}\".format(i, loss))\n",
    "\n",
    "        print(\"\\nTraining Loss for epoch {} = {}\\n\".format(epoch, train_loss))\n",
    "        total_train_loss.append(train_loss/len(train_dataloader.dataset))\n",
    "\n",
    "        model.eval()\n",
    "        validation_loss = 0\n",
    "        with torch.inference_mode():\n",
    "            val_true_labels = []\n",
    "            train_true_labels = []\n",
    "            val_pred_labels = []\n",
    "            train_pred_labels = []\n",
    "            for batch in val_dataloader:\n",
    "                caption_tokens = batch['text_indices'].to(device)\n",
    "                # caption_tokens = caption_tokens.to(torch.float32)\n",
    "                labels = batch['label'].to(device, dtype=torch.long)\n",
    "                outputs = model(caption_tokens)\n",
    "                loss = criterion(outputs, labels)\n",
    "                validation_loss += loss\n",
    "\n",
    "                outputs = torch.argmax(outputs, dim=1)\n",
    "                val_true_labels.extend(labels.cpu().numpy())\n",
    "                val_pred_labels.extend(outputs.cpu().numpy())\n",
    "\n",
    "            for batch in train_dataloader:\n",
    "                caption_tokens = batch['text_indices'].to(device)\n",
    "                # caption_tokens = caption_tokens.to(torch.float32)\n",
    "                labels = batch['label'].to(device, dtype=torch.long)\n",
    "                outputs = model(caption_tokens)\n",
    "\n",
    "                outputs = torch.argmax(outputs, dim=1)\n",
    "                train_true_labels.extend(labels.cpu().numpy())\n",
    "                train_pred_labels.extend(outputs.cpu().numpy())\n",
    "\n",
    "            if validation_loss < best_val_loss:\n",
    "                best_val_loss = validation_loss\n",
    "                epochs_without_improvement = 0\n",
    "                best_model_weights = model.state_dict()\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "\n",
    "            val_true_labels = np.array(val_true_labels)\n",
    "            train_true_labels = np.array(train_true_labels)\n",
    "            val_pred_labels = np.array(val_pred_labels)\n",
    "            train_pred_labels = np.array(train_pred_labels)\n",
    "\n",
    "            train_accuracy = accuracy_score(train_true_labels, train_pred_labels)\n",
    "            val_accuracy = accuracy_score(val_true_labels, val_pred_labels)\n",
    "\n",
    "            train_accuracies.append(train_accuracy)\n",
    "            val_accuracies.append(val_accuracy)\n",
    "\n",
    "            print(f\"Current Validation Loss = {validation_loss}\")\n",
    "            print(f\"Best Validation Loss = {best_val_loss}\")\n",
    "            print(f\"Epochs without Improvement = {epochs_without_improvement}\")\n",
    "\n",
    "            print(f\"Train Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "            print(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
    "\n",
    "        total_val_loss.append(validation_loss/len(val_dataloader.dataset))\n",
    "        scheduler.step(validation_loss)\n",
    "        if epochs_without_improvement == early_stopping_rounds:\n",
    "            break\n",
    "\n",
    "    if return_best_model == True:\n",
    "        model.load_state_dict(best_model_weights)\n",
    "    total_train_loss = [item.cpu().detach().numpy() for item in total_train_loss]\n",
    "    total_val_loss = [item.cpu().detach().numpy() for item in total_val_loss]\n",
    "\n",
    "    total_train_loss = np.array(total_train_loss)\n",
    "    total_val_loss = np.array(total_val_loss)\n",
    "\n",
    "    train_accuracies = np.array(train_accuracies)\n",
    "    val_accuracies = np.array(val_accuracies)\n",
    "\n",
    "    x_train = np.arange(len(total_train_loss))\n",
    "    x_val = np.arange(len(total_val_loss))\n",
    "    \n",
    "    sns.set_style('whitegrid')\n",
    "    plt.figure(figsize=(14,5))\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    sns.lineplot(x=x_train, y=total_train_loss, label='Training Loss')\n",
    "    sns.lineplot(x=x_val, y=total_val_loss, label='Validation Loss')\n",
    "    plt.title(\"Loss over {} Epochs\".format(len(total_train_loss)))\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xticks(np.arange(len(total_train_loss)))\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    sns.lineplot(x=x_train, y=train_accuracies, label='Training Accuracy')\n",
    "    sns.lineplot(x=x_val, y=val_accuracies, label='Validation Accuracy')\n",
    "    plt.title(\"Accuracy over {} Epochs\".format(len(total_train_loss)))\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xticks(np.arange(len(total_train_loss)))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d5c8a39c99d4d59865ca78e62d2705b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0\n",
      "----------\n",
      "Loss for batch 0 = 1.0954129695892334\n",
      "Loss for batch 1 = 1.117844820022583\n",
      "Loss for batch 2 = 1.0888476371765137\n",
      "Loss for batch 3 = 1.134114146232605\n",
      "Loss for batch 4 = 1.1121652126312256\n",
      "Loss for batch 5 = 1.0995631217956543\n",
      "Loss for batch 6 = 1.1006276607513428\n",
      "Loss for batch 7 = 1.0872316360473633\n",
      "Loss for batch 8 = 1.1005913019180298\n",
      "Loss for batch 9 = 1.0858919620513916\n",
      "Loss for batch 10 = 1.0892263650894165\n",
      "Loss for batch 11 = 1.0724705457687378\n",
      "Loss for batch 12 = 1.3935279846191406\n",
      "Loss for batch 13 = 1.0823384523391724\n",
      "Loss for batch 14 = 1.1162610054016113\n",
      "Loss for batch 15 = 1.0946420431137085\n",
      "Loss for batch 16 = 1.1036063432693481\n",
      "Loss for batch 17 = 1.0854909420013428\n",
      "Loss for batch 18 = 1.118122935295105\n",
      "Loss for batch 19 = 1.0858770608901978\n",
      "Loss for batch 20 = 1.1070743799209595\n",
      "Loss for batch 21 = 1.0969454050064087\n",
      "Loss for batch 22 = 1.097622036933899\n",
      "Loss for batch 23 = 1.0685104131698608\n",
      "Loss for batch 24 = 0.9649468064308167\n",
      "Loss for batch 25 = 2.265057325363159\n",
      "Loss for batch 26 = 1.0983952283859253\n",
      "Loss for batch 27 = 1.089799165725708\n",
      "Loss for batch 28 = 1.099594235420227\n",
      "Loss for batch 29 = 1.094146966934204\n",
      "Loss for batch 30 = 1.0530239343643188\n",
      "Loss for batch 31 = 1.098107933998108\n",
      "Loss for batch 32 = 1.1014652252197266\n",
      "Loss for batch 33 = 1.0643560886383057\n",
      "Loss for batch 34 = 0.9938948154449463\n",
      "Loss for batch 35 = 1.1092015504837036\n",
      "Loss for batch 36 = 1.1105461120605469\n",
      "Loss for batch 37 = 1.0648151636123657\n",
      "Loss for batch 38 = 1.1110305786132812\n",
      "Loss for batch 39 = 1.047552227973938\n",
      "Loss for batch 40 = 1.0815566778182983\n",
      "Loss for batch 41 = 1.1121464967727661\n",
      "Loss for batch 42 = 1.0537627935409546\n",
      "Loss for batch 43 = 1.057166576385498\n",
      "Loss for batch 44 = 1.098836064338684\n",
      "Loss for batch 45 = 1.0476044416427612\n",
      "Loss for batch 46 = 0.9741117358207703\n",
      "Loss for batch 47 = 1.1299408674240112\n",
      "Loss for batch 48 = 1.0383532047271729\n",
      "Loss for batch 49 = 1.0473588705062866\n",
      "Loss for batch 50 = 1.0881109237670898\n",
      "Loss for batch 51 = 0.9816046357154846\n",
      "Loss for batch 52 = 0.9848947525024414\n",
      "Loss for batch 53 = 1.1722804307937622\n",
      "Loss for batch 54 = 1.0478113889694214\n",
      "Loss for batch 55 = 1.2042537927627563\n",
      "Loss for batch 56 = 1.0765869617462158\n",
      "Loss for batch 57 = 1.0563832521438599\n",
      "Loss for batch 58 = 1.1191847324371338\n",
      "Loss for batch 59 = 1.043541431427002\n",
      "Loss for batch 60 = 1.045018196105957\n",
      "Loss for batch 61 = 1.0791834592819214\n",
      "Loss for batch 62 = 1.0181740522384644\n",
      "Loss for batch 63 = 1.0235544443130493\n",
      "Loss for batch 64 = 1.0516332387924194\n",
      "Loss for batch 65 = 1.1014552116394043\n",
      "Loss for batch 66 = 1.1285324096679688\n",
      "Loss for batch 67 = 1.0567408800125122\n",
      "Loss for batch 68 = 1.0108999013900757\n",
      "Loss for batch 69 = 1.0086359977722168\n",
      "Loss for batch 70 = 1.0610947608947754\n",
      "Loss for batch 71 = 0.9080917835235596\n",
      "Loss for batch 72 = 1.063747763633728\n",
      "Loss for batch 73 = 0.942078173160553\n",
      "Loss for batch 74 = 1.0397460460662842\n",
      "Loss for batch 75 = 1.1239745616912842\n",
      "Loss for batch 76 = 1.0562670230865479\n",
      "Loss for batch 77 = 1.0565829277038574\n",
      "Loss for batch 78 = 0.9929596185684204\n",
      "Loss for batch 79 = 1.0269395112991333\n",
      "Loss for batch 80 = 1.0364998579025269\n",
      "Loss for batch 81 = 1.0692795515060425\n",
      "Loss for batch 82 = 1.08514404296875\n",
      "Loss for batch 83 = 1.0674906969070435\n",
      "Loss for batch 84 = 1.0890295505523682\n",
      "Loss for batch 85 = 1.2695740461349487\n",
      "Loss for batch 86 = 1.0632258653640747\n",
      "Loss for batch 87 = 1.0493940114974976\n",
      "Loss for batch 88 = 1.1088064908981323\n",
      "Loss for batch 89 = 1.0778250694274902\n",
      "Loss for batch 90 = 1.0744816064834595\n",
      "Loss for batch 91 = 1.0720152854919434\n",
      "Loss for batch 92 = 1.0963441133499146\n",
      "Loss for batch 93 = 1.0757055282592773\n",
      "Loss for batch 94 = 1.0756784677505493\n",
      "Loss for batch 95 = 1.1216216087341309\n",
      "Loss for batch 96 = 0.9943913221359253\n",
      "Loss for batch 97 = 1.028125286102295\n",
      "Loss for batch 98 = 1.0345171689987183\n",
      "Loss for batch 99 = 1.0361205339431763\n",
      "Loss for batch 100 = 1.0794377326965332\n",
      "Loss for batch 101 = 0.9844875335693359\n",
      "Loss for batch 102 = 1.0040967464447021\n",
      "Loss for batch 103 = 1.0972986221313477\n",
      "Loss for batch 104 = 0.9816648960113525\n",
      "Loss for batch 105 = 1.0298186540603638\n",
      "Loss for batch 106 = 1.068364143371582\n",
      "\n",
      "Training Loss for epoch 0 = 115.98118591308594\n",
      "\n",
      "Current Validation Loss = 16.57320213317871\n",
      "Best Validation Loss = 16.57320213317871\n",
      "Epochs without Improvement = 0\n",
      "Train Accuracy: 44.34%\n",
      "Validation Accuracy: 44.15%\n",
      "\n",
      "Epoch 1\n",
      "----------\n",
      "Loss for batch 0 = 0.9785456657409668\n",
      "Loss for batch 1 = 0.9763322472572327\n",
      "Loss for batch 2 = 1.1911613941192627\n",
      "Loss for batch 3 = 1.0022127628326416\n",
      "Loss for batch 4 = 1.1437866687774658\n",
      "Loss for batch 5 = 1.0737944841384888\n",
      "Loss for batch 6 = 1.1222753524780273\n",
      "Loss for batch 7 = 1.1032414436340332\n",
      "Loss for batch 8 = 1.1002944707870483\n",
      "Loss for batch 9 = 1.097428798675537\n",
      "Loss for batch 10 = 1.054267406463623\n",
      "Loss for batch 11 = 1.0535318851470947\n",
      "Loss for batch 12 = 1.172721028327942\n",
      "Loss for batch 13 = 1.0317522287368774\n",
      "Loss for batch 14 = 1.0948225259780884\n",
      "Loss for batch 15 = 1.0448217391967773\n",
      "Loss for batch 16 = 1.1018548011779785\n",
      "Loss for batch 17 = 1.057669758796692\n",
      "Loss for batch 18 = 1.334452509880066\n",
      "Loss for batch 19 = 1.095892071723938\n",
      "Loss for batch 20 = 1.1480408906936646\n",
      "Loss for batch 21 = 1.1164056062698364\n",
      "Loss for batch 22 = 1.1047828197479248\n",
      "Loss for batch 23 = 1.0748482942581177\n",
      "Loss for batch 24 = 1.0401946306228638\n",
      "Loss for batch 25 = 1.162559986114502\n",
      "Loss for batch 26 = 1.0390901565551758\n",
      "Loss for batch 27 = 1.076022744178772\n",
      "Loss for batch 28 = 1.1118227243423462\n",
      "Loss for batch 29 = 1.1553314924240112\n",
      "Loss for batch 30 = 1.0724812746047974\n",
      "Loss for batch 31 = 1.0971620082855225\n",
      "Loss for batch 32 = 1.055971384048462\n",
      "Loss for batch 33 = 1.063367486000061\n",
      "Loss for batch 34 = 1.0529958009719849\n",
      "Loss for batch 35 = 1.042442798614502\n",
      "Loss for batch 36 = 1.0729330778121948\n",
      "Loss for batch 37 = 1.0939656496047974\n",
      "Loss for batch 38 = 1.096853494644165\n",
      "Loss for batch 39 = 1.0256030559539795\n",
      "Loss for batch 40 = 1.075811505317688\n",
      "Loss for batch 41 = 1.1085638999938965\n",
      "Loss for batch 42 = 1.0130069255828857\n",
      "Loss for batch 43 = 1.0837284326553345\n",
      "Loss for batch 44 = 1.0553383827209473\n",
      "Loss for batch 45 = 0.9832512736320496\n",
      "Loss for batch 46 = 1.0167211294174194\n",
      "Loss for batch 47 = 1.1078805923461914\n",
      "Loss for batch 48 = 1.1253072023391724\n",
      "Loss for batch 49 = 1.0580875873565674\n",
      "Loss for batch 50 = 1.081233024597168\n",
      "Loss for batch 51 = 1.0070303678512573\n",
      "Loss for batch 52 = 1.0293362140655518\n",
      "Loss for batch 53 = 1.0864808559417725\n",
      "Loss for batch 54 = 1.0177425146102905\n",
      "Loss for batch 55 = 1.1397687196731567\n",
      "Loss for batch 56 = 1.0776623487472534\n",
      "Loss for batch 57 = 1.0272786617279053\n",
      "Loss for batch 58 = 1.133635401725769\n",
      "Loss for batch 59 = 1.0612596273422241\n",
      "Loss for batch 60 = 1.025370717048645\n",
      "Loss for batch 61 = 1.0788053274154663\n",
      "Loss for batch 62 = 1.0717319250106812\n",
      "Loss for batch 63 = 1.0527515411376953\n",
      "Loss for batch 64 = 1.02437424659729\n",
      "Loss for batch 65 = 1.2038376331329346\n",
      "Loss for batch 66 = 1.0789222717285156\n",
      "Loss for batch 67 = 1.015647292137146\n",
      "Loss for batch 68 = 0.9464668035507202\n",
      "Loss for batch 69 = 1.0804842710494995\n",
      "Loss for batch 70 = 1.037184476852417\n",
      "Loss for batch 71 = 0.8860018253326416\n",
      "Loss for batch 72 = 1.0705679655075073\n",
      "Loss for batch 73 = 0.9012621641159058\n",
      "Loss for batch 74 = 1.0990031957626343\n",
      "Loss for batch 75 = 1.128278136253357\n",
      "Loss for batch 76 = 1.0456550121307373\n",
      "Loss for batch 77 = 1.0437216758728027\n",
      "Loss for batch 78 = 0.9950422048568726\n",
      "Loss for batch 79 = 1.0048178434371948\n",
      "Loss for batch 80 = 0.9739649295806885\n",
      "Loss for batch 81 = 1.028415322303772\n",
      "Loss for batch 82 = 0.9951729774475098\n",
      "Loss for batch 83 = 0.9594747424125671\n",
      "Loss for batch 84 = 1.0460032224655151\n",
      "Loss for batch 85 = 1.3041049242019653\n",
      "Loss for batch 86 = 0.9822993278503418\n",
      "Loss for batch 87 = 1.0268239974975586\n",
      "Loss for batch 88 = 1.054376244544983\n",
      "Loss for batch 89 = 1.0050183534622192\n",
      "Loss for batch 90 = 1.0755077600479126\n",
      "Loss for batch 91 = 1.0053520202636719\n",
      "Loss for batch 92 = 1.0134034156799316\n",
      "Loss for batch 93 = 1.1149464845657349\n",
      "Loss for batch 94 = 1.0278441905975342\n",
      "Loss for batch 95 = 1.0944175720214844\n",
      "Loss for batch 96 = 0.9736872911453247\n",
      "Loss for batch 97 = 0.9984020590782166\n",
      "Loss for batch 98 = 1.0159937143325806\n",
      "Loss for batch 99 = 0.9850947260856628\n",
      "Loss for batch 100 = 0.9841971397399902\n",
      "Loss for batch 101 = 0.9052379131317139\n",
      "Loss for batch 102 = 0.8264787197113037\n",
      "Loss for batch 103 = 1.239029049873352\n",
      "Loss for batch 104 = 0.9250692129135132\n",
      "Loss for batch 105 = 0.9543275833129883\n",
      "Loss for batch 106 = 0.9798526167869568\n",
      "\n",
      "Training Loss for epoch 1 = 113.10728454589844\n",
      "\n",
      "Current Validation Loss = 16.05353546142578\n",
      "Best Validation Loss = 16.05353546142578\n",
      "Epochs without Improvement = 0\n",
      "Train Accuracy: 50.44%\n",
      "Validation Accuracy: 50.31%\n",
      "\n",
      "Epoch 2\n",
      "----------\n",
      "Loss for batch 0 = 0.8812623023986816\n",
      "Loss for batch 1 = 0.9821946024894714\n",
      "Loss for batch 2 = 1.1290640830993652\n",
      "Loss for batch 3 = 0.9914624691009521\n",
      "Loss for batch 4 = 1.1040235757827759\n",
      "Loss for batch 5 = 0.9989518523216248\n",
      "Loss for batch 6 = 1.1257057189941406\n",
      "Loss for batch 7 = 1.0512334108352661\n",
      "Loss for batch 8 = 1.1109497547149658\n",
      "Loss for batch 9 = 1.0022457838058472\n",
      "Loss for batch 10 = 1.0009338855743408\n",
      "Loss for batch 11 = 1.062791109085083\n",
      "Loss for batch 12 = 0.9436761736869812\n",
      "Loss for batch 13 = 1.0276715755462646\n",
      "Loss for batch 14 = 1.0530438423156738\n",
      "Loss for batch 15 = 1.0588515996932983\n",
      "Loss for batch 16 = 1.0940123796463013\n",
      "Loss for batch 17 = 0.9958285093307495\n",
      "Loss for batch 18 = 1.0518752336502075\n",
      "Loss for batch 19 = 0.9314484596252441\n",
      "Loss for batch 20 = 1.0251848697662354\n",
      "Loss for batch 21 = 1.1097846031188965\n",
      "Loss for batch 22 = 1.1463096141815186\n",
      "Loss for batch 23 = 0.9468497633934021\n",
      "Loss for batch 24 = 0.9485942125320435\n",
      "Loss for batch 25 = 1.0214840173721313\n",
      "Loss for batch 26 = 1.064208984375\n",
      "Loss for batch 27 = 0.9762815833091736\n",
      "Loss for batch 28 = 1.0781798362731934\n",
      "Loss for batch 29 = 0.9339699149131775\n",
      "Loss for batch 30 = 1.0104339122772217\n",
      "Loss for batch 31 = 1.1484417915344238\n",
      "Loss for batch 32 = 1.0807408094406128\n",
      "Loss for batch 33 = 0.9983193278312683\n",
      "Loss for batch 34 = 0.9794792532920837\n",
      "Loss for batch 35 = 0.9944294691085815\n",
      "Loss for batch 36 = 1.033428430557251\n",
      "Loss for batch 37 = 0.8642073273658752\n",
      "Loss for batch 38 = 1.0778555870056152\n",
      "Loss for batch 39 = 0.8821517825126648\n",
      "Loss for batch 40 = 1.0629467964172363\n",
      "Loss for batch 41 = 1.0225508213043213\n",
      "Loss for batch 42 = 0.9646461606025696\n",
      "Loss for batch 43 = 1.0289403200149536\n",
      "Loss for batch 44 = 0.9829996228218079\n",
      "Loss for batch 45 = 1.0256950855255127\n",
      "Loss for batch 46 = 0.8475638628005981\n",
      "Loss for batch 47 = 1.0552536249160767\n",
      "Loss for batch 48 = 0.9726232886314392\n",
      "Loss for batch 49 = 0.9782096743583679\n",
      "Loss for batch 50 = 1.0147535800933838\n",
      "Loss for batch 51 = 0.9682437181472778\n",
      "Loss for batch 52 = 1.0514475107192993\n",
      "Loss for batch 53 = 1.077080488204956\n",
      "Loss for batch 54 = 0.9164166450500488\n",
      "Loss for batch 55 = 1.0235077142715454\n",
      "Loss for batch 56 = 1.0153191089630127\n",
      "Loss for batch 57 = 0.9027020335197449\n",
      "Loss for batch 58 = 1.1113719940185547\n",
      "Loss for batch 59 = 0.9012619853019714\n",
      "Loss for batch 60 = 0.9831186532974243\n",
      "Loss for batch 61 = 1.0752520561218262\n",
      "Loss for batch 62 = 0.9506239891052246\n",
      "Loss for batch 63 = 1.024783968925476\n",
      "Loss for batch 64 = 1.034705638885498\n",
      "Loss for batch 65 = 1.044379472732544\n",
      "Loss for batch 66 = 1.1408928632736206\n",
      "Loss for batch 67 = 0.9362583160400391\n",
      "Loss for batch 68 = 0.9217946529388428\n",
      "Loss for batch 69 = 0.9563101530075073\n",
      "Loss for batch 70 = 1.0409579277038574\n",
      "Loss for batch 71 = 0.85719895362854\n",
      "Loss for batch 72 = 1.038193702697754\n",
      "Loss for batch 73 = 0.879133403301239\n",
      "Loss for batch 74 = 1.0498316287994385\n",
      "Loss for batch 75 = 1.0530822277069092\n",
      "Loss for batch 76 = 0.9768823981285095\n",
      "Loss for batch 77 = 0.9714083671569824\n",
      "Loss for batch 78 = 0.9677162170410156\n",
      "Loss for batch 79 = 0.9045202732086182\n",
      "Loss for batch 80 = 0.8700321316719055\n",
      "Loss for batch 81 = 0.9260625839233398\n",
      "Loss for batch 82 = 0.9914718270301819\n",
      "Loss for batch 83 = 1.1203418970108032\n",
      "Loss for batch 84 = 0.9171070456504822\n",
      "Loss for batch 85 = 1.178875207901001\n",
      "Loss for batch 86 = 0.8007735013961792\n",
      "Loss for batch 87 = 0.9798246026039124\n",
      "Loss for batch 88 = 1.161902904510498\n",
      "Loss for batch 89 = 1.0441744327545166\n",
      "Loss for batch 90 = 0.9755591154098511\n",
      "Loss for batch 91 = 0.9793752431869507\n",
      "Loss for batch 92 = 0.9975602030754089\n",
      "Loss for batch 93 = 0.8852814435958862\n",
      "Loss for batch 94 = 0.9899774789810181\n",
      "Loss for batch 95 = 0.9914080500602722\n",
      "Loss for batch 96 = 0.8985047936439514\n",
      "Loss for batch 97 = 0.9241002798080444\n",
      "Loss for batch 98 = 1.028915524482727\n",
      "Loss for batch 99 = 0.8598803877830505\n",
      "Loss for batch 100 = 0.9466012120246887\n",
      "Loss for batch 101 = 0.8865713477134705\n",
      "Loss for batch 102 = 0.848208487033844\n",
      "Loss for batch 103 = 0.997348427772522\n",
      "Loss for batch 104 = 0.8532602190971375\n",
      "Loss for batch 105 = 0.9363080859184265\n",
      "Loss for batch 106 = 1.0368326902389526\n",
      "\n",
      "Training Loss for epoch 2 = 106.67439270019531\n",
      "\n",
      "Current Validation Loss = 16.33306884765625\n",
      "Best Validation Loss = 16.05353546142578\n",
      "Epochs without Improvement = 1\n",
      "Train Accuracy: 45.57%\n",
      "Validation Accuracy: 44.76%\n",
      "\n",
      "Epoch 3\n",
      "----------\n",
      "Loss for batch 0 = 0.9665511250495911\n",
      "Loss for batch 1 = 0.8496668934822083\n",
      "Loss for batch 2 = 1.0256465673446655\n",
      "Loss for batch 3 = 0.9062154293060303\n",
      "Loss for batch 4 = 0.9410660862922668\n",
      "Loss for batch 5 = 0.899658739566803\n",
      "Loss for batch 6 = 1.0563149452209473\n",
      "Loss for batch 7 = 0.9868362545967102\n",
      "Loss for batch 8 = 1.1680954694747925\n",
      "Loss for batch 9 = 0.8769900798797607\n",
      "Loss for batch 10 = 1.0123777389526367\n",
      "Loss for batch 11 = 1.0370688438415527\n",
      "Loss for batch 12 = 0.8826707601547241\n",
      "Loss for batch 13 = 1.0169167518615723\n",
      "Loss for batch 14 = 0.9693560600280762\n",
      "Loss for batch 15 = 1.0662394762039185\n",
      "Loss for batch 16 = 1.0088006258010864\n",
      "Loss for batch 17 = 0.9825188517570496\n",
      "Loss for batch 18 = 0.989733099937439\n",
      "Loss for batch 19 = 0.9353135228157043\n",
      "Loss for batch 20 = 0.9663548469543457\n",
      "Loss for batch 21 = 0.8924446105957031\n",
      "Loss for batch 22 = 1.2232279777526855\n",
      "Loss for batch 23 = 0.8730299472808838\n",
      "Loss for batch 24 = 0.8277130722999573\n",
      "Loss for batch 25 = 0.9766084551811218\n",
      "Loss for batch 26 = 1.0228362083435059\n",
      "Loss for batch 27 = 0.8708334565162659\n",
      "Loss for batch 28 = 1.03567636013031\n",
      "Loss for batch 29 = 1.0834861993789673\n",
      "Loss for batch 30 = 0.9750909805297852\n",
      "Loss for batch 31 = 1.0937365293502808\n",
      "Loss for batch 32 = 0.9798629879951477\n",
      "Loss for batch 33 = 0.8234394788742065\n",
      "Loss for batch 34 = 0.9173716306686401\n",
      "Loss for batch 35 = 0.9582557678222656\n",
      "Loss for batch 36 = 0.9051490426063538\n",
      "Loss for batch 37 = 0.8675394058227539\n",
      "Loss for batch 38 = 0.9278848767280579\n",
      "Loss for batch 39 = 0.8115234375\n",
      "Loss for batch 40 = 0.9399808049201965\n",
      "Loss for batch 41 = 1.0909990072250366\n",
      "Loss for batch 42 = 0.951835036277771\n",
      "Loss for batch 43 = 0.9911127686500549\n",
      "Loss for batch 44 = 0.9975101351737976\n",
      "Loss for batch 45 = 0.9833472371101379\n",
      "Loss for batch 46 = 0.83392333984375\n",
      "Loss for batch 47 = 1.0293611288070679\n",
      "Loss for batch 48 = 0.7668733596801758\n",
      "Loss for batch 49 = 0.8667032122612\n",
      "Loss for batch 50 = 1.091756820678711\n",
      "Loss for batch 51 = 0.8133049011230469\n",
      "Loss for batch 52 = 0.9899505376815796\n",
      "Loss for batch 53 = 0.9818089604377747\n",
      "Loss for batch 54 = 0.826522171497345\n",
      "Loss for batch 55 = 0.9379096627235413\n",
      "Loss for batch 56 = 0.8912307024002075\n",
      "Loss for batch 57 = 0.7221375703811646\n",
      "Loss for batch 58 = 1.049331545829773\n",
      "Loss for batch 59 = 0.9406251907348633\n",
      "Loss for batch 60 = 0.8639107942581177\n",
      "Loss for batch 61 = 1.0427453517913818\n",
      "Loss for batch 62 = 0.8527856469154358\n",
      "Loss for batch 63 = 0.912787139415741\n",
      "Loss for batch 64 = 0.9564440846443176\n",
      "Loss for batch 65 = 0.8786860704421997\n",
      "Loss for batch 66 = 0.906980574131012\n",
      "Loss for batch 67 = 0.7769642472267151\n",
      "Loss for batch 68 = 1.2060307264328003\n",
      "Loss for batch 69 = 1.042866826057434\n",
      "Loss for batch 70 = 0.8978029489517212\n",
      "Loss for batch 71 = 0.7505471706390381\n",
      "Loss for batch 72 = 1.0220539569854736\n",
      "Loss for batch 73 = 0.8702808618545532\n",
      "Loss for batch 74 = 1.026218056678772\n",
      "Loss for batch 75 = 1.065365195274353\n",
      "Loss for batch 76 = 1.0843582153320312\n",
      "Loss for batch 77 = 1.0302551984786987\n",
      "Loss for batch 78 = 0.986295223236084\n",
      "Loss for batch 79 = 0.8650978803634644\n",
      "Loss for batch 80 = 0.8002067804336548\n",
      "Loss for batch 81 = 0.8312399387359619\n",
      "Loss for batch 82 = 0.9506925344467163\n",
      "Loss for batch 83 = 0.9604588150978088\n",
      "Loss for batch 84 = 1.0458770990371704\n",
      "Loss for batch 85 = 1.2066686153411865\n",
      "Loss for batch 86 = 0.8530517816543579\n",
      "Loss for batch 87 = 0.8221673965454102\n",
      "Loss for batch 88 = 0.8542798757553101\n",
      "Loss for batch 89 = 0.9840215444564819\n",
      "Loss for batch 90 = 0.8653426170349121\n",
      "Loss for batch 91 = 0.8784502148628235\n",
      "Loss for batch 92 = 0.9778496026992798\n",
      "Loss for batch 93 = 1.0128512382507324\n",
      "Loss for batch 94 = 0.9106113910675049\n",
      "Loss for batch 95 = 1.0577130317687988\n",
      "Loss for batch 96 = 0.9680110812187195\n",
      "Loss for batch 97 = 0.9462635517120361\n",
      "Loss for batch 98 = 0.8981587290763855\n",
      "Loss for batch 99 = 0.8552383184432983\n",
      "Loss for batch 100 = 0.931682288646698\n",
      "Loss for batch 101 = 0.7881942987442017\n",
      "Loss for batch 102 = 0.7592864632606506\n",
      "Loss for batch 103 = 0.937345027923584\n",
      "Loss for batch 104 = 0.6983606219291687\n",
      "Loss for batch 105 = 0.9481973052024841\n",
      "Loss for batch 106 = 0.9208904504776001\n",
      "\n",
      "Training Loss for epoch 3 = 101.0079116821289\n",
      "\n",
      "Current Validation Loss = 15.067033767700195\n",
      "Best Validation Loss = 15.067033767700195\n",
      "Epochs without Improvement = 0\n",
      "Train Accuracy: 51.82%\n",
      "Validation Accuracy: 50.31%\n",
      "\n",
      "Epoch 4\n",
      "----------\n",
      "Loss for batch 0 = 0.8248603940010071\n",
      "Loss for batch 1 = 0.8308644890785217\n",
      "Loss for batch 2 = 1.1674885749816895\n",
      "Loss for batch 3 = 0.8140045404434204\n",
      "Loss for batch 4 = 0.8737154603004456\n",
      "Loss for batch 5 = 0.7978026866912842\n",
      "Loss for batch 6 = 0.880014181137085\n",
      "Loss for batch 7 = 0.9722148180007935\n",
      "Loss for batch 8 = 1.0796605348587036\n",
      "Loss for batch 9 = 0.8420345187187195\n",
      "Loss for batch 10 = 0.7989262342453003\n",
      "Loss for batch 11 = 0.8662383556365967\n",
      "Loss for batch 12 = 0.7957566976547241\n",
      "Loss for batch 13 = 0.8781000971794128\n",
      "Loss for batch 14 = 0.8627235293388367\n",
      "Loss for batch 15 = 1.0302761793136597\n",
      "Loss for batch 16 = 1.0044931173324585\n",
      "Loss for batch 17 = 0.8326555490493774\n",
      "Loss for batch 18 = 0.9671425819396973\n",
      "Loss for batch 19 = 0.7413617372512817\n",
      "Loss for batch 20 = 0.990480363368988\n",
      "Loss for batch 21 = 0.8309231400489807\n",
      "Loss for batch 22 = 1.2067393064498901\n",
      "Loss for batch 23 = 0.8334229588508606\n",
      "Loss for batch 24 = 0.7627219557762146\n",
      "Loss for batch 25 = 0.9498052597045898\n",
      "Loss for batch 26 = 0.8494382500648499\n",
      "Loss for batch 27 = 0.8584933876991272\n",
      "Loss for batch 28 = 0.8079172372817993\n",
      "Loss for batch 29 = 0.9213677644729614\n",
      "Loss for batch 30 = 0.8488091230392456\n",
      "Loss for batch 31 = 1.0487847328186035\n",
      "Loss for batch 32 = 0.8892844319343567\n",
      "Loss for batch 33 = 0.77989262342453\n",
      "Loss for batch 34 = 0.8621158003807068\n",
      "Loss for batch 35 = 0.9096212387084961\n",
      "Loss for batch 36 = 0.7941051125526428\n",
      "Loss for batch 37 = 0.8204585313796997\n",
      "Loss for batch 38 = 0.8890519142150879\n",
      "Loss for batch 39 = 0.6465775370597839\n",
      "Loss for batch 40 = 0.8083416819572449\n",
      "Loss for batch 41 = 0.9637636542320251\n",
      "Loss for batch 42 = 0.8444409966468811\n",
      "Loss for batch 43 = 0.9787840843200684\n",
      "Loss for batch 44 = 0.8479790091514587\n",
      "Loss for batch 45 = 0.8843064308166504\n",
      "Loss for batch 46 = 0.7604147791862488\n",
      "Loss for batch 47 = 0.8606208562850952\n",
      "Loss for batch 48 = 0.640670657157898\n",
      "Loss for batch 49 = 0.7656105756759644\n",
      "Loss for batch 50 = 0.8905364871025085\n",
      "Loss for batch 51 = 0.8449952006340027\n",
      "Loss for batch 52 = 0.8378587365150452\n",
      "Loss for batch 53 = 0.847287118434906\n",
      "Loss for batch 54 = 0.7575123310089111\n",
      "Loss for batch 55 = 0.7423291802406311\n",
      "Loss for batch 56 = 0.6967261433601379\n",
      "Loss for batch 57 = 0.5863996148109436\n",
      "Loss for batch 58 = 0.6984383463859558\n",
      "Loss for batch 59 = 0.7148651480674744\n",
      "Loss for batch 60 = 0.7278081178665161\n",
      "Loss for batch 61 = 1.0467374324798584\n",
      "Loss for batch 62 = 0.7261726260185242\n",
      "Loss for batch 63 = 0.7101205587387085\n",
      "Loss for batch 64 = 0.7575432658195496\n",
      "Loss for batch 65 = 0.7255764007568359\n",
      "Loss for batch 66 = 1.0830496549606323\n",
      "Loss for batch 67 = 0.7272214889526367\n",
      "Loss for batch 68 = 0.8450019955635071\n",
      "Loss for batch 69 = 0.7229907512664795\n",
      "Loss for batch 70 = 0.7377442121505737\n",
      "Loss for batch 71 = 0.5336342453956604\n",
      "Loss for batch 72 = 0.8538988828659058\n",
      "Loss for batch 73 = 0.866157591342926\n",
      "Loss for batch 74 = 0.843198835849762\n",
      "Loss for batch 75 = 0.9100779294967651\n",
      "Loss for batch 76 = 0.953529953956604\n",
      "Loss for batch 77 = 0.9423127174377441\n",
      "Loss for batch 78 = 0.7305724024772644\n",
      "Loss for batch 79 = 0.6358203887939453\n",
      "Loss for batch 80 = 0.5088884234428406\n",
      "Loss for batch 81 = 0.7386962175369263\n",
      "Loss for batch 82 = 0.6208739876747131\n",
      "Loss for batch 83 = 0.9098190665245056\n",
      "Loss for batch 84 = 0.8074654340744019\n",
      "Loss for batch 85 = 1.20420241355896\n",
      "Loss for batch 86 = 0.5348000526428223\n",
      "Loss for batch 87 = 0.8106894493103027\n",
      "Loss for batch 88 = 0.8132569789886475\n",
      "Loss for batch 89 = 0.776268720626831\n",
      "Loss for batch 90 = 0.6928791403770447\n",
      "Loss for batch 91 = 0.6287539601325989\n",
      "Loss for batch 92 = 0.7871415019035339\n",
      "Loss for batch 93 = 0.6637842059135437\n",
      "Loss for batch 94 = 0.7140607833862305\n",
      "Loss for batch 95 = 0.8363983631134033\n",
      "Loss for batch 96 = 0.7269155979156494\n",
      "Loss for batch 97 = 0.8328599333763123\n",
      "Loss for batch 98 = 0.7284170389175415\n",
      "Loss for batch 99 = 0.5505099296569824\n",
      "Loss for batch 100 = 0.6030625700950623\n",
      "Loss for batch 101 = 0.5833041667938232\n",
      "Loss for batch 102 = 0.5903178453445435\n",
      "Loss for batch 103 = 0.8544926643371582\n",
      "Loss for batch 104 = 0.8193466067314148\n",
      "Loss for batch 105 = 0.9216902256011963\n",
      "Loss for batch 106 = 0.8833237290382385\n",
      "\n",
      "Training Loss for epoch 4 = 87.55360412597656\n",
      "\n",
      "Current Validation Loss = 13.047027587890625\n",
      "Best Validation Loss = 13.047027587890625\n",
      "Epochs without Improvement = 0\n",
      "Train Accuracy: 69.42%\n",
      "Validation Accuracy: 61.81%\n",
      "\n",
      "Epoch 5\n",
      "----------\n",
      "Loss for batch 0 = 0.6226552128791809\n",
      "Loss for batch 1 = 0.4415551722049713\n",
      "Loss for batch 2 = 0.6910516023635864\n",
      "Loss for batch 3 = 0.5535019040107727\n",
      "Loss for batch 4 = 0.4725606143474579\n",
      "Loss for batch 5 = 0.532291829586029\n",
      "Loss for batch 6 = 0.7164915204048157\n",
      "Loss for batch 7 = 0.7192597389221191\n",
      "Loss for batch 8 = 0.7541738748550415\n",
      "Loss for batch 9 = 0.70318603515625\n",
      "Loss for batch 10 = 0.6482052206993103\n",
      "Loss for batch 11 = 0.9082644581794739\n",
      "Loss for batch 12 = 0.7076141834259033\n",
      "Loss for batch 13 = 0.7315824031829834\n",
      "Loss for batch 14 = 0.6453925371170044\n",
      "Loss for batch 15 = 0.8798166513442993\n",
      "Loss for batch 16 = 0.6905211210250854\n",
      "Loss for batch 17 = 0.665713906288147\n",
      "Loss for batch 18 = 0.717160701751709\n",
      "Loss for batch 19 = 0.447194367647171\n",
      "Loss for batch 20 = 0.7507622838020325\n",
      "Loss for batch 21 = 0.664431631565094\n",
      "Loss for batch 22 = 1.0159211158752441\n",
      "Loss for batch 23 = 0.7250791788101196\n",
      "Loss for batch 24 = 0.6687214970588684\n",
      "Loss for batch 25 = 0.815906286239624\n",
      "Loss for batch 26 = 0.6351919174194336\n",
      "Loss for batch 27 = 0.728890061378479\n",
      "Loss for batch 28 = 0.6071712970733643\n",
      "Loss for batch 29 = 0.7012161612510681\n",
      "Loss for batch 30 = 0.904895007610321\n",
      "Loss for batch 31 = 1.0634897947311401\n",
      "Loss for batch 32 = 0.5697726011276245\n",
      "Loss for batch 33 = 0.5772680044174194\n",
      "Loss for batch 34 = 0.7067406177520752\n",
      "Loss for batch 35 = 0.7489659786224365\n",
      "Loss for batch 36 = 0.5940558910369873\n",
      "Loss for batch 37 = 0.5952341556549072\n",
      "Loss for batch 38 = 0.6629414558410645\n",
      "Loss for batch 39 = 0.49833643436431885\n",
      "Loss for batch 40 = 0.7105429172515869\n",
      "Loss for batch 41 = 0.6233236789703369\n",
      "Loss for batch 42 = 0.7104553580284119\n",
      "Loss for batch 43 = 0.9745156764984131\n",
      "Loss for batch 44 = 0.6459397673606873\n",
      "Loss for batch 45 = 0.8106400966644287\n",
      "Loss for batch 46 = 0.6533733606338501\n",
      "Loss for batch 47 = 0.5825708508491516\n",
      "Loss for batch 48 = 0.5848969221115112\n",
      "Loss for batch 49 = 0.9053159952163696\n",
      "Loss for batch 50 = 0.8672714233398438\n",
      "Loss for batch 51 = 0.70049649477005\n",
      "Loss for batch 52 = 0.7920452356338501\n",
      "Loss for batch 53 = 0.8607065677642822\n",
      "Loss for batch 54 = 0.6113377213478088\n",
      "Loss for batch 55 = 0.6770891547203064\n",
      "Loss for batch 56 = 0.5042316913604736\n",
      "Loss for batch 57 = 0.589711606502533\n",
      "Loss for batch 58 = 0.7344249486923218\n",
      "Loss for batch 59 = 0.580949068069458\n",
      "Loss for batch 60 = 0.6543038487434387\n",
      "Loss for batch 61 = 0.9776003956794739\n",
      "Loss for batch 62 = 0.531560480594635\n",
      "Loss for batch 63 = 0.6141003370285034\n",
      "Loss for batch 64 = 0.7634676694869995\n",
      "Loss for batch 65 = 0.644359290599823\n",
      "Loss for batch 66 = 0.9120442271232605\n",
      "Loss for batch 67 = 0.5692389607429504\n",
      "Loss for batch 68 = 0.74346923828125\n",
      "Loss for batch 69 = 0.6404126286506653\n",
      "Loss for batch 70 = 0.5143988132476807\n",
      "Loss for batch 71 = 0.5127464532852173\n",
      "Loss for batch 72 = 0.8216358423233032\n",
      "Loss for batch 73 = 0.547958254814148\n",
      "Loss for batch 74 = 0.7939780950546265\n",
      "Loss for batch 75 = 0.8830481767654419\n",
      "Loss for batch 76 = 0.9140768051147461\n",
      "Loss for batch 77 = 0.815701961517334\n",
      "Loss for batch 78 = 0.6857926249504089\n",
      "Loss for batch 79 = 0.5276100039482117\n",
      "Loss for batch 80 = 0.46210092306137085\n",
      "Loss for batch 81 = 0.6716926693916321\n",
      "Loss for batch 82 = 0.5411151051521301\n",
      "Loss for batch 83 = 0.7561405897140503\n",
      "Loss for batch 84 = 0.6896339654922485\n",
      "Loss for batch 85 = 1.1371608972549438\n",
      "Loss for batch 86 = 0.4832638204097748\n",
      "Loss for batch 87 = 0.6859939098358154\n",
      "Loss for batch 88 = 0.661368727684021\n",
      "Loss for batch 89 = 0.6987903118133545\n",
      "Loss for batch 90 = 0.5054572820663452\n",
      "Loss for batch 91 = 0.5474810600280762\n",
      "Loss for batch 92 = 0.5994546413421631\n",
      "Loss for batch 93 = 0.6672562956809998\n",
      "Loss for batch 94 = 0.6009629964828491\n",
      "Loss for batch 95 = 0.6681573390960693\n",
      "Loss for batch 96 = 0.7544819712638855\n",
      "Loss for batch 97 = 0.7826077938079834\n",
      "Loss for batch 98 = 0.6752386093139648\n",
      "Loss for batch 99 = 0.5038343667984009\n",
      "Loss for batch 100 = 0.40430372953414917\n",
      "Loss for batch 101 = 0.44516366720199585\n",
      "Loss for batch 102 = 0.5927245616912842\n",
      "Loss for batch 103 = 0.6994223594665527\n",
      "Loss for batch 104 = 0.8215331435203552\n",
      "Loss for batch 105 = 0.8089562654495239\n",
      "Loss for batch 106 = 0.527927041053772\n",
      "\n",
      "Training Loss for epoch 5 = 73.06275939941406\n",
      "\n",
      "Current Validation Loss = 13.224742889404297\n",
      "Best Validation Loss = 13.047027587890625\n",
      "Epochs without Improvement = 1\n",
      "Train Accuracy: 75.85%\n",
      "Validation Accuracy: 63.86%\n",
      "\n",
      "Epoch 6\n",
      "----------\n",
      "Loss for batch 0 = 0.5230903029441833\n",
      "Loss for batch 1 = 0.44296643137931824\n",
      "Loss for batch 2 = 0.642846941947937\n",
      "Loss for batch 3 = 0.4847947955131531\n",
      "Loss for batch 4 = 0.4007999897003174\n",
      "Loss for batch 5 = 0.462175577878952\n",
      "Loss for batch 6 = 0.6569231748580933\n",
      "Loss for batch 7 = 0.6941869258880615\n",
      "Loss for batch 8 = 0.5836371779441833\n",
      "Loss for batch 9 = 0.5798085331916809\n",
      "Loss for batch 10 = 0.5383204221725464\n",
      "Loss for batch 11 = 0.8224068284034729\n",
      "Loss for batch 12 = 0.6209355592727661\n",
      "Loss for batch 13 = 0.4484568238258362\n",
      "Loss for batch 14 = 0.4730124771595001\n",
      "Loss for batch 15 = 0.8969420790672302\n",
      "Loss for batch 16 = 0.5356682538986206\n",
      "Loss for batch 17 = 0.6821292638778687\n",
      "Loss for batch 18 = 0.5880519151687622\n",
      "Loss for batch 19 = 0.39189785718917847\n",
      "Loss for batch 20 = 0.5436036586761475\n",
      "Loss for batch 21 = 0.5169497132301331\n",
      "Loss for batch 22 = 0.9428242444992065\n",
      "Loss for batch 23 = 0.4796774387359619\n",
      "Loss for batch 24 = 0.581615686416626\n",
      "Loss for batch 25 = 0.6862621903419495\n",
      "Loss for batch 26 = 0.49854257702827454\n",
      "Loss for batch 27 = 0.7621910572052002\n",
      "Loss for batch 28 = 0.5648729801177979\n",
      "Loss for batch 29 = 0.7018464207649231\n",
      "Loss for batch 30 = 0.6814872622489929\n",
      "Loss for batch 31 = 0.9263830184936523\n",
      "Loss for batch 32 = 0.5864848494529724\n",
      "Loss for batch 33 = 0.4929511845111847\n",
      "Loss for batch 34 = 0.547668993473053\n",
      "Loss for batch 35 = 0.6696528196334839\n",
      "Loss for batch 36 = 0.5027841329574585\n",
      "Loss for batch 37 = 0.5207749605178833\n",
      "Loss for batch 38 = 0.6170137524604797\n",
      "Loss for batch 39 = 0.4936525523662567\n",
      "Loss for batch 40 = 0.5633881092071533\n",
      "Loss for batch 41 = 0.5685690641403198\n",
      "Loss for batch 42 = 0.6883990168571472\n",
      "Loss for batch 43 = 0.7903828024864197\n",
      "Loss for batch 44 = 0.5089573264122009\n",
      "Loss for batch 45 = 0.7803963422775269\n",
      "Loss for batch 46 = 0.7289248108863831\n",
      "Loss for batch 47 = 0.802268385887146\n",
      "Loss for batch 48 = 0.4675636887550354\n",
      "Loss for batch 49 = 0.5523172616958618\n",
      "Loss for batch 50 = 0.5776799321174622\n",
      "Loss for batch 51 = 0.6865724325180054\n",
      "Loss for batch 52 = 0.6855000853538513\n",
      "Loss for batch 53 = 0.7160252928733826\n",
      "Loss for batch 54 = 0.6172405481338501\n",
      "Loss for batch 55 = 0.5384833812713623\n",
      "Loss for batch 56 = 0.396982342004776\n",
      "Loss for batch 57 = 0.546642541885376\n",
      "Loss for batch 58 = 0.5484104752540588\n",
      "Loss for batch 59 = 0.4821770191192627\n",
      "Loss for batch 60 = 0.4749845862388611\n",
      "Loss for batch 61 = 0.7630797028541565\n",
      "Loss for batch 62 = 0.476055771112442\n",
      "Loss for batch 63 = 0.4915703237056732\n",
      "Loss for batch 64 = 0.5392022132873535\n",
      "Loss for batch 65 = 0.543522834777832\n",
      "Loss for batch 66 = 0.8719743490219116\n",
      "Loss for batch 67 = 0.3907230794429779\n",
      "Loss for batch 68 = 0.7434234023094177\n",
      "Loss for batch 69 = 0.40810608863830566\n",
      "Loss for batch 70 = 0.5273365378379822\n",
      "Loss for batch 71 = 0.5009607672691345\n",
      "Loss for batch 72 = 0.6761094331741333\n",
      "Loss for batch 73 = 0.42973393201828003\n",
      "Loss for batch 74 = 0.7736634612083435\n",
      "Loss for batch 75 = 0.669141948223114\n",
      "Loss for batch 76 = 0.7162564396858215\n",
      "Loss for batch 77 = 0.639177680015564\n",
      "Loss for batch 78 = 0.6088146567344666\n",
      "Loss for batch 79 = 0.5346299409866333\n",
      "Loss for batch 80 = 0.4580845832824707\n",
      "Loss for batch 81 = 0.6451438665390015\n",
      "Loss for batch 82 = 0.527675211429596\n",
      "Loss for batch 83 = 0.6621941328048706\n",
      "Loss for batch 84 = 0.7145262360572815\n",
      "Loss for batch 85 = 0.8716002106666565\n",
      "Loss for batch 86 = 0.426062673330307\n",
      "Loss for batch 87 = 0.5318065881729126\n",
      "Loss for batch 88 = 0.627729594707489\n",
      "Loss for batch 89 = 0.6799010038375854\n",
      "Loss for batch 90 = 0.5163593888282776\n",
      "Loss for batch 91 = 0.589145839214325\n",
      "Loss for batch 92 = 0.5328724980354309\n",
      "Loss for batch 93 = 0.7817413210868835\n",
      "Loss for batch 94 = 0.5845836400985718\n",
      "Loss for batch 95 = 0.5695984363555908\n",
      "Loss for batch 96 = 0.5692955255508423\n",
      "Loss for batch 97 = 0.6824438571929932\n",
      "Loss for batch 98 = 0.5312361121177673\n",
      "Loss for batch 99 = 0.44393640756607056\n",
      "Loss for batch 100 = 0.2986966073513031\n",
      "Loss for batch 101 = 0.41023194789886475\n",
      "Loss for batch 102 = 0.4816601276397705\n",
      "Loss for batch 103 = 0.6949025988578796\n",
      "Loss for batch 104 = 0.6534285545349121\n",
      "Loss for batch 105 = 0.640277087688446\n",
      "Loss for batch 106 = 0.6102370023727417\n",
      "\n",
      "Training Loss for epoch 6 = 63.54702377319336\n",
      "\n",
      "Current Validation Loss = 12.73818588256836\n",
      "Best Validation Loss = 12.73818588256836\n",
      "Epochs without Improvement = 0\n",
      "Train Accuracy: 78.20%\n",
      "Validation Accuracy: 66.32%\n",
      "\n",
      "Epoch 7\n",
      "----------\n",
      "Loss for batch 0 = 0.5805240273475647\n",
      "Loss for batch 1 = 0.34816741943359375\n",
      "Loss for batch 2 = 0.5472283959388733\n",
      "Loss for batch 3 = 0.45728349685668945\n",
      "Loss for batch 4 = 0.3012070059776306\n",
      "Loss for batch 5 = 0.3856610655784607\n",
      "Loss for batch 6 = 0.6140645742416382\n",
      "Loss for batch 7 = 0.6671870350837708\n",
      "Loss for batch 8 = 0.49308910965919495\n",
      "Loss for batch 9 = 0.4690820574760437\n",
      "Loss for batch 10 = 0.37337836623191833\n",
      "Loss for batch 11 = 0.6977331042289734\n",
      "Loss for batch 12 = 0.6001377701759338\n",
      "Loss for batch 13 = 0.3914535939693451\n",
      "Loss for batch 14 = 0.39538872241973877\n",
      "Loss for batch 15 = 0.8431075811386108\n",
      "Loss for batch 16 = 0.4585074186325073\n",
      "Loss for batch 17 = 0.4598357677459717\n",
      "Loss for batch 18 = 0.49038735032081604\n",
      "Loss for batch 19 = 0.26325780153274536\n",
      "Loss for batch 20 = 0.4605782926082611\n",
      "Loss for batch 21 = 0.4812256395816803\n",
      "Loss for batch 22 = 1.0065677165985107\n",
      "Loss for batch 23 = 0.33009108901023865\n",
      "Loss for batch 24 = 0.5394647717475891\n",
      "Loss for batch 25 = 0.5145243406295776\n",
      "Loss for batch 26 = 0.4308498501777649\n",
      "Loss for batch 27 = 0.5717623233795166\n",
      "Loss for batch 28 = 0.4497610926628113\n",
      "Loss for batch 29 = 0.5694016814231873\n",
      "Loss for batch 30 = 0.686827540397644\n",
      "Loss for batch 31 = 0.8774628639221191\n",
      "Loss for batch 32 = 0.39093148708343506\n",
      "Loss for batch 33 = 0.3134460747241974\n",
      "Loss for batch 34 = 0.47346144914627075\n",
      "Loss for batch 35 = 0.5504220724105835\n",
      "Loss for batch 36 = 0.2869224548339844\n",
      "Loss for batch 37 = 0.3693319261074066\n",
      "Loss for batch 38 = 0.4586350917816162\n",
      "Loss for batch 39 = 0.5231536030769348\n",
      "Loss for batch 40 = 0.43874484300613403\n",
      "Loss for batch 41 = 0.46166154742240906\n",
      "Loss for batch 42 = 0.6866557598114014\n",
      "Loss for batch 43 = 0.71417236328125\n",
      "Loss for batch 44 = 0.41285932064056396\n",
      "Loss for batch 45 = 0.5893341302871704\n",
      "Loss for batch 46 = 0.5776365399360657\n",
      "Loss for batch 47 = 0.45406121015548706\n",
      "Loss for batch 48 = 0.26929375529289246\n",
      "Loss for batch 49 = 0.5162063837051392\n",
      "Loss for batch 50 = 0.45950764417648315\n",
      "Loss for batch 51 = 0.5964112281799316\n",
      "Loss for batch 52 = 0.5758916735649109\n",
      "Loss for batch 53 = 0.5669487714767456\n",
      "Loss for batch 54 = 0.5313377976417542\n",
      "Loss for batch 55 = 0.42353466153144836\n",
      "Loss for batch 56 = 0.33010005950927734\n",
      "Loss for batch 57 = 0.30956122279167175\n",
      "Loss for batch 58 = 0.5060265064239502\n",
      "Loss for batch 59 = 0.4035438001155853\n",
      "Loss for batch 60 = 0.4083697199821472\n",
      "Loss for batch 61 = 0.6663616895675659\n",
      "Loss for batch 62 = 0.3509703278541565\n",
      "Loss for batch 63 = 0.3308587372303009\n",
      "Loss for batch 64 = 0.48702681064605713\n",
      "Loss for batch 65 = 0.4744759798049927\n",
      "Loss for batch 66 = 0.5876317024230957\n",
      "Loss for batch 67 = 0.2769335210323334\n",
      "Loss for batch 68 = 0.6785922646522522\n",
      "Loss for batch 69 = 0.28492847084999084\n",
      "Loss for batch 70 = 0.3711796998977661\n",
      "Loss for batch 71 = 0.37780922651290894\n",
      "Loss for batch 72 = 0.6364843249320984\n",
      "Loss for batch 73 = 0.5063417553901672\n",
      "Loss for batch 74 = 0.49399906396865845\n",
      "Loss for batch 75 = 0.7144985198974609\n",
      "Loss for batch 76 = 0.682101309299469\n",
      "Loss for batch 77 = 0.5418935418128967\n",
      "Loss for batch 78 = 0.5146549344062805\n",
      "Loss for batch 79 = 0.4321475923061371\n",
      "Loss for batch 80 = 0.30468112230300903\n",
      "Loss for batch 81 = 0.5502058267593384\n",
      "Loss for batch 82 = 0.35237976908683777\n",
      "Loss for batch 83 = 0.5527379512786865\n",
      "Loss for batch 84 = 0.6256598830223083\n",
      "Loss for batch 85 = 0.7547515630722046\n",
      "Loss for batch 86 = 0.30083325505256653\n",
      "Loss for batch 87 = 0.5274321436882019\n",
      "Loss for batch 88 = 0.44813162088394165\n",
      "Loss for batch 89 = 0.6135929226875305\n",
      "Loss for batch 90 = 0.3595162630081177\n",
      "Loss for batch 91 = 0.48934701085090637\n",
      "Loss for batch 92 = 0.38132256269454956\n",
      "Loss for batch 93 = 0.5280987620353699\n",
      "Loss for batch 94 = 0.5249767303466797\n",
      "Loss for batch 95 = 0.45664599537849426\n",
      "Loss for batch 96 = 0.5005137920379639\n",
      "Loss for batch 97 = 0.6382383704185486\n",
      "Loss for batch 98 = 0.4966953694820404\n",
      "Loss for batch 99 = 0.44283974170684814\n",
      "Loss for batch 100 = 0.2380661964416504\n",
      "Loss for batch 101 = 0.3850400149822235\n",
      "Loss for batch 102 = 0.4317406415939331\n",
      "Loss for batch 103 = 0.683689534664154\n",
      "Loss for batch 104 = 0.335318922996521\n",
      "Loss for batch 105 = 0.5243600010871887\n",
      "Loss for batch 106 = 0.32840412855148315\n",
      "\n",
      "Training Loss for epoch 7 = 52.5374755859375\n",
      "\n",
      "Current Validation Loss = 12.187006950378418\n",
      "Best Validation Loss = 12.187006950378418\n",
      "Epochs without Improvement = 0\n",
      "Train Accuracy: 84.45%\n",
      "Validation Accuracy: 69.82%\n",
      "\n",
      "Epoch 8\n",
      "----------\n",
      "Loss for batch 0 = 0.4285956919193268\n",
      "Loss for batch 1 = 0.2785889208316803\n",
      "Loss for batch 2 = 0.4052736163139343\n",
      "Loss for batch 3 = 0.3306018114089966\n",
      "Loss for batch 4 = 0.21726632118225098\n",
      "Loss for batch 5 = 0.24677912890911102\n",
      "Loss for batch 6 = 0.6358758211135864\n",
      "Loss for batch 7 = 0.43148869276046753\n",
      "Loss for batch 8 = 0.4294023811817169\n",
      "Loss for batch 9 = 0.3628672659397125\n",
      "Loss for batch 10 = 0.2607649266719818\n",
      "Loss for batch 11 = 0.4695897698402405\n",
      "Loss for batch 12 = 0.35958632826805115\n",
      "Loss for batch 13 = 0.26536524295806885\n",
      "Loss for batch 14 = 0.29354652762413025\n",
      "Loss for batch 15 = 0.7801318764686584\n",
      "Loss for batch 16 = 0.3166571855545044\n",
      "Loss for batch 17 = 0.40457895398139954\n",
      "Loss for batch 18 = 0.4233151376247406\n",
      "Loss for batch 19 = 0.13511793315410614\n",
      "Loss for batch 20 = 0.35001271963119507\n",
      "Loss for batch 21 = 0.34469884634017944\n",
      "Loss for batch 22 = 0.8240377306938171\n",
      "Loss for batch 23 = 0.26378369331359863\n",
      "Loss for batch 24 = 0.5261819958686829\n",
      "Loss for batch 25 = 0.4104313850402832\n",
      "Loss for batch 26 = 0.2904992997646332\n",
      "Loss for batch 27 = 0.47283947467803955\n",
      "Loss for batch 28 = 0.3796258866786957\n",
      "Loss for batch 29 = 0.49457278847694397\n",
      "Loss for batch 30 = 0.4870353639125824\n",
      "Loss for batch 31 = 0.7623690962791443\n",
      "Loss for batch 32 = 0.26052409410476685\n",
      "Loss for batch 33 = 0.24609984457492828\n",
      "Loss for batch 34 = 0.3156854212284088\n",
      "Loss for batch 35 = 0.4417164623737335\n",
      "Loss for batch 36 = 0.22607411444187164\n",
      "Loss for batch 37 = 0.3006187081336975\n",
      "Loss for batch 38 = 0.28833886981010437\n",
      "Loss for batch 39 = 0.5069829821586609\n",
      "Loss for batch 40 = 0.2592666447162628\n",
      "Loss for batch 41 = 0.43507230281829834\n",
      "Loss for batch 42 = 0.4518749713897705\n",
      "Loss for batch 43 = 0.6240922808647156\n",
      "Loss for batch 44 = 0.45459747314453125\n",
      "Loss for batch 45 = 0.5034179091453552\n",
      "Loss for batch 46 = 0.6002437472343445\n",
      "Loss for batch 47 = 0.37432581186294556\n",
      "Loss for batch 48 = 0.30213412642478943\n",
      "Loss for batch 49 = 0.3934125304222107\n",
      "Loss for batch 50 = 0.3230149447917938\n",
      "Loss for batch 51 = 0.4528178870677948\n",
      "Loss for batch 52 = 0.5277162790298462\n",
      "Loss for batch 53 = 0.4487459361553192\n",
      "Loss for batch 54 = 0.459159255027771\n",
      "Loss for batch 55 = 0.36295631527900696\n",
      "Loss for batch 56 = 0.27220258116722107\n",
      "Loss for batch 57 = 0.24621254205703735\n",
      "Loss for batch 58 = 0.3806363642215729\n",
      "Loss for batch 59 = 0.24567200243473053\n",
      "Loss for batch 60 = 0.44530731439590454\n",
      "Loss for batch 61 = 0.6100196838378906\n",
      "Loss for batch 62 = 0.3011140823364258\n",
      "Loss for batch 63 = 0.21757596731185913\n",
      "Loss for batch 64 = 0.3341459631919861\n",
      "Loss for batch 65 = 0.3788034915924072\n",
      "Loss for batch 66 = 0.5792639255523682\n",
      "Loss for batch 67 = 0.18875020742416382\n",
      "Loss for batch 68 = 0.4704298973083496\n",
      "Loss for batch 69 = 0.22039200365543365\n",
      "Loss for batch 70 = 0.17099671065807343\n",
      "Loss for batch 71 = 0.4230929911136627\n",
      "Loss for batch 72 = 0.4434296190738678\n",
      "Loss for batch 73 = 0.36700326204299927\n",
      "Loss for batch 74 = 0.624933660030365\n",
      "Loss for batch 75 = 0.5735684633255005\n",
      "Loss for batch 76 = 0.5961378812789917\n",
      "Loss for batch 77 = 0.4525630474090576\n",
      "Loss for batch 78 = 0.38814693689346313\n",
      "Loss for batch 79 = 0.3100937008857727\n",
      "Loss for batch 80 = 0.24711044132709503\n",
      "Loss for batch 81 = 0.46499356627464294\n",
      "Loss for batch 82 = 0.2578427791595459\n",
      "Loss for batch 83 = 0.4503702223300934\n",
      "Loss for batch 84 = 0.5976057648658752\n",
      "Loss for batch 85 = 0.5781384706497192\n",
      "Loss for batch 86 = 0.19337686896324158\n",
      "Loss for batch 87 = 0.41972023248672485\n",
      "Loss for batch 88 = 0.38547980785369873\n",
      "Loss for batch 89 = 0.6228627562522888\n",
      "Loss for batch 90 = 0.28061163425445557\n",
      "Loss for batch 91 = 0.4865480959415436\n",
      "Loss for batch 92 = 0.37899115681648254\n",
      "Loss for batch 93 = 0.5161551237106323\n",
      "Loss for batch 94 = 0.4922869801521301\n",
      "Loss for batch 95 = 0.34480181336402893\n",
      "Loss for batch 96 = 0.31496068835258484\n",
      "Loss for batch 97 = 0.4719902276992798\n",
      "Loss for batch 98 = 0.3329390585422516\n",
      "Loss for batch 99 = 0.23395311832427979\n",
      "Loss for batch 100 = 0.1309788078069687\n",
      "Loss for batch 101 = 0.3510390520095825\n",
      "Loss for batch 102 = 0.32945939898490906\n",
      "Loss for batch 103 = 0.6866060495376587\n",
      "Loss for batch 104 = 0.3064618408679962\n",
      "Loss for batch 105 = 0.5813032388687134\n",
      "Loss for batch 106 = 0.3497468829154968\n",
      "\n",
      "Training Loss for epoch 8 = 42.68720245361328\n",
      "\n",
      "Current Validation Loss = 14.2062349319458\n",
      "Best Validation Loss = 12.187006950378418\n",
      "Epochs without Improvement = 1\n",
      "Train Accuracy: 76.61%\n",
      "Validation Accuracy: 63.86%\n",
      "\n",
      "Epoch 9\n",
      "----------\n",
      "Loss for batch 0 = 0.37117987871170044\n",
      "Loss for batch 1 = 0.3295724391937256\n",
      "Loss for batch 2 = 0.35440462827682495\n",
      "Loss for batch 3 = 0.3443332314491272\n",
      "Loss for batch 4 = 0.18256647884845734\n",
      "Loss for batch 5 = 0.21939581632614136\n",
      "Loss for batch 6 = 0.5204723477363586\n",
      "Loss for batch 7 = 0.33488136529922485\n",
      "Loss for batch 8 = 0.3446769714355469\n",
      "Loss for batch 9 = 0.29986754059791565\n",
      "Loss for batch 10 = 0.23160237073898315\n",
      "Loss for batch 11 = 0.3668646812438965\n",
      "Loss for batch 12 = 0.28569021821022034\n",
      "Loss for batch 13 = 0.2746458649635315\n",
      "Loss for batch 14 = 0.36473700404167175\n",
      "Loss for batch 15 = 0.7436017990112305\n",
      "Loss for batch 16 = 0.3054659962654114\n",
      "Loss for batch 17 = 0.3238474130630493\n",
      "Loss for batch 18 = 0.4076955318450928\n",
      "Loss for batch 19 = 0.12638650834560394\n",
      "Loss for batch 20 = 0.14583978056907654\n",
      "Loss for batch 21 = 0.2963499128818512\n",
      "Loss for batch 22 = 0.6062985062599182\n",
      "Loss for batch 23 = 0.21422414481639862\n",
      "Loss for batch 24 = 0.4660649597644806\n",
      "Loss for batch 25 = 0.30100855231285095\n",
      "Loss for batch 26 = 0.16034525632858276\n",
      "Loss for batch 27 = 0.26624882221221924\n",
      "Loss for batch 28 = 0.2902269959449768\n",
      "Loss for batch 29 = 0.2891772985458374\n",
      "Loss for batch 30 = 0.42401936650276184\n",
      "Loss for batch 31 = 0.6817429065704346\n",
      "Loss for batch 32 = 0.1822878122329712\n",
      "Loss for batch 33 = 0.171427920460701\n",
      "Loss for batch 34 = 0.21126903593540192\n",
      "Loss for batch 35 = 0.47836488485336304\n",
      "Loss for batch 36 = 0.15923118591308594\n",
      "Loss for batch 37 = 0.3616209328174591\n",
      "Loss for batch 38 = 0.35892558097839355\n",
      "Loss for batch 39 = 0.20012633502483368\n",
      "Loss for batch 40 = 0.26624611020088196\n",
      "Loss for batch 41 = 0.2506835162639618\n",
      "Loss for batch 42 = 0.4189072251319885\n",
      "Loss for batch 43 = 0.48030149936676025\n",
      "Loss for batch 44 = 0.3096156716346741\n",
      "Loss for batch 45 = 0.2663181722164154\n",
      "Loss for batch 46 = 0.42465803027153015\n",
      "Loss for batch 47 = 0.4092077910900116\n",
      "Loss for batch 48 = 0.2071259319782257\n",
      "Loss for batch 49 = 0.23223106563091278\n",
      "Loss for batch 50 = 0.2952595353126526\n",
      "Loss for batch 51 = 0.3611598312854767\n",
      "Loss for batch 52 = 0.502524197101593\n",
      "Loss for batch 53 = 0.481519877910614\n",
      "Loss for batch 54 = 0.43779146671295166\n",
      "Loss for batch 55 = 0.24211789667606354\n",
      "Loss for batch 56 = 0.24270133674144745\n",
      "Loss for batch 57 = 0.2280978560447693\n",
      "Loss for batch 58 = 0.26648372411727905\n",
      "Loss for batch 59 = 0.2958618402481079\n",
      "Loss for batch 60 = 0.29807206988334656\n",
      "Loss for batch 61 = 0.5835064649581909\n",
      "Loss for batch 62 = 0.26576104760169983\n",
      "Loss for batch 63 = 0.09196632355451584\n",
      "Loss for batch 64 = 0.31880059838294983\n",
      "Loss for batch 65 = 0.3989461064338684\n",
      "Loss for batch 66 = 0.49633267521858215\n",
      "Loss for batch 67 = 0.4005366265773773\n",
      "Loss for batch 68 = 0.29195764660835266\n",
      "Loss for batch 69 = 0.27125197649002075\n",
      "Loss for batch 70 = 0.31162330508232117\n",
      "Loss for batch 71 = 0.378383994102478\n",
      "Loss for batch 72 = 0.40770038962364197\n",
      "Loss for batch 73 = 0.35720768570899963\n",
      "Loss for batch 74 = 0.49597498774528503\n",
      "Loss for batch 75 = 0.5201451182365417\n",
      "Loss for batch 76 = 0.46166595816612244\n",
      "Loss for batch 77 = 0.5171865820884705\n",
      "Loss for batch 78 = 0.42307817935943604\n",
      "Loss for batch 79 = 0.2597985565662384\n",
      "Loss for batch 80 = 0.23840467631816864\n",
      "Loss for batch 81 = 0.4799128770828247\n",
      "Loss for batch 82 = 0.25977185368537903\n",
      "Loss for batch 83 = 0.3155319392681122\n",
      "Loss for batch 84 = 0.44628751277923584\n",
      "Loss for batch 85 = 0.512969434261322\n",
      "Loss for batch 86 = 0.1952097862958908\n",
      "Loss for batch 87 = 0.3561415672302246\n",
      "Loss for batch 88 = 0.3904244005680084\n",
      "Loss for batch 89 = 0.5618172287940979\n",
      "Loss for batch 90 = 0.25731056928634644\n",
      "Loss for batch 91 = 0.3915775418281555\n",
      "Loss for batch 92 = 0.3456136882305145\n",
      "Loss for batch 93 = 0.484197199344635\n",
      "Loss for batch 94 = 0.30338096618652344\n",
      "Loss for batch 95 = 0.2547881007194519\n",
      "Loss for batch 96 = 0.1882665902376175\n",
      "Loss for batch 97 = 0.32843518257141113\n",
      "Loss for batch 98 = 0.2879040241241455\n",
      "Loss for batch 99 = 0.15331333875656128\n",
      "Loss for batch 100 = 0.13900703191757202\n",
      "Loss for batch 101 = 0.33354809880256653\n",
      "Loss for batch 102 = 0.15463799238204956\n",
      "Loss for batch 103 = 0.5438127517700195\n",
      "Loss for batch 104 = 0.214517742395401\n",
      "Loss for batch 105 = 0.6864931583404541\n",
      "Loss for batch 106 = 0.18906232714653015\n",
      "\n",
      "Training Loss for epoch 9 = 36.177734375\n",
      "\n",
      "Current Validation Loss = 14.623551368713379\n",
      "Best Validation Loss = 12.187006950378418\n",
      "Epochs without Improvement = 2\n",
      "Train Accuracy: 76.35%\n",
      "Validation Accuracy: 63.66%\n",
      "\n",
      "Epoch 10\n",
      "----------\n",
      "Loss for batch 0 = 0.25937148928642273\n",
      "Loss for batch 1 = 0.29751235246658325\n",
      "Loss for batch 2 = 0.22396458685398102\n",
      "Loss for batch 3 = 0.32619333267211914\n",
      "Loss for batch 4 = 0.23656974732875824\n",
      "Loss for batch 5 = 0.25825610756874084\n",
      "Loss for batch 6 = 0.4575144946575165\n",
      "Loss for batch 7 = 0.3837226629257202\n",
      "Loss for batch 8 = 0.19644775986671448\n",
      "Loss for batch 9 = 0.3001151978969574\n",
      "Loss for batch 10 = 0.27308210730552673\n",
      "Loss for batch 11 = 0.2802678644657135\n",
      "Loss for batch 12 = 0.29544657468795776\n",
      "Loss for batch 13 = 0.3746631145477295\n",
      "Loss for batch 14 = 0.19554449617862701\n",
      "Loss for batch 15 = 0.9730154871940613\n",
      "Loss for batch 16 = 0.3865097463130951\n",
      "Loss for batch 17 = 0.28552544116973877\n",
      "Loss for batch 18 = 0.3427160084247589\n",
      "Loss for batch 19 = 0.16121035814285278\n",
      "Loss for batch 20 = 0.1638372838497162\n",
      "Loss for batch 21 = 0.18340307474136353\n",
      "Loss for batch 22 = 0.6128330230712891\n",
      "Loss for batch 23 = 0.11986392736434937\n",
      "Loss for batch 24 = 0.2495724856853485\n",
      "Loss for batch 25 = 0.19727154076099396\n",
      "Loss for batch 26 = 0.1571892499923706\n",
      "Loss for batch 27 = 0.4692433476448059\n",
      "Loss for batch 28 = 0.2627854645252228\n",
      "Loss for batch 29 = 0.32280299067497253\n",
      "Loss for batch 30 = 0.3848207890987396\n",
      "Loss for batch 31 = 0.705494225025177\n",
      "Loss for batch 32 = 0.16156215965747833\n",
      "Loss for batch 33 = 0.17218104004859924\n",
      "Loss for batch 34 = 0.22829248011112213\n",
      "Loss for batch 35 = 0.36292576789855957\n",
      "Loss for batch 36 = 0.117665596306324\n",
      "Loss for batch 37 = 0.2295905202627182\n",
      "Loss for batch 38 = 0.1344631314277649\n",
      "Loss for batch 39 = 0.2665267884731293\n",
      "Loss for batch 40 = 0.19780316948890686\n",
      "Loss for batch 41 = 0.29465776681900024\n",
      "Loss for batch 42 = 0.3713570237159729\n",
      "Loss for batch 43 = 0.34502074122428894\n",
      "Loss for batch 44 = 0.3263798654079437\n",
      "Loss for batch 45 = 0.19500596821308136\n",
      "Loss for batch 46 = 0.3584866523742676\n",
      "Loss for batch 47 = 0.1984328329563141\n",
      "Loss for batch 48 = 0.14319723844528198\n",
      "Loss for batch 49 = 0.46303457021713257\n",
      "Loss for batch 50 = 0.2823309600353241\n",
      "Loss for batch 51 = 0.25563299655914307\n",
      "Loss for batch 52 = 0.4831063449382782\n",
      "Loss for batch 53 = 0.3098146617412567\n",
      "Loss for batch 54 = 0.373152494430542\n",
      "Loss for batch 55 = 0.19922055304050446\n",
      "Loss for batch 56 = 0.2384433150291443\n",
      "Loss for batch 57 = 0.15515124797821045\n",
      "Loss for batch 58 = 0.22333860397338867\n",
      "Loss for batch 59 = 0.227426216006279\n",
      "Loss for batch 60 = 0.30820024013519287\n",
      "Loss for batch 61 = 0.3489515781402588\n",
      "Loss for batch 62 = 0.1766296923160553\n",
      "Loss for batch 63 = 0.08351178467273712\n",
      "Loss for batch 64 = 0.24448688328266144\n",
      "Loss for batch 65 = 0.34095919132232666\n",
      "Loss for batch 66 = 0.308937132358551\n",
      "Loss for batch 67 = 0.17091940343379974\n",
      "Loss for batch 68 = 0.30605217814445496\n",
      "Loss for batch 69 = 0.22310297191143036\n",
      "Loss for batch 70 = 0.07925554364919662\n",
      "Loss for batch 71 = 0.4378826320171356\n",
      "Loss for batch 72 = 0.291658878326416\n",
      "Loss for batch 73 = 0.33439910411834717\n",
      "Loss for batch 74 = 0.42955857515335083\n",
      "Loss for batch 75 = 0.43712276220321655\n",
      "Loss for batch 76 = 0.3910004794597626\n",
      "Loss for batch 77 = 0.3854515254497528\n",
      "Loss for batch 78 = 0.37895163893699646\n",
      "Loss for batch 79 = 0.17433694005012512\n",
      "Loss for batch 80 = 0.2772037088871002\n",
      "Loss for batch 81 = 0.29008904099464417\n",
      "Loss for batch 82 = 0.21501222252845764\n",
      "Loss for batch 83 = 0.26156479120254517\n",
      "Loss for batch 84 = 0.3941444456577301\n",
      "Loss for batch 85 = 0.3361014127731323\n",
      "Loss for batch 86 = 0.18008509278297424\n",
      "Loss for batch 87 = 0.23739883303642273\n",
      "Loss for batch 88 = 0.2948131263256073\n",
      "Loss for batch 89 = 0.4297047257423401\n",
      "Loss for batch 90 = 0.2655661106109619\n",
      "Loss for batch 91 = 0.40088531374931335\n",
      "Loss for batch 92 = 0.21789909899234772\n",
      "Loss for batch 93 = 0.2223857194185257\n",
      "Loss for batch 94 = 0.25081580877304077\n",
      "Loss for batch 95 = 0.41239142417907715\n",
      "Loss for batch 96 = 0.11599114537239075\n",
      "Loss for batch 97 = 0.2991616725921631\n",
      "Loss for batch 98 = 0.2380276620388031\n",
      "Loss for batch 99 = 0.09686288237571716\n",
      "Loss for batch 100 = 0.0958467423915863\n",
      "Loss for batch 101 = 0.2620207965373993\n",
      "Loss for batch 102 = 0.07792901992797852\n",
      "Loss for batch 103 = 0.45888012647628784\n",
      "Loss for batch 104 = 0.16300056874752045\n",
      "Loss for batch 105 = 0.4966573417186737\n",
      "Loss for batch 106 = 0.0724053904414177\n",
      "\n",
      "Training Loss for epoch 10 = 30.565174102783203\n",
      "\n",
      "Current Validation Loss = 14.878656387329102\n",
      "Best Validation Loss = 12.187006950378418\n",
      "Epochs without Improvement = 3\n",
      "Train Accuracy: 94.07%\n",
      "Validation Accuracy: 71.87%\n",
      "\n",
      "Epoch 11\n",
      "----------\n",
      "Loss for batch 0 = 0.3422597646713257\n",
      "Loss for batch 1 = 0.17962683737277985\n",
      "Loss for batch 2 = 0.2771652638912201\n",
      "Loss for batch 3 = 0.4241589605808258\n",
      "Loss for batch 4 = 0.22067339718341827\n",
      "Loss for batch 5 = 0.19729800522327423\n",
      "Loss for batch 6 = 0.2742243707180023\n",
      "Loss for batch 7 = 0.28341418504714966\n",
      "Loss for batch 8 = 0.07001923024654388\n",
      "Loss for batch 9 = 0.22283421456813812\n",
      "Loss for batch 10 = 0.07255279272794724\n",
      "Loss for batch 11 = 0.22888320684432983\n",
      "Loss for batch 12 = 0.23766133189201355\n",
      "Loss for batch 13 = 0.22560225427150726\n",
      "Loss for batch 14 = 0.22704392671585083\n",
      "Loss for batch 15 = 0.4822359085083008\n",
      "Loss for batch 16 = 0.193669855594635\n",
      "Loss for batch 17 = 0.25445762276649475\n",
      "Loss for batch 18 = 0.3354581594467163\n",
      "Loss for batch 19 = 0.09082376211881638\n",
      "Loss for batch 20 = 0.15622985363006592\n",
      "Loss for batch 21 = 0.17217020690441132\n",
      "Loss for batch 22 = 0.5043617486953735\n",
      "Loss for batch 23 = 0.03390961512923241\n",
      "Loss for batch 24 = 0.22322715818881989\n",
      "Loss for batch 25 = 0.2270757257938385\n",
      "Loss for batch 26 = 0.15145805478096008\n",
      "Loss for batch 27 = 0.23913250863552094\n",
      "Loss for batch 28 = 0.18605321645736694\n",
      "Loss for batch 29 = 0.24490217864513397\n",
      "Loss for batch 30 = 0.2556462287902832\n",
      "Loss for batch 31 = 0.5715096592903137\n",
      "Loss for batch 32 = 0.0722789615392685\n",
      "Loss for batch 33 = 0.04583829641342163\n",
      "Loss for batch 34 = 0.27016422152519226\n",
      "Loss for batch 35 = 0.5021665692329407\n",
      "Loss for batch 36 = 0.05987760052084923\n",
      "Loss for batch 37 = 0.20395366847515106\n",
      "Loss for batch 38 = 0.07991570234298706\n",
      "Loss for batch 39 = 0.3351247012615204\n",
      "Loss for batch 40 = 0.19912269711494446\n",
      "Loss for batch 41 = 0.1773555874824524\n",
      "Loss for batch 42 = 0.3050186336040497\n",
      "Loss for batch 43 = 0.3116881549358368\n",
      "Loss for batch 44 = 0.24616704881191254\n",
      "Loss for batch 45 = 0.08827156573534012\n",
      "Loss for batch 46 = 0.3211645185947418\n",
      "Loss for batch 47 = 0.11872872710227966\n",
      "Loss for batch 48 = 0.08741296082735062\n",
      "Loss for batch 49 = 0.31052783131599426\n",
      "Loss for batch 50 = 0.1581980437040329\n",
      "Loss for batch 51 = 0.3463807702064514\n",
      "Loss for batch 52 = 0.42221665382385254\n",
      "Loss for batch 53 = 0.16318145394325256\n",
      "Loss for batch 54 = 0.26497703790664673\n",
      "Loss for batch 55 = 0.18502338230609894\n",
      "Loss for batch 56 = 0.25253742933273315\n",
      "Loss for batch 57 = 0.1422625631093979\n",
      "Loss for batch 58 = 0.13069523870944977\n",
      "Loss for batch 59 = 0.09992004185914993\n",
      "Loss for batch 60 = 0.24000313878059387\n",
      "Loss for batch 61 = 0.17149220407009125\n",
      "Loss for batch 62 = 0.09876536577939987\n",
      "Loss for batch 63 = 0.02654738910496235\n",
      "Loss for batch 64 = 0.20886875689029694\n",
      "Loss for batch 65 = 0.3969549536705017\n",
      "Loss for batch 66 = 0.2517038583755493\n",
      "Loss for batch 67 = 0.06782153248786926\n",
      "Loss for batch 68 = 0.19033998250961304\n",
      "Loss for batch 69 = 0.08633198589086533\n",
      "Loss for batch 70 = 0.07590378075838089\n",
      "Loss for batch 71 = 0.3560568392276764\n",
      "Loss for batch 72 = 0.2439045011997223\n",
      "Loss for batch 73 = 0.3586071729660034\n",
      "Loss for batch 74 = 0.37856969237327576\n",
      "Loss for batch 75 = 0.327767014503479\n",
      "Loss for batch 76 = 0.15828807651996613\n",
      "Loss for batch 77 = 0.37504884600639343\n",
      "Loss for batch 78 = 0.3497302234172821\n",
      "Loss for batch 79 = 0.2211998999118805\n",
      "Loss for batch 80 = 0.1955185830593109\n",
      "Loss for batch 81 = 0.4507400393486023\n",
      "Loss for batch 82 = 0.17080584168434143\n",
      "Loss for batch 83 = 0.2346503585577011\n",
      "Loss for batch 84 = 0.1692618727684021\n",
      "Loss for batch 85 = 0.18721707165241241\n",
      "Loss for batch 86 = 0.1714521199464798\n",
      "Loss for batch 87 = 0.13402491807937622\n",
      "Loss for batch 88 = 0.28102076053619385\n",
      "Loss for batch 89 = 0.3757491111755371\n",
      "Loss for batch 90 = 0.22739525139331818\n",
      "Loss for batch 91 = 0.24557837843894958\n",
      "Loss for batch 92 = 0.16859054565429688\n",
      "Loss for batch 93 = 0.08313532173633575\n",
      "Loss for batch 94 = 0.18768009543418884\n",
      "Loss for batch 95 = 0.09551123529672623\n",
      "Loss for batch 96 = 0.1058376207947731\n",
      "Loss for batch 97 = 0.2743612825870514\n",
      "Loss for batch 98 = 0.20243896543979645\n",
      "Loss for batch 99 = 0.09473655372858047\n",
      "Loss for batch 100 = 0.10424377769231796\n",
      "Loss for batch 101 = 0.21519246697425842\n",
      "Loss for batch 102 = 0.05542192980647087\n",
      "Loss for batch 103 = 0.4603450298309326\n",
      "Loss for batch 104 = 0.2545129656791687\n",
      "Loss for batch 105 = 0.45619118213653564\n",
      "Loss for batch 106 = 0.0336507149040699\n",
      "\n",
      "Training Loss for epoch 11 = 23.925079345703125\n",
      "\n",
      "Current Validation Loss = 16.590665817260742\n",
      "Best Validation Loss = 12.187006950378418\n",
      "Epochs without Improvement = 4\n",
      "Train Accuracy: 94.16%\n",
      "Validation Accuracy: 70.23%\n",
      "\n",
      "Epoch 12\n",
      "----------\n",
      "Loss for batch 0 = 0.37462225556373596\n",
      "Loss for batch 1 = 0.07649438083171844\n",
      "Loss for batch 2 = 0.20410968363285065\n",
      "Loss for batch 3 = 0.27059072256088257\n",
      "Loss for batch 4 = 0.13974159955978394\n",
      "Loss for batch 5 = 0.17718595266342163\n",
      "Loss for batch 6 = 0.18879768252372742\n",
      "Loss for batch 7 = 0.2429991066455841\n",
      "Loss for batch 8 = 0.04623175412416458\n",
      "Loss for batch 9 = 0.29439792037010193\n",
      "Loss for batch 10 = 0.07235247641801834\n",
      "Loss for batch 11 = 0.22560377418994904\n",
      "Loss for batch 12 = 0.2770144045352936\n",
      "Loss for batch 13 = 0.2508159577846527\n",
      "Loss for batch 14 = 0.17883193492889404\n",
      "Loss for batch 15 = 0.4216441214084625\n",
      "Loss for batch 16 = 0.1596137285232544\n",
      "Loss for batch 17 = 0.05528944358229637\n",
      "Loss for batch 18 = 0.25870293378829956\n",
      "Loss for batch 19 = 0.08876407146453857\n",
      "Loss for batch 20 = 0.11627206206321716\n",
      "Loss for batch 21 = 0.170557901263237\n",
      "Loss for batch 22 = 0.24393795430660248\n",
      "Loss for batch 23 = 0.020913923159241676\n",
      "Loss for batch 24 = 0.13696445524692535\n",
      "Loss for batch 25 = 0.26613596081733704\n",
      "Loss for batch 26 = 0.0394618958234787\n",
      "Loss for batch 27 = 0.30887913703918457\n",
      "Loss for batch 28 = 0.07596825063228607\n",
      "Loss for batch 29 = 0.25356900691986084\n",
      "Loss for batch 30 = 0.23433996737003326\n",
      "Loss for batch 31 = 0.5865526795387268\n",
      "Loss for batch 32 = 0.05378102511167526\n",
      "Loss for batch 33 = 0.12866416573524475\n",
      "Loss for batch 34 = 0.19329337775707245\n",
      "Loss for batch 35 = 0.4213619828224182\n",
      "Loss for batch 36 = 0.04896128550171852\n",
      "Loss for batch 37 = 0.1339503973722458\n",
      "Loss for batch 38 = 0.07464649528265\n",
      "Loss for batch 39 = 0.12780310213565826\n",
      "Loss for batch 40 = 0.1373925358057022\n",
      "Loss for batch 41 = 0.19878974556922913\n",
      "Loss for batch 42 = 0.2803274393081665\n",
      "Loss for batch 43 = 0.3730248510837555\n",
      "Loss for batch 44 = 0.15428277850151062\n",
      "Loss for batch 45 = 0.12365996837615967\n",
      "Loss for batch 46 = 0.21452875435352325\n",
      "Loss for batch 47 = 0.0740395188331604\n",
      "Loss for batch 48 = 0.07734333723783493\n",
      "Loss for batch 49 = 0.10135296732187271\n",
      "Loss for batch 50 = 0.2141745686531067\n",
      "Loss for batch 51 = 0.23845060169696808\n",
      "Loss for batch 52 = 0.3359144926071167\n",
      "Loss for batch 53 = 0.11185218393802643\n",
      "Loss for batch 54 = 0.22722002863883972\n",
      "Loss for batch 55 = 0.09372852742671967\n",
      "Loss for batch 56 = 0.2863885164260864\n",
      "Loss for batch 57 = 0.09901128709316254\n",
      "Loss for batch 58 = 0.06364236027002335\n",
      "Loss for batch 59 = 0.05508974939584732\n",
      "Loss for batch 60 = 0.21333083510398865\n",
      "Loss for batch 61 = 0.12642677128314972\n",
      "Loss for batch 62 = 0.02519354596734047\n",
      "Loss for batch 63 = 0.03437391296029091\n",
      "Loss for batch 64 = 0.07818876206874847\n",
      "Loss for batch 65 = 0.22163808345794678\n",
      "Loss for batch 66 = 0.1821853667497635\n",
      "Loss for batch 67 = 0.05793806537985802\n",
      "Loss for batch 68 = 0.2582307457923889\n",
      "Loss for batch 69 = 0.08636735379695892\n",
      "Loss for batch 70 = 0.07962870597839355\n",
      "Loss for batch 71 = 0.19180849194526672\n",
      "Loss for batch 72 = 0.3663701117038727\n",
      "Loss for batch 73 = 0.35124853253364563\n",
      "Loss for batch 74 = 0.3336155116558075\n",
      "Loss for batch 75 = 0.27089205384254456\n",
      "Loss for batch 76 = 0.19078117609024048\n",
      "Loss for batch 77 = 0.15019109845161438\n",
      "Loss for batch 78 = 0.2757663130760193\n",
      "Loss for batch 79 = 0.15810592472553253\n",
      "Loss for batch 80 = 0.25384846329689026\n",
      "Loss for batch 81 = 0.15700937807559967\n",
      "Loss for batch 82 = 0.10084449499845505\n",
      "Loss for batch 83 = 0.1109500601887703\n",
      "Loss for batch 84 = 0.16270864009857178\n",
      "Loss for batch 85 = 0.10882090777158737\n",
      "Loss for batch 86 = 0.0669659748673439\n",
      "Loss for batch 87 = 0.06238960847258568\n",
      "Loss for batch 88 = 0.26436710357666016\n",
      "Loss for batch 89 = 0.24704940617084503\n",
      "Loss for batch 90 = 0.34945347905158997\n",
      "Loss for batch 91 = 0.2756011486053467\n",
      "Loss for batch 92 = 0.10503517836332321\n",
      "Loss for batch 93 = 0.03986373171210289\n",
      "Loss for batch 94 = 0.12039609998464584\n",
      "Loss for batch 95 = 0.17848211526870728\n",
      "Loss for batch 96 = 0.06426308304071426\n",
      "Loss for batch 97 = 0.3670533001422882\n",
      "Loss for batch 98 = 0.24673673510551453\n",
      "Loss for batch 99 = 0.047723691910505295\n",
      "Loss for batch 100 = 0.04181728884577751\n",
      "Loss for batch 101 = 0.2131929248571396\n",
      "Loss for batch 102 = 0.0559779554605484\n",
      "Loss for batch 103 = 0.4140549600124359\n",
      "Loss for batch 104 = 0.17435383796691895\n",
      "Loss for batch 105 = 0.4537958800792694\n",
      "Loss for batch 106 = 0.20586493611335754\n",
      "\n",
      "Training Loss for epoch 12 = 19.64093589782715\n",
      "\n",
      "Current Validation Loss = 15.92055606842041\n",
      "Best Validation Loss = 12.187006950378418\n",
      "Epochs without Improvement = 5\n",
      "Train Accuracy: 94.95%\n",
      "Validation Accuracy: 73.31%\n",
      "\n",
      "Epoch 13\n",
      "----------\n",
      "Loss for batch 0 = 0.10340309143066406\n",
      "Loss for batch 1 = 0.08376381546258926\n",
      "Loss for batch 2 = 0.20908533036708832\n",
      "Loss for batch 3 = 0.34766510128974915\n",
      "Loss for batch 4 = 0.10764826089143753\n",
      "Loss for batch 5 = 0.07907956093549728\n",
      "Loss for batch 6 = 0.21994686126708984\n",
      "Loss for batch 7 = 0.23636220395565033\n",
      "Loss for batch 8 = 0.04678749293088913\n",
      "Loss for batch 9 = 0.18221399188041687\n",
      "Loss for batch 10 = 0.020042503252625465\n",
      "Loss for batch 11 = 0.09562178701162338\n",
      "Loss for batch 12 = 0.32764050364494324\n",
      "Loss for batch 13 = 0.2022506594657898\n",
      "Loss for batch 14 = 0.06844671070575714\n",
      "Loss for batch 15 = 0.38326650857925415\n",
      "Loss for batch 16 = 0.04918753728270531\n",
      "Loss for batch 17 = 0.03895367309451103\n",
      "Loss for batch 18 = 0.3002057671546936\n",
      "Loss for batch 19 = 0.06975107640028\n",
      "Loss for batch 20 = 0.30250853300094604\n",
      "Loss for batch 21 = 0.13171613216400146\n",
      "Loss for batch 22 = 0.17876523733139038\n",
      "Loss for batch 23 = 0.10904205590486526\n",
      "Loss for batch 24 = 0.05426371470093727\n",
      "Loss for batch 25 = 0.17879146337509155\n",
      "Loss for batch 26 = 0.08769019693136215\n",
      "Loss for batch 27 = 0.18307489156723022\n",
      "Loss for batch 28 = 0.06499110907316208\n",
      "Loss for batch 29 = 0.2892334759235382\n",
      "Loss for batch 30 = 0.13218814134597778\n",
      "Loss for batch 31 = 0.5133496522903442\n",
      "Loss for batch 32 = 0.05833521857857704\n",
      "Loss for batch 33 = 0.08176720887422562\n",
      "Loss for batch 34 = 0.1539069265127182\n",
      "Loss for batch 35 = 0.2829786539077759\n",
      "Loss for batch 36 = 0.06159697100520134\n",
      "Loss for batch 37 = 0.11164480447769165\n",
      "Loss for batch 38 = 0.04408012330532074\n",
      "Loss for batch 39 = 0.11726392060518265\n",
      "Loss for batch 40 = 0.10701754689216614\n",
      "Loss for batch 41 = 0.09572518616914749\n",
      "Loss for batch 42 = 0.2681296765804291\n",
      "Loss for batch 43 = 0.27856212854385376\n",
      "Loss for batch 44 = 0.2695702314376831\n",
      "Loss for batch 45 = 0.06333520263433456\n",
      "Loss for batch 46 = 0.08645845204591751\n",
      "Loss for batch 47 = 0.03661851957440376\n",
      "Loss for batch 48 = 0.1603563129901886\n",
      "Loss for batch 49 = 0.053408294916152954\n",
      "Loss for batch 50 = 0.105027936398983\n",
      "Loss for batch 51 = 0.28923025727272034\n",
      "Loss for batch 52 = 0.42770570516586304\n",
      "Loss for batch 53 = 0.09631649404764175\n",
      "Loss for batch 54 = 0.26757898926734924\n",
      "Loss for batch 55 = 0.08426571637392044\n",
      "Loss for batch 56 = 0.21327365934848785\n",
      "Loss for batch 57 = 0.1286364495754242\n",
      "Loss for batch 58 = 0.10819033533334732\n",
      "Loss for batch 59 = 0.10353740304708481\n",
      "Loss for batch 60 = 0.21917934715747833\n",
      "Loss for batch 61 = 0.12488631159067154\n",
      "Loss for batch 62 = 0.06424286216497421\n",
      "Loss for batch 63 = 0.03347308933734894\n",
      "Loss for batch 64 = 0.05321856588125229\n",
      "Loss for batch 65 = 0.18074406683444977\n",
      "Loss for batch 66 = 0.4576784074306488\n",
      "Loss for batch 67 = 0.0761897936463356\n",
      "Loss for batch 68 = 0.17108117043972015\n",
      "Loss for batch 69 = 0.14915457367897034\n",
      "Loss for batch 70 = 0.04189339652657509\n",
      "Loss for batch 71 = 0.22604098916053772\n",
      "Loss for batch 72 = 0.2668997049331665\n",
      "Loss for batch 73 = 0.1756412535905838\n",
      "Loss for batch 74 = 0.503996729850769\n",
      "Loss for batch 75 = 0.2684011161327362\n",
      "Loss for batch 76 = 0.1952124387025833\n",
      "Loss for batch 77 = 0.13614068925380707\n",
      "Loss for batch 78 = 0.2827647030353546\n",
      "Loss for batch 79 = 0.1679787039756775\n",
      "Loss for batch 80 = 0.1297958493232727\n",
      "Loss for batch 81 = 0.09684827923774719\n",
      "Loss for batch 82 = 0.05691513419151306\n",
      "Loss for batch 83 = 0.03434949368238449\n",
      "Loss for batch 84 = 0.06491751968860626\n",
      "Loss for batch 85 = 0.06871384382247925\n",
      "Loss for batch 86 = 0.04332035407423973\n",
      "Loss for batch 87 = 0.026987791061401367\n",
      "Loss for batch 88 = 0.1360931694507599\n",
      "Loss for batch 89 = 0.24147608876228333\n",
      "Loss for batch 90 = 0.3489941358566284\n",
      "Loss for batch 91 = 0.1984965205192566\n",
      "Loss for batch 92 = 0.16627110540866852\n",
      "Loss for batch 93 = 0.07603435963392258\n",
      "Loss for batch 94 = 0.08183574676513672\n",
      "Loss for batch 95 = 0.0800795927643776\n",
      "Loss for batch 96 = 0.03490747511386871\n",
      "Loss for batch 97 = 0.2716059982776642\n",
      "Loss for batch 98 = 0.2146630883216858\n",
      "Loss for batch 99 = 0.03136593475937843\n",
      "Loss for batch 100 = 0.03428904712200165\n",
      "Loss for batch 101 = 0.13774582743644714\n",
      "Loss for batch 102 = 0.05219626426696777\n",
      "Loss for batch 103 = 0.4140527844429016\n",
      "Loss for batch 104 = 0.16815757751464844\n",
      "Loss for batch 105 = 0.28952810168266296\n",
      "Loss for batch 106 = 0.3677311837673187\n",
      "\n",
      "Training Loss for epoch 13 = 17.213651657104492\n",
      "\n",
      "Current Validation Loss = 17.001359939575195\n",
      "Best Validation Loss = 12.187006950378418\n",
      "Epochs without Improvement = 6\n",
      "Train Accuracy: 95.22%\n",
      "Validation Accuracy: 71.25%\n",
      "\n",
      "Epoch 14\n",
      "----------\n",
      "Loss for batch 0 = 0.07466703653335571\n",
      "Loss for batch 1 = 0.15454724431037903\n",
      "Loss for batch 2 = 0.023614713922142982\n",
      "Loss for batch 3 = 0.31338751316070557\n",
      "Loss for batch 4 = 0.08565344661474228\n",
      "Loss for batch 5 = 0.13109637796878815\n",
      "Loss for batch 6 = 0.17009060084819794\n",
      "Loss for batch 7 = 0.19068515300750732\n",
      "Loss for batch 8 = 0.15867559611797333\n",
      "Loss for batch 9 = 0.19446450471878052\n",
      "Loss for batch 10 = 0.04436330124735832\n",
      "Loss for batch 11 = 0.052909012883901596\n",
      "Loss for batch 12 = 0.20392335951328278\n",
      "Loss for batch 13 = 0.26300495862960815\n",
      "Loss for batch 14 = 0.08718211203813553\n",
      "Loss for batch 15 = 0.35018035769462585\n",
      "Loss for batch 16 = 0.1347011774778366\n",
      "Loss for batch 17 = 0.051582470536231995\n",
      "Loss for batch 18 = 0.26023757457733154\n",
      "Loss for batch 19 = 0.03492911532521248\n",
      "Loss for batch 20 = 0.19217462837696075\n",
      "Loss for batch 21 = 0.02958616614341736\n",
      "Loss for batch 22 = 0.2610038220882416\n",
      "Loss for batch 23 = 0.04864725098013878\n",
      "Loss for batch 24 = 0.06030580773949623\n",
      "Loss for batch 25 = 0.15214961767196655\n",
      "Loss for batch 26 = 0.019887976348400116\n",
      "Loss for batch 27 = 0.1889561414718628\n",
      "Loss for batch 28 = 0.07954194396734238\n",
      "Loss for batch 29 = 0.2355167716741562\n",
      "Loss for batch 30 = 0.2669646739959717\n",
      "Loss for batch 31 = 0.5634244680404663\n",
      "Loss for batch 32 = 0.06744063645601273\n",
      "Loss for batch 33 = 0.025934921577572823\n",
      "Loss for batch 34 = 0.26140716671943665\n",
      "Loss for batch 35 = 0.1659545749425888\n",
      "Loss for batch 36 = 0.09681867808103561\n",
      "Loss for batch 37 = 0.11113473773002625\n",
      "Loss for batch 38 = 0.04547074809670448\n",
      "Loss for batch 39 = 0.03609606623649597\n",
      "Loss for batch 40 = 0.07633008062839508\n",
      "Loss for batch 41 = 0.02398012764751911\n",
      "Loss for batch 42 = 0.15982992947101593\n",
      "Loss for batch 43 = 0.19594620168209076\n",
      "Loss for batch 44 = 0.029502417892217636\n",
      "Loss for batch 45 = 0.3181585967540741\n",
      "Loss for batch 46 = 0.09500011056661606\n",
      "Loss for batch 47 = 0.03122095949947834\n",
      "Loss for batch 48 = 0.10848331451416016\n",
      "Loss for batch 49 = 0.03690008074045181\n",
      "Loss for batch 50 = 0.14095507562160492\n",
      "Loss for batch 51 = 0.1863994598388672\n",
      "Loss for batch 52 = 0.3381879925727844\n",
      "Loss for batch 53 = 0.10534144937992096\n",
      "Loss for batch 54 = 0.21750031411647797\n",
      "Loss for batch 55 = 0.02569933421909809\n",
      "Loss for batch 56 = 0.16512513160705566\n",
      "Loss for batch 57 = 0.14673259854316711\n",
      "Loss for batch 58 = 0.04758373275399208\n",
      "Loss for batch 59 = 0.03142338618636131\n",
      "Loss for batch 60 = 0.20046588778495789\n",
      "Loss for batch 61 = 0.10073916614055634\n",
      "Loss for batch 62 = 0.03413202986121178\n",
      "Loss for batch 63 = 0.0099026495590806\n",
      "Loss for batch 64 = 0.03367973119020462\n",
      "Loss for batch 65 = 0.16006210446357727\n",
      "Loss for batch 66 = 0.2817155122756958\n",
      "Loss for batch 67 = 0.04962393268942833\n",
      "Loss for batch 68 = 0.08552005887031555\n",
      "Loss for batch 69 = 0.09939762204885483\n",
      "Loss for batch 70 = 0.1041674017906189\n",
      "Loss for batch 71 = 0.12113769352436066\n",
      "Loss for batch 72 = 0.2158493846654892\n",
      "Loss for batch 73 = 0.1670394241809845\n",
      "Loss for batch 74 = 0.3569284677505493\n",
      "Loss for batch 75 = 0.20804309844970703\n",
      "Loss for batch 76 = 0.20723332464694977\n",
      "Loss for batch 77 = 0.0648028701543808\n",
      "Loss for batch 78 = 0.1368192881345749\n",
      "Loss for batch 79 = 0.05465693026781082\n",
      "Loss for batch 80 = 0.02299274317920208\n",
      "Loss for batch 81 = 0.17452308535575867\n",
      "Loss for batch 82 = 0.037957582622766495\n",
      "Loss for batch 83 = 0.08986774832010269\n",
      "Loss for batch 84 = 0.0833154246211052\n",
      "Loss for batch 85 = 0.050477415323257446\n",
      "Loss for batch 86 = 0.02358214743435383\n",
      "Loss for batch 87 = 0.01983342319726944\n",
      "Loss for batch 88 = 0.1483917385339737\n",
      "Loss for batch 89 = 0.187962144613266\n",
      "Loss for batch 90 = 0.28910574316978455\n",
      "Loss for batch 91 = 0.25674110651016235\n",
      "Loss for batch 92 = 0.16089414060115814\n",
      "Loss for batch 93 = 0.0308526661247015\n",
      "Loss for batch 94 = 0.09852559119462967\n",
      "Loss for batch 95 = 0.09458860754966736\n",
      "Loss for batch 96 = 0.030970722436904907\n",
      "Loss for batch 97 = 0.24720335006713867\n",
      "Loss for batch 98 = 0.2036149501800537\n",
      "Loss for batch 99 = 0.02625516802072525\n",
      "Loss for batch 100 = 0.013770461082458496\n",
      "Loss for batch 101 = 0.12678860127925873\n",
      "Loss for batch 102 = 0.01857985183596611\n",
      "Loss for batch 103 = 0.2842380702495575\n",
      "Loss for batch 104 = 0.07053323835134506\n",
      "Loss for batch 105 = 0.4125540852546692\n",
      "Loss for batch 106 = 0.21596120297908783\n",
      "\n",
      "Training Loss for epoch 14 = 14.480608940124512\n",
      "\n",
      "Current Validation Loss = 16.786785125732422\n",
      "Best Validation Loss = 12.187006950378418\n",
      "Epochs without Improvement = 7\n",
      "Train Accuracy: 95.33%\n",
      "Validation Accuracy: 70.23%\n",
      "\n",
      "Epoch 15\n",
      "----------\n",
      "Loss for batch 0 = 0.03781823068857193\n",
      "Loss for batch 1 = 0.02090795896947384\n",
      "Loss for batch 2 = 0.023699691519141197\n",
      "Loss for batch 3 = 0.26900988817214966\n",
      "Loss for batch 4 = 0.08560610562562943\n",
      "Loss for batch 5 = 0.17462629079818726\n",
      "Loss for batch 6 = 0.16806286573410034\n",
      "Loss for batch 7 = 0.12815693020820618\n",
      "Loss for batch 8 = 0.03058386594057083\n",
      "Loss for batch 9 = 0.21963529288768768\n",
      "Loss for batch 10 = 0.02207091823220253\n",
      "Loss for batch 11 = 0.02782140113413334\n",
      "Loss for batch 12 = 0.15074996650218964\n",
      "Loss for batch 13 = 0.24611346423625946\n",
      "Loss for batch 14 = 0.04824156314134598\n",
      "Loss for batch 15 = 0.34564435482025146\n",
      "Loss for batch 16 = 0.12968862056732178\n",
      "Loss for batch 17 = 0.07049243152141571\n",
      "Loss for batch 18 = 0.20728546380996704\n",
      "Loss for batch 19 = 0.061194825917482376\n",
      "Loss for batch 20 = 0.22627398371696472\n",
      "Loss for batch 21 = 0.11933663487434387\n",
      "Loss for batch 22 = 0.376220166683197\n",
      "Loss for batch 23 = 0.04137253761291504\n",
      "Loss for batch 24 = 0.09576187282800674\n",
      "Loss for batch 25 = 0.12134826928377151\n",
      "Loss for batch 26 = 0.03412822633981705\n",
      "Loss for batch 27 = 0.18265078961849213\n",
      "Loss for batch 28 = 0.037551600486040115\n",
      "Loss for batch 29 = 0.18594375252723694\n",
      "Loss for batch 30 = 0.1315750628709793\n",
      "Loss for batch 31 = 0.5342508554458618\n",
      "Loss for batch 32 = 0.1093168631196022\n",
      "Loss for batch 33 = 0.03559846431016922\n",
      "Loss for batch 34 = 0.1795741319656372\n",
      "Loss for batch 35 = 0.10524225980043411\n",
      "Loss for batch 36 = 0.16941510140895844\n",
      "Loss for batch 37 = 0.09792189300060272\n",
      "Loss for batch 38 = 0.07676877826452255\n",
      "Loss for batch 39 = 0.020902398973703384\n",
      "Loss for batch 40 = 0.016535066068172455\n",
      "Loss for batch 41 = 0.02389833889901638\n",
      "Loss for batch 42 = 0.15593698620796204\n",
      "Loss for batch 43 = 0.3399510085582733\n",
      "Loss for batch 44 = 0.024060145020484924\n",
      "Loss for batch 45 = 0.023525230586528778\n",
      "Loss for batch 46 = 0.027193831279873848\n",
      "Loss for batch 47 = 0.049242060631513596\n",
      "Loss for batch 48 = 0.030759286135435104\n",
      "Loss for batch 49 = 0.032710373401641846\n",
      "Loss for batch 50 = 0.09397407621145248\n",
      "Loss for batch 51 = 0.21407514810562134\n",
      "Loss for batch 52 = 0.35347801446914673\n",
      "Loss for batch 53 = 0.05068138986825943\n",
      "Loss for batch 54 = 0.24319560825824738\n",
      "Loss for batch 55 = 0.01806746982038021\n",
      "Loss for batch 56 = 0.16362817585468292\n",
      "Loss for batch 57 = 0.12508875131607056\n",
      "Loss for batch 58 = 0.12903361022472382\n",
      "Loss for batch 59 = 0.2135215550661087\n",
      "Loss for batch 60 = 0.23284420371055603\n",
      "Loss for batch 61 = 0.13900165259838104\n",
      "Loss for batch 62 = 0.01820133626461029\n",
      "Loss for batch 63 = 0.028170662000775337\n",
      "Loss for batch 64 = 0.025011567398905754\n",
      "Loss for batch 65 = 0.14730463922023773\n",
      "Loss for batch 66 = 0.14989645779132843\n",
      "Loss for batch 67 = 0.020829908549785614\n",
      "Loss for batch 68 = 0.039352450519800186\n",
      "Loss for batch 69 = 0.07830671221017838\n",
      "Loss for batch 70 = 0.013437503017485142\n",
      "Loss for batch 71 = 0.10272587835788727\n",
      "Loss for batch 72 = 0.17114873230457306\n",
      "Loss for batch 73 = 0.1445155292749405\n",
      "Loss for batch 74 = 0.3327431082725525\n",
      "Loss for batch 75 = 0.19671307504177094\n",
      "Loss for batch 76 = 0.15459014475345612\n",
      "Loss for batch 77 = 0.04569061100482941\n",
      "Loss for batch 78 = 0.17496119439601898\n",
      "Loss for batch 79 = 0.02510232850909233\n",
      "Loss for batch 80 = 0.020037617534399033\n",
      "Loss for batch 81 = 0.039024513214826584\n",
      "Loss for batch 82 = 0.08005532622337341\n",
      "Loss for batch 83 = 0.031390950083732605\n",
      "Loss for batch 84 = 0.06843428313732147\n",
      "Loss for batch 85 = 0.028370540589094162\n",
      "Loss for batch 86 = 0.12283854931592941\n",
      "Loss for batch 87 = 0.024453232064843178\n",
      "Loss for batch 88 = 0.15702980756759644\n",
      "Loss for batch 89 = 0.15496134757995605\n",
      "Loss for batch 90 = 0.15797820687294006\n",
      "Loss for batch 91 = 0.2168070673942566\n",
      "Loss for batch 92 = 0.26800528168678284\n",
      "Loss for batch 93 = 0.022131266072392464\n",
      "Loss for batch 94 = 0.04398062825202942\n",
      "Loss for batch 95 = 0.16047629714012146\n",
      "Loss for batch 96 = 0.027960723266005516\n",
      "Loss for batch 97 = 0.24772968888282776\n",
      "Loss for batch 98 = 0.16007764637470245\n",
      "Loss for batch 99 = 0.028451479971408844\n",
      "Loss for batch 100 = 0.02835480496287346\n",
      "Loss for batch 101 = 0.051140330731868744\n",
      "Loss for batch 102 = 0.0724540576338768\n",
      "Loss for batch 103 = 0.3726012110710144\n",
      "Loss for batch 104 = 0.041694700717926025\n",
      "Loss for batch 105 = 0.32451096177101135\n",
      "Loss for batch 106 = 0.17709438502788544\n",
      "\n",
      "Training Loss for epoch 15 = 13.045717239379883\n",
      "\n",
      "Current Validation Loss = 18.767559051513672\n",
      "Best Validation Loss = 12.187006950378418\n",
      "Epochs without Improvement = 8\n",
      "Train Accuracy: 96.27%\n",
      "Validation Accuracy: 71.25%\n",
      "\n",
      "Epoch 16\n",
      "----------\n",
      "Loss for batch 0 = 0.18447010219097137\n",
      "Loss for batch 1 = 0.10574838519096375\n",
      "Loss for batch 2 = 0.13376116752624512\n",
      "Loss for batch 3 = 0.22427277266979218\n",
      "Loss for batch 4 = 0.11056782305240631\n",
      "Loss for batch 5 = 0.056738223880529404\n",
      "Loss for batch 6 = 0.16195200383663177\n",
      "Loss for batch 7 = 0.08021282404661179\n",
      "Loss for batch 8 = 0.019404003396630287\n",
      "Loss for batch 9 = 0.10359766334295273\n",
      "Loss for batch 10 = 0.08083812147378922\n",
      "Loss for batch 11 = 0.026008935645222664\n",
      "Loss for batch 12 = 0.396706759929657\n",
      "Loss for batch 13 = 0.1381136029958725\n",
      "Loss for batch 14 = 0.05125973001122475\n",
      "Loss for batch 15 = 0.29456496238708496\n",
      "Loss for batch 16 = 0.06895769387483597\n",
      "Loss for batch 17 = 0.02675572969019413\n",
      "Loss for batch 18 = 0.1063353568315506\n",
      "Loss for batch 19 = 0.03236638382077217\n",
      "Loss for batch 20 = 0.06929758936166763\n",
      "Loss for batch 21 = 0.0935889482498169\n",
      "Loss for batch 22 = 0.29154548048973083\n",
      "Loss for batch 23 = 0.013685743324458599\n",
      "Loss for batch 24 = 0.0511518269777298\n",
      "Loss for batch 25 = 0.13110046088695526\n",
      "Loss for batch 26 = 0.02142944186925888\n",
      "Loss for batch 27 = 0.17633607983589172\n",
      "Loss for batch 28 = 0.09636060148477554\n",
      "Loss for batch 29 = 0.2109367549419403\n",
      "Loss for batch 30 = 0.13796433806419373\n",
      "Loss for batch 31 = 0.4885062277317047\n",
      "Loss for batch 32 = 0.035809073597192764\n",
      "Loss for batch 33 = 0.039619699120521545\n",
      "Loss for batch 34 = 0.37017035484313965\n",
      "Loss for batch 35 = 0.1153932586312294\n",
      "Loss for batch 36 = 0.03681487962603569\n",
      "Loss for batch 37 = 0.08800021559000015\n",
      "Loss for batch 38 = 0.02619401179254055\n",
      "Loss for batch 39 = 0.01911155693233013\n",
      "Loss for batch 40 = 0.11077272891998291\n",
      "Loss for batch 41 = 0.022689905017614365\n",
      "Loss for batch 42 = 0.1528438925743103\n",
      "Loss for batch 43 = 0.3076549172401428\n",
      "Loss for batch 44 = 0.04483151063323021\n",
      "Loss for batch 45 = 0.029930703341960907\n",
      "Loss for batch 46 = 0.16896125674247742\n",
      "Loss for batch 47 = 0.014944489113986492\n",
      "Loss for batch 48 = 0.014950071461498737\n",
      "Loss for batch 49 = 0.05343183875083923\n",
      "Loss for batch 50 = 0.2061915248632431\n",
      "Loss for batch 51 = 0.18992988765239716\n",
      "Loss for batch 52 = 0.2812654972076416\n",
      "Loss for batch 53 = 0.10666500777006149\n",
      "Loss for batch 54 = 0.2385837733745575\n",
      "Loss for batch 55 = 0.06562581658363342\n",
      "Loss for batch 56 = 0.29792580008506775\n",
      "Loss for batch 57 = 0.0496644452214241\n",
      "Loss for batch 58 = 0.02085300162434578\n",
      "Loss for batch 59 = 0.1632251739501953\n",
      "Loss for batch 60 = 0.3312014937400818\n",
      "Loss for batch 61 = 0.1067705750465393\n",
      "Loss for batch 62 = 0.025006262585520744\n",
      "Loss for batch 63 = 0.020773809403181076\n",
      "Loss for batch 64 = 0.042556021362543106\n",
      "Loss for batch 65 = 0.2172325998544693\n",
      "Loss for batch 66 = 0.2443782240152359\n",
      "Loss for batch 67 = 0.01935812085866928\n",
      "Loss for batch 68 = 0.09074896574020386\n",
      "Loss for batch 69 = 0.07829638570547104\n",
      "Loss for batch 70 = 0.050485000014305115\n",
      "Loss for batch 71 = 0.09057328850030899\n",
      "Loss for batch 72 = 0.30642086267471313\n",
      "Loss for batch 73 = 0.17181295156478882\n",
      "Loss for batch 74 = 0.31854498386383057\n",
      "Loss for batch 75 = 0.19998696446418762\n",
      "Loss for batch 76 = 0.1408614218235016\n",
      "Loss for batch 77 = 0.13776066899299622\n",
      "Loss for batch 78 = 0.18496380746364594\n",
      "Loss for batch 79 = 0.08486130833625793\n",
      "Loss for batch 80 = 0.02035072073340416\n",
      "Loss for batch 81 = 0.0601256899535656\n",
      "Loss for batch 82 = 0.13176003098487854\n",
      "Loss for batch 83 = 0.11414038389921188\n",
      "Loss for batch 84 = 0.05512165278196335\n",
      "Loss for batch 85 = 0.04178120940923691\n",
      "Loss for batch 86 = 0.021015943959355354\n",
      "Loss for batch 87 = 0.024457484483718872\n",
      "Loss for batch 88 = 0.05378584936261177\n",
      "Loss for batch 89 = 0.12967140972614288\n",
      "Loss for batch 90 = 0.19663304090499878\n",
      "Loss for batch 91 = 0.05762933939695358\n",
      "Loss for batch 92 = 0.03615608438849449\n",
      "Loss for batch 93 = 0.0951138511300087\n",
      "Loss for batch 94 = 0.13926799595355988\n",
      "Loss for batch 95 = 0.06023379787802696\n",
      "Loss for batch 96 = 0.012334535829722881\n",
      "Loss for batch 97 = 0.19161619246006012\n",
      "Loss for batch 98 = 0.17508414387702942\n",
      "Loss for batch 99 = 0.03856635466217995\n",
      "Loss for batch 100 = 0.025955742225050926\n",
      "Loss for batch 101 = 0.10874257981777191\n",
      "Loss for batch 102 = 0.012936118058860302\n",
      "Loss for batch 103 = 0.2811433970928192\n",
      "Loss for batch 104 = 0.01787409745156765\n",
      "Loss for batch 105 = 0.3440953195095062\n",
      "Loss for batch 106 = 0.034310560673475266\n",
      "\n",
      "Training Loss for epoch 16 = 12.799089431762695\n",
      "\n",
      "Current Validation Loss = 20.199872970581055\n",
      "Best Validation Loss = 12.187006950378418\n",
      "Epochs without Improvement = 9\n",
      "Train Accuracy: 94.66%\n",
      "Validation Accuracy: 71.66%\n",
      "\n",
      "Epoch 17\n",
      "----------\n",
      "Loss for batch 0 = 0.06963542103767395\n",
      "Loss for batch 1 = 0.06474721431732178\n",
      "Loss for batch 2 = 0.009361027739942074\n",
      "Loss for batch 3 = 0.20963847637176514\n",
      "Loss for batch 4 = 0.06565061211585999\n",
      "Loss for batch 5 = 0.07132785022258759\n",
      "Loss for batch 6 = 0.343183308839798\n",
      "Loss for batch 7 = 0.14367155730724335\n",
      "Loss for batch 8 = 0.01081230491399765\n",
      "Loss for batch 9 = 0.23937390744686127\n",
      "Loss for batch 10 = 0.02820376679301262\n",
      "Loss for batch 11 = 0.01669749617576599\n",
      "Loss for batch 12 = 0.2559697926044464\n",
      "Loss for batch 13 = 0.23865963518619537\n",
      "Loss for batch 14 = 0.03038359060883522\n",
      "Loss for batch 15 = 0.19974087178707123\n",
      "Loss for batch 16 = 0.13776208460330963\n",
      "Loss for batch 17 = 0.022609032690525055\n",
      "Loss for batch 18 = 0.19292989373207092\n",
      "Loss for batch 19 = 0.02729232795536518\n",
      "Loss for batch 20 = 0.024811062961816788\n",
      "Loss for batch 21 = 0.0340680368244648\n",
      "Loss for batch 22 = 0.19363108277320862\n",
      "Loss for batch 23 = 0.010604098439216614\n",
      "Loss for batch 24 = 0.02636769227683544\n",
      "Loss for batch 25 = 0.08962307870388031\n",
      "Loss for batch 26 = 0.029717085883021355\n",
      "Loss for batch 27 = 0.1605900079011917\n",
      "Loss for batch 28 = 0.03741774335503578\n",
      "Loss for batch 29 = 0.18123573064804077\n",
      "Loss for batch 30 = 0.10675588995218277\n",
      "Loss for batch 31 = 0.4895271956920624\n",
      "Loss for batch 32 = 0.017962004989385605\n",
      "Loss for batch 33 = 0.018711138516664505\n",
      "Loss for batch 34 = 0.10833701491355896\n",
      "Loss for batch 35 = 0.039217472076416016\n",
      "Loss for batch 36 = 0.008216407150030136\n",
      "Loss for batch 37 = 0.13982853293418884\n",
      "Loss for batch 38 = 0.035997357219457626\n",
      "Loss for batch 39 = 0.01170653011649847\n",
      "Loss for batch 40 = 0.02043706178665161\n",
      "Loss for batch 41 = 0.0084126191213727\n",
      "Loss for batch 42 = 0.139812171459198\n",
      "Loss for batch 43 = 0.24119050800800323\n",
      "Loss for batch 44 = 0.03649428114295006\n",
      "Loss for batch 45 = 0.02175445854663849\n",
      "Loss for batch 46 = 0.1877179592847824\n",
      "Loss for batch 47 = 0.0402219295501709\n",
      "Loss for batch 48 = 0.048360202461481094\n",
      "Loss for batch 49 = 0.022020313888788223\n",
      "Loss for batch 50 = 0.09485851228237152\n",
      "Loss for batch 51 = 0.15337251126766205\n",
      "Loss for batch 52 = 0.25564396381378174\n",
      "Loss for batch 53 = 0.018404457718133926\n",
      "Loss for batch 54 = 0.1969943344593048\n",
      "Loss for batch 55 = 0.023335600271821022\n",
      "Loss for batch 56 = 0.08536069840192795\n",
      "Loss for batch 57 = 0.039219167083501816\n",
      "Loss for batch 58 = 0.017109936103224754\n",
      "Loss for batch 59 = 0.015111839398741722\n",
      "Loss for batch 60 = 0.19758762419223785\n",
      "Loss for batch 61 = 0.14950326085090637\n",
      "Loss for batch 62 = 0.012163227424025536\n",
      "Loss for batch 63 = 0.010465958155691624\n",
      "Loss for batch 64 = 0.012276152148842812\n",
      "Loss for batch 65 = 0.16704489290714264\n",
      "Loss for batch 66 = 0.16806115210056305\n",
      "Loss for batch 67 = 0.03729808330535889\n",
      "Loss for batch 68 = 0.012870239093899727\n",
      "Loss for batch 69 = 0.062468308955430984\n",
      "Loss for batch 70 = 0.01163078099489212\n",
      "Loss for batch 71 = 0.053851764649152756\n",
      "Loss for batch 72 = 0.16872189939022064\n",
      "Loss for batch 73 = 0.10815797746181488\n",
      "Loss for batch 74 = 0.2081458568572998\n",
      "Loss for batch 75 = 0.22831085324287415\n",
      "Loss for batch 76 = 0.1375054121017456\n",
      "Loss for batch 77 = 0.13761267066001892\n",
      "Loss for batch 78 = 0.13697578012943268\n",
      "Loss for batch 79 = 0.01580124907195568\n",
      "Loss for batch 80 = 0.011122533120214939\n",
      "Loss for batch 81 = 0.024358930066227913\n",
      "Loss for batch 82 = 0.012704127468168736\n",
      "Loss for batch 83 = 0.03569519892334938\n",
      "Loss for batch 84 = 0.14556965231895447\n",
      "Loss for batch 85 = 0.06706269085407257\n",
      "Loss for batch 86 = 0.05098658800125122\n",
      "Loss for batch 87 = 0.013748628087341785\n",
      "Loss for batch 88 = 0.016086196526885033\n",
      "Loss for batch 89 = 0.2253967970609665\n",
      "Loss for batch 90 = 0.18545864522457123\n",
      "Loss for batch 91 = 0.09661421179771423\n",
      "Loss for batch 92 = 0.01207144558429718\n",
      "Loss for batch 93 = 0.03546930477023125\n",
      "Loss for batch 94 = 0.018170157447457314\n",
      "Loss for batch 95 = 0.15619659423828125\n",
      "Loss for batch 96 = 0.015439515002071857\n",
      "Loss for batch 97 = 0.18383537232875824\n",
      "Loss for batch 98 = 0.17844244837760925\n",
      "Loss for batch 99 = 0.023056499660015106\n",
      "Loss for batch 100 = 0.02733113244175911\n",
      "Loss for batch 101 = 0.01902349852025509\n",
      "Loss for batch 102 = 0.014228694140911102\n",
      "Loss for batch 103 = 0.2506701350212097\n",
      "Loss for batch 104 = 0.020260846242308617\n",
      "Loss for batch 105 = 0.12865102291107178\n",
      "Loss for batch 106 = 0.10013104975223541\n",
      "\n",
      "Training Loss for epoch 17 = 9.912041664123535\n",
      "\n",
      "Current Validation Loss = 19.292308807373047\n",
      "Best Validation Loss = 12.187006950378418\n",
      "Epochs without Improvement = 10\n",
      "Train Accuracy: 97.77%\n",
      "Validation Accuracy: 72.07%\n",
      "\n",
      "Epoch 18\n",
      "----------\n",
      "Loss for batch 0 = 0.023590927943587303\n",
      "Loss for batch 1 = 0.014030811376869678\n",
      "Loss for batch 2 = 0.00967344455420971\n",
      "Loss for batch 3 = 0.20449601113796234\n",
      "Loss for batch 4 = 0.07026445120573044\n",
      "Loss for batch 5 = 0.1225234717130661\n",
      "Loss for batch 6 = 0.17460842430591583\n",
      "Loss for batch 7 = 0.04832705110311508\n",
      "Loss for batch 8 = 0.018949171528220177\n",
      "Loss for batch 9 = 0.028063656762242317\n",
      "Loss for batch 10 = 0.011845896020531654\n",
      "Loss for batch 11 = 0.010038227774202824\n",
      "Loss for batch 12 = 0.14363814890384674\n",
      "Loss for batch 13 = 0.2015083283185959\n",
      "Loss for batch 14 = 0.015532998368144035\n",
      "Loss for batch 15 = 0.19820500910282135\n",
      "Loss for batch 16 = 0.07391037791967392\n",
      "Loss for batch 17 = 0.02202542871236801\n",
      "Loss for batch 18 = 0.10285871475934982\n",
      "Loss for batch 19 = 0.014633892104029655\n",
      "Loss for batch 20 = 0.026229167357087135\n",
      "Loss for batch 21 = 0.025031086057424545\n",
      "Loss for batch 22 = 0.1682785004377365\n",
      "Loss for batch 23 = 0.00988128874450922\n",
      "Loss for batch 24 = 0.017413729801774025\n",
      "Loss for batch 25 = 0.07295621186494827\n",
      "Loss for batch 26 = 0.007780154701322317\n",
      "Loss for batch 27 = 0.1582697182893753\n",
      "Loss for batch 28 = 0.007605591788887978\n",
      "Loss for batch 29 = 0.16953890025615692\n",
      "Loss for batch 30 = 0.07815476506948471\n",
      "Loss for batch 31 = 0.5158721804618835\n",
      "Loss for batch 32 = 0.012834210880100727\n",
      "Loss for batch 33 = 0.02208152413368225\n",
      "Loss for batch 34 = 0.1511206328868866\n",
      "Loss for batch 35 = 0.09498372673988342\n",
      "Loss for batch 36 = 0.007118808105587959\n",
      "Loss for batch 37 = 0.12269607931375504\n",
      "Loss for batch 38 = 0.04522867128252983\n",
      "Loss for batch 39 = 0.012242978438735008\n",
      "Loss for batch 40 = 0.019445747137069702\n",
      "Loss for batch 41 = 0.010256489738821983\n",
      "Loss for batch 42 = 0.10777069628238678\n",
      "Loss for batch 43 = 0.2068268209695816\n",
      "Loss for batch 44 = 0.01045578345656395\n",
      "Loss for batch 45 = 0.013372684828937054\n",
      "Loss for batch 46 = 0.10335693508386612\n",
      "Loss for batch 47 = 0.009689679369330406\n",
      "Loss for batch 48 = 0.009779735468327999\n",
      "Loss for batch 49 = 0.02282349206507206\n",
      "Loss for batch 50 = 0.08060889691114426\n",
      "Loss for batch 51 = 0.15492933988571167\n",
      "Loss for batch 52 = 0.22566303610801697\n",
      "Loss for batch 53 = 0.020792076364159584\n",
      "Loss for batch 54 = 0.10501374304294586\n",
      "Loss for batch 55 = 0.011813368648290634\n",
      "Loss for batch 56 = 0.04599027708172798\n",
      "Loss for batch 57 = 0.023415129631757736\n",
      "Loss for batch 58 = 0.019221344962716103\n",
      "Loss for batch 59 = 0.010544945485889912\n",
      "Loss for batch 60 = 0.19829149544239044\n",
      "Loss for batch 61 = 0.02834349125623703\n",
      "Loss for batch 62 = 0.0072293938137590885\n",
      "Loss for batch 63 = 0.012488274835050106\n",
      "Loss for batch 64 = 0.008641758002340794\n",
      "Loss for batch 65 = 0.07733242213726044\n",
      "Loss for batch 66 = 0.16419266164302826\n",
      "Loss for batch 67 = 0.009268390946090221\n",
      "Loss for batch 68 = 0.009250215254724026\n",
      "Loss for batch 69 = 0.05085169896483421\n",
      "Loss for batch 70 = 0.010774247348308563\n",
      "Loss for batch 71 = 0.03919889032840729\n",
      "Loss for batch 72 = 0.1604229211807251\n",
      "Loss for batch 73 = 0.10684746503829956\n",
      "Loss for batch 74 = 0.10786871612071991\n",
      "Loss for batch 75 = 0.20264489948749542\n",
      "Loss for batch 76 = 0.10248634964227676\n",
      "Loss for batch 77 = 0.03271273151040077\n",
      "Loss for batch 78 = 0.1110001876950264\n",
      "Loss for batch 79 = 0.006571582052856684\n",
      "Loss for batch 80 = 0.007814847864210606\n",
      "Loss for batch 81 = 0.018572136759757996\n",
      "Loss for batch 82 = 0.009235058911144733\n",
      "Loss for batch 83 = 0.019858235493302345\n",
      "Loss for batch 84 = 0.01334342360496521\n",
      "Loss for batch 85 = 0.008316759020090103\n",
      "Loss for batch 86 = 0.010274520143866539\n",
      "Loss for batch 87 = 0.00921024102717638\n",
      "Loss for batch 88 = 0.006412936374545097\n",
      "Loss for batch 89 = 0.09074519574642181\n",
      "Loss for batch 90 = 0.18398670852184296\n",
      "Loss for batch 91 = 0.032508548349142075\n",
      "Loss for batch 92 = 0.008055605925619602\n",
      "Loss for batch 93 = 0.007204745896160603\n",
      "Loss for batch 94 = 0.00939183495938778\n",
      "Loss for batch 95 = 0.05181216448545456\n",
      "Loss for batch 96 = 0.013661623932421207\n",
      "Loss for batch 97 = 0.14877931773662567\n",
      "Loss for batch 98 = 0.16996777057647705\n",
      "Loss for batch 99 = 0.00717203551903367\n",
      "Loss for batch 100 = 0.006528335157781839\n",
      "Loss for batch 101 = 0.009252982214093208\n",
      "Loss for batch 102 = 0.009640654549002647\n",
      "Loss for batch 103 = 0.1142827719449997\n",
      "Loss for batch 104 = 0.0077046058140695095\n",
      "Loss for batch 105 = 0.12815243005752563\n",
      "Loss for batch 106 = 0.04083620011806488\n",
      "\n",
      "Training Loss for epoch 18 = 7.034951686859131\n",
      "\n",
      "Current Validation Loss = 24.454559326171875\n",
      "Best Validation Loss = 12.187006950378418\n",
      "Epochs without Improvement = 11\n",
      "Train Accuracy: 98.15%\n",
      "Validation Accuracy: 72.69%\n",
      "\n",
      "Epoch 19\n",
      "----------\n",
      "Loss for batch 0 = 0.0068223061971366405\n",
      "Loss for batch 1 = 0.005247016437351704\n",
      "Loss for batch 2 = 0.004845784977078438\n",
      "Loss for batch 3 = 0.21505612134933472\n",
      "Loss for batch 4 = 0.05772645026445389\n",
      "Loss for batch 5 = 0.041006725281476974\n",
      "Loss for batch 6 = 0.18247582018375397\n",
      "Loss for batch 7 = 0.007782509084790945\n",
      "Loss for batch 8 = 0.00473432894796133\n",
      "Loss for batch 9 = 0.01217059139162302\n",
      "Loss for batch 10 = 0.009251897223293781\n",
      "Loss for batch 11 = 0.005759200546890497\n",
      "Loss for batch 12 = 0.25127890706062317\n",
      "Loss for batch 13 = 0.2015993446111679\n",
      "Loss for batch 14 = 0.007710379082709551\n",
      "Loss for batch 15 = 0.19821614027023315\n",
      "Loss for batch 16 = 0.01600026525557041\n",
      "Loss for batch 17 = 0.006013392936438322\n",
      "Loss for batch 18 = 0.10668161511421204\n",
      "Loss for batch 19 = 0.007459487300366163\n",
      "Loss for batch 20 = 0.03882922604680061\n",
      "Loss for batch 21 = 0.005561226047575474\n",
      "Loss for batch 22 = 0.1696077138185501\n",
      "Loss for batch 23 = 0.01290693785995245\n",
      "Loss for batch 24 = 0.0073557146824896336\n",
      "Loss for batch 25 = 0.06621988862752914\n",
      "Loss for batch 26 = 0.007699187379330397\n",
      "Loss for batch 27 = 0.16306579113006592\n",
      "Loss for batch 28 = 0.02424195222556591\n",
      "Loss for batch 29 = 0.1190471276640892\n",
      "Loss for batch 30 = 0.05279392749071121\n",
      "Loss for batch 31 = 0.43403682112693787\n",
      "Loss for batch 32 = 0.009926902130246162\n",
      "Loss for batch 33 = 0.010551312007009983\n",
      "Loss for batch 34 = 0.1517096906900406\n",
      "Loss for batch 35 = 0.010215934365987778\n",
      "Loss for batch 36 = 0.10486041009426117\n",
      "Loss for batch 37 = 0.10547733306884766\n",
      "Loss for batch 38 = 0.012788766995072365\n",
      "Loss for batch 39 = 0.012821651063859463\n",
      "Loss for batch 40 = 0.015052095986902714\n",
      "Loss for batch 41 = 0.007235589437186718\n",
      "Loss for batch 42 = 0.10346987843513489\n",
      "Loss for batch 43 = 0.2049711048603058\n",
      "Loss for batch 44 = 0.007528812158852816\n",
      "Loss for batch 45 = 0.011024228297173977\n",
      "Loss for batch 46 = 0.031906601041555405\n",
      "Loss for batch 47 = 0.007557438220828772\n",
      "Loss for batch 48 = 0.0251875352114439\n",
      "Loss for batch 49 = 0.01734163798391819\n",
      "Loss for batch 50 = 0.05058502033352852\n",
      "Loss for batch 51 = 0.15167121589183807\n",
      "Loss for batch 52 = 0.17990075051784515\n",
      "Loss for batch 53 = 0.014831188134849072\n",
      "Loss for batch 54 = 0.11352391541004181\n",
      "Loss for batch 55 = 0.013937776908278465\n",
      "Loss for batch 56 = 0.02959372289478779\n",
      "Loss for batch 57 = 0.023690253496170044\n",
      "Loss for batch 58 = 0.010654493235051632\n",
      "Loss for batch 59 = 0.008482662960886955\n",
      "Loss for batch 60 = 0.19912314414978027\n",
      "Loss for batch 61 = 0.015394876711070538\n",
      "Loss for batch 62 = 0.0065718647092580795\n",
      "Loss for batch 63 = 0.00965726375579834\n",
      "Loss for batch 64 = 0.006616841536015272\n",
      "Loss for batch 65 = 0.06298353523015976\n",
      "Loss for batch 66 = 0.15373343229293823\n",
      "Loss for batch 67 = 0.0081513412296772\n",
      "Loss for batch 68 = 0.007206812500953674\n",
      "Loss for batch 69 = 0.04780729487538338\n",
      "Loss for batch 70 = 0.015476050786674023\n",
      "Loss for batch 71 = 0.08573645353317261\n",
      "Loss for batch 72 = 0.16874851286411285\n",
      "Loss for batch 73 = 0.08547026664018631\n",
      "Loss for batch 74 = 0.22213000059127808\n",
      "Loss for batch 75 = 0.19391454756259918\n",
      "Loss for batch 76 = 0.1439991593360901\n",
      "Loss for batch 77 = 0.009542995132505894\n",
      "Loss for batch 78 = 0.12917017936706543\n",
      "Loss for batch 79 = 0.006622776389122009\n",
      "Loss for batch 80 = 0.008013502694666386\n",
      "Loss for batch 81 = 0.015692176297307014\n",
      "Loss for batch 82 = 0.011359691619873047\n",
      "Loss for batch 83 = 0.009717394597828388\n",
      "Loss for batch 84 = 0.0208886805921793\n",
      "Loss for batch 85 = 0.04576786234974861\n",
      "Loss for batch 86 = 0.01023823767900467\n",
      "Loss for batch 87 = 0.008314352482557297\n",
      "Loss for batch 88 = 0.014846919104456902\n",
      "Loss for batch 89 = 0.0599740631878376\n",
      "Loss for batch 90 = 0.17945319414138794\n",
      "Loss for batch 91 = 0.008305279538035393\n",
      "Loss for batch 92 = 0.0070142848417162895\n",
      "Loss for batch 93 = 0.006970669142901897\n",
      "Loss for batch 94 = 0.006753186695277691\n",
      "Loss for batch 95 = 0.015168541111052036\n",
      "Loss for batch 96 = 0.008794143795967102\n",
      "Loss for batch 97 = 0.1578909456729889\n",
      "Loss for batch 98 = 0.17021538317203522\n",
      "Loss for batch 99 = 0.006948783993721008\n",
      "Loss for batch 100 = 0.006232639774680138\n",
      "Loss for batch 101 = 0.009488925337791443\n",
      "Loss for batch 102 = 0.008444488048553467\n",
      "Loss for batch 103 = 0.07588237524032593\n",
      "Loss for batch 104 = 0.006901262793689966\n",
      "Loss for batch 105 = 0.04748208448290825\n",
      "Loss for batch 106 = 0.007828953675925732\n",
      "\n",
      "Training Loss for epoch 19 = 6.408347129821777\n",
      "\n",
      "Current Validation Loss = 24.800586700439453\n",
      "Best Validation Loss = 12.187006950378418\n",
      "Epochs without Improvement = 12\n",
      "Train Accuracy: 98.65%\n",
      "Validation Accuracy: 72.69%\n",
      "\n",
      "Epoch 20\n",
      "----------\n",
      "Loss for batch 0 = 0.030752960592508316\n",
      "Loss for batch 1 = 0.00631191860884428\n",
      "Loss for batch 2 = 0.004986794199794531\n",
      "Loss for batch 3 = 0.21865153312683105\n",
      "Loss for batch 4 = 0.144086092710495\n",
      "Loss for batch 5 = 0.04709066450595856\n",
      "Loss for batch 6 = 0.1739681214094162\n",
      "Loss for batch 7 = 0.007166357710957527\n",
      "Loss for batch 8 = 0.005209083668887615\n",
      "Loss for batch 9 = 0.12001077085733414\n",
      "Loss for batch 10 = 0.006223819684237242\n",
      "Loss for batch 11 = 0.006153683643788099\n",
      "Loss for batch 12 = 0.0756039097905159\n",
      "Loss for batch 13 = 0.20114347338676453\n",
      "Loss for batch 14 = 0.009380361996591091\n",
      "Loss for batch 15 = 0.20267213881015778\n",
      "Loss for batch 16 = 0.06701838225126266\n",
      "Loss for batch 17 = 0.009158567525446415\n",
      "Loss for batch 18 = 0.06692002713680267\n",
      "Loss for batch 19 = 0.011689533479511738\n",
      "Loss for batch 20 = 0.006411035545170307\n",
      "Loss for batch 21 = 0.0057504912838339806\n",
      "Loss for batch 22 = 0.16346776485443115\n",
      "Loss for batch 23 = 0.008530079387128353\n",
      "Loss for batch 24 = 0.008831449784338474\n",
      "Loss for batch 25 = 0.05318259820342064\n",
      "Loss for batch 26 = 0.006325915455818176\n",
      "Loss for batch 27 = 0.16435270011425018\n",
      "Loss for batch 28 = 0.013631646521389484\n",
      "Loss for batch 29 = 0.16930025815963745\n",
      "Loss for batch 30 = 0.05054129660129547\n",
      "Loss for batch 31 = 0.39966291189193726\n",
      "Loss for batch 32 = 0.009553005918860435\n",
      "Loss for batch 33 = 0.013831785880029202\n",
      "Loss for batch 34 = 0.16223551332950592\n",
      "Loss for batch 35 = 0.01762917824089527\n",
      "Loss for batch 36 = 0.006645031739026308\n",
      "Loss for batch 37 = 0.14644677937030792\n",
      "Loss for batch 38 = 0.014680927619338036\n",
      "Loss for batch 39 = 0.008395959623157978\n",
      "Loss for batch 40 = 0.024944880977272987\n",
      "Loss for batch 41 = 0.004720969591289759\n",
      "Loss for batch 42 = 0.051727645099163055\n",
      "Loss for batch 43 = 0.1990087926387787\n",
      "Loss for batch 44 = 0.005308244377374649\n",
      "Loss for batch 45 = 0.016032911837100983\n",
      "Loss for batch 46 = 0.024271158501505852\n",
      "Loss for batch 47 = 0.0057159652933478355\n",
      "Loss for batch 48 = 0.004975877236574888\n",
      "Loss for batch 49 = 0.017990076914429665\n",
      "Loss for batch 50 = 0.0373629629611969\n",
      "Loss for batch 51 = 0.16229794919490814\n",
      "Loss for batch 52 = 0.19346365332603455\n",
      "Loss for batch 53 = 0.030796894803643227\n",
      "Loss for batch 54 = 0.04225023463368416\n",
      "Loss for batch 55 = 0.011999797075986862\n",
      "Loss for batch 56 = 0.01943574845790863\n",
      "Loss for batch 57 = 0.017734024673700333\n",
      "Loss for batch 58 = 0.0049161347560584545\n",
      "Loss for batch 59 = 0.011017203330993652\n",
      "Loss for batch 60 = 0.19525502622127533\n",
      "Loss for batch 61 = 0.016738737002015114\n",
      "Loss for batch 62 = 0.009051001630723476\n",
      "Loss for batch 63 = 0.004665721673518419\n",
      "Loss for batch 64 = 0.005384543910622597\n",
      "Loss for batch 65 = 0.042797237634658813\n",
      "Loss for batch 66 = 0.1652040183544159\n",
      "Loss for batch 67 = 0.007550376933068037\n",
      "Loss for batch 68 = 0.005522988736629486\n",
      "Loss for batch 69 = 0.039876531809568405\n",
      "Loss for batch 70 = 0.00683694239705801\n",
      "Loss for batch 71 = 0.03573888540267944\n",
      "Loss for batch 72 = 0.16602495312690735\n",
      "Loss for batch 73 = 0.06495630741119385\n",
      "Loss for batch 74 = 0.1566651165485382\n",
      "Loss for batch 75 = 0.1914270669221878\n",
      "Loss for batch 76 = 0.08342695236206055\n",
      "Loss for batch 77 = 0.013561157509684563\n",
      "Loss for batch 78 = 0.10977955162525177\n",
      "Loss for batch 79 = 0.0060007101856172085\n",
      "Loss for batch 80 = 0.007267880719155073\n",
      "Loss for batch 81 = 0.02033066377043724\n",
      "Loss for batch 82 = 0.0075429268181324005\n",
      "Loss for batch 83 = 0.008813371881842613\n",
      "Loss for batch 84 = 0.009670127183198929\n",
      "Loss for batch 85 = 0.007820509374141693\n",
      "Loss for batch 86 = 0.0478452667593956\n",
      "Loss for batch 87 = 0.008654719218611717\n",
      "Loss for batch 88 = 0.006210670806467533\n",
      "Loss for batch 89 = 0.04344679042696953\n",
      "Loss for batch 90 = 0.16970711946487427\n",
      "Loss for batch 91 = 0.0171455517411232\n",
      "Loss for batch 92 = 0.0061902874149382114\n",
      "Loss for batch 93 = 0.0063772560097277164\n",
      "Loss for batch 94 = 0.005390499252825975\n",
      "Loss for batch 95 = 0.054664671421051025\n",
      "Loss for batch 96 = 0.006870459299534559\n",
      "Loss for batch 97 = 0.17168743908405304\n",
      "Loss for batch 98 = 0.1583171784877777\n",
      "Loss for batch 99 = 0.006445837207138538\n",
      "Loss for batch 100 = 0.006263186223804951\n",
      "Loss for batch 101 = 0.01161553617566824\n",
      "Loss for batch 102 = 0.008356492966413498\n",
      "Loss for batch 103 = 0.04288702458143234\n",
      "Loss for batch 104 = 0.006767194718122482\n",
      "Loss for batch 105 = 0.058832041919231415\n",
      "Loss for batch 106 = 0.015163257718086243\n",
      "\n",
      "Training Loss for epoch 20 = 5.976324558258057\n",
      "\n",
      "Current Validation Loss = 24.320819854736328\n",
      "Best Validation Loss = 12.187006950378418\n",
      "Epochs without Improvement = 13\n",
      "Train Accuracy: 98.88%\n",
      "Validation Accuracy: 72.48%\n",
      "\n",
      "Epoch 21\n",
      "----------\n",
      "Loss for batch 0 = 0.006159651093184948\n",
      "Loss for batch 1 = 0.009819874539971352\n",
      "Loss for batch 2 = 0.030514981597661972\n",
      "Loss for batch 3 = 0.2242261916399002\n",
      "Loss for batch 4 = 0.10264910757541656\n",
      "Loss for batch 5 = 0.04749872162938118\n",
      "Loss for batch 6 = 0.1691047102212906\n",
      "Loss for batch 7 = 0.00702192448079586\n",
      "Loss for batch 8 = 0.004393868148326874\n",
      "Loss for batch 9 = 0.010341311804950237\n",
      "Loss for batch 10 = 0.00562688522040844\n",
      "Loss for batch 11 = 0.005833255127072334\n",
      "Loss for batch 12 = 0.2604064643383026\n",
      "Loss for batch 13 = 0.19433997571468353\n",
      "Loss for batch 14 = 0.006024457514286041\n",
      "Loss for batch 15 = 0.16658733785152435\n",
      "Loss for batch 16 = 0.026139885187149048\n",
      "Loss for batch 17 = 0.006531057879328728\n",
      "Loss for batch 18 = 0.02774195186793804\n",
      "Loss for batch 19 = 0.007527648471295834\n",
      "Loss for batch 20 = 0.007590198423713446\n",
      "Loss for batch 21 = 0.007860730402171612\n",
      "Loss for batch 22 = 0.15429528057575226\n",
      "Loss for batch 23 = 0.006698561832308769\n",
      "Loss for batch 24 = 0.008280349895358086\n",
      "Loss for batch 25 = 0.0322423130273819\n",
      "Loss for batch 26 = 0.005931337829679251\n",
      "Loss for batch 27 = 0.16874274611473083\n",
      "Loss for batch 28 = 0.00639402074739337\n",
      "Loss for batch 29 = 0.041515104472637177\n",
      "Loss for batch 30 = 0.054884038865566254\n",
      "Loss for batch 31 = 0.36786389350891113\n",
      "Loss for batch 32 = 0.015561309643089771\n",
      "Loss for batch 33 = 0.014127064496278763\n",
      "Loss for batch 34 = 0.08397267013788223\n",
      "Loss for batch 35 = 0.007329096551984549\n",
      "Loss for batch 36 = 0.004951917100697756\n",
      "Loss for batch 37 = 0.0483645461499691\n",
      "Loss for batch 38 = 0.008041031658649445\n",
      "Loss for batch 39 = 0.006712579634040594\n",
      "Loss for batch 40 = 0.011005393229424953\n",
      "Loss for batch 41 = 0.0036798107903450727\n",
      "Loss for batch 42 = 0.03760681673884392\n",
      "Loss for batch 43 = 0.1983351856470108\n",
      "Loss for batch 44 = 0.022489158436655998\n",
      "Loss for batch 45 = 0.007098419591784477\n",
      "Loss for batch 46 = 0.03567495569586754\n",
      "Loss for batch 47 = 0.004275101236999035\n",
      "Loss for batch 48 = 0.004330895375460386\n",
      "Loss for batch 49 = 0.017358317971229553\n",
      "Loss for batch 50 = 0.02747415192425251\n",
      "Loss for batch 51 = 0.17760354280471802\n",
      "Loss for batch 52 = 0.18561552464962006\n",
      "Loss for batch 53 = 0.0260377936065197\n",
      "Loss for batch 54 = 0.02605711668729782\n",
      "Loss for batch 55 = 0.01024059858173132\n",
      "Loss for batch 56 = 0.011063027195632458\n",
      "Loss for batch 57 = 0.01620130054652691\n",
      "Loss for batch 58 = 0.007395302876830101\n",
      "Loss for batch 59 = 0.005236807744950056\n",
      "Loss for batch 60 = 0.18811467289924622\n",
      "Loss for batch 61 = 0.009700583294034004\n",
      "Loss for batch 62 = 0.003934341482818127\n",
      "Loss for batch 63 = 0.0037169428542256355\n",
      "Loss for batch 64 = 0.004466295707970858\n",
      "Loss for batch 65 = 0.1283656507730484\n",
      "Loss for batch 66 = 0.11029846966266632\n",
      "Loss for batch 67 = 0.033289942890405655\n",
      "Loss for batch 68 = 0.006812010891735554\n",
      "Loss for batch 69 = 0.09050525724887848\n",
      "Loss for batch 70 = 0.011571315117180347\n",
      "Loss for batch 71 = 0.08099691569805145\n",
      "Loss for batch 72 = 0.15941494703292847\n",
      "Loss for batch 73 = 0.06387187540531158\n",
      "Loss for batch 74 = 0.15183097124099731\n",
      "Loss for batch 75 = 0.1782238483428955\n",
      "Loss for batch 76 = 0.0744909942150116\n",
      "Loss for batch 77 = 0.018923072144389153\n",
      "Loss for batch 78 = 0.10511483252048492\n",
      "Loss for batch 79 = 0.007311291992664337\n",
      "Loss for batch 80 = 0.008493563160300255\n",
      "Loss for batch 81 = 0.050203267484903336\n",
      "Loss for batch 82 = 0.010116767138242722\n",
      "Loss for batch 83 = 0.015824593603610992\n",
      "Loss for batch 84 = 0.014135529287159443\n",
      "Loss for batch 85 = 0.007737174164503813\n",
      "Loss for batch 86 = 0.008577577769756317\n",
      "Loss for batch 87 = 0.00817671325057745\n",
      "Loss for batch 88 = 0.006715632043778896\n",
      "Loss for batch 89 = 0.04042919725179672\n",
      "Loss for batch 90 = 0.18748940527439117\n",
      "Loss for batch 91 = 0.03128138184547424\n",
      "Loss for batch 92 = 0.007424445822834969\n",
      "Loss for batch 93 = 0.006737939547747374\n",
      "Loss for batch 94 = 0.0042605274356901646\n",
      "Loss for batch 95 = 0.010546348989009857\n",
      "Loss for batch 96 = 0.006403688341379166\n",
      "Loss for batch 97 = 0.1536840945482254\n",
      "Loss for batch 98 = 0.1611126959323883\n",
      "Loss for batch 99 = 0.006734175607562065\n",
      "Loss for batch 100 = 0.007660590577870607\n",
      "Loss for batch 101 = 0.03889082744717598\n",
      "Loss for batch 102 = 0.0086327213793993\n",
      "Loss for batch 103 = 0.04328272491693497\n",
      "Loss for batch 104 = 0.006632089614868164\n",
      "Loss for batch 105 = 0.09917565435171127\n",
      "Loss for batch 106 = 0.0058457818813622\n",
      "\n",
      "Training Loss for epoch 21 = 5.591786861419678\n",
      "\n",
      "Current Validation Loss = 27.06476593017578\n",
      "Best Validation Loss = 12.187006950378418\n",
      "Epochs without Improvement = 14\n",
      "Train Accuracy: 98.74%\n",
      "Validation Accuracy: 71.25%\n",
      "\n",
      "Epoch 22\n",
      "----------\n",
      "Loss for batch 0 = 0.008572107180953026\n",
      "Loss for batch 1 = 0.00949967559427023\n",
      "Loss for batch 2 = 0.013010763563215733\n",
      "Loss for batch 3 = 0.22773227095603943\n",
      "Loss for batch 4 = 0.06326469779014587\n",
      "Loss for batch 5 = 0.051520198583602905\n",
      "Loss for batch 6 = 0.16705434024333954\n",
      "Loss for batch 7 = 0.012593502178788185\n",
      "Loss for batch 8 = 0.00448259711265564\n",
      "Loss for batch 9 = 0.009081629104912281\n",
      "Loss for batch 10 = 0.005755633115768433\n",
      "Loss for batch 11 = 0.005810005124658346\n",
      "Loss for batch 12 = 0.22187964618206024\n",
      "Loss for batch 13 = 0.18836843967437744\n",
      "Loss for batch 14 = 0.005823307204991579\n",
      "Loss for batch 15 = 0.13746005296707153\n",
      "Loss for batch 16 = 0.007746412418782711\n",
      "Loss for batch 17 = 0.006955889984965324\n",
      "Loss for batch 18 = 0.018468234688043594\n",
      "Loss for batch 19 = 0.0065498193725943565\n",
      "Loss for batch 20 = 0.006702490150928497\n",
      "Loss for batch 21 = 0.004397948272526264\n",
      "Loss for batch 22 = 0.15170788764953613\n",
      "Loss for batch 23 = 0.005838792771100998\n",
      "Loss for batch 24 = 0.007268313318490982\n",
      "Loss for batch 25 = 0.03524980694055557\n",
      "Loss for batch 26 = 0.005094017367810011\n",
      "Loss for batch 27 = 0.16869144141674042\n",
      "Loss for batch 28 = 0.01594654656946659\n",
      "Loss for batch 29 = 0.06117502972483635\n",
      "Loss for batch 30 = 0.09064903855323792\n",
      "Loss for batch 31 = 0.3606105148792267\n",
      "Loss for batch 32 = 0.008343980647623539\n",
      "Loss for batch 33 = 0.007820338942110538\n",
      "Loss for batch 34 = 0.1563127487897873\n",
      "Loss for batch 35 = 0.007788756862282753\n",
      "Loss for batch 36 = 0.006191601511090994\n",
      "Loss for batch 37 = 0.10252444446086884\n",
      "Loss for batch 38 = 0.010878072120249271\n",
      "Loss for batch 39 = 0.009115570224821568\n",
      "Loss for batch 40 = 0.016762420535087585\n",
      "Loss for batch 41 = 0.004540144931524992\n",
      "Loss for batch 42 = 0.03304333984851837\n",
      "Loss for batch 43 = 0.19828104972839355\n",
      "Loss for batch 44 = 0.005682547111064196\n",
      "Loss for batch 45 = 0.06888888031244278\n",
      "Loss for batch 46 = 0.012110823765397072\n",
      "Loss for batch 47 = 0.006467869970947504\n",
      "Loss for batch 48 = 0.00593170803040266\n",
      "Loss for batch 49 = 0.021867536008358\n",
      "Loss for batch 50 = 0.03857426717877388\n",
      "Loss for batch 51 = 0.1549306958913803\n",
      "Loss for batch 52 = 0.20992374420166016\n",
      "Loss for batch 53 = 0.006737795192748308\n",
      "Loss for batch 54 = 0.04334656894207001\n",
      "Loss for batch 55 = 0.011148524470627308\n",
      "Loss for batch 56 = 0.012830530293285847\n",
      "Loss for batch 57 = 0.024829814210534096\n",
      "Loss for batch 58 = 0.005594042129814625\n",
      "Loss for batch 59 = 0.010268285870552063\n",
      "Loss for batch 60 = 0.1883394718170166\n",
      "Loss for batch 61 = 0.008326799608767033\n",
      "Loss for batch 62 = 0.016386333853006363\n",
      "Loss for batch 63 = 0.006192357745021582\n",
      "Loss for batch 64 = 0.006881298031657934\n",
      "Loss for batch 65 = 0.07960470765829086\n",
      "Loss for batch 66 = 0.16976115107536316\n",
      "Loss for batch 67 = 0.013403447344899178\n",
      "Loss for batch 68 = 0.005172816105186939\n",
      "Loss for batch 69 = 0.0175859946757555\n",
      "Loss for batch 70 = 0.005253160372376442\n",
      "Loss for batch 71 = 0.026069367304444313\n",
      "Loss for batch 72 = 0.15273737907409668\n",
      "Loss for batch 73 = 0.04197237640619278\n",
      "Loss for batch 74 = 0.16643406450748444\n",
      "Loss for batch 75 = 0.1887536495923996\n",
      "Loss for batch 76 = 0.05178282782435417\n",
      "Loss for batch 77 = 0.00729388277977705\n",
      "Loss for batch 78 = 0.06950430572032928\n",
      "Loss for batch 79 = 0.00606473907828331\n",
      "Loss for batch 80 = 0.006766443606466055\n",
      "Loss for batch 81 = 0.024061664938926697\n",
      "Loss for batch 82 = 0.008211493492126465\n",
      "Loss for batch 83 = 0.018693914636969566\n",
      "Loss for batch 84 = 0.009061558172106743\n",
      "Loss for batch 85 = 0.00631610956043005\n",
      "Loss for batch 86 = 0.007363227661699057\n",
      "Loss for batch 87 = 0.006428910885006189\n",
      "Loss for batch 88 = 0.005394915118813515\n",
      "Loss for batch 89 = 0.03893344849348068\n",
      "Loss for batch 90 = 0.18942472338676453\n",
      "Loss for batch 91 = 0.00867373775690794\n",
      "Loss for batch 92 = 0.005462355446070433\n",
      "Loss for batch 93 = 0.006025811657309532\n",
      "Loss for batch 94 = 0.003495437791571021\n",
      "Loss for batch 95 = 0.06367479264736176\n",
      "Loss for batch 96 = 0.007365463301539421\n",
      "Loss for batch 97 = 0.16533856093883514\n",
      "Loss for batch 98 = 0.1477537900209427\n",
      "Loss for batch 99 = 0.010638565756380558\n",
      "Loss for batch 100 = 0.005474880803376436\n",
      "Loss for batch 101 = 0.016186654567718506\n",
      "Loss for batch 102 = 0.009560172446072102\n",
      "Loss for batch 103 = 0.03194115683436394\n",
      "Loss for batch 104 = 0.0069429329596459866\n",
      "Loss for batch 105 = 0.03322460874915123\n",
      "Loss for batch 106 = 0.018268417567014694\n",
      "\n",
      "Training Loss for epoch 22 = 5.373898506164551\n",
      "\n",
      "Current Validation Loss = 27.34474754333496\n",
      "Best Validation Loss = 12.187006950378418\n",
      "Epochs without Improvement = 15\n",
      "Train Accuracy: 99.00%\n",
      "Validation Accuracy: 71.46%\n",
      "\n",
      "Epoch 23\n",
      "----------\n",
      "Loss for batch 0 = 0.006951248738914728\n",
      "Loss for batch 1 = 0.021733447909355164\n",
      "Loss for batch 2 = 0.0045463009737432\n",
      "Loss for batch 3 = 0.22853894531726837\n",
      "Loss for batch 4 = 0.08350429683923721\n",
      "Loss for batch 5 = 0.06039610877633095\n",
      "Loss for batch 6 = 0.16668933629989624\n",
      "Loss for batch 7 = 0.0053080483339726925\n",
      "Loss for batch 8 = 0.0039790719747543335\n",
      "Loss for batch 9 = 0.010207860730588436\n",
      "Loss for batch 10 = 0.004847981501370668\n",
      "Loss for batch 11 = 0.007575266528874636\n",
      "Loss for batch 12 = 0.07097161561250687\n",
      "Loss for batch 13 = 0.21247044205665588\n",
      "Loss for batch 14 = 0.005908106453716755\n",
      "Loss for batch 15 = 0.19258277118206024\n",
      "Loss for batch 16 = 0.014627175405621529\n",
      "Loss for batch 17 = 0.0050326064229011536\n",
      "Loss for batch 18 = 0.009185748174786568\n",
      "Loss for batch 19 = 0.005157408304512501\n",
      "Loss for batch 20 = 0.007862092927098274\n",
      "Loss for batch 21 = 0.003424043068662286\n",
      "Loss for batch 22 = 0.13481344282627106\n",
      "Loss for batch 23 = 0.005018139258027077\n",
      "Loss for batch 24 = 0.007988951168954372\n",
      "Loss for batch 25 = 0.003206969704478979\n",
      "Loss for batch 26 = 0.004094724543392658\n",
      "Loss for batch 27 = 0.17118825018405914\n",
      "Loss for batch 28 = 0.009204648435115814\n",
      "Loss for batch 29 = 0.06517048180103302\n",
      "Loss for batch 30 = 0.05224347487092018\n",
      "Loss for batch 31 = 0.38316482305526733\n",
      "Loss for batch 32 = 0.008876337669789791\n",
      "Loss for batch 33 = 0.015942469239234924\n",
      "Loss for batch 34 = 0.16945478320121765\n",
      "Loss for batch 35 = 0.03155717998743057\n",
      "Loss for batch 36 = 0.005020733457058668\n",
      "Loss for batch 37 = 0.030900338664650917\n",
      "Loss for batch 38 = 0.010956786572933197\n",
      "Loss for batch 39 = 0.008545681834220886\n",
      "Loss for batch 40 = 0.023019559681415558\n",
      "Loss for batch 41 = 0.0033911974169313908\n",
      "Loss for batch 42 = 0.04722742736339569\n",
      "Loss for batch 43 = 0.20601023733615875\n",
      "Loss for batch 44 = 0.0088734682649374\n",
      "Loss for batch 45 = 0.006015719845890999\n",
      "Loss for batch 46 = 0.026523912325501442\n",
      "Loss for batch 47 = 0.004841668531298637\n",
      "Loss for batch 48 = 0.006472086068242788\n",
      "Loss for batch 49 = 0.008419805206358433\n",
      "Loss for batch 50 = 0.025740640237927437\n",
      "Loss for batch 51 = 0.1720922440290451\n",
      "Loss for batch 52 = 0.13002361357212067\n",
      "Loss for batch 53 = 0.004363420885056257\n",
      "Loss for batch 54 = 0.02237199805676937\n",
      "Loss for batch 55 = 0.007015736773610115\n",
      "Loss for batch 56 = 0.00996951200067997\n",
      "Loss for batch 57 = 0.026606908068060875\n",
      "Loss for batch 58 = 0.004150429740548134\n",
      "Loss for batch 59 = 0.00584799749776721\n",
      "Loss for batch 60 = 0.1887068897485733\n",
      "Loss for batch 61 = 0.0055980379693210125\n",
      "Loss for batch 62 = 0.0037298500537872314\n",
      "Loss for batch 63 = 0.005158737767487764\n",
      "Loss for batch 64 = 0.004877928178757429\n",
      "Loss for batch 65 = 0.03078162483870983\n",
      "Loss for batch 66 = 0.08434566110372543\n",
      "Loss for batch 67 = 0.004538921173661947\n",
      "Loss for batch 68 = 0.005035329610109329\n",
      "Loss for batch 69 = 0.019096707925200462\n",
      "Loss for batch 70 = 0.011249925009906292\n",
      "Loss for batch 71 = 0.07361847162246704\n",
      "Loss for batch 72 = 0.1435907781124115\n",
      "Loss for batch 73 = 0.11214126646518707\n",
      "Loss for batch 74 = 0.035927169024944305\n",
      "Loss for batch 75 = 0.2131669521331787\n",
      "Loss for batch 76 = 0.053000155836343765\n",
      "Loss for batch 77 = 0.007835060358047485\n",
      "Loss for batch 78 = 0.034699056297540665\n",
      "Loss for batch 79 = 0.0051664444617927074\n",
      "Loss for batch 80 = 0.004620123654603958\n",
      "Loss for batch 81 = 0.013593370094895363\n",
      "Loss for batch 82 = 0.0047043701633811\n",
      "Loss for batch 83 = 0.004268760327249765\n",
      "Loss for batch 84 = 0.005874954164028168\n",
      "Loss for batch 85 = 0.008905086666345596\n",
      "Loss for batch 86 = 0.004739017691463232\n",
      "Loss for batch 87 = 0.0063716634176671505\n",
      "Loss for batch 88 = 0.013306831009685993\n",
      "Loss for batch 89 = 0.02475564368069172\n",
      "Loss for batch 90 = 0.15356320142745972\n",
      "Loss for batch 91 = 0.005060533992946148\n",
      "Loss for batch 92 = 0.004138367250561714\n",
      "Loss for batch 93 = 0.006087802350521088\n",
      "Loss for batch 94 = 0.005014271475374699\n",
      "Loss for batch 95 = 0.0036886988673359156\n",
      "Loss for batch 96 = 0.004788782447576523\n",
      "Loss for batch 97 = 0.17894552648067474\n",
      "Loss for batch 98 = 0.31676390767097473\n",
      "Loss for batch 99 = 0.011521050706505775\n",
      "Loss for batch 100 = 0.004888865631073713\n",
      "Loss for batch 101 = 0.05812207609415054\n",
      "Loss for batch 102 = 0.006336227059364319\n",
      "Loss for batch 103 = 0.0277774166315794\n",
      "Loss for batch 104 = 0.0071525718085467815\n",
      "Loss for batch 105 = 0.027603503316640854\n",
      "Loss for batch 106 = 0.012872081249952316\n",
      "\n",
      "Training Loss for epoch 23 = 4.965554237365723\n",
      "\n",
      "Current Validation Loss = 29.852113723754883\n",
      "Best Validation Loss = 12.187006950378418\n",
      "Epochs without Improvement = 16\n",
      "Train Accuracy: 98.94%\n",
      "Validation Accuracy: 70.23%\n",
      "\n",
      "Epoch 24\n",
      "----------\n",
      "Loss for batch 0 = 0.006596799474209547\n",
      "Loss for batch 1 = 0.011540310457348824\n",
      "Loss for batch 2 = 0.0053426227532327175\n",
      "Loss for batch 3 = 0.2224029004573822\n",
      "Loss for batch 4 = 0.10945159941911697\n",
      "Loss for batch 5 = 0.05654098093509674\n",
      "Loss for batch 6 = 0.17058835923671722\n",
      "Loss for batch 7 = 0.008681723847985268\n",
      "Loss for batch 8 = 0.003917274065315723\n",
      "Loss for batch 9 = 0.03188466653227806\n",
      "Loss for batch 10 = 0.00463341549038887\n",
      "Loss for batch 11 = 0.015540710650384426\n",
      "Loss for batch 12 = 0.05015076324343681\n",
      "Loss for batch 13 = 0.16865812242031097\n",
      "Loss for batch 14 = 0.006899053696542978\n",
      "Loss for batch 15 = 0.18128040432929993\n",
      "Loss for batch 16 = 0.020992500707507133\n",
      "Loss for batch 17 = 0.00605953810736537\n",
      "Loss for batch 18 = 0.07270736247301102\n",
      "Loss for batch 19 = 0.006277164909988642\n",
      "Loss for batch 20 = 0.009320391342043877\n",
      "Loss for batch 21 = 0.003789479611441493\n",
      "Loss for batch 22 = 0.1148131787776947\n",
      "Loss for batch 23 = 0.005205056164413691\n",
      "Loss for batch 24 = 0.010717799887061119\n",
      "Loss for batch 25 = 0.0037610905710607767\n",
      "Loss for batch 26 = 0.003952081315219402\n",
      "Loss for batch 27 = 0.17661993205547333\n",
      "Loss for batch 28 = 0.01112691592425108\n",
      "Loss for batch 29 = 0.1555251181125641\n",
      "Loss for batch 30 = 0.04157974198460579\n",
      "Loss for batch 31 = 0.35895127058029175\n",
      "Loss for batch 32 = 0.007572770584374666\n",
      "Loss for batch 33 = 0.008475683629512787\n",
      "Loss for batch 34 = 0.17007184028625488\n",
      "Loss for batch 35 = 0.01576642133295536\n",
      "Loss for batch 36 = 0.006735408212989569\n",
      "Loss for batch 37 = 0.038393452763557434\n",
      "Loss for batch 38 = 0.021388066932559013\n",
      "Loss for batch 39 = 0.006780720315873623\n",
      "Loss for batch 40 = 0.022864796221256256\n",
      "Loss for batch 41 = 0.003873188979923725\n",
      "Loss for batch 42 = 0.042025648057460785\n",
      "Loss for batch 43 = 0.2081192433834076\n",
      "Loss for batch 44 = 0.006203009281307459\n",
      "Loss for batch 45 = 0.008003917522728443\n",
      "Loss for batch 46 = 0.01527478452771902\n",
      "Loss for batch 47 = 0.006428035907447338\n",
      "Loss for batch 48 = 0.0071126255206763744\n",
      "Loss for batch 49 = 0.013467113487422466\n",
      "Loss for batch 50 = 0.16657041013240814\n",
      "Loss for batch 51 = 0.17230172455310822\n",
      "Loss for batch 52 = 0.1024184450507164\n",
      "Loss for batch 53 = 0.006316235288977623\n",
      "Loss for batch 54 = 0.024095747619867325\n",
      "Loss for batch 55 = 0.022288592532277107\n",
      "Loss for batch 56 = 0.008937264792621136\n",
      "Loss for batch 57 = 0.011820130981504917\n",
      "Loss for batch 58 = 0.005871409550309181\n",
      "Loss for batch 59 = 0.00742732360959053\n",
      "Loss for batch 60 = 0.18861672282218933\n",
      "Loss for batch 61 = 0.007392682135105133\n",
      "Loss for batch 62 = 0.003495448036119342\n",
      "Loss for batch 63 = 0.004826831631362438\n",
      "Loss for batch 64 = 0.009219758212566376\n",
      "Loss for batch 65 = 0.0249969232827425\n",
      "Loss for batch 66 = 0.053714919835329056\n",
      "Loss for batch 67 = 0.00452303234487772\n",
      "Loss for batch 68 = 0.26797980070114136\n",
      "Loss for batch 69 = 0.007253032177686691\n",
      "Loss for batch 70 = 0.011362615041434765\n",
      "Loss for batch 71 = 0.015842145308852196\n",
      "Loss for batch 72 = 0.023424848914146423\n",
      "Loss for batch 73 = 0.042766887694597244\n",
      "Loss for batch 74 = 0.142287477850914\n",
      "Loss for batch 75 = 0.20049534738063812\n",
      "Loss for batch 76 = 0.026749078184366226\n",
      "Loss for batch 77 = 0.0046478742733597755\n",
      "Loss for batch 78 = 0.019434679299592972\n",
      "Loss for batch 79 = 0.0052799684926867485\n",
      "Loss for batch 80 = 0.004674247466027737\n",
      "Loss for batch 81 = 0.0050160945393145084\n",
      "Loss for batch 82 = 0.005090327933430672\n",
      "Loss for batch 83 = 0.0050847395323216915\n",
      "Loss for batch 84 = 0.005118710920214653\n",
      "Loss for batch 85 = 0.006936471909284592\n",
      "Loss for batch 86 = 0.004148254636675119\n",
      "Loss for batch 87 = 0.004210543353110552\n",
      "Loss for batch 88 = 0.0037387118209153414\n",
      "Loss for batch 89 = 0.02294623851776123\n",
      "Loss for batch 90 = 0.13244566321372986\n",
      "Loss for batch 91 = 0.006871469784528017\n",
      "Loss for batch 92 = 0.004248665180057287\n",
      "Loss for batch 93 = 0.0042121014557778835\n",
      "Loss for batch 94 = 0.0028787986375391483\n",
      "Loss for batch 95 = 0.02400076948106289\n",
      "Loss for batch 96 = 0.006534225773066282\n",
      "Loss for batch 97 = 0.18243971467018127\n",
      "Loss for batch 98 = 0.10426514595746994\n",
      "Loss for batch 99 = 0.005242360755801201\n",
      "Loss for batch 100 = 0.004023473709821701\n",
      "Loss for batch 101 = 0.009308618493378162\n",
      "Loss for batch 102 = 0.006153169088065624\n",
      "Loss for batch 103 = 0.023160111159086227\n",
      "Loss for batch 104 = 0.010493689216673374\n",
      "Loss for batch 105 = 0.01662258990108967\n",
      "Loss for batch 106 = 0.014493695460259914\n",
      "\n",
      "Training Loss for epoch 24 = 4.8807806968688965\n",
      "\n",
      "Current Validation Loss = 25.80914306640625\n",
      "Best Validation Loss = 12.187006950378418\n",
      "Epochs without Improvement = 17\n",
      "Train Accuracy: 99.03%\n",
      "Validation Accuracy: 70.23%\n",
      "\n",
      "Epoch 25\n",
      "----------\n",
      "Loss for batch 0 = 0.0042563509196043015\n",
      "Loss for batch 1 = 0.008628267794847488\n",
      "Loss for batch 2 = 0.006970349233597517\n",
      "Loss for batch 3 = 0.22554625570774078\n",
      "Loss for batch 4 = 0.0942888855934143\n",
      "Loss for batch 5 = 0.06435012072324753\n",
      "Loss for batch 6 = 0.17117592692375183\n",
      "Loss for batch 7 = 0.022236373275518417\n",
      "Loss for batch 8 = 0.05773073807358742\n",
      "Loss for batch 9 = 0.008052634075284004\n",
      "Loss for batch 10 = 0.0041394163854420185\n",
      "Loss for batch 11 = 0.004728125873953104\n",
      "Loss for batch 12 = 0.08028601855039597\n",
      "Loss for batch 13 = 0.11979831010103226\n",
      "Loss for batch 14 = 0.007813334465026855\n",
      "Loss for batch 15 = 0.17776887118816376\n",
      "Loss for batch 16 = 0.02589675784111023\n",
      "Loss for batch 17 = 0.005275601986795664\n",
      "Loss for batch 18 = 0.010536991991102695\n",
      "Loss for batch 19 = 0.006507751997560263\n",
      "Loss for batch 20 = 0.0053543103858828545\n",
      "Loss for batch 21 = 0.00721419882029295\n",
      "Loss for batch 22 = 0.10219809412956238\n",
      "Loss for batch 23 = 0.004799249581992626\n",
      "Loss for batch 24 = 0.10798729211091995\n",
      "Loss for batch 25 = 0.004098278004676104\n",
      "Loss for batch 26 = 0.003489759052172303\n",
      "Loss for batch 27 = 0.1787780523300171\n",
      "Loss for batch 28 = 0.0039005379658192396\n",
      "Loss for batch 29 = 0.02329959161579609\n",
      "Loss for batch 30 = 0.01545169111341238\n",
      "Loss for batch 31 = 0.2939267158508301\n",
      "Loss for batch 32 = 0.006803437136113644\n",
      "Loss for batch 33 = 0.0091808857396245\n",
      "Loss for batch 34 = 0.15776854753494263\n",
      "Loss for batch 35 = 0.04083128646016121\n",
      "Loss for batch 36 = 0.004975888412445784\n",
      "Loss for batch 37 = 0.13145452737808228\n",
      "Loss for batch 38 = 0.009578523226082325\n",
      "Loss for batch 39 = 0.006172046530991793\n",
      "Loss for batch 40 = 0.006050826981663704\n",
      "Loss for batch 41 = 0.004296588245779276\n",
      "Loss for batch 42 = 0.021512052044272423\n",
      "Loss for batch 43 = 0.2376011461019516\n",
      "Loss for batch 44 = 0.010654349811375141\n",
      "Loss for batch 45 = 0.006402071565389633\n",
      "Loss for batch 46 = 0.16717305779457092\n",
      "Loss for batch 47 = 0.006518683396279812\n",
      "Loss for batch 48 = 0.006210364867001772\n",
      "Loss for batch 49 = 0.018541991710662842\n",
      "Loss for batch 50 = 0.016854675486683846\n",
      "Loss for batch 51 = 0.17247049510478973\n",
      "Loss for batch 52 = 0.08902275562286377\n",
      "Loss for batch 53 = 0.005766920745372772\n",
      "Loss for batch 54 = 0.02763201668858528\n",
      "Loss for batch 55 = 0.0061579798348248005\n",
      "Loss for batch 56 = 0.10335935652256012\n",
      "Loss for batch 57 = 0.14160382747650146\n",
      "Loss for batch 58 = 0.006115978583693504\n",
      "Loss for batch 59 = 0.00955921970307827\n",
      "Loss for batch 60 = 0.1947064846754074\n",
      "Loss for batch 61 = 0.006666144356131554\n",
      "Loss for batch 62 = 0.0034437859430909157\n",
      "Loss for batch 63 = 0.00573504576459527\n",
      "Loss for batch 64 = 0.007228604052215815\n",
      "Loss for batch 65 = 0.04471937566995621\n",
      "Loss for batch 66 = 0.014843943528831005\n",
      "Loss for batch 67 = 0.0054721347987651825\n",
      "Loss for batch 68 = 0.15328136086463928\n",
      "Loss for batch 69 = 0.06643594801425934\n",
      "Loss for batch 70 = 0.008256976492702961\n",
      "Loss for batch 71 = 0.012680403888225555\n",
      "Loss for batch 72 = 0.24926334619522095\n",
      "Loss for batch 73 = 0.02660108543932438\n",
      "Loss for batch 74 = 0.03827837109565735\n",
      "Loss for batch 75 = 0.20894263684749603\n",
      "Loss for batch 76 = 0.01587737537920475\n",
      "Loss for batch 77 = 0.005519549828022718\n",
      "Loss for batch 78 = 0.013729344122111797\n",
      "Loss for batch 79 = 0.005376005079597235\n",
      "Loss for batch 80 = 0.004184287507086992\n",
      "Loss for batch 81 = 0.07942498475313187\n",
      "Loss for batch 82 = 0.004675331525504589\n",
      "Loss for batch 83 = 0.006422671023756266\n",
      "Loss for batch 84 = 0.005081412848085165\n",
      "Loss for batch 85 = 0.007378309033811092\n",
      "Loss for batch 86 = 0.00996587984263897\n",
      "Loss for batch 87 = 0.007062449585646391\n",
      "Loss for batch 88 = 0.004331169184297323\n",
      "Loss for batch 89 = 0.022018559277057648\n",
      "Loss for batch 90 = 0.1331476867198944\n",
      "Loss for batch 91 = 0.03138614818453789\n",
      "Loss for batch 92 = 0.006127879023551941\n",
      "Loss for batch 93 = 0.003814113326370716\n",
      "Loss for batch 94 = 0.004104606807231903\n",
      "Loss for batch 95 = 0.010958664119243622\n",
      "Loss for batch 96 = 0.006509261671453714\n",
      "Loss for batch 97 = 0.19836841523647308\n",
      "Loss for batch 98 = 0.1047937199473381\n",
      "Loss for batch 99 = 0.005533016286790371\n",
      "Loss for batch 100 = 0.004623636603355408\n",
      "Loss for batch 101 = 0.02208796516060829\n",
      "Loss for batch 102 = 0.005065948702394962\n",
      "Loss for batch 103 = 0.023524651303887367\n",
      "Loss for batch 104 = 0.008658166974782944\n",
      "Loss for batch 105 = 0.02188987471163273\n",
      "Loss for batch 106 = 0.013431395404040813\n",
      "\n",
      "Training Loss for epoch 25 = 5.128381729125977\n",
      "\n",
      "Current Validation Loss = 25.40517807006836\n",
      "Best Validation Loss = 12.187006950378418\n",
      "Epochs without Improvement = 18\n",
      "Train Accuracy: 99.12%\n",
      "Validation Accuracy: 70.23%\n",
      "\n",
      "Epoch 26\n",
      "----------\n",
      "Loss for batch 0 = 0.0047697387635707855\n",
      "Loss for batch 1 = 0.008262274786829948\n",
      "Loss for batch 2 = 0.008057348430156708\n",
      "Loss for batch 3 = 0.23409733176231384\n",
      "Loss for batch 4 = 0.07302864640951157\n",
      "Loss for batch 5 = 0.07466810196638107\n",
      "Loss for batch 6 = 0.17358769476413727\n",
      "Loss for batch 7 = 0.004781858995556831\n",
      "Loss for batch 8 = 0.003135891631245613\n",
      "Loss for batch 9 = 0.01381051354110241\n",
      "Loss for batch 10 = 0.004626931622624397\n",
      "Loss for batch 11 = 0.005966169759631157\n",
      "Loss for batch 12 = 0.038985900580883026\n",
      "Loss for batch 13 = 0.05717933177947998\n",
      "Loss for batch 14 = 0.021866798400878906\n",
      "Loss for batch 15 = 0.1229524314403534\n",
      "Loss for batch 16 = 0.013660936616361141\n",
      "Loss for batch 17 = 0.004461050499230623\n",
      "Loss for batch 18 = 0.005716245621442795\n",
      "Loss for batch 19 = 0.005585752427577972\n",
      "Loss for batch 20 = 0.053393639624118805\n",
      "Loss for batch 21 = 0.0028808526694774628\n",
      "Loss for batch 22 = 0.09166531264781952\n",
      "Loss for batch 23 = 0.004741298966109753\n",
      "Loss for batch 24 = 0.009328792802989483\n",
      "Loss for batch 25 = 0.0029545400757342577\n",
      "Loss for batch 26 = 0.002993627917021513\n",
      "Loss for batch 27 = 0.17884165048599243\n",
      "Loss for batch 28 = 0.0028000979218631983\n",
      "Loss for batch 29 = 0.0127205029129982\n",
      "Loss for batch 30 = 0.018702175468206406\n",
      "Loss for batch 31 = 0.363839715719223\n",
      "Loss for batch 32 = 0.005908307619392872\n",
      "Loss for batch 33 = 0.004596509039402008\n",
      "Loss for batch 34 = 0.02791154757142067\n",
      "Loss for batch 35 = 0.0070066554471850395\n",
      "Loss for batch 36 = 0.002965342253446579\n",
      "Loss for batch 37 = 0.1307673156261444\n",
      "Loss for batch 38 = 0.00689005758613348\n",
      "Loss for batch 39 = 0.004492699634283781\n",
      "Loss for batch 40 = 0.00641722371801734\n",
      "Loss for batch 41 = 0.0027235993184149265\n",
      "Loss for batch 42 = 0.026651574298739433\n",
      "Loss for batch 43 = 0.21501080691814423\n",
      "Loss for batch 44 = 0.003067675745114684\n",
      "Loss for batch 45 = 0.024865856394171715\n",
      "Loss for batch 46 = 0.009569422341883183\n",
      "Loss for batch 47 = 0.006393583957105875\n",
      "Loss for batch 48 = 0.007299988996237516\n",
      "Loss for batch 49 = 0.008936859667301178\n",
      "Loss for batch 50 = 0.019773568958044052\n",
      "Loss for batch 51 = 0.16832350194454193\n",
      "Loss for batch 52 = 0.10128583014011383\n",
      "Loss for batch 53 = 0.00531862024217844\n",
      "Loss for batch 54 = 0.0175554808229208\n",
      "Loss for batch 55 = 0.005127481650561094\n",
      "Loss for batch 56 = 0.011471026577055454\n",
      "Loss for batch 57 = 0.02096823789179325\n",
      "Loss for batch 58 = 0.0074853841215372086\n",
      "Loss for batch 59 = 0.009895727969706059\n",
      "Loss for batch 60 = 0.1974416822195053\n",
      "Loss for batch 61 = 0.005377990659326315\n",
      "Loss for batch 62 = 0.0036402528639882803\n",
      "Loss for batch 63 = 0.004014177713543177\n",
      "Loss for batch 64 = 0.0038992860354483128\n",
      "Loss for batch 65 = 0.016615191474556923\n",
      "Loss for batch 66 = 0.010900833643972874\n",
      "Loss for batch 67 = 0.0037088075187057257\n",
      "Loss for batch 68 = 0.024654751643538475\n",
      "Loss for batch 69 = 0.01137076411396265\n",
      "Loss for batch 70 = 0.00479644350707531\n",
      "Loss for batch 71 = 0.0074247089214622974\n",
      "Loss for batch 72 = 0.14020943641662598\n",
      "Loss for batch 73 = 0.03294558823108673\n",
      "Loss for batch 74 = 0.07096463441848755\n",
      "Loss for batch 75 = 0.1922486275434494\n",
      "Loss for batch 76 = 0.021282978355884552\n",
      "Loss for batch 77 = 0.004560855682939291\n",
      "Loss for batch 78 = 0.012320950627326965\n",
      "Loss for batch 79 = 0.006707449443638325\n",
      "Loss for batch 80 = 0.003683410817757249\n",
      "Loss for batch 81 = 0.011436229571700096\n",
      "Loss for batch 82 = 0.005563589744269848\n",
      "Loss for batch 83 = 0.005869591608643532\n",
      "Loss for batch 84 = 0.004330895375460386\n",
      "Loss for batch 85 = 0.003463936038315296\n",
      "Loss for batch 86 = 0.004280186258256435\n",
      "Loss for batch 87 = 0.004123007412999868\n",
      "Loss for batch 88 = 0.0030690983403474092\n",
      "Loss for batch 89 = 0.008624967187643051\n",
      "Loss for batch 90 = 0.10297634452581406\n",
      "Loss for batch 91 = 0.09983852505683899\n",
      "Loss for batch 92 = 0.003664072137326002\n",
      "Loss for batch 93 = 0.003663323586806655\n",
      "Loss for batch 94 = 0.0037227575667202473\n",
      "Loss for batch 95 = 0.004683848470449448\n",
      "Loss for batch 96 = 0.007643763907253742\n",
      "Loss for batch 97 = 0.19174212217330933\n",
      "Loss for batch 98 = 0.09130825102329254\n",
      "Loss for batch 99 = 0.003910480532795191\n",
      "Loss for batch 100 = 0.005547851789742708\n",
      "Loss for batch 101 = 0.00543893501162529\n",
      "Loss for batch 102 = 0.005144686438143253\n",
      "Loss for batch 103 = 0.013679807074368\n",
      "Loss for batch 104 = 0.010364828631281853\n",
      "Loss for batch 105 = 0.021028820425271988\n",
      "Loss for batch 106 = 0.01332781370729208\n",
      "\n",
      "Training Loss for epoch 26 = 3.909958839416504\n",
      "\n",
      "Current Validation Loss = 27.149797439575195\n",
      "Best Validation Loss = 12.187006950378418\n",
      "Epochs without Improvement = 19\n",
      "Train Accuracy: 98.62%\n",
      "Validation Accuracy: 67.97%\n",
      "\n",
      "Epoch 27\n",
      "----------\n",
      "Loss for batch 0 = 0.0085372906178236\n",
      "Loss for batch 1 = 0.016009705141186714\n",
      "Loss for batch 2 = 0.20906199514865875\n",
      "Loss for batch 3 = 0.2124791294336319\n",
      "Loss for batch 4 = 0.12683305144309998\n",
      "Loss for batch 5 = 0.07690321654081345\n",
      "Loss for batch 6 = 0.1729888767004013\n",
      "Loss for batch 7 = 0.024775158613920212\n",
      "Loss for batch 8 = 0.004890247248113155\n",
      "Loss for batch 9 = 0.011894403956830502\n",
      "Loss for batch 10 = 0.014279728755354881\n",
      "Loss for batch 11 = 0.05686786770820618\n",
      "Loss for batch 12 = 0.029121369123458862\n",
      "Loss for batch 13 = 0.07052920758724213\n",
      "Loss for batch 14 = 0.008091907948255539\n",
      "Loss for batch 15 = 0.0986415445804596\n",
      "Loss for batch 16 = 0.005619425792247057\n",
      "Loss for batch 17 = 0.003848258638754487\n",
      "Loss for batch 18 = 0.00492678489536047\n",
      "Loss for batch 19 = 0.006572534795850515\n",
      "Loss for batch 20 = 0.0073010800406336784\n",
      "Loss for batch 21 = 0.0033521857112646103\n",
      "Loss for batch 22 = 0.08973342925310135\n",
      "Loss for batch 23 = 0.004017332103103399\n",
      "Loss for batch 24 = 0.00919044017791748\n",
      "Loss for batch 25 = 0.005043624900281429\n",
      "Loss for batch 26 = 0.005587340332567692\n",
      "Loss for batch 27 = 0.1751628965139389\n",
      "Loss for batch 28 = 0.003861931385472417\n",
      "Loss for batch 29 = 0.011369744315743446\n",
      "Loss for batch 30 = 0.011369922198355198\n",
      "Loss for batch 31 = 0.290539026260376\n",
      "Loss for batch 32 = 0.005509871989488602\n",
      "Loss for batch 33 = 0.005108687561005354\n",
      "Loss for batch 34 = 0.10982109606266022\n",
      "Loss for batch 35 = 0.014012729749083519\n",
      "Loss for batch 36 = 0.0034895907156169415\n",
      "Loss for batch 37 = 0.04256587103009224\n",
      "Loss for batch 38 = 0.013063223101198673\n",
      "Loss for batch 39 = 0.005362660624086857\n",
      "Loss for batch 40 = 0.23275242745876312\n",
      "Loss for batch 41 = 0.002479123417288065\n",
      "Loss for batch 42 = 0.014323239214718342\n",
      "Loss for batch 43 = 0.21573112905025482\n",
      "Loss for batch 44 = 0.004390940070152283\n",
      "Loss for batch 45 = 0.004415746312588453\n",
      "Loss for batch 46 = 0.009026897139847279\n",
      "Loss for batch 47 = 0.005128865595906973\n",
      "Loss for batch 48 = 0.005877490155398846\n",
      "Loss for batch 49 = 0.00688081094995141\n",
      "Loss for batch 50 = 0.015050954185426235\n",
      "Loss for batch 51 = 0.16867408156394958\n",
      "Loss for batch 52 = 0.0735476091504097\n",
      "Loss for batch 53 = 0.002729069674387574\n",
      "Loss for batch 54 = 0.02090088650584221\n",
      "Loss for batch 55 = 0.004032312426716089\n",
      "Loss for batch 56 = 0.008949416689574718\n",
      "Loss for batch 57 = 0.022272856906056404\n",
      "Loss for batch 58 = 0.0069811418652534485\n",
      "Loss for batch 59 = 0.009399168193340302\n",
      "Loss for batch 60 = 0.19452881813049316\n",
      "Loss for batch 61 = 0.004430882632732391\n",
      "Loss for batch 62 = 0.0030693511944264174\n",
      "Loss for batch 63 = 0.0035168910399079323\n",
      "Loss for batch 64 = 0.003467939095571637\n",
      "Loss for batch 65 = 0.019197218120098114\n",
      "Loss for batch 66 = 0.03090173751115799\n",
      "Loss for batch 67 = 0.003395814448595047\n",
      "Loss for batch 68 = 0.003426732262596488\n",
      "Loss for batch 69 = 0.01296525914222002\n",
      "Loss for batch 70 = 0.004770760424435139\n",
      "Loss for batch 71 = 0.01742478460073471\n",
      "Loss for batch 72 = 0.013502819463610649\n",
      "Loss for batch 73 = 0.019126297906041145\n",
      "Loss for batch 74 = 0.020955059677362442\n",
      "Loss for batch 75 = 0.19847403466701508\n",
      "Loss for batch 76 = 0.021421413868665695\n",
      "Loss for batch 77 = 0.06578455120325089\n",
      "Loss for batch 78 = 0.012599757872521877\n",
      "Loss for batch 79 = 0.005232958123087883\n",
      "Loss for batch 80 = 0.0031738977413624525\n",
      "Loss for batch 81 = 0.1033371090888977\n",
      "Loss for batch 82 = 0.003309532767161727\n",
      "Loss for batch 83 = 0.002961551072075963\n",
      "Loss for batch 84 = 0.0030254279263317585\n",
      "Loss for batch 85 = 0.003275864524766803\n",
      "Loss for batch 86 = 0.0027882212307304144\n",
      "Loss for batch 87 = 0.00307160010561347\n",
      "Loss for batch 88 = 0.002591690281406045\n",
      "Loss for batch 89 = 0.011231175623834133\n",
      "Loss for batch 90 = 0.11201134324073792\n",
      "Loss for batch 91 = 0.002574359066784382\n",
      "Loss for batch 92 = 0.002789590507745743\n",
      "Loss for batch 93 = 0.002958994358778\n",
      "Loss for batch 94 = 0.002008071169257164\n",
      "Loss for batch 95 = 0.0025214150082319975\n",
      "Loss for batch 96 = 0.005652394611388445\n",
      "Loss for batch 97 = 0.19656039774417877\n",
      "Loss for batch 98 = 0.09545864909887314\n",
      "Loss for batch 99 = 0.0028433233965188265\n",
      "Loss for batch 100 = 0.00232054409570992\n",
      "Loss for batch 101 = 0.006686644162982702\n",
      "Loss for batch 102 = 0.00353650888428092\n",
      "Loss for batch 103 = 0.01223174761980772\n",
      "Loss for batch 104 = 0.007530623581260443\n",
      "Loss for batch 105 = 0.03310815989971161\n",
      "Loss for batch 106 = 0.01274376641958952\n",
      "\n",
      "Training Loss for epoch 27 = 4.112375259399414\n",
      "\n",
      "Current Validation Loss = 26.384796142578125\n",
      "Best Validation Loss = 12.187006950378418\n",
      "Epochs without Improvement = 20\n",
      "Train Accuracy: 99.27%\n",
      "Validation Accuracy: 70.64%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TrainLoop(modelv1, optimizer, loss_fn, train_loader, val_loader, scheduler, 100, 20, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 70.74%\n"
     ]
    }
   ],
   "source": [
    "test_set = CustomTextClassificationDataset(test_x, test_y, word_to_index)\n",
    "test_loader = DataLoader(test_set, 32)\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "with torch.inference_mode():\n",
    "    for batch in test_loader:\n",
    "        caption_tokens = batch['text_indices'].to('cuda')\n",
    "        labels = batch['label'].to('cuda', dtype=torch.long)\n",
    "        preds = torch.argmax(modelv1(caption_tokens), dim=1)\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        pred_labels.extend(preds.cpu().numpy())\n",
    "\n",
    "true_labels = np.array(true_labels)\n",
    "pred_labels = np.array(pred_labels)\n",
    "\n",
    "modelv1_accuracy = accuracy_score(true_labels, pred_labels)\n",
    "\n",
    "print(f'Accuracy: {modelv1_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(modelv1.state_dict(), 'lstm11.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelv2 = LSTMmodel(3, 25, 256, bidirectionality=True)\n",
    "loss_fn_2 = torch.nn.NLLLoss()\n",
    "optimizer_2 = torch.optim.NAdam(modelv2.parameters(), 0.002)\n",
    "scheduler_2 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_2, 'min', 0.3, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33572893"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in modelv2.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3636de7d354420d8aa7e757c2cd1094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0\n",
      "----------\n",
      "Loss for batch 0 = 1.0917292833328247\n",
      "Loss for batch 1 = 1.134045958518982\n",
      "Loss for batch 2 = 1.0867518186569214\n",
      "Loss for batch 3 = 1.1193300485610962\n",
      "Loss for batch 4 = 1.1089468002319336\n",
      "Loss for batch 5 = 1.1012405157089233\n",
      "Loss for batch 6 = 1.1010876893997192\n",
      "Loss for batch 7 = 1.0839693546295166\n",
      "Loss for batch 8 = 1.104689598083496\n",
      "Loss for batch 9 = 1.084059476852417\n",
      "Loss for batch 10 = 1.0884870290756226\n",
      "Loss for batch 11 = 1.0791072845458984\n",
      "Loss for batch 12 = 1.0685181617736816\n",
      "Loss for batch 13 = 1.5048125982284546\n",
      "Loss for batch 14 = 1.1129525899887085\n",
      "Loss for batch 15 = 1.0836416482925415\n",
      "Loss for batch 16 = 1.1113070249557495\n",
      "Loss for batch 17 = 1.071638584136963\n",
      "Loss for batch 18 = 1.124261498451233\n",
      "Loss for batch 19 = 1.0869672298431396\n",
      "Loss for batch 20 = 1.110395908355713\n",
      "Loss for batch 21 = 1.1025261878967285\n",
      "Loss for batch 22 = 1.0979725122451782\n",
      "Loss for batch 23 = 1.0856883525848389\n",
      "Loss for batch 24 = 1.0725948810577393\n",
      "Loss for batch 25 = 1.0996898412704468\n",
      "Loss for batch 26 = 1.0476539134979248\n",
      "Loss for batch 27 = 1.0903834104537964\n",
      "Loss for batch 28 = 1.0860581398010254\n",
      "Loss for batch 29 = 1.1315138339996338\n",
      "Loss for batch 30 = 1.0913207530975342\n",
      "Loss for batch 31 = 1.0909491777420044\n",
      "Loss for batch 32 = 1.0983082056045532\n",
      "Loss for batch 33 = 1.0727009773254395\n",
      "Loss for batch 34 = 1.0054882764816284\n",
      "Loss for batch 35 = 1.2025794982910156\n",
      "Loss for batch 36 = 1.0692609548568726\n",
      "Loss for batch 37 = 1.0706270933151245\n",
      "Loss for batch 38 = 1.1199167966842651\n",
      "Loss for batch 39 = 1.0472525358200073\n",
      "Loss for batch 40 = 1.075153112411499\n",
      "Loss for batch 41 = 1.1359925270080566\n",
      "Loss for batch 42 = 1.075750708580017\n",
      "Loss for batch 43 = 1.0940810441970825\n",
      "Loss for batch 44 = 1.0885194540023804\n",
      "Loss for batch 45 = 1.0524169206619263\n",
      "Loss for batch 46 = 1.0429909229278564\n",
      "Loss for batch 47 = 1.0714497566223145\n",
      "Loss for batch 48 = 1.0527507066726685\n",
      "Loss for batch 49 = 1.1141608953475952\n",
      "Loss for batch 50 = 1.1171435117721558\n",
      "Loss for batch 51 = 1.0557321310043335\n",
      "Loss for batch 52 = 1.0539191961288452\n",
      "Loss for batch 53 = 1.0338642597198486\n",
      "Loss for batch 54 = 1.0464121103286743\n",
      "Loss for batch 55 = 1.1197625398635864\n",
      "Loss for batch 56 = 1.122117042541504\n",
      "Loss for batch 57 = 1.0640734434127808\n",
      "Loss for batch 58 = 1.1187160015106201\n",
      "Loss for batch 59 = 1.0581958293914795\n",
      "Loss for batch 60 = 1.017338752746582\n",
      "Loss for batch 61 = 1.0448477268218994\n",
      "Loss for batch 62 = 1.1131869554519653\n",
      "Loss for batch 63 = 1.087134599685669\n",
      "Loss for batch 64 = 1.056626558303833\n",
      "Loss for batch 65 = 1.1419546604156494\n",
      "Loss for batch 66 = 1.1013802289962769\n",
      "Loss for batch 67 = 1.0649522542953491\n",
      "Loss for batch 68 = 1.0159318447113037\n",
      "Loss for batch 69 = 0.959312915802002\n",
      "Loss for batch 70 = 1.110719084739685\n",
      "Loss for batch 71 = 1.0759567022323608\n",
      "Loss for batch 72 = 1.0915827751159668\n",
      "Loss for batch 73 = 0.9572511911392212\n",
      "Loss for batch 74 = 1.3673800230026245\n",
      "Loss for batch 75 = 1.0774489641189575\n",
      "Loss for batch 76 = 1.1825889348983765\n",
      "Loss for batch 77 = 1.058896780014038\n",
      "Loss for batch 78 = 1.1429613828659058\n",
      "Loss for batch 79 = 1.1167478561401367\n",
      "Loss for batch 80 = 1.0809658765792847\n",
      "Loss for batch 81 = 1.082621693611145\n",
      "Loss for batch 82 = 1.089906930923462\n",
      "Loss for batch 83 = 1.0791128873825073\n",
      "Loss for batch 84 = 1.0799007415771484\n",
      "Loss for batch 85 = 1.1792516708374023\n",
      "Loss for batch 86 = 1.0711524486541748\n",
      "Loss for batch 87 = 1.0424392223358154\n",
      "Loss for batch 88 = 1.1001224517822266\n",
      "Loss for batch 89 = 1.0628705024719238\n",
      "Loss for batch 90 = 1.0762302875518799\n",
      "Loss for batch 91 = 1.029104232788086\n",
      "Loss for batch 92 = 1.0520601272583008\n",
      "Loss for batch 93 = 1.083229899406433\n",
      "Loss for batch 94 = 1.076505422592163\n",
      "Loss for batch 95 = 1.1220431327819824\n",
      "Loss for batch 96 = 1.056235671043396\n",
      "Loss for batch 97 = 1.0317758321762085\n",
      "Loss for batch 98 = 1.0714956521987915\n",
      "Loss for batch 99 = 1.0912953615188599\n",
      "Loss for batch 100 = 1.0764433145523071\n",
      "Loss for batch 101 = 1.0511523485183716\n",
      "Loss for batch 102 = 0.9606191515922546\n",
      "Loss for batch 103 = 0.9604920744895935\n",
      "Loss for batch 104 = 1.1509428024291992\n",
      "Loss for batch 105 = 1.1500270366668701\n",
      "Loss for batch 106 = 1.4548448324203491\n",
      "\n",
      "Training Loss for epoch 0 = 116.88065338134766\n",
      "\n",
      "Current Validation Loss = 17.538808822631836\n",
      "Best Validation Loss = 17.538808822631836\n",
      "Epochs without Improvement = 0\n",
      "Train Accuracy: 41.26%\n",
      "Validation Accuracy: 40.45%\n",
      "\n",
      "Epoch 1\n",
      "----------\n",
      "Loss for batch 0 = 1.0743446350097656\n",
      "Loss for batch 1 = 1.0553489923477173\n",
      "Loss for batch 2 = 1.0564687252044678\n",
      "Loss for batch 3 = 1.0899821519851685\n",
      "Loss for batch 4 = 1.1067290306091309\n",
      "Loss for batch 5 = 1.1897459030151367\n",
      "Loss for batch 6 = 1.0929534435272217\n",
      "Loss for batch 7 = 1.0820549726486206\n",
      "Loss for batch 8 = 1.0979286432266235\n",
      "Loss for batch 9 = 1.0645596981048584\n",
      "Loss for batch 10 = 1.0699727535247803\n",
      "Loss for batch 11 = 1.0740388631820679\n",
      "Loss for batch 12 = 1.0405441522598267\n",
      "Loss for batch 13 = 1.0699454545974731\n",
      "Loss for batch 14 = 1.0978257656097412\n",
      "Loss for batch 15 = 1.0897568464279175\n",
      "Loss for batch 16 = 1.093258261680603\n",
      "Loss for batch 17 = 1.0624287128448486\n",
      "Loss for batch 18 = 1.120558500289917\n",
      "Loss for batch 19 = 1.1073055267333984\n",
      "Loss for batch 20 = 1.1174176931381226\n",
      "Loss for batch 21 = 1.059391975402832\n",
      "Loss for batch 22 = 1.109753131866455\n",
      "Loss for batch 23 = 1.0865195989608765\n",
      "Loss for batch 24 = 1.0911657810211182\n",
      "Loss for batch 25 = 1.0901241302490234\n",
      "Loss for batch 26 = 1.1067248582839966\n",
      "Loss for batch 27 = 1.083250880241394\n",
      "Loss for batch 28 = 1.0986407995224\n",
      "Loss for batch 29 = 1.0691003799438477\n",
      "Loss for batch 30 = 1.0792564153671265\n",
      "Loss for batch 31 = 1.1197932958602905\n",
      "Loss for batch 32 = 1.0742923021316528\n",
      "Loss for batch 33 = 1.058236837387085\n",
      "Loss for batch 34 = 1.0488277673721313\n",
      "Loss for batch 35 = 1.0759197473526\n",
      "Loss for batch 36 = 1.0882978439331055\n",
      "Loss for batch 37 = 1.07392418384552\n",
      "Loss for batch 38 = 1.0539685487747192\n",
      "Loss for batch 39 = 1.0662598609924316\n",
      "Loss for batch 40 = 1.1090786457061768\n",
      "Loss for batch 41 = 1.1128089427947998\n",
      "Loss for batch 42 = 1.0460824966430664\n",
      "Loss for batch 43 = 0.995948076248169\n",
      "Loss for batch 44 = 1.1314055919647217\n",
      "Loss for batch 45 = 1.0447019338607788\n",
      "Loss for batch 46 = 1.0140712261199951\n",
      "Loss for batch 47 = 1.0901213884353638\n",
      "Loss for batch 48 = 1.0671051740646362\n",
      "Loss for batch 49 = 1.0808050632476807\n",
      "Loss for batch 50 = 1.1690315008163452\n",
      "Loss for batch 51 = 1.0299192667007446\n",
      "Loss for batch 52 = 1.0676170587539673\n",
      "Loss for batch 53 = 1.1113861799240112\n",
      "Loss for batch 54 = 1.0468014478683472\n",
      "Loss for batch 55 = 1.10140860080719\n",
      "Loss for batch 56 = 1.0548979043960571\n",
      "Loss for batch 57 = 1.0486923456192017\n",
      "Loss for batch 58 = 1.0683671236038208\n",
      "Loss for batch 59 = 1.0136890411376953\n",
      "Loss for batch 60 = 1.0651012659072876\n",
      "Loss for batch 61 = 1.1203502416610718\n",
      "Loss for batch 62 = 1.0806483030319214\n",
      "Loss for batch 63 = 1.108113408088684\n",
      "Loss for batch 64 = 1.0381078720092773\n",
      "Loss for batch 65 = 1.0717129707336426\n",
      "Loss for batch 66 = 1.1028227806091309\n",
      "Loss for batch 67 = 1.0369020700454712\n",
      "Loss for batch 68 = 1.0094342231750488\n",
      "Loss for batch 69 = 1.0716215372085571\n",
      "Loss for batch 70 = 1.0790225267410278\n",
      "Loss for batch 71 = 1.0629849433898926\n",
      "Loss for batch 72 = 1.0326248407363892\n",
      "Loss for batch 73 = 0.9040982127189636\n",
      "Loss for batch 74 = 1.1743618249893188\n",
      "Loss for batch 75 = 1.0802147388458252\n",
      "Loss for batch 76 = 1.0490546226501465\n",
      "Loss for batch 77 = 1.066763162612915\n",
      "Loss for batch 78 = 1.0710653066635132\n",
      "Loss for batch 79 = 1.0644031763076782\n",
      "Loss for batch 80 = 1.09235417842865\n",
      "Loss for batch 81 = 1.0860614776611328\n",
      "Loss for batch 82 = 1.0365428924560547\n",
      "Loss for batch 83 = 1.0963929891586304\n",
      "Loss for batch 84 = 1.0325548648834229\n",
      "Loss for batch 85 = 1.177101492881775\n",
      "Loss for batch 86 = 1.0711073875427246\n",
      "Loss for batch 87 = 1.0606757402420044\n",
      "Loss for batch 88 = 1.0675158500671387\n",
      "Loss for batch 89 = 1.11268150806427\n",
      "Loss for batch 90 = 1.1073133945465088\n",
      "Loss for batch 91 = 1.0838329792022705\n",
      "Loss for batch 92 = 1.1181132793426514\n",
      "Loss for batch 93 = 1.0625914335250854\n",
      "Loss for batch 94 = 1.0767662525177002\n",
      "Loss for batch 95 = 1.1327177286148071\n",
      "Loss for batch 96 = 1.073294997215271\n",
      "Loss for batch 97 = 1.0704824924468994\n",
      "Loss for batch 98 = 1.0617517232894897\n",
      "Loss for batch 99 = 1.0624098777770996\n",
      "Loss for batch 100 = 1.1116966009140015\n",
      "Loss for batch 101 = 1.0680627822875977\n",
      "Loss for batch 102 = 1.0860763788223267\n",
      "Loss for batch 103 = 1.0943502187728882\n",
      "Loss for batch 104 = 1.0346348285675049\n",
      "Loss for batch 105 = 1.1008121967315674\n",
      "Loss for batch 106 = 1.0765525102615356\n",
      "\n",
      "Training Loss for epoch 1 = 115.33443450927734\n",
      "\n",
      "Current Validation Loss = 17.23939323425293\n",
      "Best Validation Loss = 17.23939323425293\n",
      "Epochs without Improvement = 0\n",
      "Train Accuracy: 37.88%\n",
      "Validation Accuracy: 37.37%\n",
      "\n",
      "Epoch 2\n",
      "----------\n",
      "Loss for batch 0 = 1.028298020362854\n",
      "Loss for batch 1 = 1.027016520500183\n",
      "Loss for batch 2 = 1.042454719543457\n",
      "Loss for batch 3 = 1.076772928237915\n",
      "Loss for batch 4 = 1.0852112770080566\n",
      "Loss for batch 5 = 1.1351374387741089\n",
      "Loss for batch 6 = 1.0743966102600098\n",
      "Loss for batch 7 = 1.0839800834655762\n",
      "Loss for batch 8 = 1.095394492149353\n",
      "Loss for batch 9 = 0.992434024810791\n",
      "Loss for batch 10 = 1.0669828653335571\n",
      "Loss for batch 11 = 1.058809757232666\n",
      "Loss for batch 12 = 0.9967271685600281\n",
      "Loss for batch 13 = 1.0752665996551514\n",
      "Loss for batch 14 = 1.1330355405807495\n",
      "Loss for batch 15 = 1.106160283088684\n",
      "Loss for batch 16 = 1.1086859703063965\n",
      "Loss for batch 17 = 1.0101323127746582\n",
      "Loss for batch 18 = 1.1079951524734497\n",
      "Loss for batch 19 = 1.0671820640563965\n",
      "Loss for batch 20 = 1.1069377660751343\n",
      "Loss for batch 21 = 1.0660539865493774\n",
      "Loss for batch 22 = 1.1058486700057983\n",
      "Loss for batch 23 = 1.0692133903503418\n",
      "Loss for batch 24 = 1.0553537607192993\n",
      "Loss for batch 25 = 1.0740900039672852\n",
      "Loss for batch 26 = 1.0935734510421753\n",
      "Loss for batch 27 = 1.088147759437561\n",
      "Loss for batch 28 = 1.0969101190567017\n",
      "Loss for batch 29 = 1.079825758934021\n",
      "Loss for batch 30 = 1.0745850801467896\n",
      "Loss for batch 31 = 1.1610767841339111\n",
      "Loss for batch 32 = 1.0767741203308105\n",
      "Loss for batch 33 = 1.031018614768982\n",
      "Loss for batch 34 = 1.0379453897476196\n",
      "Loss for batch 35 = 1.0667259693145752\n",
      "Loss for batch 36 = 1.0763087272644043\n",
      "Loss for batch 37 = 1.0688681602478027\n",
      "Loss for batch 38 = 1.0444893836975098\n",
      "Loss for batch 39 = 1.0588524341583252\n",
      "Loss for batch 40 = 1.1075681447982788\n",
      "Loss for batch 41 = 1.1044530868530273\n",
      "Loss for batch 42 = 1.046170711517334\n",
      "Loss for batch 43 = 0.988232433795929\n",
      "Loss for batch 44 = 1.131143569946289\n",
      "Loss for batch 45 = 1.0383145809173584\n",
      "Loss for batch 46 = 1.0053701400756836\n",
      "Loss for batch 47 = 1.0901272296905518\n",
      "Loss for batch 48 = 1.0634196996688843\n",
      "Loss for batch 49 = 1.0354504585266113\n",
      "Loss for batch 50 = 1.149921178817749\n",
      "Loss for batch 51 = 0.9986055493354797\n",
      "Loss for batch 52 = 1.0564038753509521\n",
      "Loss for batch 53 = 1.1184934377670288\n",
      "Loss for batch 54 = 1.0434054136276245\n",
      "Loss for batch 55 = 1.101359248161316\n",
      "Loss for batch 56 = 1.0118825435638428\n",
      "Loss for batch 57 = 1.0417641401290894\n",
      "Loss for batch 58 = 1.1067476272583008\n",
      "Loss for batch 59 = 0.9833133220672607\n",
      "Loss for batch 60 = 1.058592677116394\n",
      "Loss for batch 61 = 1.1154552698135376\n",
      "Loss for batch 62 = 1.022026777267456\n",
      "Loss for batch 63 = 1.0761308670043945\n",
      "Loss for batch 64 = 1.034151554107666\n",
      "Loss for batch 65 = 1.0738478899002075\n",
      "Loss for batch 66 = 1.1026345491409302\n",
      "Loss for batch 67 = 1.0420284271240234\n",
      "Loss for batch 68 = 1.0043590068817139\n",
      "Loss for batch 69 = 1.040330171585083\n",
      "Loss for batch 70 = 1.0765037536621094\n",
      "Loss for batch 71 = 1.0305758714675903\n",
      "Loss for batch 72 = 1.035298228263855\n",
      "Loss for batch 73 = 0.8757354021072388\n",
      "Loss for batch 74 = 1.1456435918807983\n",
      "Loss for batch 75 = 1.077449083328247\n",
      "Loss for batch 76 = 1.048317790031433\n",
      "Loss for batch 77 = 1.0649161338806152\n",
      "Loss for batch 78 = 1.0690010786056519\n",
      "Loss for batch 79 = 1.0635427236557007\n",
      "Loss for batch 80 = 1.0901139974594116\n",
      "Loss for batch 81 = 1.088212013244629\n",
      "Loss for batch 82 = 1.0325589179992676\n",
      "Loss for batch 83 = 1.0870097875595093\n",
      "Loss for batch 84 = 1.0140734910964966\n",
      "Loss for batch 85 = 1.191195011138916\n",
      "Loss for batch 86 = 1.067685842514038\n",
      "Loss for batch 87 = 1.0588421821594238\n",
      "Loss for batch 88 = 1.0634009838104248\n",
      "Loss for batch 89 = 1.0842264890670776\n",
      "Loss for batch 90 = 1.111522912979126\n",
      "Loss for batch 91 = 1.0798646211624146\n",
      "Loss for batch 92 = 1.1286906003952026\n",
      "Loss for batch 93 = 1.043782353401184\n",
      "Loss for batch 94 = 1.0741206407546997\n",
      "Loss for batch 95 = 1.137460708618164\n",
      "Loss for batch 96 = 1.0446909666061401\n",
      "Loss for batch 97 = 1.0633643865585327\n",
      "Loss for batch 98 = 1.06093430519104\n",
      "Loss for batch 99 = 1.0522165298461914\n",
      "Loss for batch 100 = 1.0961068868637085\n",
      "Loss for batch 101 = 1.0532658100128174\n",
      "Loss for batch 102 = 1.0416810512542725\n",
      "Loss for batch 103 = 1.0936223268508911\n",
      "Loss for batch 104 = 1.0051265954971313\n",
      "Loss for batch 105 = 1.0990647077560425\n",
      "Loss for batch 106 = 1.1139249801635742\n",
      "\n",
      "Training Loss for epoch 2 = 114.2601318359375\n",
      "\n",
      "Current Validation Loss = 17.53484535217285\n",
      "Best Validation Loss = 17.23939323425293\n",
      "Epochs without Improvement = 1\n",
      "Train Accuracy: 34.04%\n",
      "Validation Accuracy: 34.09%\n",
      "\n",
      "Epoch 3\n",
      "----------\n",
      "Loss for batch 0 = 1.0913221836090088\n",
      "Loss for batch 1 = 1.0980408191680908\n",
      "Loss for batch 2 = 1.0940351486206055\n",
      "Loss for batch 3 = 1.0997073650360107\n",
      "Loss for batch 4 = 1.1095143556594849\n",
      "Loss for batch 5 = 1.090670108795166\n",
      "Loss for batch 6 = 1.1046665906906128\n",
      "Loss for batch 7 = 1.0816956758499146\n",
      "Loss for batch 8 = 1.0963959693908691\n",
      "Loss for batch 9 = 1.0934406518936157\n",
      "Loss for batch 10 = 1.0963510274887085\n",
      "Loss for batch 11 = 1.1061344146728516\n",
      "Loss for batch 12 = 1.080862283706665\n",
      "Loss for batch 13 = 1.1031826734542847\n",
      "Loss for batch 14 = 1.0988776683807373\n",
      "Loss for batch 15 = 1.0896382331848145\n",
      "Loss for batch 16 = 1.105928659439087\n",
      "Loss for batch 17 = 1.0812346935272217\n",
      "Loss for batch 18 = 1.115011215209961\n",
      "Loss for batch 19 = 1.0904384851455688\n",
      "Loss for batch 20 = 1.1085081100463867\n",
      "Loss for batch 21 = 1.1000854969024658\n",
      "Loss for batch 22 = 1.0961458683013916\n",
      "Loss for batch 23 = 1.0905362367630005\n",
      "Loss for batch 24 = 1.088996171951294\n",
      "Loss for batch 25 = 1.1000275611877441\n",
      "Loss for batch 26 = 1.0882070064544678\n",
      "Loss for batch 27 = 1.099431037902832\n",
      "Loss for batch 28 = 1.0966160297393799\n",
      "Loss for batch 29 = 1.0882445573806763\n",
      "Loss for batch 30 = 1.115175724029541\n",
      "Loss for batch 31 = 1.097305417060852\n",
      "Loss for batch 32 = 1.099379301071167\n",
      "Loss for batch 33 = 1.099240779876709\n",
      "Loss for batch 34 = 1.0704097747802734\n",
      "Loss for batch 35 = 1.089725375175476\n",
      "Loss for batch 36 = 1.1135584115982056\n",
      "Loss for batch 37 = 1.0982317924499512\n",
      "Loss for batch 38 = 1.0860673189163208\n",
      "Loss for batch 39 = 1.0938975811004639\n",
      "Loss for batch 40 = 1.1001859903335571\n",
      "Loss for batch 41 = 1.1234471797943115\n",
      "Loss for batch 42 = 1.0927841663360596\n",
      "Loss for batch 43 = 1.0903867483139038\n",
      "Loss for batch 44 = 1.099918246269226\n",
      "Loss for batch 45 = 1.0784342288970947\n",
      "Loss for batch 46 = 1.0657685995101929\n",
      "Loss for batch 47 = 1.084939956665039\n",
      "Loss for batch 48 = 1.0852855443954468\n",
      "Loss for batch 49 = 1.0938242673873901\n",
      "Loss for batch 50 = 1.149006724357605\n",
      "Loss for batch 51 = 1.0859886407852173\n",
      "Loss for batch 52 = 1.0842641592025757\n",
      "Loss for batch 53 = 1.0976600646972656\n",
      "Loss for batch 54 = 1.0841944217681885\n",
      "Loss for batch 55 = 1.0837554931640625\n",
      "Loss for batch 56 = 1.0938016176223755\n",
      "Loss for batch 57 = 1.0963481664657593\n",
      "Loss for batch 58 = 1.1047765016555786\n",
      "Loss for batch 59 = 1.084577202796936\n",
      "Loss for batch 60 = 1.069204568862915\n",
      "Loss for batch 61 = 1.1059722900390625\n",
      "Loss for batch 62 = 1.1042817831039429\n",
      "Loss for batch 63 = 1.109575867652893\n",
      "Loss for batch 64 = 1.0847136974334717\n",
      "Loss for batch 65 = 1.0579696893692017\n",
      "Loss for batch 66 = 1.0618867874145508\n",
      "Loss for batch 67 = 1.0405166149139404\n",
      "Loss for batch 68 = 1.0820934772491455\n",
      "Loss for batch 69 = 1.2984496355056763\n",
      "Loss for batch 70 = 1.1212389469146729\n",
      "Loss for batch 71 = 1.08730149269104\n",
      "Loss for batch 72 = 1.0872403383255005\n",
      "Loss for batch 73 = 1.1046152114868164\n",
      "Loss for batch 74 = 1.0933161973953247\n",
      "Loss for batch 75 = 1.1111204624176025\n",
      "Loss for batch 76 = 1.11416757106781\n",
      "Loss for batch 77 = 1.080916404724121\n",
      "Loss for batch 78 = 1.100447177886963\n",
      "Loss for batch 79 = 1.0951132774353027\n",
      "Loss for batch 80 = 1.0906131267547607\n",
      "Loss for batch 81 = 1.0764598846435547\n",
      "Loss for batch 82 = 1.090574860572815\n",
      "Loss for batch 83 = 1.0879303216934204\n",
      "Loss for batch 84 = 1.0860645771026611\n",
      "Loss for batch 85 = 1.14374577999115\n",
      "Loss for batch 86 = 1.0713986158370972\n",
      "Loss for batch 87 = 1.0702277421951294\n",
      "Loss for batch 88 = 1.0887147188186646\n",
      "Loss for batch 89 = 1.0891658067703247\n",
      "Loss for batch 90 = 1.0926868915557861\n",
      "Loss for batch 91 = 1.0988215208053589\n",
      "Loss for batch 92 = 1.1142789125442505\n",
      "Loss for batch 93 = 1.06778085231781\n",
      "Loss for batch 94 = 1.0836256742477417\n",
      "Loss for batch 95 = 1.0650098323822021\n",
      "Loss for batch 96 = 1.0621460676193237\n",
      "Loss for batch 97 = 1.0429397821426392\n",
      "Loss for batch 98 = 1.0824328660964966\n",
      "Loss for batch 99 = 1.0706552267074585\n",
      "Loss for batch 100 = 1.1709580421447754\n",
      "Loss for batch 101 = 1.1932132244110107\n",
      "Loss for batch 102 = 1.1038944721221924\n",
      "Loss for batch 103 = 1.0802077054977417\n",
      "Loss for batch 104 = 1.132457971572876\n",
      "Loss for batch 105 = 1.1041752099990845\n",
      "Loss for batch 106 = 1.1429469585418701\n",
      "\n",
      "Training Loss for epoch 3 = 117.33956909179688\n",
      "\n",
      "Current Validation Loss = 17.539072036743164\n",
      "Best Validation Loss = 17.23939323425293\n",
      "Epochs without Improvement = 2\n",
      "Train Accuracy: 37.18%\n",
      "Validation Accuracy: 34.29%\n",
      "\n",
      "Epoch 4\n",
      "----------\n",
      "Loss for batch 0 = 1.0881925821304321\n",
      "Loss for batch 1 = 1.097174048423767\n",
      "Loss for batch 2 = 1.0855995416641235\n",
      "Loss for batch 3 = 1.1020337343215942\n",
      "Loss for batch 4 = 1.11574387550354\n",
      "Loss for batch 5 = 1.1140027046203613\n",
      "Loss for batch 6 = 1.1033796072006226\n",
      "Loss for batch 7 = 1.0867878198623657\n",
      "Loss for batch 8 = 1.0911681652069092\n",
      "Loss for batch 9 = 1.0857751369476318\n",
      "Loss for batch 10 = 1.0924994945526123\n",
      "Loss for batch 11 = 1.0973351001739502\n",
      "Loss for batch 12 = 1.0951061248779297\n",
      "Loss for batch 13 = 1.1028413772583008\n",
      "Loss for batch 14 = 1.0975760221481323\n",
      "Loss for batch 15 = 1.0806705951690674\n",
      "Loss for batch 16 = 1.0862438678741455\n",
      "Loss for batch 17 = 1.0533722639083862\n",
      "Loss for batch 18 = 1.1082351207733154\n",
      "Loss for batch 19 = 1.0735657215118408\n",
      "Loss for batch 20 = 1.103520393371582\n",
      "Loss for batch 21 = 1.0678846836090088\n",
      "Loss for batch 22 = 1.1000497341156006\n",
      "Loss for batch 23 = 1.0855602025985718\n",
      "Loss for batch 24 = 1.0801178216934204\n",
      "Loss for batch 25 = 1.1043788194656372\n",
      "Loss for batch 26 = 1.064953088760376\n",
      "Loss for batch 27 = 1.070633888244629\n",
      "Loss for batch 28 = 1.0760596990585327\n",
      "Loss for batch 29 = 1.0682011842727661\n",
      "Loss for batch 30 = 1.0556448698043823\n",
      "Loss for batch 31 = 1.0715227127075195\n",
      "Loss for batch 32 = 1.0803519487380981\n",
      "Loss for batch 33 = 1.0604254007339478\n",
      "Loss for batch 34 = 1.0409835577011108\n",
      "Loss for batch 35 = 1.0842375755310059\n",
      "Loss for batch 36 = 1.1148117780685425\n",
      "Loss for batch 37 = 1.0897380113601685\n",
      "Loss for batch 38 = 1.1790250539779663\n",
      "Loss for batch 39 = 1.002655267715454\n",
      "Loss for batch 40 = 1.034005880355835\n",
      "Loss for batch 41 = 1.2075027227401733\n",
      "Loss for batch 42 = 1.1181584596633911\n",
      "Loss for batch 43 = 1.0801048278808594\n",
      "Loss for batch 44 = 1.1334439516067505\n",
      "Loss for batch 45 = 1.0832895040512085\n",
      "Loss for batch 46 = 1.0562679767608643\n",
      "Loss for batch 47 = 1.0846431255340576\n",
      "Loss for batch 48 = 1.110224723815918\n",
      "Loss for batch 49 = 1.110341191291809\n",
      "Loss for batch 50 = 1.0693762302398682\n",
      "Loss for batch 51 = 1.0908215045928955\n",
      "Loss for batch 52 = 1.0199838876724243\n",
      "Loss for batch 53 = 1.1231566667556763\n",
      "Loss for batch 54 = 1.0743836164474487\n",
      "Loss for batch 55 = 1.093015432357788\n",
      "Loss for batch 56 = 1.0755414962768555\n",
      "Loss for batch 57 = 1.0529612302780151\n",
      "Loss for batch 58 = 1.1214426755905151\n",
      "Loss for batch 59 = 1.0430694818496704\n",
      "Loss for batch 60 = 1.0115665197372437\n",
      "Loss for batch 61 = 1.0657455921173096\n",
      "Loss for batch 62 = 1.117760419845581\n",
      "Loss for batch 63 = 1.078680157661438\n",
      "Loss for batch 64 = 1.0805821418762207\n",
      "Loss for batch 65 = 1.1164913177490234\n",
      "Loss for batch 66 = 1.0929880142211914\n",
      "Loss for batch 67 = 1.1127737760543823\n",
      "Loss for batch 68 = 1.0240368843078613\n",
      "Loss for batch 69 = 1.0315203666687012\n",
      "Loss for batch 70 = 1.087625503540039\n",
      "Loss for batch 71 = 0.996595561504364\n",
      "Loss for batch 72 = 1.0729084014892578\n",
      "Loss for batch 73 = 1.0588353872299194\n",
      "Loss for batch 74 = 1.0756787061691284\n",
      "Loss for batch 75 = 1.094865083694458\n",
      "Loss for batch 76 = 1.1143683195114136\n",
      "Loss for batch 77 = 1.0166107416152954\n",
      "Loss for batch 78 = 1.12157142162323\n",
      "Loss for batch 79 = 1.1024303436279297\n",
      "Loss for batch 80 = 1.0734142065048218\n",
      "Loss for batch 81 = 1.1205086708068848\n",
      "Loss for batch 82 = 1.1497235298156738\n",
      "Loss for batch 83 = 1.0697412490844727\n",
      "Loss for batch 84 = 1.112290620803833\n",
      "Loss for batch 85 = 1.138671875\n",
      "Loss for batch 86 = 1.0858734846115112\n",
      "Loss for batch 87 = 1.0785307884216309\n",
      "Loss for batch 88 = 1.0964874029159546\n",
      "Loss for batch 89 = 1.0948302745819092\n",
      "Loss for batch 90 = 1.096660852432251\n",
      "Loss for batch 91 = 1.1142048835754395\n",
      "Loss for batch 92 = 1.112382173538208\n",
      "Loss for batch 93 = 1.054113507270813\n",
      "Loss for batch 94 = 1.068747878074646\n",
      "Loss for batch 95 = 1.070799708366394\n",
      "Loss for batch 96 = 1.0401458740234375\n",
      "Loss for batch 97 = 1.0112892389297485\n",
      "Loss for batch 98 = 1.0795811414718628\n",
      "Loss for batch 99 = 1.096366286277771\n",
      "Loss for batch 100 = 1.1046489477157593\n",
      "Loss for batch 101 = 1.1212756633758545\n",
      "Loss for batch 102 = 1.113435983657837\n",
      "Loss for batch 103 = 1.0763286352157593\n",
      "Loss for batch 104 = 1.0702378749847412\n",
      "Loss for batch 105 = 1.1005135774612427\n",
      "Loss for batch 106 = 1.1052428483963013\n",
      "\n",
      "Training Loss for epoch 4 = 116.16948699951172\n",
      "\n",
      "Current Validation Loss = 17.298948287963867\n",
      "Best Validation Loss = 17.23939323425293\n",
      "Epochs without Improvement = 3\n",
      "Train Accuracy: 40.02%\n",
      "Validation Accuracy: 39.43%\n",
      "\n",
      "Epoch 5\n",
      "----------\n",
      "Loss for batch 0 = 1.0565506219863892\n",
      "Loss for batch 1 = 1.1051088571548462\n",
      "Loss for batch 2 = 1.1214078664779663\n",
      "Loss for batch 3 = 1.0725127458572388\n",
      "Loss for batch 4 = 1.0816562175750732\n",
      "Loss for batch 5 = 1.1114733219146729\n",
      "Loss for batch 6 = 1.0747952461242676\n",
      "Loss for batch 7 = 1.0514967441558838\n",
      "Loss for batch 8 = 1.0937092304229736\n",
      "Loss for batch 9 = 1.0595142841339111\n",
      "Loss for batch 10 = 1.0627810955047607\n",
      "Loss for batch 11 = 1.0514376163482666\n",
      "Loss for batch 12 = 1.140092372894287\n",
      "Loss for batch 13 = 1.085329294204712\n",
      "Loss for batch 14 = 1.1043438911437988\n",
      "Loss for batch 15 = 1.1048007011413574\n",
      "Loss for batch 16 = 1.0766595602035522\n",
      "Loss for batch 17 = 1.0468021631240845\n",
      "Loss for batch 18 = 1.1430679559707642\n",
      "Loss for batch 19 = 1.0248503684997559\n",
      "Loss for batch 20 = 1.117478370666504\n",
      "Loss for batch 21 = 1.091088891029358\n",
      "Loss for batch 22 = 1.0822633504867554\n",
      "Loss for batch 23 = 1.0596911907196045\n",
      "Loss for batch 24 = 1.0257338285446167\n",
      "Loss for batch 25 = 1.1188673973083496\n",
      "Loss for batch 26 = 1.0579512119293213\n",
      "Loss for batch 27 = 1.1127694845199585\n",
      "Loss for batch 28 = 1.0838608741760254\n",
      "Loss for batch 29 = 1.1008729934692383\n",
      "Loss for batch 30 = 1.0780398845672607\n",
      "Loss for batch 31 = 1.0552425384521484\n",
      "Loss for batch 32 = 1.063399314880371\n",
      "Loss for batch 33 = 1.0344527959823608\n",
      "Loss for batch 34 = 1.028203010559082\n",
      "Loss for batch 35 = 1.1022844314575195\n",
      "Loss for batch 36 = 1.0837286710739136\n",
      "Loss for batch 37 = 1.096086859703064\n",
      "Loss for batch 38 = 1.0375031232833862\n",
      "Loss for batch 39 = 0.9765647053718567\n",
      "Loss for batch 40 = 1.0679206848144531\n",
      "Loss for batch 41 = 1.1195217370986938\n",
      "Loss for batch 42 = 1.0720536708831787\n",
      "Loss for batch 43 = 1.070481538772583\n",
      "Loss for batch 44 = 1.077160358428955\n",
      "Loss for batch 45 = 1.016708493232727\n",
      "Loss for batch 46 = 1.0113005638122559\n",
      "Loss for batch 47 = 1.1033827066421509\n",
      "Loss for batch 48 = 1.1053482294082642\n",
      "Loss for batch 49 = 1.0921589136123657\n",
      "Loss for batch 50 = 1.1100512742996216\n",
      "Loss for batch 51 = 1.068005919456482\n",
      "Loss for batch 52 = 0.9560273289680481\n",
      "Loss for batch 53 = 1.0510486364364624\n",
      "Loss for batch 54 = 1.0692750215530396\n",
      "Loss for batch 55 = 1.0920158624649048\n",
      "Loss for batch 56 = 1.1114870309829712\n",
      "Loss for batch 57 = 0.9822155833244324\n",
      "Loss for batch 58 = 1.1221401691436768\n",
      "Loss for batch 59 = 1.0590306520462036\n",
      "Loss for batch 60 = 0.9610033631324768\n",
      "Loss for batch 61 = 1.0942871570587158\n",
      "Loss for batch 62 = 1.079245686531067\n",
      "Loss for batch 63 = 1.0187957286834717\n",
      "Loss for batch 64 = 1.035827398300171\n",
      "Loss for batch 65 = 1.006316900253296\n",
      "Loss for batch 66 = 1.1573456525802612\n",
      "Loss for batch 67 = 0.9835071563720703\n",
      "Loss for batch 68 = 0.9721782207489014\n",
      "Loss for batch 69 = 1.0877833366394043\n",
      "Loss for batch 70 = 1.1140624284744263\n",
      "Loss for batch 71 = 1.0507029294967651\n",
      "Loss for batch 72 = 1.0619399547576904\n",
      "Loss for batch 73 = 1.0640795230865479\n",
      "Loss for batch 74 = 1.0753968954086304\n",
      "Loss for batch 75 = 1.0952973365783691\n",
      "Loss for batch 76 = 1.0709723234176636\n",
      "Loss for batch 77 = 0.9996299743652344\n",
      "Loss for batch 78 = 1.1608206033706665\n",
      "Loss for batch 79 = 1.0685166120529175\n",
      "Loss for batch 80 = 1.054097056388855\n",
      "Loss for batch 81 = 1.1350877285003662\n",
      "Loss for batch 82 = 1.0583136081695557\n",
      "Loss for batch 83 = 1.0532753467559814\n",
      "Loss for batch 84 = 1.1311733722686768\n",
      "Loss for batch 85 = 1.1969783306121826\n",
      "Loss for batch 86 = 1.0221960544586182\n",
      "Loss for batch 87 = 1.0711084604263306\n",
      "Loss for batch 88 = 1.0910866260528564\n",
      "Loss for batch 89 = 1.0737738609313965\n",
      "Loss for batch 90 = 1.0886294841766357\n",
      "Loss for batch 91 = 1.0573011636734009\n",
      "Loss for batch 92 = 1.0882809162139893\n",
      "Loss for batch 93 = 0.9992144703865051\n",
      "Loss for batch 94 = 1.0428255796432495\n",
      "Loss for batch 95 = 1.0343033075332642\n",
      "Loss for batch 96 = 0.9455016255378723\n",
      "Loss for batch 97 = 0.9752271175384521\n",
      "Loss for batch 98 = 1.059008002281189\n",
      "Loss for batch 99 = 1.2360509634017944\n",
      "Loss for batch 100 = 1.094655156135559\n",
      "Loss for batch 101 = 1.0628303289413452\n",
      "Loss for batch 102 = 1.0271947383880615\n",
      "Loss for batch 103 = 1.1066468954086304\n",
      "Loss for batch 104 = 1.0127263069152832\n",
      "Loss for batch 105 = 1.0577861070632935\n",
      "Loss for batch 106 = 1.1276791095733643\n",
      "\n",
      "Training Loss for epoch 5 = 114.49244689941406\n",
      "\n",
      "Current Validation Loss = 17.02273178100586\n",
      "Best Validation Loss = 17.02273178100586\n",
      "Epochs without Improvement = 0\n",
      "Train Accuracy: 43.69%\n",
      "Validation Accuracy: 41.27%\n",
      "\n",
      "Epoch 6\n",
      "----------\n",
      "Loss for batch 0 = 1.0178492069244385\n",
      "Loss for batch 1 = 1.032989263534546\n",
      "Loss for batch 2 = 1.0892925262451172\n",
      "Loss for batch 3 = 1.039335012435913\n",
      "Loss for batch 4 = 1.026332974433899\n",
      "Loss for batch 5 = 1.0920815467834473\n",
      "Loss for batch 6 = 1.0655837059020996\n",
      "Loss for batch 7 = 1.020127534866333\n",
      "Loss for batch 8 = 1.1775872707366943\n",
      "Loss for batch 9 = 1.0159207582473755\n",
      "Loss for batch 10 = 0.999842643737793\n",
      "Loss for batch 11 = 1.083250880241394\n",
      "Loss for batch 12 = 1.3224444389343262\n",
      "Loss for batch 13 = 1.0905829668045044\n",
      "Loss for batch 14 = 1.0753724575042725\n",
      "Loss for batch 15 = 1.0591150522232056\n",
      "Loss for batch 16 = 1.1120225191116333\n",
      "Loss for batch 17 = 1.0247337818145752\n",
      "Loss for batch 18 = 1.1264288425445557\n",
      "Loss for batch 19 = 1.0244780778884888\n",
      "Loss for batch 20 = 1.0916283130645752\n",
      "Loss for batch 21 = 1.0439648628234863\n",
      "Loss for batch 22 = 1.0459744930267334\n",
      "Loss for batch 23 = 1.0493996143341064\n",
      "Loss for batch 24 = 0.9382612705230713\n",
      "Loss for batch 25 = 1.1356894969940186\n",
      "Loss for batch 26 = 1.0374881029129028\n",
      "Loss for batch 27 = 1.069413423538208\n",
      "Loss for batch 28 = 1.052490234375\n",
      "Loss for batch 29 = 1.109356164932251\n",
      "Loss for batch 30 = 1.0764716863632202\n",
      "Loss for batch 31 = 1.0521180629730225\n",
      "Loss for batch 32 = 1.0627501010894775\n",
      "Loss for batch 33 = 0.9910735487937927\n",
      "Loss for batch 34 = 1.0142159461975098\n",
      "Loss for batch 35 = 1.0531442165374756\n",
      "Loss for batch 36 = 1.0671024322509766\n",
      "Loss for batch 37 = 1.0771031379699707\n",
      "Loss for batch 38 = 1.012251615524292\n",
      "Loss for batch 39 = 0.8856697082519531\n",
      "Loss for batch 40 = 1.0799994468688965\n",
      "Loss for batch 41 = 1.324163556098938\n",
      "Loss for batch 42 = 1.077543020248413\n",
      "Loss for batch 43 = 1.1121784448623657\n",
      "Loss for batch 44 = 1.0736247301101685\n",
      "Loss for batch 45 = 0.9757966995239258\n",
      "Loss for batch 46 = 0.9403635263442993\n",
      "Loss for batch 47 = 1.0406684875488281\n",
      "Loss for batch 48 = 1.2021993398666382\n",
      "Loss for batch 49 = 1.099611759185791\n",
      "Loss for batch 50 = 1.0850533246994019\n",
      "Loss for batch 51 = 1.0465670824050903\n",
      "Loss for batch 52 = 1.093974232673645\n",
      "Loss for batch 53 = 1.015189528465271\n",
      "Loss for batch 54 = 1.0289744138717651\n",
      "Loss for batch 55 = 1.0476179122924805\n",
      "Loss for batch 56 = 1.1035248041152954\n",
      "Loss for batch 57 = 1.0088664293289185\n",
      "Loss for batch 58 = 1.1035106182098389\n",
      "Loss for batch 59 = 1.0005167722702026\n",
      "Loss for batch 60 = 0.9492951035499573\n",
      "Loss for batch 61 = 1.148985505104065\n",
      "Loss for batch 62 = 1.0991017818450928\n",
      "Loss for batch 63 = 0.9822394847869873\n",
      "Loss for batch 64 = 1.0480382442474365\n",
      "Loss for batch 65 = 1.0743305683135986\n",
      "Loss for batch 66 = 1.032138466835022\n",
      "Loss for batch 67 = 1.1478257179260254\n",
      "Loss for batch 68 = 1.0100436210632324\n",
      "Loss for batch 69 = 0.9954895973205566\n",
      "Loss for batch 70 = 1.0747473239898682\n",
      "Loss for batch 71 = 0.9994964003562927\n",
      "Loss for batch 72 = 1.0364840030670166\n",
      "Loss for batch 73 = 1.0424044132232666\n",
      "Loss for batch 74 = 1.0339983701705933\n",
      "Loss for batch 75 = 1.092881441116333\n",
      "Loss for batch 76 = 1.027209758758545\n",
      "Loss for batch 77 = 1.038500428199768\n",
      "Loss for batch 78 = 1.1261781454086304\n",
      "Loss for batch 79 = 1.078147053718567\n",
      "Loss for batch 80 = 1.0738416910171509\n",
      "Loss for batch 81 = 1.0339429378509521\n",
      "Loss for batch 82 = 1.0575428009033203\n",
      "Loss for batch 83 = 1.038130521774292\n",
      "Loss for batch 84 = 1.0897401571273804\n",
      "Loss for batch 85 = 1.2267705202102661\n",
      "Loss for batch 86 = 1.0170618295669556\n",
      "Loss for batch 87 = 1.0297914743423462\n",
      "Loss for batch 88 = 1.091071605682373\n",
      "Loss for batch 89 = 1.0608713626861572\n",
      "Loss for batch 90 = 1.059105634689331\n",
      "Loss for batch 91 = 1.1200560331344604\n",
      "Loss for batch 92 = 1.1075507402420044\n",
      "Loss for batch 93 = 1.0085846185684204\n",
      "Loss for batch 94 = 1.0222883224487305\n",
      "Loss for batch 95 = 1.0272231101989746\n",
      "Loss for batch 96 = 0.9402458071708679\n",
      "Loss for batch 97 = 1.0259737968444824\n",
      "Loss for batch 98 = 1.0591033697128296\n",
      "Loss for batch 99 = 1.0669465065002441\n",
      "Loss for batch 100 = 1.0623699426651\n",
      "Loss for batch 101 = 1.1451836824417114\n",
      "Loss for batch 102 = 0.972030520439148\n",
      "Loss for batch 103 = 1.0721955299377441\n",
      "Loss for batch 104 = 0.9549334645271301\n",
      "Loss for batch 105 = 1.0650666952133179\n",
      "Loss for batch 106 = 1.12416672706604\n",
      "\n",
      "Training Loss for epoch 6 = 113.41018676757812\n",
      "\n",
      "Current Validation Loss = 16.849172592163086\n",
      "Best Validation Loss = 16.849172592163086\n",
      "Epochs without Improvement = 0\n",
      "Train Accuracy: 47.59%\n",
      "Validation Accuracy: 43.53%\n",
      "\n",
      "Epoch 7\n",
      "----------\n",
      "Loss for batch 0 = 0.9860818982124329\n",
      "Loss for batch 1 = 1.042068362236023\n",
      "Loss for batch 2 = 1.0868278741836548\n",
      "Loss for batch 3 = 0.9960396885871887\n",
      "Loss for batch 4 = 1.0811257362365723\n",
      "Loss for batch 5 = 1.1358819007873535\n",
      "Loss for batch 6 = 1.0501047372817993\n",
      "Loss for batch 7 = 0.9830202460289001\n",
      "Loss for batch 8 = 1.1045341491699219\n",
      "Loss for batch 9 = 0.9943464398384094\n",
      "Loss for batch 10 = 1.0012022256851196\n",
      "Loss for batch 11 = 1.1459808349609375\n",
      "Loss for batch 12 = 1.0621259212493896\n",
      "Loss for batch 13 = 1.0947203636169434\n",
      "Loss for batch 14 = 1.113169550895691\n",
      "Loss for batch 15 = 1.005496621131897\n",
      "Loss for batch 16 = 1.0659747123718262\n",
      "Loss for batch 17 = 1.0391297340393066\n",
      "Loss for batch 18 = 1.1157283782958984\n",
      "Loss for batch 19 = 0.989705502986908\n",
      "Loss for batch 20 = 1.0927079916000366\n",
      "Loss for batch 21 = 0.9642540812492371\n",
      "Loss for batch 22 = 1.0613864660263062\n",
      "Loss for batch 23 = 1.0419039726257324\n",
      "Loss for batch 24 = 0.8957424163818359\n",
      "Loss for batch 25 = 1.1526280641555786\n",
      "Loss for batch 26 = 0.967938244342804\n",
      "Loss for batch 27 = 1.2171577215194702\n",
      "Loss for batch 28 = 1.067989706993103\n",
      "Loss for batch 29 = 1.0289794206619263\n",
      "Loss for batch 30 = 0.9793804883956909\n",
      "Loss for batch 31 = 1.0452561378479004\n",
      "Loss for batch 32 = 1.1412328481674194\n",
      "Loss for batch 33 = 1.0038899183273315\n",
      "Loss for batch 34 = 0.9362350702285767\n",
      "Loss for batch 35 = 1.098757266998291\n",
      "Loss for batch 36 = 0.9954838156700134\n",
      "Loss for batch 37 = 1.0781030654907227\n",
      "Loss for batch 38 = 0.971753716468811\n",
      "Loss for batch 39 = 0.8401371240615845\n",
      "Loss for batch 40 = 1.0209641456604004\n",
      "Loss for batch 41 = 1.1146315336227417\n",
      "Loss for batch 42 = 1.0355565547943115\n",
      "Loss for batch 43 = 0.999332845211029\n",
      "Loss for batch 44 = 1.0687536001205444\n",
      "Loss for batch 45 = 1.3322259187698364\n",
      "Loss for batch 46 = 1.0092474222183228\n",
      "Loss for batch 47 = 1.0329660177230835\n",
      "Loss for batch 48 = 1.0633740425109863\n",
      "Loss for batch 49 = 0.9232101440429688\n",
      "Loss for batch 50 = 1.05107581615448\n",
      "Loss for batch 51 = 1.0112626552581787\n",
      "Loss for batch 52 = 1.0179554224014282\n",
      "Loss for batch 53 = 1.0334430932998657\n",
      "Loss for batch 54 = 1.0007035732269287\n",
      "Loss for batch 55 = 1.0406475067138672\n",
      "Loss for batch 56 = 1.1238826513290405\n",
      "Loss for batch 57 = 0.9040054678916931\n",
      "Loss for batch 58 = 1.078782081604004\n",
      "Loss for batch 59 = 0.9739893078804016\n",
      "Loss for batch 60 = 0.8997997045516968\n",
      "Loss for batch 61 = 1.0057837963104248\n",
      "Loss for batch 62 = 1.0981765985488892\n",
      "Loss for batch 63 = 0.9605386257171631\n",
      "Loss for batch 64 = 1.0586050748825073\n",
      "Loss for batch 65 = 1.0434967279434204\n",
      "Loss for batch 66 = 0.9977980256080627\n",
      "Loss for batch 67 = 0.9915699362754822\n",
      "Loss for batch 68 = 1.0088250637054443\n",
      "Loss for batch 69 = 1.0954543352127075\n",
      "Loss for batch 70 = 1.09734046459198\n",
      "Loss for batch 71 = 0.9844921231269836\n",
      "Loss for batch 72 = 1.0309242010116577\n",
      "Loss for batch 73 = 0.909978449344635\n",
      "Loss for batch 74 = 1.0666544437408447\n",
      "Loss for batch 75 = 1.06273353099823\n",
      "Loss for batch 76 = 1.0012136697769165\n",
      "Loss for batch 77 = 0.9747461080551147\n",
      "Loss for batch 78 = 0.960666835308075\n",
      "Loss for batch 79 = 1.067276954650879\n",
      "Loss for batch 80 = 1.0127298831939697\n",
      "Loss for batch 81 = 1.0989282131195068\n",
      "Loss for batch 82 = 0.9872852563858032\n",
      "Loss for batch 83 = 1.0107102394104004\n",
      "Loss for batch 84 = 0.9840781688690186\n",
      "Loss for batch 85 = 1.25547194480896\n",
      "Loss for batch 86 = 0.9197221994400024\n",
      "Loss for batch 87 = 1.0008257627487183\n",
      "Loss for batch 88 = 1.1304748058319092\n",
      "Loss for batch 89 = 1.024074912071228\n",
      "Loss for batch 90 = 1.0138726234436035\n",
      "Loss for batch 91 = 1.0984892845153809\n",
      "Loss for batch 92 = 1.0091267824172974\n",
      "Loss for batch 93 = 0.9442054033279419\n",
      "Loss for batch 94 = 0.9450063705444336\n",
      "Loss for batch 95 = 1.1303304433822632\n",
      "Loss for batch 96 = 0.8409821391105652\n",
      "Loss for batch 97 = 1.0944314002990723\n",
      "Loss for batch 98 = 1.075765609741211\n",
      "Loss for batch 99 = 1.0182406902313232\n",
      "Loss for batch 100 = 1.074110746383667\n",
      "Loss for batch 101 = 1.0526422262191772\n",
      "Loss for batch 102 = 0.9342941641807556\n",
      "Loss for batch 103 = 1.045735239982605\n",
      "Loss for batch 104 = 0.9340962767601013\n",
      "Loss for batch 105 = 1.074644684791565\n",
      "Loss for batch 106 = 1.094618558883667\n",
      "\n",
      "Training Loss for epoch 7 = 110.63117218017578\n",
      "\n",
      "Current Validation Loss = 16.988988876342773\n",
      "Best Validation Loss = 16.849172592163086\n",
      "Epochs without Improvement = 1\n",
      "Train Accuracy: 43.98%\n",
      "Validation Accuracy: 42.71%\n",
      "\n",
      "Epoch 8\n",
      "----------\n",
      "Loss for batch 0 = 0.9949343204498291\n",
      "Loss for batch 1 = 0.9234511256217957\n",
      "Loss for batch 2 = 1.0184582471847534\n",
      "Loss for batch 3 = 1.1514230966567993\n",
      "Loss for batch 4 = 1.0471611022949219\n",
      "Loss for batch 5 = 1.147354006767273\n",
      "Loss for batch 6 = 1.1166716814041138\n",
      "Loss for batch 7 = 0.9942053556442261\n",
      "Loss for batch 8 = 1.0761686563491821\n",
      "Loss for batch 9 = 0.9819325804710388\n",
      "Loss for batch 10 = 0.9365906715393066\n",
      "Loss for batch 11 = 0.9375512599945068\n",
      "Loss for batch 12 = 1.0168761014938354\n",
      "Loss for batch 13 = 0.9818675518035889\n",
      "Loss for batch 14 = 1.078134536743164\n",
      "Loss for batch 15 = 0.928176760673523\n",
      "Loss for batch 16 = 0.9919401407241821\n",
      "Loss for batch 17 = 0.9811872243881226\n",
      "Loss for batch 18 = 1.0011128187179565\n",
      "Loss for batch 19 = 0.9450216293334961\n",
      "Loss for batch 20 = 1.0543771982192993\n",
      "Loss for batch 21 = 0.8563400506973267\n",
      "Loss for batch 22 = 1.127150058746338\n",
      "Loss for batch 23 = 0.9539268016815186\n",
      "Loss for batch 24 = 0.8102903962135315\n",
      "Loss for batch 25 = 1.0869929790496826\n",
      "Loss for batch 26 = 0.9585900902748108\n",
      "Loss for batch 27 = 0.9073889851570129\n",
      "Loss for batch 28 = 1.1284749507904053\n",
      "Loss for batch 29 = 1.0101925134658813\n",
      "Loss for batch 30 = 1.0889813899993896\n",
      "Loss for batch 31 = 1.3201767206192017\n",
      "Loss for batch 32 = 1.270171046257019\n",
      "Loss for batch 33 = 1.0334607362747192\n",
      "Loss for batch 34 = 0.9941982626914978\n",
      "Loss for batch 35 = 0.9947476983070374\n",
      "Loss for batch 36 = 0.9580131769180298\n",
      "Loss for batch 37 = 1.0282318592071533\n",
      "Loss for batch 38 = 0.9364922642707825\n",
      "Loss for batch 39 = 0.8007250428199768\n",
      "Loss for batch 40 = 0.994849681854248\n",
      "Loss for batch 41 = 1.109812617301941\n",
      "Loss for batch 42 = 1.0254106521606445\n",
      "Loss for batch 43 = 0.9696723222732544\n",
      "Loss for batch 44 = 1.0744705200195312\n",
      "Loss for batch 45 = 0.9688840508460999\n",
      "Loss for batch 46 = 0.8357093930244446\n",
      "Loss for batch 47 = 1.0348997116088867\n",
      "Loss for batch 48 = 0.9850586652755737\n",
      "Loss for batch 49 = 0.844966471195221\n",
      "Loss for batch 50 = 0.9353315234184265\n",
      "Loss for batch 51 = 0.9743763208389282\n",
      "Loss for batch 52 = 0.8872170448303223\n",
      "Loss for batch 53 = 0.8978095650672913\n",
      "Loss for batch 54 = 0.9431357383728027\n",
      "Loss for batch 55 = 0.9525575637817383\n",
      "Loss for batch 56 = 1.06444251537323\n",
      "Loss for batch 57 = 0.8714218735694885\n",
      "Loss for batch 58 = 1.1221245527267456\n",
      "Loss for batch 59 = 1.049910545349121\n",
      "Loss for batch 60 = 0.8740271329879761\n",
      "Loss for batch 61 = 0.9622840285301208\n",
      "Loss for batch 62 = 1.0541660785675049\n",
      "Loss for batch 63 = 1.003096580505371\n",
      "Loss for batch 64 = 1.2902116775512695\n",
      "Loss for batch 65 = 1.0318042039871216\n",
      "Loss for batch 66 = 0.9475299119949341\n",
      "Loss for batch 67 = 0.932220995426178\n",
      "Loss for batch 68 = 0.8768467307090759\n",
      "Loss for batch 69 = 0.9188117980957031\n",
      "Loss for batch 70 = 1.2337560653686523\n",
      "Loss for batch 71 = 0.893812894821167\n",
      "Loss for batch 72 = 0.9242224097251892\n",
      "Loss for batch 73 = 0.9249091744422913\n",
      "Loss for batch 74 = 1.0437201261520386\n",
      "Loss for batch 75 = 1.1164458990097046\n",
      "Loss for batch 76 = 0.9550262689590454\n",
      "Loss for batch 77 = 0.9108735918998718\n",
      "Loss for batch 78 = 0.9936134219169617\n",
      "Loss for batch 79 = 1.0464378595352173\n",
      "Loss for batch 80 = 0.9750038385391235\n",
      "Loss for batch 81 = 1.0425835847854614\n",
      "Loss for batch 82 = 0.9406713247299194\n",
      "Loss for batch 83 = 0.9372926354408264\n",
      "Loss for batch 84 = 0.9726107120513916\n",
      "Loss for batch 85 = 1.3205050230026245\n",
      "Loss for batch 86 = 1.0679432153701782\n",
      "Loss for batch 87 = 1.0596270561218262\n",
      "Loss for batch 88 = 0.954236626625061\n",
      "Loss for batch 89 = 0.9800963401794434\n",
      "Loss for batch 90 = 1.0194591283798218\n",
      "Loss for batch 91 = 0.9902490973472595\n",
      "Loss for batch 92 = 0.9829013347625732\n",
      "Loss for batch 93 = 0.9333328604698181\n",
      "Loss for batch 94 = 0.9430202841758728\n",
      "Loss for batch 95 = 0.9539928436279297\n",
      "Loss for batch 96 = 0.7979146838188171\n",
      "Loss for batch 97 = 0.9604851603507996\n",
      "Loss for batch 98 = 1.0124822854995728\n",
      "Loss for batch 99 = 1.1520873308181763\n",
      "Loss for batch 100 = 1.0576729774475098\n",
      "Loss for batch 101 = 1.041688084602356\n",
      "Loss for batch 102 = 0.8133094906806946\n",
      "Loss for batch 103 = 1.1220996379852295\n",
      "Loss for batch 104 = 0.8594280481338501\n",
      "Loss for batch 105 = 1.0313585996627808\n",
      "Loss for batch 106 = 1.0325963497161865\n",
      "\n",
      "Training Loss for epoch 8 = 106.99967193603516\n",
      "\n",
      "Current Validation Loss = 16.784347534179688\n",
      "Best Validation Loss = 16.784347534179688\n",
      "Epochs without Improvement = 0\n",
      "Train Accuracy: 46.42%\n",
      "Validation Accuracy: 42.30%\n",
      "\n",
      "Epoch 9\n",
      "----------\n",
      "Loss for batch 0 = 0.9644583463668823\n",
      "Loss for batch 1 = 0.8766981959342957\n",
      "Loss for batch 2 = 0.9925020337104797\n",
      "Loss for batch 3 = 0.9071831703186035\n",
      "Loss for batch 4 = 1.025978922843933\n",
      "Loss for batch 5 = 1.1531749963760376\n",
      "Loss for batch 6 = 1.111788034439087\n",
      "Loss for batch 7 = 0.9490243196487427\n",
      "Loss for batch 8 = 0.9943974614143372\n",
      "Loss for batch 9 = 0.8912787437438965\n",
      "Loss for batch 10 = 0.8492212891578674\n",
      "Loss for batch 11 = 0.8989169001579285\n",
      "Loss for batch 12 = 0.9605545401573181\n",
      "Loss for batch 13 = 0.9455424547195435\n",
      "Loss for batch 14 = 1.0097678899765015\n",
      "Loss for batch 15 = 0.877886950969696\n",
      "Loss for batch 16 = 0.9648088216781616\n",
      "Loss for batch 17 = 0.9293552041053772\n",
      "Loss for batch 18 = 1.028486728668213\n",
      "Loss for batch 19 = 0.8377203345298767\n",
      "Loss for batch 20 = 0.889483630657196\n",
      "Loss for batch 21 = 1.0940771102905273\n",
      "Loss for batch 22 = 0.997783899307251\n",
      "Loss for batch 23 = 0.8692011833190918\n",
      "Loss for batch 24 = 0.7744548320770264\n",
      "Loss for batch 25 = 1.1717966794967651\n",
      "Loss for batch 26 = 0.9508265256881714\n",
      "Loss for batch 27 = 0.8158597946166992\n",
      "Loss for batch 28 = 1.0246236324310303\n",
      "Loss for batch 29 = 1.01809823513031\n",
      "Loss for batch 30 = 0.8977237939834595\n",
      "Loss for batch 31 = 1.066979169845581\n",
      "Loss for batch 32 = 0.9306938052177429\n",
      "Loss for batch 33 = 0.9053140878677368\n",
      "Loss for batch 34 = 0.8757132291793823\n",
      "Loss for batch 35 = 0.9597182273864746\n",
      "Loss for batch 36 = 0.8484060764312744\n",
      "Loss for batch 37 = 0.9619454145431519\n",
      "Loss for batch 38 = 0.882335901260376\n",
      "Loss for batch 39 = 0.6807975769042969\n",
      "Loss for batch 40 = 0.9481327533721924\n",
      "Loss for batch 41 = 1.5013585090637207\n",
      "Loss for batch 42 = 1.4079408645629883\n",
      "Loss for batch 43 = 0.9814379215240479\n",
      "Loss for batch 44 = 0.9591387510299683\n",
      "Loss for batch 45 = 1.0167382955551147\n",
      "Loss for batch 46 = 0.8612176179885864\n",
      "Loss for batch 47 = 1.0192761421203613\n",
      "Loss for batch 48 = 0.8694502711296082\n",
      "Loss for batch 49 = 0.9829409718513489\n",
      "Loss for batch 50 = 1.0456029176712036\n",
      "Loss for batch 51 = 0.8996434211730957\n",
      "Loss for batch 52 = 0.8941859006881714\n",
      "Loss for batch 53 = 0.9482137560844421\n",
      "Loss for batch 54 = 0.9378805756568909\n",
      "Loss for batch 55 = 0.8910742402076721\n",
      "Loss for batch 56 = 0.8890188932418823\n",
      "Loss for batch 57 = 0.7118961215019226\n",
      "Loss for batch 58 = 0.9079864025115967\n",
      "Loss for batch 59 = 0.8620228171348572\n",
      "Loss for batch 60 = 0.752560555934906\n",
      "Loss for batch 61 = 0.8223236799240112\n",
      "Loss for batch 62 = 1.0807973146438599\n",
      "Loss for batch 63 = 1.3198963403701782\n",
      "Loss for batch 64 = 1.129206895828247\n",
      "Loss for batch 65 = 0.897097647190094\n",
      "Loss for batch 66 = 0.8631563186645508\n",
      "Loss for batch 67 = 0.8248675465583801\n",
      "Loss for batch 68 = 0.8913379907608032\n",
      "Loss for batch 69 = 0.7987755537033081\n",
      "Loss for batch 70 = 1.0121665000915527\n",
      "Loss for batch 71 = 0.8269355893135071\n",
      "Loss for batch 72 = 0.9160051941871643\n",
      "Loss for batch 73 = 1.772588849067688\n",
      "Loss for batch 74 = 1.2903618812561035\n",
      "Loss for batch 75 = 1.2672795057296753\n",
      "Loss for batch 76 = 1.207785964012146\n",
      "Loss for batch 77 = 1.08357572555542\n",
      "Loss for batch 78 = 1.1644775867462158\n",
      "Loss for batch 79 = 1.0828943252563477\n",
      "Loss for batch 80 = 1.0479888916015625\n",
      "Loss for batch 81 = 1.0706366300582886\n",
      "Loss for batch 82 = 1.0303750038146973\n",
      "Loss for batch 83 = 1.0126597881317139\n",
      "Loss for batch 84 = 1.104011058807373\n",
      "Loss for batch 85 = 1.0843678712844849\n",
      "Loss for batch 86 = 0.9750745296478271\n",
      "Loss for batch 87 = 1.0019563436508179\n",
      "Loss for batch 88 = 0.9535359740257263\n",
      "Loss for batch 89 = 1.0125982761383057\n",
      "Loss for batch 90 = 1.054044246673584\n",
      "Loss for batch 91 = 1.011555552482605\n",
      "Loss for batch 92 = 0.9533488750457764\n",
      "Loss for batch 93 = 1.1935110092163086\n",
      "Loss for batch 94 = 0.9607158899307251\n",
      "Loss for batch 95 = 0.8376623392105103\n",
      "Loss for batch 96 = 0.7897297739982605\n",
      "Loss for batch 97 = 0.8806173205375671\n",
      "Loss for batch 98 = 0.8703305721282959\n",
      "Loss for batch 99 = 0.8974176645278931\n",
      "Loss for batch 100 = 1.381989598274231\n",
      "Loss for batch 101 = 0.9475653767585754\n",
      "Loss for batch 102 = 0.7686203122138977\n",
      "Loss for batch 103 = 0.8563473224639893\n",
      "Loss for batch 104 = 0.7630242109298706\n",
      "Loss for batch 105 = 1.0634604692459106\n",
      "Loss for batch 106 = 1.002587080001831\n",
      "\n",
      "Training Loss for epoch 9 = 104.84952545166016\n",
      "\n",
      "Current Validation Loss = 18.929534912109375\n",
      "Best Validation Loss = 16.784347534179688\n",
      "Epochs without Improvement = 1\n",
      "Train Accuracy: 34.07%\n",
      "Validation Accuracy: 36.55%\n",
      "\n",
      "Epoch 10\n",
      "----------\n",
      "Loss for batch 0 = 1.1585140228271484\n",
      "Loss for batch 1 = 0.7516440153121948\n",
      "Loss for batch 2 = 0.9834250211715698\n",
      "Loss for batch 3 = 0.8227444887161255\n",
      "Loss for batch 4 = 0.923715353012085\n",
      "Loss for batch 5 = 1.2695034742355347\n",
      "Loss for batch 6 = 1.1675269603729248\n",
      "Loss for batch 7 = 0.938525915145874\n",
      "Loss for batch 8 = 0.9552967548370361\n",
      "Loss for batch 9 = 0.8858626484870911\n",
      "Loss for batch 10 = 0.8041954040527344\n",
      "Loss for batch 11 = 0.93284010887146\n",
      "Loss for batch 12 = 0.9945767521858215\n",
      "Loss for batch 13 = 1.0456907749176025\n",
      "Loss for batch 14 = 1.003597378730774\n",
      "Loss for batch 15 = 0.8703885078430176\n",
      "Loss for batch 16 = 0.890923023223877\n",
      "Loss for batch 17 = 0.9471871256828308\n",
      "Loss for batch 18 = 0.8821770548820496\n",
      "Loss for batch 19 = 0.7278803586959839\n",
      "Loss for batch 20 = 0.8644219636917114\n",
      "Loss for batch 21 = 0.7267192602157593\n",
      "Loss for batch 22 = 0.9097331166267395\n",
      "Loss for batch 23 = 0.8959397077560425\n",
      "Loss for batch 24 = 0.772854745388031\n",
      "Loss for batch 25 = 0.9852622151374817\n",
      "Loss for batch 26 = 0.8365893363952637\n",
      "Loss for batch 27 = 0.6644121408462524\n",
      "Loss for batch 28 = 0.8772230744361877\n",
      "Loss for batch 29 = 0.8049271702766418\n",
      "Loss for batch 30 = 0.7375568747520447\n",
      "Loss for batch 31 = 0.8965205550193787\n",
      "Loss for batch 32 = 1.0937795639038086\n",
      "Loss for batch 33 = 1.1846768856048584\n",
      "Loss for batch 34 = 0.9310728907585144\n",
      "Loss for batch 35 = 0.8877729773521423\n",
      "Loss for batch 36 = 0.7252206802368164\n",
      "Loss for batch 37 = 0.7720493674278259\n",
      "Loss for batch 38 = 1.3229200839996338\n",
      "Loss for batch 39 = 0.8141713738441467\n",
      "Loss for batch 40 = 0.9732723832130432\n",
      "Loss for batch 41 = 1.0297410488128662\n",
      "Loss for batch 42 = 0.8197022676467896\n",
      "Loss for batch 43 = 0.973376452922821\n",
      "Loss for batch 44 = 0.7854977250099182\n",
      "Loss for batch 45 = 0.844745934009552\n",
      "Loss for batch 46 = 0.7319970726966858\n",
      "Loss for batch 47 = 1.0110317468643188\n",
      "Loss for batch 48 = 0.724632740020752\n",
      "Loss for batch 49 = 0.6869516372680664\n",
      "Loss for batch 50 = 0.7034154534339905\n",
      "Loss for batch 51 = 0.9453709125518799\n",
      "Loss for batch 52 = 0.9513771533966064\n",
      "Loss for batch 53 = 0.6817647814750671\n",
      "Loss for batch 54 = 0.7958893775939941\n",
      "Loss for batch 55 = 0.9267346858978271\n",
      "Loss for batch 56 = 0.7624973058700562\n",
      "Loss for batch 57 = 0.6844646334648132\n",
      "Loss for batch 58 = 0.841320276260376\n",
      "Loss for batch 59 = 0.868898332118988\n",
      "Loss for batch 60 = 0.7756420373916626\n",
      "Loss for batch 61 = 0.7446305751800537\n",
      "Loss for batch 62 = 0.8029942512512207\n",
      "Loss for batch 63 = 0.8497349619865417\n",
      "Loss for batch 64 = 0.8631888628005981\n",
      "Loss for batch 65 = 0.8852589130401611\n",
      "Loss for batch 66 = 0.6606689095497131\n",
      "Loss for batch 67 = 0.6210395693778992\n",
      "Loss for batch 68 = 0.9029671549797058\n",
      "Loss for batch 69 = 0.5754058361053467\n",
      "Loss for batch 70 = 0.8194448351860046\n",
      "Loss for batch 71 = 0.9601820707321167\n",
      "Loss for batch 72 = 1.58235502243042\n",
      "Loss for batch 73 = 0.8120000958442688\n",
      "Loss for batch 74 = 1.1196208000183105\n",
      "Loss for batch 75 = 0.9111471772193909\n",
      "Loss for batch 76 = 0.8590110540390015\n",
      "Loss for batch 77 = 0.9417037963867188\n",
      "Loss for batch 78 = 1.1067370176315308\n",
      "Loss for batch 79 = 0.8204370737075806\n",
      "Loss for batch 80 = 0.7188509106636047\n",
      "Loss for batch 81 = 0.996863603591919\n",
      "Loss for batch 82 = 0.7172954082489014\n",
      "Loss for batch 83 = 0.8320390582084656\n",
      "Loss for batch 84 = 1.0779719352722168\n",
      "Loss for batch 85 = 1.0494458675384521\n",
      "Loss for batch 86 = 0.6189567446708679\n",
      "Loss for batch 87 = 0.7087686657905579\n",
      "Loss for batch 88 = 0.7211953997612\n",
      "Loss for batch 89 = 0.7381237149238586\n",
      "Loss for batch 90 = 0.8231103420257568\n",
      "Loss for batch 91 = 0.8725074529647827\n",
      "Loss for batch 92 = 0.9018414616584778\n",
      "Loss for batch 93 = 0.9747785329818726\n",
      "Loss for batch 94 = 0.7589631080627441\n",
      "Loss for batch 95 = 0.7538917660713196\n",
      "Loss for batch 96 = 0.6548986434936523\n",
      "Loss for batch 97 = 0.7918444871902466\n",
      "Loss for batch 98 = 0.6490294933319092\n",
      "Loss for batch 99 = 0.608993649482727\n",
      "Loss for batch 100 = 0.8147722482681274\n",
      "Loss for batch 101 = 0.8172147870063782\n",
      "Loss for batch 102 = 0.6525290608406067\n",
      "Loss for batch 103 = 0.725854754447937\n",
      "Loss for batch 104 = 0.5940014719963074\n",
      "Loss for batch 105 = 0.8644868731498718\n",
      "Loss for batch 106 = 0.9232509136199951\n",
      "\n",
      "Training Loss for epoch 10 = 92.37835693359375\n",
      "\n",
      "Current Validation Loss = 14.9857177734375\n",
      "Best Validation Loss = 14.9857177734375\n",
      "Epochs without Improvement = 0\n",
      "Train Accuracy: 70.36%\n",
      "Validation Accuracy: 56.67%\n",
      "\n",
      "Epoch 11\n",
      "----------\n",
      "Loss for batch 0 = 0.639947772026062\n",
      "Loss for batch 1 = 0.5412330031394958\n",
      "Loss for batch 2 = 0.8906373381614685\n",
      "Loss for batch 3 = 0.9350921511650085\n",
      "Loss for batch 4 = 0.8882324695587158\n",
      "Loss for batch 5 = 0.7494959235191345\n",
      "Loss for batch 6 = 0.6766499876976013\n",
      "Loss for batch 7 = 0.8819146156311035\n",
      "Loss for batch 8 = 0.6377003192901611\n",
      "Loss for batch 9 = 0.7078063488006592\n",
      "Loss for batch 10 = 0.5664722919464111\n",
      "Loss for batch 11 = 0.6800811886787415\n",
      "Loss for batch 12 = 0.7565593123435974\n",
      "Loss for batch 13 = 1.1054840087890625\n",
      "Loss for batch 14 = 0.9291685819625854\n",
      "Loss for batch 15 = 0.6758830547332764\n",
      "Loss for batch 16 = 0.7282223701477051\n",
      "Loss for batch 17 = 0.6509864330291748\n",
      "Loss for batch 18 = 0.7686140537261963\n",
      "Loss for batch 19 = 0.6236982941627502\n",
      "Loss for batch 20 = 0.7734999060630798\n",
      "Loss for batch 21 = 0.5354220271110535\n",
      "Loss for batch 22 = 0.6525187492370605\n",
      "Loss for batch 23 = 0.5590764880180359\n",
      "Loss for batch 24 = 0.6317939758300781\n",
      "Loss for batch 25 = 0.8726418018341064\n",
      "Loss for batch 26 = 0.6156220436096191\n",
      "Loss for batch 27 = 0.5332814455032349\n",
      "Loss for batch 28 = 0.8658041954040527\n",
      "Loss for batch 29 = 0.8553600311279297\n",
      "Loss for batch 30 = 0.5950064063072205\n",
      "Loss for batch 31 = 0.932640552520752\n",
      "Loss for batch 32 = 0.7671569585800171\n",
      "Loss for batch 33 = 0.5411732196807861\n",
      "Loss for batch 34 = 0.6903197765350342\n",
      "Loss for batch 35 = 0.8015999794006348\n",
      "Loss for batch 36 = 0.5967324376106262\n",
      "Loss for batch 37 = 0.6182284355163574\n",
      "Loss for batch 38 = 0.7392235398292542\n",
      "Loss for batch 39 = 0.5088626146316528\n",
      "Loss for batch 40 = 0.8575032949447632\n",
      "Loss for batch 41 = 0.7291226983070374\n",
      "Loss for batch 42 = 0.7281611561775208\n",
      "Loss for batch 43 = 0.8226900100708008\n",
      "Loss for batch 44 = 0.6433196663856506\n",
      "Loss for batch 45 = 0.6743422746658325\n",
      "Loss for batch 46 = 0.6691022515296936\n",
      "Loss for batch 47 = 0.7912412285804749\n",
      "Loss for batch 48 = 0.544141411781311\n",
      "Loss for batch 49 = 0.5556224584579468\n",
      "Loss for batch 50 = 0.6372207403182983\n",
      "Loss for batch 51 = 0.7031615972518921\n",
      "Loss for batch 52 = 0.8039536476135254\n",
      "Loss for batch 53 = 0.699393093585968\n",
      "Loss for batch 54 = 0.6427938938140869\n",
      "Loss for batch 55 = 0.7029080986976624\n",
      "Loss for batch 56 = 0.644265353679657\n",
      "Loss for batch 57 = 0.5333347320556641\n",
      "Loss for batch 58 = 0.6154717206954956\n",
      "Loss for batch 59 = 0.6656957268714905\n",
      "Loss for batch 60 = 0.8286175727844238\n",
      "Loss for batch 61 = 0.823440432548523\n",
      "Loss for batch 62 = 0.7180554866790771\n",
      "Loss for batch 63 = 0.6012586951255798\n",
      "Loss for batch 64 = 0.6752627491950989\n",
      "Loss for batch 65 = 0.782673180103302\n",
      "Loss for batch 66 = 0.5814930200576782\n",
      "Loss for batch 67 = 0.46540942788124084\n",
      "Loss for batch 68 = 0.6838507056236267\n",
      "Loss for batch 69 = 0.5328812003135681\n",
      "Loss for batch 70 = 0.6180067658424377\n",
      "Loss for batch 71 = 0.5621992349624634\n",
      "Loss for batch 72 = 0.7521385550498962\n",
      "Loss for batch 73 = 0.5980547666549683\n",
      "Loss for batch 74 = 1.1325105428695679\n",
      "Loss for batch 75 = 0.7918481826782227\n",
      "Loss for batch 76 = 0.6571886539459229\n",
      "Loss for batch 77 = 0.7762907147407532\n",
      "Loss for batch 78 = 0.9605321288108826\n",
      "Loss for batch 79 = 0.5469825863838196\n",
      "Loss for batch 80 = 0.5809827446937561\n",
      "Loss for batch 81 = 0.7679648995399475\n",
      "Loss for batch 82 = 0.8152623176574707\n",
      "Loss for batch 83 = 0.6718044877052307\n",
      "Loss for batch 84 = 0.6353941559791565\n",
      "Loss for batch 85 = 0.9906024932861328\n",
      "Loss for batch 86 = 0.5861507654190063\n",
      "Loss for batch 87 = 0.5723374485969543\n",
      "Loss for batch 88 = 0.6189796924591064\n",
      "Loss for batch 89 = 0.676375687122345\n",
      "Loss for batch 90 = 0.7288205623626709\n",
      "Loss for batch 91 = 0.6768468618392944\n",
      "Loss for batch 92 = 0.6529563665390015\n",
      "Loss for batch 93 = 0.6812758445739746\n",
      "Loss for batch 94 = 0.6372839212417603\n",
      "Loss for batch 95 = 0.5459893345832825\n",
      "Loss for batch 96 = 0.5313316583633423\n",
      "Loss for batch 97 = 0.6438629031181335\n",
      "Loss for batch 98 = 0.5291205644607544\n",
      "Loss for batch 99 = 0.4136865735054016\n",
      "Loss for batch 100 = 0.5775493383407593\n",
      "Loss for batch 101 = 0.7587286233901978\n",
      "Loss for batch 102 = 0.5816747546195984\n",
      "Loss for batch 103 = 0.6836879253387451\n",
      "Loss for batch 104 = 0.4484509229660034\n",
      "Loss for batch 105 = 0.7609403729438782\n",
      "Loss for batch 106 = 0.8074125051498413\n",
      "\n",
      "Training Loss for epoch 11 = 74.04745483398438\n",
      "\n",
      "Current Validation Loss = 15.756418228149414\n",
      "Best Validation Loss = 14.9857177734375\n",
      "Epochs without Improvement = 1\n",
      "Train Accuracy: 75.62%\n",
      "Validation Accuracy: 58.93%\n",
      "\n",
      "Epoch 12\n",
      "----------\n",
      "Loss for batch 0 = 0.4895443618297577\n",
      "Loss for batch 1 = 0.28896400332450867\n",
      "Loss for batch 2 = 0.6742932200431824\n",
      "Loss for batch 3 = 0.6253760457038879\n",
      "Loss for batch 4 = 0.5291377902030945\n",
      "Loss for batch 5 = 0.4921724498271942\n",
      "Loss for batch 6 = 0.5624668598175049\n",
      "Loss for batch 7 = 0.7193955183029175\n",
      "Loss for batch 8 = 0.5394672155380249\n",
      "Loss for batch 9 = 0.544594407081604\n",
      "Loss for batch 10 = 0.5666133165359497\n",
      "Loss for batch 11 = 0.7668629884719849\n",
      "Loss for batch 12 = 1.4418693780899048\n",
      "Loss for batch 13 = 0.7376338243484497\n",
      "Loss for batch 14 = 0.8327956199645996\n",
      "Loss for batch 15 = 0.6820775270462036\n",
      "Loss for batch 16 = 0.7783481478691101\n",
      "Loss for batch 17 = 0.48841622471809387\n",
      "Loss for batch 18 = 0.6496849656105042\n",
      "Loss for batch 19 = 0.4498409032821655\n",
      "Loss for batch 20 = 0.5945717096328735\n",
      "Loss for batch 21 = 0.3312879800796509\n",
      "Loss for batch 22 = 0.640800416469574\n",
      "Loss for batch 23 = 0.7869269847869873\n",
      "Loss for batch 24 = 0.6249218583106995\n",
      "Loss for batch 25 = 0.7595028877258301\n",
      "Loss for batch 26 = 0.5349234342575073\n",
      "Loss for batch 27 = 0.49492138624191284\n",
      "Loss for batch 28 = 0.8279751539230347\n",
      "Loss for batch 29 = 0.5644973516464233\n",
      "Loss for batch 30 = 0.4971582293510437\n",
      "Loss for batch 31 = 0.7864779233932495\n",
      "Loss for batch 32 = 0.5077670812606812\n",
      "Loss for batch 33 = 0.45932862162590027\n",
      "Loss for batch 34 = 0.5525621175765991\n",
      "Loss for batch 35 = 0.5882982015609741\n",
      "Loss for batch 36 = 0.4072633683681488\n",
      "Loss for batch 37 = 0.49902233481407166\n",
      "Loss for batch 38 = 0.6588620543479919\n",
      "Loss for batch 39 = 0.43021610379219055\n",
      "Loss for batch 40 = 0.5114119648933411\n",
      "Loss for batch 41 = 0.7571227550506592\n",
      "Loss for batch 42 = 0.654863178730011\n",
      "Loss for batch 43 = 0.8036723732948303\n",
      "Loss for batch 44 = 0.7197888493537903\n",
      "Loss for batch 45 = 0.6173341274261475\n",
      "Loss for batch 46 = 0.5695353150367737\n",
      "Loss for batch 47 = 0.644284725189209\n",
      "Loss for batch 48 = 0.47444814443588257\n",
      "Loss for batch 49 = 0.6392141580581665\n",
      "Loss for batch 50 = 0.4368842840194702\n",
      "Loss for batch 51 = 0.4718409478664398\n",
      "Loss for batch 52 = 0.7303869724273682\n",
      "Loss for batch 53 = 0.7957476377487183\n",
      "Loss for batch 54 = 0.7878421545028687\n",
      "Loss for batch 55 = 0.6207941770553589\n",
      "Loss for batch 56 = 0.5439457297325134\n",
      "Loss for batch 57 = 0.40249741077423096\n",
      "Loss for batch 58 = 0.5648125410079956\n",
      "Loss for batch 59 = 0.6121140718460083\n",
      "Loss for batch 60 = 0.5612426400184631\n",
      "Loss for batch 61 = 0.606833815574646\n",
      "Loss for batch 62 = 0.4990622401237488\n",
      "Loss for batch 63 = 0.4558500051498413\n",
      "Loss for batch 64 = 0.5181201100349426\n",
      "Loss for batch 65 = 0.6055462956428528\n",
      "Loss for batch 66 = 0.5566390156745911\n",
      "Loss for batch 67 = 0.3914654850959778\n",
      "Loss for batch 68 = 0.6885979771614075\n",
      "Loss for batch 69 = 0.34137144684791565\n",
      "Loss for batch 70 = 0.4353303909301758\n",
      "Loss for batch 71 = 0.5315442085266113\n",
      "Loss for batch 72 = 0.6930973529815674\n",
      "Loss for batch 73 = 0.4784719944000244\n",
      "Loss for batch 74 = 1.041520357131958\n",
      "Loss for batch 75 = 0.7645915746688843\n",
      "Loss for batch 76 = 0.47818759083747864\n",
      "Loss for batch 77 = 0.6601027250289917\n",
      "Loss for batch 78 = 0.6326164603233337\n",
      "Loss for batch 79 = 0.3784638047218323\n",
      "Loss for batch 80 = 0.36890339851379395\n",
      "Loss for batch 81 = 0.7842633128166199\n",
      "Loss for batch 82 = 0.589787483215332\n",
      "Loss for batch 83 = 0.5815680623054504\n",
      "Loss for batch 84 = 0.4519711136817932\n",
      "Loss for batch 85 = 0.8216674327850342\n",
      "Loss for batch 86 = 0.4255983531475067\n",
      "Loss for batch 87 = 0.5064348578453064\n",
      "Loss for batch 88 = 0.4938933253288269\n",
      "Loss for batch 89 = 0.6039431691169739\n",
      "Loss for batch 90 = 0.49082690477371216\n",
      "Loss for batch 91 = 0.6069899797439575\n",
      "Loss for batch 92 = 0.461611270904541\n",
      "Loss for batch 93 = 0.742205798625946\n",
      "Loss for batch 94 = 0.5206974148750305\n",
      "Loss for batch 95 = 0.6059225797653198\n",
      "Loss for batch 96 = 0.4927011728286743\n",
      "Loss for batch 97 = 0.5599337816238403\n",
      "Loss for batch 98 = 0.40640416741371155\n",
      "Loss for batch 99 = 0.33971497416496277\n",
      "Loss for batch 100 = 0.46687695384025574\n",
      "Loss for batch 101 = 0.6415916681289673\n",
      "Loss for batch 102 = 0.43942567706108093\n",
      "Loss for batch 103 = 0.5921616554260254\n",
      "Loss for batch 104 = 0.31098201870918274\n",
      "Loss for batch 105 = 0.7175724506378174\n",
      "Loss for batch 106 = 0.6093685626983643\n",
      "\n",
      "Training Loss for epoch 12 = 62.717041015625\n",
      "\n",
      "Current Validation Loss = 17.242406845092773\n",
      "Best Validation Loss = 14.9857177734375\n",
      "Epochs without Improvement = 2\n",
      "Train Accuracy: 78.08%\n",
      "Validation Accuracy: 58.52%\n",
      "\n",
      "Epoch 13\n",
      "----------\n",
      "Loss for batch 0 = 0.4534410536289215\n",
      "Loss for batch 1 = 0.2484179437160492\n",
      "Loss for batch 2 = 0.5915478467941284\n",
      "Loss for batch 3 = 0.4502097964286804\n",
      "Loss for batch 4 = 0.49781695008277893\n",
      "Loss for batch 5 = 0.35045117139816284\n",
      "Loss for batch 6 = 0.597798228263855\n",
      "Loss for batch 7 = 0.5812370181083679\n",
      "Loss for batch 8 = 0.6330890655517578\n",
      "Loss for batch 9 = 0.4883700907230377\n",
      "Loss for batch 10 = 0.5052457451820374\n",
      "Loss for batch 11 = 0.3935961127281189\n",
      "Loss for batch 12 = 0.46097561717033386\n",
      "Loss for batch 13 = 0.4377591013908386\n",
      "Loss for batch 14 = 0.5239360332489014\n",
      "Loss for batch 15 = 0.5965210795402527\n",
      "Loss for batch 16 = 0.38785120844841003\n",
      "Loss for batch 17 = 0.347746342420578\n",
      "Loss for batch 18 = 0.31682589650154114\n",
      "Loss for batch 19 = 0.5446969270706177\n",
      "Loss for batch 20 = 0.5135757327079773\n",
      "Loss for batch 21 = 0.22628533840179443\n",
      "Loss for batch 22 = 0.44763585925102234\n",
      "Loss for batch 23 = 0.4038670063018799\n",
      "Loss for batch 24 = 0.34721460938453674\n",
      "Loss for batch 25 = 0.6545727252960205\n",
      "Loss for batch 26 = 0.4864727258682251\n",
      "Loss for batch 27 = 0.4890202283859253\n",
      "Loss for batch 28 = 0.8234589099884033\n",
      "Loss for batch 29 = 0.47107526659965515\n",
      "Loss for batch 30 = 0.3399015963077545\n",
      "Loss for batch 31 = 0.7759490609169006\n",
      "Loss for batch 32 = 0.31871750950813293\n",
      "Loss for batch 33 = 0.3828403651714325\n",
      "Loss for batch 34 = 0.429866224527359\n",
      "Loss for batch 35 = 0.3739633858203888\n",
      "Loss for batch 36 = 0.2734008729457855\n",
      "Loss for batch 37 = 0.5143064856529236\n",
      "Loss for batch 38 = 0.6861608028411865\n",
      "Loss for batch 39 = 0.35063236951828003\n",
      "Loss for batch 40 = 0.536998987197876\n",
      "Loss for batch 41 = 0.8797276020050049\n",
      "Loss for batch 42 = 0.7222524285316467\n",
      "Loss for batch 43 = 0.813057541847229\n",
      "Loss for batch 44 = 0.5435928702354431\n",
      "Loss for batch 45 = 0.42533212900161743\n",
      "Loss for batch 46 = 0.521245002746582\n",
      "Loss for batch 47 = 0.47206950187683105\n",
      "Loss for batch 48 = 0.30350828170776367\n",
      "Loss for batch 49 = 0.4562755525112152\n",
      "Loss for batch 50 = 0.446186900138855\n",
      "Loss for batch 51 = 0.44219186902046204\n",
      "Loss for batch 52 = 0.5896697640419006\n",
      "Loss for batch 53 = 0.3141669034957886\n",
      "Loss for batch 54 = 0.4237573742866516\n",
      "Loss for batch 55 = 0.4960319399833679\n",
      "Loss for batch 56 = 0.7418007850646973\n",
      "Loss for batch 57 = 0.27574247121810913\n",
      "Loss for batch 58 = 0.3889024555683136\n",
      "Loss for batch 59 = 0.4654642641544342\n",
      "Loss for batch 60 = 0.6973651647567749\n",
      "Loss for batch 61 = 0.6171582341194153\n",
      "Loss for batch 62 = 0.3390763998031616\n",
      "Loss for batch 63 = 0.37021228671073914\n",
      "Loss for batch 64 = 0.39704248309135437\n",
      "Loss for batch 65 = 0.4780733287334442\n",
      "Loss for batch 66 = 0.5489943027496338\n",
      "Loss for batch 67 = 0.3615609109401703\n",
      "Loss for batch 68 = 0.45369210839271545\n",
      "Loss for batch 69 = 0.2602187991142273\n",
      "Loss for batch 70 = 0.3265048563480377\n",
      "Loss for batch 71 = 0.37487438321113586\n",
      "Loss for batch 72 = 0.5991487503051758\n",
      "Loss for batch 73 = 0.4731374979019165\n",
      "Loss for batch 74 = 0.8666799664497375\n",
      "Loss for batch 75 = 0.5074952244758606\n",
      "Loss for batch 76 = 0.5639148354530334\n",
      "Loss for batch 77 = 0.8564594388008118\n",
      "Loss for batch 78 = 0.6417954564094543\n",
      "Loss for batch 79 = 0.32361263036727905\n",
      "Loss for batch 80 = 0.29514873027801514\n",
      "Loss for batch 81 = 0.6031159162521362\n",
      "Loss for batch 82 = 0.4495466947555542\n",
      "Loss for batch 83 = 0.5540960431098938\n",
      "Loss for batch 84 = 0.3528125286102295\n",
      "Loss for batch 85 = 0.5486322045326233\n",
      "Loss for batch 86 = 0.48392024636268616\n",
      "Loss for batch 87 = 0.4363596737384796\n",
      "Loss for batch 88 = 0.4342247545719147\n",
      "Loss for batch 89 = 0.5805673003196716\n",
      "Loss for batch 90 = 0.3823159635066986\n",
      "Loss for batch 91 = 0.8293856978416443\n",
      "Loss for batch 92 = 0.4617660641670227\n",
      "Loss for batch 93 = 0.6401755809783936\n",
      "Loss for batch 94 = 0.5294055938720703\n",
      "Loss for batch 95 = 0.39042139053344727\n",
      "Loss for batch 96 = 0.3596425950527191\n",
      "Loss for batch 97 = 0.5015228390693665\n",
      "Loss for batch 98 = 0.34204933047294617\n",
      "Loss for batch 99 = 0.4210256338119507\n",
      "Loss for batch 100 = 0.4291108250617981\n",
      "Loss for batch 101 = 0.6742455959320068\n",
      "Loss for batch 102 = 0.37336841225624084\n",
      "Loss for batch 103 = 0.45276564359664917\n",
      "Loss for batch 104 = 0.1701454222202301\n",
      "Loss for batch 105 = 0.6908696889877319\n",
      "Loss for batch 106 = 0.5908140540122986\n",
      "\n",
      "Training Loss for epoch 13 = 52.00889205932617\n",
      "\n",
      "Current Validation Loss = 17.99212646484375\n",
      "Best Validation Loss = 14.9857177734375\n",
      "Epochs without Improvement = 3\n",
      "Train Accuracy: 83.89%\n",
      "Validation Accuracy: 60.37%\n",
      "\n",
      "Epoch 14\n",
      "----------\n",
      "Loss for batch 0 = 0.27301087975502014\n",
      "Loss for batch 1 = 0.23828867077827454\n",
      "Loss for batch 2 = 0.5469295978546143\n",
      "Loss for batch 3 = 0.356199711561203\n",
      "Loss for batch 4 = 0.36652159690856934\n",
      "Loss for batch 5 = 0.2634662985801697\n",
      "Loss for batch 6 = 0.5302991271018982\n",
      "Loss for batch 7 = 0.7999712824821472\n",
      "Loss for batch 8 = 0.45644351840019226\n",
      "Loss for batch 9 = 0.44648799300193787\n",
      "Loss for batch 10 = 0.4080442488193512\n",
      "Loss for batch 11 = 0.29581570625305176\n",
      "Loss for batch 12 = 0.36055415868759155\n",
      "Loss for batch 13 = 0.2715837359428406\n",
      "Loss for batch 14 = 0.3810075521469116\n",
      "Loss for batch 15 = 0.4509490728378296\n",
      "Loss for batch 16 = 0.2545607089996338\n",
      "Loss for batch 17 = 0.33627650141716003\n",
      "Loss for batch 18 = 0.277495801448822\n",
      "Loss for batch 19 = 0.23970277607440948\n",
      "Loss for batch 20 = 0.39624130725860596\n",
      "Loss for batch 21 = 0.22137345373630524\n",
      "Loss for batch 22 = 0.40052157640457153\n",
      "Loss for batch 23 = 0.3006541430950165\n",
      "Loss for batch 24 = 0.28509676456451416\n",
      "Loss for batch 25 = 0.5573461651802063\n",
      "Loss for batch 26 = 0.38846930861473083\n",
      "Loss for batch 27 = 0.5611145496368408\n",
      "Loss for batch 28 = 0.601499617099762\n",
      "Loss for batch 29 = 0.3936895728111267\n",
      "Loss for batch 30 = 0.28042495250701904\n",
      "Loss for batch 31 = 0.6545279622077942\n",
      "Loss for batch 32 = 0.2071802318096161\n",
      "Loss for batch 33 = 0.20671971142292023\n",
      "Loss for batch 34 = 0.3285681903362274\n",
      "Loss for batch 35 = 0.25181859731674194\n",
      "Loss for batch 36 = 0.44466832280158997\n",
      "Loss for batch 37 = 0.4258442223072052\n",
      "Loss for batch 38 = 0.6261159777641296\n",
      "Loss for batch 39 = 0.34154319763183594\n",
      "Loss for batch 40 = 0.49981170892715454\n",
      "Loss for batch 41 = 0.4563124477863312\n",
      "Loss for batch 42 = 0.6246955394744873\n",
      "Loss for batch 43 = 0.7241591811180115\n",
      "Loss for batch 44 = 0.5278838872909546\n",
      "Loss for batch 45 = 0.3661988377571106\n",
      "Loss for batch 46 = 0.48507076501846313\n",
      "Loss for batch 47 = 0.3901996314525604\n",
      "Loss for batch 48 = 0.14262421429157257\n",
      "Loss for batch 49 = 0.2481498122215271\n",
      "Loss for batch 50 = 0.33405712246894836\n",
      "Loss for batch 51 = 0.45835256576538086\n",
      "Loss for batch 52 = 0.5917549729347229\n",
      "Loss for batch 53 = 0.2764831781387329\n",
      "Loss for batch 54 = 0.32852452993392944\n",
      "Loss for batch 55 = 0.42816492915153503\n",
      "Loss for batch 56 = 0.3837284445762634\n",
      "Loss for batch 57 = 0.2846030294895172\n",
      "Loss for batch 58 = 0.3380123972892761\n",
      "Loss for batch 59 = 0.4394340515136719\n",
      "Loss for batch 60 = 0.5138823390007019\n",
      "Loss for batch 61 = 0.5281839966773987\n",
      "Loss for batch 62 = 0.25952762365341187\n",
      "Loss for batch 63 = 0.2929217517375946\n",
      "Loss for batch 64 = 0.38783881068229675\n",
      "Loss for batch 65 = 0.3948937654495239\n",
      "Loss for batch 66 = 0.505547821521759\n",
      "Loss for batch 67 = 0.273491233587265\n",
      "Loss for batch 68 = 0.48539239168167114\n",
      "Loss for batch 69 = 0.24274183809757233\n",
      "Loss for batch 70 = 0.2686091959476471\n",
      "Loss for batch 71 = 0.2596757709980011\n",
      "Loss for batch 72 = 0.6555835604667664\n",
      "Loss for batch 73 = 0.4205878973007202\n",
      "Loss for batch 74 = 0.6200578212738037\n",
      "Loss for batch 75 = 0.4833020865917206\n",
      "Loss for batch 76 = 0.427154004573822\n",
      "Loss for batch 77 = 0.43722257018089294\n",
      "Loss for batch 78 = 0.4913981556892395\n",
      "Loss for batch 79 = 0.24272935092449188\n",
      "Loss for batch 80 = 0.25437089800834656\n",
      "Loss for batch 81 = 0.6768251657485962\n",
      "Loss for batch 82 = 0.33250829577445984\n",
      "Loss for batch 83 = 0.48901307582855225\n",
      "Loss for batch 84 = 0.2718051075935364\n",
      "Loss for batch 85 = 0.4681045114994049\n",
      "Loss for batch 86 = 0.35968104004859924\n",
      "Loss for batch 87 = 0.3365051746368408\n",
      "Loss for batch 88 = 0.3301425874233246\n",
      "Loss for batch 89 = 0.5071684718132019\n",
      "Loss for batch 90 = 0.2862103581428528\n",
      "Loss for batch 91 = 0.5971083045005798\n",
      "Loss for batch 92 = 0.35825949907302856\n",
      "Loss for batch 93 = 0.5760093927383423\n",
      "Loss for batch 94 = 0.5387016534805298\n",
      "Loss for batch 95 = 0.3148215413093567\n",
      "Loss for batch 96 = 0.27509644627571106\n",
      "Loss for batch 97 = 0.49381348490715027\n",
      "Loss for batch 98 = 0.24726778268814087\n",
      "Loss for batch 99 = 0.3927687704563141\n",
      "Loss for batch 100 = 0.38490188121795654\n",
      "Loss for batch 101 = 0.6187905669212341\n",
      "Loss for batch 102 = 0.23413307964801788\n",
      "Loss for batch 103 = 0.3332047462463379\n",
      "Loss for batch 104 = 0.11877627670764923\n",
      "Loss for batch 105 = 0.6670121550559998\n",
      "Loss for batch 106 = 0.483601838350296\n",
      "\n",
      "Training Loss for epoch 14 = 42.89259338378906\n",
      "\n",
      "Current Validation Loss = 19.26840591430664\n",
      "Best Validation Loss = 14.9857177734375\n",
      "Epochs without Improvement = 4\n",
      "Train Accuracy: 85.53%\n",
      "Validation Accuracy: 60.99%\n",
      "\n",
      "Epoch 15\n",
      "----------\n",
      "Loss for batch 0 = 0.2253791093826294\n",
      "Loss for batch 1 = 0.14229145646095276\n",
      "Loss for batch 2 = 0.5302340388298035\n",
      "Loss for batch 3 = 0.29920056462287903\n",
      "Loss for batch 4 = 0.24500849843025208\n",
      "Loss for batch 5 = 0.25014805793762207\n",
      "Loss for batch 6 = 0.47691577672958374\n",
      "Loss for batch 7 = 0.6291013956069946\n",
      "Loss for batch 8 = 0.3624274730682373\n",
      "Loss for batch 9 = 0.4411045014858246\n",
      "Loss for batch 10 = 0.35363006591796875\n",
      "Loss for batch 11 = 0.24540473520755768\n",
      "Loss for batch 12 = 0.33479800820350647\n",
      "Loss for batch 13 = 0.15639512240886688\n",
      "Loss for batch 14 = 0.38790398836135864\n",
      "Loss for batch 15 = 0.29663723707199097\n",
      "Loss for batch 16 = 0.34361732006073\n",
      "Loss for batch 17 = 0.2680906653404236\n",
      "Loss for batch 18 = 0.17461878061294556\n",
      "Loss for batch 19 = 0.11040149629116058\n",
      "Loss for batch 20 = 0.30284321308135986\n",
      "Loss for batch 21 = 0.16814669966697693\n",
      "Loss for batch 22 = 0.3696920573711395\n",
      "Loss for batch 23 = 0.11613407731056213\n",
      "Loss for batch 24 = 0.20863008499145508\n",
      "Loss for batch 25 = 0.37921059131622314\n",
      "Loss for batch 26 = 0.3766302466392517\n",
      "Loss for batch 27 = 0.5252662301063538\n",
      "Loss for batch 28 = 0.45633620023727417\n",
      "Loss for batch 29 = 0.2828667461872101\n",
      "Loss for batch 30 = 0.22858871519565582\n",
      "Loss for batch 31 = 0.6488528847694397\n",
      "Loss for batch 32 = 0.146077498793602\n",
      "Loss for batch 33 = 0.17451462149620056\n",
      "Loss for batch 34 = 0.2416948527097702\n",
      "Loss for batch 35 = 0.20794697105884552\n",
      "Loss for batch 36 = 0.336617648601532\n",
      "Loss for batch 37 = 0.5686047673225403\n",
      "Loss for batch 38 = 0.8815711736679077\n",
      "Loss for batch 39 = 0.5649481415748596\n",
      "Loss for batch 40 = 0.3878200948238373\n",
      "Loss for batch 41 = 0.38131898641586304\n",
      "Loss for batch 42 = 0.44906315207481384\n",
      "Loss for batch 43 = 0.4944128692150116\n",
      "Loss for batch 44 = 0.4767092764377594\n",
      "Loss for batch 45 = 0.2490697056055069\n",
      "Loss for batch 46 = 0.4602341651916504\n",
      "Loss for batch 47 = 0.3312057554721832\n",
      "Loss for batch 48 = 0.15139542520046234\n",
      "Loss for batch 49 = 0.2207777500152588\n",
      "Loss for batch 50 = 0.3283833861351013\n",
      "Loss for batch 51 = 0.33728665113449097\n",
      "Loss for batch 52 = 0.4538251757621765\n",
      "Loss for batch 53 = 0.32164108753204346\n",
      "Loss for batch 54 = 0.32619744539260864\n",
      "Loss for batch 55 = 0.21306945383548737\n",
      "Loss for batch 56 = 0.30536043643951416\n",
      "Loss for batch 57 = 0.29916203022003174\n",
      "Loss for batch 58 = 0.2992730736732483\n",
      "Loss for batch 59 = 0.3260517120361328\n",
      "Loss for batch 60 = 0.5828396677970886\n",
      "Loss for batch 61 = 0.40716782212257385\n",
      "Loss for batch 62 = 0.20537808537483215\n",
      "Loss for batch 63 = 0.2886800169944763\n",
      "Loss for batch 64 = 0.3452594578266144\n",
      "Loss for batch 65 = 0.32086992263793945\n",
      "Loss for batch 66 = 0.47003793716430664\n",
      "Loss for batch 67 = 0.10456684231758118\n",
      "Loss for batch 68 = 0.3654260039329529\n",
      "Loss for batch 69 = 0.1651698648929596\n",
      "Loss for batch 70 = 0.20940527319908142\n",
      "Loss for batch 71 = 0.18734042346477509\n",
      "Loss for batch 72 = 0.5477220416069031\n",
      "Loss for batch 73 = 0.36328595876693726\n",
      "Loss for batch 74 = 0.6517757177352905\n",
      "Loss for batch 75 = 0.38838109374046326\n",
      "Loss for batch 76 = 0.2121494859457016\n",
      "Loss for batch 77 = 0.4155920147895813\n",
      "Loss for batch 78 = 0.4624733030796051\n",
      "Loss for batch 79 = 0.21280018985271454\n",
      "Loss for batch 80 = 0.27223843336105347\n",
      "Loss for batch 81 = 0.7128575444221497\n",
      "Loss for batch 82 = 0.31233200430870056\n",
      "Loss for batch 83 = 0.442534863948822\n",
      "Loss for batch 84 = 0.20175135135650635\n",
      "Loss for batch 85 = 0.403485506772995\n",
      "Loss for batch 86 = 0.26452234387397766\n",
      "Loss for batch 87 = 0.3099808394908905\n",
      "Loss for batch 88 = 0.1984127014875412\n",
      "Loss for batch 89 = 0.5051560401916504\n",
      "Loss for batch 90 = 0.2799951434135437\n",
      "Loss for batch 91 = 0.2847276031970978\n",
      "Loss for batch 92 = 0.3431141972541809\n",
      "Loss for batch 93 = 0.5533250570297241\n",
      "Loss for batch 94 = 0.41805335879325867\n",
      "Loss for batch 95 = 0.5572869777679443\n",
      "Loss for batch 96 = 0.22799459099769592\n",
      "Loss for batch 97 = 0.44555261731147766\n",
      "Loss for batch 98 = 0.20103903114795685\n",
      "Loss for batch 99 = 0.17288857698440552\n",
      "Loss for batch 100 = 0.3435121774673462\n",
      "Loss for batch 101 = 0.6207277178764343\n",
      "Loss for batch 102 = 0.19970649480819702\n",
      "Loss for batch 103 = 0.36684542894363403\n",
      "Loss for batch 104 = 0.0992560163140297\n",
      "Loss for batch 105 = 0.5344480276107788\n",
      "Loss for batch 106 = 0.4644768238067627\n",
      "\n",
      "Training Loss for epoch 15 = 36.91326904296875\n",
      "\n",
      "Current Validation Loss = 19.96861457824707\n",
      "Best Validation Loss = 14.9857177734375\n",
      "Epochs without Improvement = 5\n",
      "Train Accuracy: 86.21%\n",
      "Validation Accuracy: 61.60%\n",
      "\n",
      "Epoch 16\n",
      "----------\n",
      "Loss for batch 0 = 0.18844634294509888\n",
      "Loss for batch 1 = 0.10946010053157806\n",
      "Loss for batch 2 = 0.42722299695014954\n",
      "Loss for batch 3 = 0.26868653297424316\n",
      "Loss for batch 4 = 0.17081475257873535\n",
      "Loss for batch 5 = 0.12495927512645721\n",
      "Loss for batch 6 = 0.36751118302345276\n",
      "Loss for batch 7 = 0.5834639072418213\n",
      "Loss for batch 8 = 0.2865506112575531\n",
      "Loss for batch 9 = 0.34503069519996643\n",
      "Loss for batch 10 = 0.36543789505958557\n",
      "Loss for batch 11 = 0.17196215689182281\n",
      "Loss for batch 12 = 0.19325515627861023\n",
      "Loss for batch 13 = 0.19678595662117004\n",
      "Loss for batch 14 = 0.44188418984413147\n",
      "Loss for batch 15 = 0.25093597173690796\n",
      "Loss for batch 16 = 0.20724394917488098\n",
      "Loss for batch 17 = 0.19846808910369873\n",
      "Loss for batch 18 = 0.10232937335968018\n",
      "Loss for batch 19 = 0.0657549649477005\n",
      "Loss for batch 20 = 0.2050608992576599\n",
      "Loss for batch 21 = 0.18815994262695312\n",
      "Loss for batch 22 = 0.318462610244751\n",
      "Loss for batch 23 = 0.11242073774337769\n",
      "Loss for batch 24 = 0.20216093957424164\n",
      "Loss for batch 25 = 0.30785679817199707\n",
      "Loss for batch 26 = 0.2617790997028351\n",
      "Loss for batch 27 = 0.6021439433097839\n",
      "Loss for batch 28 = 0.3840544819831848\n",
      "Loss for batch 29 = 0.25269925594329834\n",
      "Loss for batch 30 = 0.12326627224683762\n",
      "Loss for batch 31 = 0.5040751099586487\n",
      "Loss for batch 32 = 0.1625693291425705\n",
      "Loss for batch 33 = 0.09489243477582932\n",
      "Loss for batch 34 = 0.17061445116996765\n",
      "Loss for batch 35 = 0.1511315405368805\n",
      "Loss for batch 36 = 0.16410724818706512\n",
      "Loss for batch 37 = 0.4886476993560791\n",
      "Loss for batch 38 = 0.6871902942657471\n",
      "Loss for batch 39 = 0.2153543084859848\n",
      "Loss for batch 40 = 0.3070347309112549\n",
      "Loss for batch 41 = 0.2563706934452057\n",
      "Loss for batch 42 = 0.31102854013442993\n",
      "Loss for batch 43 = 0.40907523036003113\n",
      "Loss for batch 44 = 0.39903151988983154\n",
      "Loss for batch 45 = 0.2661067247390747\n",
      "Loss for batch 46 = 0.4029448628425598\n",
      "Loss for batch 47 = 0.30041199922561646\n",
      "Loss for batch 48 = 0.07954245060682297\n",
      "Loss for batch 49 = 0.17370687425136566\n",
      "Loss for batch 50 = 0.14948168396949768\n",
      "Loss for batch 51 = 0.29510775208473206\n",
      "Loss for batch 52 = 0.4143855571746826\n",
      "Loss for batch 53 = 0.18770886957645416\n",
      "Loss for batch 54 = 0.2529534101486206\n",
      "Loss for batch 55 = 0.19567671418190002\n",
      "Loss for batch 56 = 0.24188630282878876\n",
      "Loss for batch 57 = 0.23452331125736237\n",
      "Loss for batch 58 = 0.18231621384620667\n",
      "Loss for batch 59 = 0.25874805450439453\n",
      "Loss for batch 60 = 0.4247813820838928\n",
      "Loss for batch 61 = 0.34952297806739807\n",
      "Loss for batch 62 = 0.2981487214565277\n",
      "Loss for batch 63 = 0.17032921314239502\n",
      "Loss for batch 64 = 0.3268188238143921\n",
      "Loss for batch 65 = 0.2827630043029785\n",
      "Loss for batch 66 = 0.4421655237674713\n",
      "Loss for batch 67 = 0.057104725390672684\n",
      "Loss for batch 68 = 0.29618963599205017\n",
      "Loss for batch 69 = 0.11754195392131805\n",
      "Loss for batch 70 = 0.20510423183441162\n",
      "Loss for batch 71 = 0.16059842705726624\n",
      "Loss for batch 72 = 0.4976741671562195\n",
      "Loss for batch 73 = 0.3305239975452423\n",
      "Loss for batch 74 = 0.51907879114151\n",
      "Loss for batch 75 = 0.3591424524784088\n",
      "Loss for batch 76 = 0.2138277143239975\n",
      "Loss for batch 77 = 0.2998000979423523\n",
      "Loss for batch 78 = 0.17455166578292847\n",
      "Loss for batch 79 = 0.2096647173166275\n",
      "Loss for batch 80 = 0.15550534427165985\n",
      "Loss for batch 81 = 0.58799809217453\n",
      "Loss for batch 82 = 0.30426427721977234\n",
      "Loss for batch 83 = 0.34029778838157654\n",
      "Loss for batch 84 = 0.1523158699274063\n",
      "Loss for batch 85 = 0.36201098561286926\n",
      "Loss for batch 86 = 0.24792715907096863\n",
      "Loss for batch 87 = 0.24927981197834015\n",
      "Loss for batch 88 = 0.093873530626297\n",
      "Loss for batch 89 = 0.4965580701828003\n",
      "Loss for batch 90 = 0.2724328339099884\n",
      "Loss for batch 91 = 0.14930269122123718\n",
      "Loss for batch 92 = 0.20801538228988647\n",
      "Loss for batch 93 = 0.40359658002853394\n",
      "Loss for batch 94 = 0.2878287136554718\n",
      "Loss for batch 95 = 0.15717102587223053\n",
      "Loss for batch 96 = 0.20039086043834686\n",
      "Loss for batch 97 = 0.3946540653705597\n",
      "Loss for batch 98 = 0.18088503181934357\n",
      "Loss for batch 99 = 0.14665773510932922\n",
      "Loss for batch 100 = 0.27236035466194153\n",
      "Loss for batch 101 = 0.39620938897132874\n",
      "Loss for batch 102 = 0.08762173354625702\n",
      "Loss for batch 103 = 0.2939046621322632\n",
      "Loss for batch 104 = 0.3481973111629486\n",
      "Loss for batch 105 = 0.36067861318588257\n",
      "Loss for batch 106 = 0.4130779802799225\n",
      "\n",
      "Training Loss for epoch 16 = 29.27924156188965\n",
      "\n",
      "Current Validation Loss = 20.20210075378418\n",
      "Best Validation Loss = 14.9857177734375\n",
      "Epochs without Improvement = 6\n",
      "Train Accuracy: 91.67%\n",
      "Validation Accuracy: 61.81%\n",
      "\n",
      "Epoch 17\n",
      "----------\n",
      "Loss for batch 0 = 0.13540437817573547\n",
      "Loss for batch 1 = 0.0627838671207428\n",
      "Loss for batch 2 = 0.4036407768726349\n",
      "Loss for batch 3 = 0.17137707769870758\n",
      "Loss for batch 4 = 0.16691793501377106\n",
      "Loss for batch 5 = 0.06506452709436417\n",
      "Loss for batch 6 = 0.301511287689209\n",
      "Loss for batch 7 = 0.4292796850204468\n",
      "Loss for batch 8 = 0.30173105001449585\n",
      "Loss for batch 9 = 0.27164027094841003\n",
      "Loss for batch 10 = 0.3204371929168701\n",
      "Loss for batch 11 = 0.11328411847352982\n",
      "Loss for batch 12 = 0.11937477439641953\n",
      "Loss for batch 13 = 1.0983235836029053\n",
      "Loss for batch 14 = 0.44716331362724304\n",
      "Loss for batch 15 = 0.2580510377883911\n",
      "Loss for batch 16 = 0.3982295095920563\n",
      "Loss for batch 17 = 0.14196577668190002\n",
      "Loss for batch 18 = 0.08824203908443451\n",
      "Loss for batch 19 = 0.1927538365125656\n",
      "Loss for batch 20 = 0.171266108751297\n",
      "Loss for batch 21 = 0.15789316594600677\n",
      "Loss for batch 22 = 0.30426785349845886\n",
      "Loss for batch 23 = 0.08334583789110184\n",
      "Loss for batch 24 = 0.21998275816440582\n",
      "Loss for batch 25 = 0.2883000373840332\n",
      "Loss for batch 26 = 0.2846824824810028\n",
      "Loss for batch 27 = 0.5613343715667725\n",
      "Loss for batch 28 = 0.28717970848083496\n",
      "Loss for batch 29 = 0.2579188346862793\n",
      "Loss for batch 30 = 0.18046540021896362\n",
      "Loss for batch 31 = 0.5255866646766663\n",
      "Loss for batch 32 = 0.18317784368991852\n",
      "Loss for batch 33 = 0.1866908073425293\n",
      "Loss for batch 34 = 0.11511512100696564\n",
      "Loss for batch 35 = 0.17532838881015778\n",
      "Loss for batch 36 = 0.08559883385896683\n",
      "Loss for batch 37 = 0.2880111038684845\n",
      "Loss for batch 38 = 0.45374277234077454\n",
      "Loss for batch 39 = 0.21726708114147186\n",
      "Loss for batch 40 = 0.28202885389328003\n",
      "Loss for batch 41 = 0.28718817234039307\n",
      "Loss for batch 42 = 0.15873482823371887\n",
      "Loss for batch 43 = 0.2786272168159485\n",
      "Loss for batch 44 = 0.2736937999725342\n",
      "Loss for batch 45 = 0.22821612656116486\n",
      "Loss for batch 46 = 0.3541935980319977\n",
      "Loss for batch 47 = 0.25801360607147217\n",
      "Loss for batch 48 = 0.08592905849218369\n",
      "Loss for batch 49 = 0.18343380093574524\n",
      "Loss for batch 50 = 0.18435494601726532\n",
      "Loss for batch 51 = 0.19193965196609497\n",
      "Loss for batch 52 = 0.41083288192749023\n",
      "Loss for batch 53 = 0.19543012976646423\n",
      "Loss for batch 54 = 0.22738610208034515\n",
      "Loss for batch 55 = 0.22918583452701569\n",
      "Loss for batch 56 = 0.15021173655986786\n",
      "Loss for batch 57 = 0.18395067751407623\n",
      "Loss for batch 58 = 0.19180206954479218\n",
      "Loss for batch 59 = 0.20505140721797943\n",
      "Loss for batch 60 = 0.35988837480545044\n",
      "Loss for batch 61 = 0.26723405718803406\n",
      "Loss for batch 62 = 0.11793167144060135\n",
      "Loss for batch 63 = 0.12419672310352325\n",
      "Loss for batch 64 = 0.31117603182792664\n",
      "Loss for batch 65 = 0.22031401097774506\n",
      "Loss for batch 66 = 0.39162492752075195\n",
      "Loss for batch 67 = 0.06355611234903336\n",
      "Loss for batch 68 = 0.2607036530971527\n",
      "Loss for batch 69 = 0.1197044625878334\n",
      "Loss for batch 70 = 0.17169006168842316\n",
      "Loss for batch 71 = 0.1494639366865158\n",
      "Loss for batch 72 = 0.43808022141456604\n",
      "Loss for batch 73 = 0.27626511454582214\n",
      "Loss for batch 74 = 0.7773377895355225\n",
      "Loss for batch 75 = 0.255433589220047\n",
      "Loss for batch 76 = 0.11745014786720276\n",
      "Loss for batch 77 = 0.21692368388175964\n",
      "Loss for batch 78 = 0.14202839136123657\n",
      "Loss for batch 79 = 0.1475369781255722\n",
      "Loss for batch 80 = 0.18718792498111725\n",
      "Loss for batch 81 = 0.5511292219161987\n",
      "Loss for batch 82 = 0.212783083319664\n",
      "Loss for batch 83 = 0.18800298869609833\n",
      "Loss for batch 84 = 0.11151058971881866\n",
      "Loss for batch 85 = 0.2717664837837219\n",
      "Loss for batch 86 = 0.07464374601840973\n",
      "Loss for batch 87 = 0.21564792096614838\n",
      "Loss for batch 88 = 0.057332467287778854\n",
      "Loss for batch 89 = 0.40422165393829346\n",
      "Loss for batch 90 = 0.33388441801071167\n",
      "Loss for batch 91 = 0.10508954524993896\n",
      "Loss for batch 92 = 0.20824211835861206\n",
      "Loss for batch 93 = 0.4120127558708191\n",
      "Loss for batch 94 = 0.31848978996276855\n",
      "Loss for batch 95 = 0.1201714277267456\n",
      "Loss for batch 96 = 0.21268661320209503\n",
      "Loss for batch 97 = 0.39848724007606506\n",
      "Loss for batch 98 = 0.17874827980995178\n",
      "Loss for batch 99 = 0.1496848165988922\n",
      "Loss for batch 100 = 0.17758482694625854\n",
      "Loss for batch 101 = 0.223961740732193\n",
      "Loss for batch 102 = 0.04385409876704216\n",
      "Loss for batch 103 = 0.2115384340286255\n",
      "Loss for batch 104 = 0.17596620321273804\n",
      "Loss for batch 105 = 0.3107685148715973\n",
      "Loss for batch 106 = 0.4462486207485199\n",
      "\n",
      "Training Loss for epoch 17 = 26.304725646972656\n",
      "\n",
      "Current Validation Loss = 20.73685073852539\n",
      "Best Validation Loss = 14.9857177734375\n",
      "Epochs without Improvement = 7\n",
      "Train Accuracy: 93.66%\n",
      "Validation Accuracy: 61.60%\n",
      "\n",
      "Epoch 18\n",
      "----------\n",
      "Loss for batch 0 = 0.0731341540813446\n",
      "Loss for batch 1 = 0.04571332037448883\n",
      "Loss for batch 2 = 0.29792702198028564\n",
      "Loss for batch 3 = 0.24891144037246704\n",
      "Loss for batch 4 = 0.051702748984098434\n",
      "Loss for batch 5 = 0.044526949524879456\n",
      "Loss for batch 6 = 0.27851733565330505\n",
      "Loss for batch 7 = 0.4068431258201599\n",
      "Loss for batch 8 = 0.2438819706439972\n",
      "Loss for batch 9 = 0.24711844325065613\n",
      "Loss for batch 10 = 0.44483187794685364\n",
      "Loss for batch 11 = 0.09654512256383896\n",
      "Loss for batch 12 = 0.05934448540210724\n",
      "Loss for batch 13 = 0.23620463907718658\n",
      "Loss for batch 14 = 0.2761501967906952\n",
      "Loss for batch 15 = 0.2979058027267456\n",
      "Loss for batch 16 = 0.14418116211891174\n",
      "Loss for batch 17 = 0.18832308053970337\n",
      "Loss for batch 18 = 0.05238304287195206\n",
      "Loss for batch 19 = 0.04235989972949028\n",
      "Loss for batch 20 = 0.1380336731672287\n",
      "Loss for batch 21 = 0.1204719990491867\n",
      "Loss for batch 22 = 0.3355092406272888\n",
      "Loss for batch 23 = 0.05814788118004799\n",
      "Loss for batch 24 = 0.11743290722370148\n",
      "Loss for batch 25 = 0.2595609426498413\n",
      "Loss for batch 26 = 0.2022431194782257\n",
      "Loss for batch 27 = 0.509424090385437\n",
      "Loss for batch 28 = 0.3252042829990387\n",
      "Loss for batch 29 = 0.2730557322502136\n",
      "Loss for batch 30 = 0.09540437906980515\n",
      "Loss for batch 31 = 0.4050920903682709\n",
      "Loss for batch 32 = 0.1319611817598343\n",
      "Loss for batch 33 = 0.07077740877866745\n",
      "Loss for batch 34 = 0.15331575274467468\n",
      "Loss for batch 35 = 0.11954221874475479\n",
      "Loss for batch 36 = 0.0714535042643547\n",
      "Loss for batch 37 = 0.1808498203754425\n",
      "Loss for batch 38 = 0.3312564790248871\n",
      "Loss for batch 39 = 0.22105024755001068\n",
      "Loss for batch 40 = 0.20508544147014618\n",
      "Loss for batch 41 = 0.24861200153827667\n",
      "Loss for batch 42 = 0.17451438307762146\n",
      "Loss for batch 43 = 0.15474361181259155\n",
      "Loss for batch 44 = 0.2481682300567627\n",
      "Loss for batch 45 = 0.0932588204741478\n",
      "Loss for batch 46 = 0.3174270987510681\n",
      "Loss for batch 47 = 0.26032495498657227\n",
      "Loss for batch 48 = 0.06383520364761353\n",
      "Loss for batch 49 = 0.13503588736057281\n",
      "Loss for batch 50 = 0.14220255613327026\n",
      "Loss for batch 51 = 0.14660969376564026\n",
      "Loss for batch 52 = 0.22104573249816895\n",
      "Loss for batch 53 = 0.15722227096557617\n",
      "Loss for batch 54 = 0.2040489912033081\n",
      "Loss for batch 55 = 0.16986408829689026\n",
      "Loss for batch 56 = 0.3742719292640686\n",
      "Loss for batch 57 = 0.20384880900382996\n",
      "Loss for batch 58 = 0.1589006632566452\n",
      "Loss for batch 59 = 0.10990548133850098\n",
      "Loss for batch 60 = 0.25307032465934753\n",
      "Loss for batch 61 = 0.24521036446094513\n",
      "Loss for batch 62 = 0.07463900744915009\n",
      "Loss for batch 63 = 0.06372037529945374\n",
      "Loss for batch 64 = 0.12043877691030502\n",
      "Loss for batch 65 = 0.11231142282485962\n",
      "Loss for batch 66 = 0.34662747383117676\n",
      "Loss for batch 67 = 0.1102130115032196\n",
      "Loss for batch 68 = 0.20901453495025635\n",
      "Loss for batch 69 = 0.07103677839040756\n",
      "Loss for batch 70 = 0.1253465861082077\n",
      "Loss for batch 71 = 0.10803799331188202\n",
      "Loss for batch 72 = 0.41561359167099\n",
      "Loss for batch 73 = 0.3023105263710022\n",
      "Loss for batch 74 = 0.5686616897583008\n",
      "Loss for batch 75 = 0.22456005215644836\n",
      "Loss for batch 76 = 0.07047498226165771\n",
      "Loss for batch 77 = 0.2006998062133789\n",
      "Loss for batch 78 = 0.10724101215600967\n",
      "Loss for batch 79 = 0.11390399187803268\n",
      "Loss for batch 80 = 0.09148553013801575\n",
      "Loss for batch 81 = 0.5354158282279968\n",
      "Loss for batch 82 = 0.13108472526073456\n",
      "Loss for batch 83 = 0.08252377063035965\n",
      "Loss for batch 84 = 0.05831099674105644\n",
      "Loss for batch 85 = 0.213996022939682\n",
      "Loss for batch 86 = 0.046796321868896484\n",
      "Loss for batch 87 = 0.21922574937343597\n",
      "Loss for batch 88 = 0.08105668425559998\n",
      "Loss for batch 89 = 0.4987856447696686\n",
      "Loss for batch 90 = 0.20540274679660797\n",
      "Loss for batch 91 = 0.10318656265735626\n",
      "Loss for batch 92 = 0.2425212264060974\n",
      "Loss for batch 93 = 0.40745341777801514\n",
      "Loss for batch 94 = 0.22999565303325653\n",
      "Loss for batch 95 = 0.2130526304244995\n",
      "Loss for batch 96 = 0.27124762535095215\n",
      "Loss for batch 97 = 0.3171940743923187\n",
      "Loss for batch 98 = 0.1763470321893692\n",
      "Loss for batch 99 = 0.12473275512456894\n",
      "Loss for batch 100 = 0.2363390177488327\n",
      "Loss for batch 101 = 0.1773810088634491\n",
      "Loss for batch 102 = 0.0698074921965599\n",
      "Loss for batch 103 = 0.2398732304573059\n",
      "Loss for batch 104 = 0.10753155499696732\n",
      "Loss for batch 105 = 0.7271537780761719\n",
      "Loss for batch 106 = 0.3570835292339325\n",
      "\n",
      "Training Loss for epoch 18 = 21.693336486816406\n",
      "\n",
      "Current Validation Loss = 21.08793830871582\n",
      "Best Validation Loss = 14.9857177734375\n",
      "Epochs without Improvement = 8\n",
      "Train Accuracy: 92.22%\n",
      "Validation Accuracy: 62.83%\n",
      "\n",
      "Epoch 19\n",
      "----------\n",
      "Loss for batch 0 = 0.09991579502820969\n",
      "Loss for batch 1 = 0.043749939650297165\n",
      "Loss for batch 2 = 0.24875950813293457\n",
      "Loss for batch 3 = 0.1772182285785675\n",
      "Loss for batch 4 = 0.04340779408812523\n",
      "Loss for batch 5 = 0.05935546010732651\n",
      "Loss for batch 6 = 0.21316108107566833\n",
      "Loss for batch 7 = 0.4597347676753998\n",
      "Loss for batch 8 = 0.16528861224651337\n",
      "Loss for batch 9 = 0.12268240749835968\n",
      "Loss for batch 10 = 0.4736308455467224\n",
      "Loss for batch 11 = 0.07545920461416245\n",
      "Loss for batch 12 = 0.04537993669509888\n",
      "Loss for batch 13 = 0.06991653889417648\n",
      "Loss for batch 14 = 0.26988059282302856\n",
      "Loss for batch 15 = 0.11611739546060562\n",
      "Loss for batch 16 = 0.24087576568126678\n",
      "Loss for batch 17 = 0.06536082178354263\n",
      "Loss for batch 18 = 0.09708044677972794\n",
      "Loss for batch 19 = 0.045632652938365936\n",
      "Loss for batch 20 = 0.14216038584709167\n",
      "Loss for batch 21 = 0.12253354489803314\n",
      "Loss for batch 22 = 0.27479618787765503\n",
      "Loss for batch 23 = 0.028017524629831314\n",
      "Loss for batch 24 = 0.11181745678186417\n",
      "Loss for batch 25 = 0.2803742587566376\n",
      "Loss for batch 26 = 0.14343371987342834\n",
      "Loss for batch 27 = 0.4682796001434326\n",
      "Loss for batch 28 = 0.1971152424812317\n",
      "Loss for batch 29 = 0.2709510624408722\n",
      "Loss for batch 30 = 0.1224115639925003\n",
      "Loss for batch 31 = 0.3590942323207855\n",
      "Loss for batch 32 = 0.059943318367004395\n",
      "Loss for batch 33 = 0.10236057639122009\n",
      "Loss for batch 34 = 0.027790656313300133\n",
      "Loss for batch 35 = 0.17342688143253326\n",
      "Loss for batch 36 = 0.04616411775350571\n",
      "Loss for batch 37 = 0.09273318946361542\n",
      "Loss for batch 38 = 0.3786514401435852\n",
      "Loss for batch 39 = 0.33839285373687744\n",
      "Loss for batch 40 = 0.09913171082735062\n",
      "Loss for batch 41 = 0.18248403072357178\n",
      "Loss for batch 42 = 0.1022844910621643\n",
      "Loss for batch 43 = 0.07287825644016266\n",
      "Loss for batch 44 = 0.26163917779922485\n",
      "Loss for batch 45 = 0.08379024267196655\n",
      "Loss for batch 46 = 0.20963364839553833\n",
      "Loss for batch 47 = 0.2752719521522522\n",
      "Loss for batch 48 = 0.029788970947265625\n",
      "Loss for batch 49 = 0.09422469884157181\n",
      "Loss for batch 50 = 0.18289221823215485\n",
      "Loss for batch 51 = 0.10153114795684814\n",
      "Loss for batch 52 = 0.13464820384979248\n",
      "Loss for batch 53 = 0.12604524195194244\n",
      "Loss for batch 54 = 0.15166111290454865\n",
      "Loss for batch 55 = 0.0727444514632225\n",
      "Loss for batch 56 = 0.09469225257635117\n",
      "Loss for batch 57 = 0.1698022037744522\n",
      "Loss for batch 58 = 0.09936033934354782\n",
      "Loss for batch 59 = 0.16059379279613495\n",
      "Loss for batch 60 = 0.468563050031662\n",
      "Loss for batch 61 = 0.2764936089515686\n",
      "Loss for batch 62 = 0.05247391387820244\n",
      "Loss for batch 63 = 0.12292719632387161\n",
      "Loss for batch 64 = 0.1013667955994606\n",
      "Loss for batch 65 = 0.139755517244339\n",
      "Loss for batch 66 = 0.2244575321674347\n",
      "Loss for batch 67 = 0.07663754373788834\n",
      "Loss for batch 68 = 0.2724592387676239\n",
      "Loss for batch 69 = 0.08720275014638901\n",
      "Loss for batch 70 = 0.07930400222539902\n",
      "Loss for batch 71 = 0.11679013073444366\n",
      "Loss for batch 72 = 0.5855067372322083\n",
      "Loss for batch 73 = 0.07054000347852707\n",
      "Loss for batch 74 = 0.5915205478668213\n",
      "Loss for batch 75 = 0.19324657320976257\n",
      "Loss for batch 76 = 0.06290322542190552\n",
      "Loss for batch 77 = 0.1489725559949875\n",
      "Loss for batch 78 = 0.09931743890047073\n",
      "Loss for batch 79 = 0.05810721963644028\n",
      "Loss for batch 80 = 0.15802828967571259\n",
      "Loss for batch 81 = 0.336808443069458\n",
      "Loss for batch 82 = 0.1045762225985527\n",
      "Loss for batch 83 = 0.1352153867483139\n",
      "Loss for batch 84 = 0.07061228901147842\n",
      "Loss for batch 85 = 0.18390339612960815\n",
      "Loss for batch 86 = 0.02559899538755417\n",
      "Loss for batch 87 = 0.2590484321117401\n",
      "Loss for batch 88 = 0.02512018010020256\n",
      "Loss for batch 89 = 0.4527762234210968\n",
      "Loss for batch 90 = 0.1971493661403656\n",
      "Loss for batch 91 = 0.053090862929821014\n",
      "Loss for batch 92 = 0.17340390384197235\n",
      "Loss for batch 93 = 0.16867560148239136\n",
      "Loss for batch 94 = 0.14810842275619507\n",
      "Loss for batch 95 = 0.05656815692782402\n",
      "Loss for batch 96 = 0.23162047564983368\n",
      "Loss for batch 97 = 0.28577691316604614\n",
      "Loss for batch 98 = 0.21101056039333344\n",
      "Loss for batch 99 = 0.142506405711174\n",
      "Loss for batch 100 = 0.11701309680938721\n",
      "Loss for batch 101 = 0.15190932154655457\n",
      "Loss for batch 102 = 0.020888643339276314\n",
      "Loss for batch 103 = 0.08559194207191467\n",
      "Loss for batch 104 = 0.02921324409544468\n",
      "Loss for batch 105 = 0.4270162582397461\n",
      "Loss for batch 106 = 0.05657465383410454\n",
      "\n",
      "Training Loss for epoch 19 = 17.714506149291992\n",
      "\n",
      "Current Validation Loss = 24.894874572753906\n",
      "Best Validation Loss = 14.9857177734375\n",
      "Epochs without Improvement = 9\n",
      "Train Accuracy: 95.72%\n",
      "Validation Accuracy: 64.07%\n",
      "\n",
      "Epoch 20\n",
      "----------\n",
      "Loss for batch 0 = 0.0767122134566307\n",
      "Loss for batch 1 = 0.019678575918078423\n",
      "Loss for batch 2 = 0.19305098056793213\n",
      "Loss for batch 3 = 0.23939868807792664\n",
      "Loss for batch 4 = 0.023484598845243454\n",
      "Loss for batch 5 = 0.028786875307559967\n",
      "Loss for batch 6 = 0.23239374160766602\n",
      "Loss for batch 7 = 0.2752229869365692\n",
      "Loss for batch 8 = 0.09806960076093674\n",
      "Loss for batch 9 = 0.11915193498134613\n",
      "Loss for batch 10 = 0.3841693699359894\n",
      "Loss for batch 11 = 0.2241276055574417\n",
      "Loss for batch 12 = 0.14805415272712708\n",
      "Loss for batch 13 = 0.02541004866361618\n",
      "Loss for batch 14 = 0.3218320906162262\n",
      "Loss for batch 15 = 0.12568742036819458\n",
      "Loss for batch 16 = 0.048620160669088364\n",
      "Loss for batch 17 = 0.0778530165553093\n",
      "Loss for batch 18 = 0.0266739334911108\n",
      "Loss for batch 19 = 0.01956493966281414\n",
      "Loss for batch 20 = 0.1295323222875595\n",
      "Loss for batch 21 = 0.11687919497489929\n",
      "Loss for batch 22 = 0.2133377343416214\n",
      "Loss for batch 23 = 0.027031417936086655\n",
      "Loss for batch 24 = 0.07121272385120392\n",
      "Loss for batch 25 = 0.13399827480316162\n",
      "Loss for batch 26 = 0.12529146671295166\n",
      "Loss for batch 27 = 0.3834823668003082\n",
      "Loss for batch 28 = 0.13210201263427734\n",
      "Loss for batch 29 = 0.2761287987232208\n",
      "Loss for batch 30 = 0.09608224779367447\n",
      "Loss for batch 31 = 0.37156811356544495\n",
      "Loss for batch 32 = 0.02519574575126171\n",
      "Loss for batch 33 = 0.06064888834953308\n",
      "Loss for batch 34 = 0.02959495782852173\n",
      "Loss for batch 35 = 0.3248656392097473\n",
      "Loss for batch 36 = 0.03244227543473244\n",
      "Loss for batch 37 = 0.16735263168811798\n",
      "Loss for batch 38 = 0.3094184994697571\n",
      "Loss for batch 39 = 0.2320832759141922\n",
      "Loss for batch 40 = 0.05884314700961113\n",
      "Loss for batch 41 = 0.22655944526195526\n",
      "Loss for batch 42 = 0.13135288655757904\n",
      "Loss for batch 43 = 0.04716135561466217\n",
      "Loss for batch 44 = 0.23617148399353027\n",
      "Loss for batch 45 = 0.04575786739587784\n",
      "Loss for batch 46 = 0.22628115117549896\n",
      "Loss for batch 47 = 0.20742276310920715\n",
      "Loss for batch 48 = 0.026966582983732224\n",
      "Loss for batch 49 = 0.05651135742664337\n",
      "Loss for batch 50 = 0.11388048529624939\n",
      "Loss for batch 51 = 0.06421423703432083\n",
      "Loss for batch 52 = 0.13803666830062866\n",
      "Loss for batch 53 = 0.06311477720737457\n",
      "Loss for batch 54 = 0.09756667166948318\n",
      "Loss for batch 55 = 0.057734791189432144\n",
      "Loss for batch 56 = 0.12342823296785355\n",
      "Loss for batch 57 = 0.12911410629749298\n",
      "Loss for batch 58 = 0.1481616050004959\n",
      "Loss for batch 59 = 0.05604588985443115\n",
      "Loss for batch 60 = 0.11024320125579834\n",
      "Loss for batch 61 = 0.28314849734306335\n",
      "Loss for batch 62 = 0.062376949936151505\n",
      "Loss for batch 63 = 0.03458645939826965\n",
      "Loss for batch 64 = 0.07409968972206116\n",
      "Loss for batch 65 = 0.06524062901735306\n",
      "Loss for batch 66 = 0.09531015157699585\n",
      "Loss for batch 67 = 0.019244946539402008\n",
      "Loss for batch 68 = 0.08527188748121262\n",
      "Loss for batch 69 = 0.04330937936902046\n",
      "Loss for batch 70 = 0.06937142461538315\n",
      "Loss for batch 71 = 0.082636758685112\n",
      "Loss for batch 72 = 0.3542350232601166\n",
      "Loss for batch 73 = 0.017613191157579422\n",
      "Loss for batch 74 = 0.3056109845638275\n",
      "Loss for batch 75 = 0.1393432468175888\n",
      "Loss for batch 76 = 0.043669283390045166\n",
      "Loss for batch 77 = 0.08078908175230026\n",
      "Loss for batch 78 = 0.05234299600124359\n",
      "Loss for batch 79 = 0.03548542037606239\n",
      "Loss for batch 80 = 0.13827845454216003\n",
      "Loss for batch 81 = 0.2402900755405426\n",
      "Loss for batch 82 = 0.08616489171981812\n",
      "Loss for batch 83 = 0.038935527205467224\n",
      "Loss for batch 84 = 0.03574470058083534\n",
      "Loss for batch 85 = 0.30055516958236694\n",
      "Loss for batch 86 = 0.024237746372818947\n",
      "Loss for batch 87 = 0.2412518411874771\n",
      "Loss for batch 88 = 0.01898428611457348\n",
      "Loss for batch 89 = 0.23570212721824646\n",
      "Loss for batch 90 = 0.1750587522983551\n",
      "Loss for batch 91 = 0.06820419430732727\n",
      "Loss for batch 92 = 0.14115402102470398\n",
      "Loss for batch 93 = 0.15499180555343628\n",
      "Loss for batch 94 = 0.12417282164096832\n",
      "Loss for batch 95 = 0.04326039180159569\n",
      "Loss for batch 96 = 0.22352871298789978\n",
      "Loss for batch 97 = 0.26115483045578003\n",
      "Loss for batch 98 = 0.122382752597332\n",
      "Loss for batch 99 = 0.12557247281074524\n",
      "Loss for batch 100 = 0.1321168839931488\n",
      "Loss for batch 101 = 0.11944613605737686\n",
      "Loss for batch 102 = 0.02778778411448002\n",
      "Loss for batch 103 = 0.04749734699726105\n",
      "Loss for batch 104 = 0.0964793711900711\n",
      "Loss for batch 105 = 0.2721354365348816\n",
      "Loss for batch 106 = 0.05449632555246353\n",
      "\n",
      "Training Loss for epoch 20 = 13.824033737182617\n",
      "\n",
      "Current Validation Loss = 22.928316116333008\n",
      "Best Validation Loss = 14.9857177734375\n",
      "Epochs without Improvement = 10\n",
      "Train Accuracy: 97.24%\n",
      "Validation Accuracy: 62.83%\n",
      "\n",
      "Epoch 21\n",
      "----------\n",
      "Loss for batch 0 = 0.13292591273784637\n",
      "Loss for batch 1 = 0.020637527108192444\n",
      "Loss for batch 2 = 0.14201220870018005\n",
      "Loss for batch 3 = 0.12325020879507065\n",
      "Loss for batch 4 = 0.02067076601088047\n",
      "Loss for batch 5 = 0.09728891402482986\n",
      "Loss for batch 6 = 0.2103056013584137\n",
      "Loss for batch 7 = 0.24483241140842438\n",
      "Loss for batch 8 = 0.059565670788288116\n",
      "Loss for batch 9 = 0.05547189712524414\n",
      "Loss for batch 10 = 0.36598190665245056\n",
      "Loss for batch 11 = 0.08770555257797241\n",
      "Loss for batch 12 = 0.15926958620548248\n",
      "Loss for batch 13 = 0.05390465259552002\n",
      "Loss for batch 14 = 0.33142179250717163\n",
      "Loss for batch 15 = 0.04852248355746269\n",
      "Loss for batch 16 = 0.03852783143520355\n",
      "Loss for batch 17 = 0.026447992771863937\n",
      "Loss for batch 18 = 0.02201893739402294\n",
      "Loss for batch 19 = 0.0168741624802351\n",
      "Loss for batch 20 = 0.135305717587471\n",
      "Loss for batch 21 = 0.12165454775094986\n",
      "Loss for batch 22 = 0.181793674826622\n",
      "Loss for batch 23 = 0.034584056586027145\n",
      "Loss for batch 24 = 0.03832570090889931\n",
      "Loss for batch 25 = 0.13759824633598328\n",
      "Loss for batch 26 = 0.05220373719930649\n",
      "Loss for batch 27 = 0.35065245628356934\n",
      "Loss for batch 28 = 0.09428825229406357\n",
      "Loss for batch 29 = 0.28041377663612366\n",
      "Loss for batch 30 = 0.03740611672401428\n",
      "Loss for batch 31 = 0.3233879804611206\n",
      "Loss for batch 32 = 0.020284157246351242\n",
      "Loss for batch 33 = 0.03499983251094818\n",
      "Loss for batch 34 = 0.018706969916820526\n",
      "Loss for batch 35 = 0.24085041880607605\n",
      "Loss for batch 36 = 0.02749989926815033\n",
      "Loss for batch 37 = 0.13088016211986542\n",
      "Loss for batch 38 = 0.18604761362075806\n",
      "Loss for batch 39 = 0.21627599000930786\n",
      "Loss for batch 40 = 0.06199997663497925\n",
      "Loss for batch 41 = 0.21424706280231476\n",
      "Loss for batch 42 = 0.06673122942447662\n",
      "Loss for batch 43 = 0.034681789577007294\n",
      "Loss for batch 44 = 0.20128653943538666\n",
      "Loss for batch 45 = 0.04086458683013916\n",
      "Loss for batch 46 = 0.20608928799629211\n",
      "Loss for batch 47 = 0.11555235832929611\n",
      "Loss for batch 48 = 0.021151527762413025\n",
      "Loss for batch 49 = 0.037120357155799866\n",
      "Loss for batch 50 = 0.11577567458152771\n",
      "Loss for batch 51 = 0.03270493075251579\n",
      "Loss for batch 52 = 0.12269267439842224\n",
      "Loss for batch 53 = 0.04888056591153145\n",
      "Loss for batch 54 = 0.09595071524381638\n",
      "Loss for batch 55 = 0.030439861118793488\n",
      "Loss for batch 56 = 0.11995171755552292\n",
      "Loss for batch 57 = 0.08957227319478989\n",
      "Loss for batch 58 = 0.04819513112306595\n",
      "Loss for batch 59 = 0.03663095831871033\n",
      "Loss for batch 60 = 0.05414581298828125\n",
      "Loss for batch 61 = 0.2490447610616684\n",
      "Loss for batch 62 = 0.052974604070186615\n",
      "Loss for batch 63 = 0.03140392154455185\n",
      "Loss for batch 64 = 0.16914169490337372\n",
      "Loss for batch 65 = 0.04062805324792862\n",
      "Loss for batch 66 = 0.08768099546432495\n",
      "Loss for batch 67 = 0.01578206941485405\n",
      "Loss for batch 68 = 0.0527421310544014\n",
      "Loss for batch 69 = 0.04303885996341705\n",
      "Loss for batch 70 = 0.045873962342739105\n",
      "Loss for batch 71 = 0.05979987606406212\n",
      "Loss for batch 72 = 0.2764817178249359\n",
      "Loss for batch 73 = 0.016845300793647766\n",
      "Loss for batch 74 = 0.2444303333759308\n",
      "Loss for batch 75 = 0.11873965710401535\n",
      "Loss for batch 76 = 0.033155471086502075\n",
      "Loss for batch 77 = 0.0633733943104744\n",
      "Loss for batch 78 = 0.03807486221194267\n",
      "Loss for batch 79 = 0.031990692019462585\n",
      "Loss for batch 80 = 0.09081338346004486\n",
      "Loss for batch 81 = 0.1465388983488083\n",
      "Loss for batch 82 = 0.04604285582900047\n",
      "Loss for batch 83 = 0.047630731016397476\n",
      "Loss for batch 84 = 0.026185201480984688\n",
      "Loss for batch 85 = 0.23204509913921356\n",
      "Loss for batch 86 = 0.017084207385778427\n",
      "Loss for batch 87 = 0.2200845628976822\n",
      "Loss for batch 88 = 0.014158476144075394\n",
      "Loss for batch 89 = 0.18387554585933685\n",
      "Loss for batch 90 = 0.16500860452651978\n",
      "Loss for batch 91 = 0.04260673746466637\n",
      "Loss for batch 92 = 0.12445997446775436\n",
      "Loss for batch 93 = 0.15796302258968353\n",
      "Loss for batch 94 = 0.0628390833735466\n",
      "Loss for batch 95 = 0.03562745451927185\n",
      "Loss for batch 96 = 0.14833328127861023\n",
      "Loss for batch 97 = 0.23983536660671234\n",
      "Loss for batch 98 = 0.11874905973672867\n",
      "Loss for batch 99 = 0.12799392640590668\n",
      "Loss for batch 100 = 0.03274078294634819\n",
      "Loss for batch 101 = 0.11547761410474777\n",
      "Loss for batch 102 = 0.01571410335600376\n",
      "Loss for batch 103 = 0.040531277656555176\n",
      "Loss for batch 104 = 0.07226788252592087\n",
      "Loss for batch 105 = 0.23481744527816772\n",
      "Loss for batch 106 = 0.03302342817187309\n",
      "\n",
      "Training Loss for epoch 21 = 11.179108619689941\n",
      "\n",
      "Current Validation Loss = 23.592084884643555\n",
      "Best Validation Loss = 14.9857177734375\n",
      "Epochs without Improvement = 11\n",
      "Train Accuracy: 98.15%\n",
      "Validation Accuracy: 63.66%\n",
      "\n",
      "Epoch 22\n",
      "----------\n",
      "Loss for batch 0 = 0.03801178187131882\n",
      "Loss for batch 1 = 0.015456419438123703\n",
      "Loss for batch 2 = 0.10606261342763901\n",
      "Loss for batch 3 = 0.05041038990020752\n",
      "Loss for batch 4 = 0.015697313472628593\n",
      "Loss for batch 5 = 0.02525952085852623\n",
      "Loss for batch 6 = 0.20614008605480194\n",
      "Loss for batch 7 = 0.2360251545906067\n",
      "Loss for batch 8 = 0.049403708428144455\n",
      "Loss for batch 9 = 0.04828749969601631\n",
      "Loss for batch 10 = 0.35792478919029236\n",
      "Loss for batch 11 = 0.038707830011844635\n",
      "Loss for batch 12 = 0.10048002004623413\n",
      "Loss for batch 13 = 0.02797970362007618\n",
      "Loss for batch 14 = 0.23431171476840973\n",
      "Loss for batch 15 = 0.04453709349036217\n",
      "Loss for batch 16 = 0.02263234183192253\n",
      "Loss for batch 17 = 0.023826967924833298\n",
      "Loss for batch 18 = 0.015846012160182\n",
      "Loss for batch 19 = 0.014853794127702713\n",
      "Loss for batch 20 = 0.05802534520626068\n",
      "Loss for batch 21 = 0.1266973465681076\n",
      "Loss for batch 22 = 0.14655111730098724\n",
      "Loss for batch 23 = 0.02104179747402668\n",
      "Loss for batch 24 = 0.03242488205432892\n",
      "Loss for batch 25 = 0.13900278508663177\n",
      "Loss for batch 26 = 0.034029435366392136\n",
      "Loss for batch 27 = 0.293632447719574\n",
      "Loss for batch 28 = 0.07526194304227829\n",
      "Loss for batch 29 = 0.28540828824043274\n",
      "Loss for batch 30 = 0.021066704764962196\n",
      "Loss for batch 31 = 0.30226850509643555\n",
      "Loss for batch 32 = 0.015170441009104252\n",
      "Loss for batch 33 = 0.022918149828910828\n",
      "Loss for batch 34 = 0.01848176307976246\n",
      "Loss for batch 35 = 0.09637501090765\n",
      "Loss for batch 36 = 0.02297305315732956\n",
      "Loss for batch 37 = 0.08029486238956451\n",
      "Loss for batch 38 = 0.1442057341337204\n",
      "Loss for batch 39 = 0.2125229388475418\n",
      "Loss for batch 40 = 0.040628280490636826\n",
      "Loss for batch 41 = 0.212735116481781\n",
      "Loss for batch 42 = 0.048592399805784225\n",
      "Loss for batch 43 = 0.026628980413079262\n",
      "Loss for batch 44 = 0.18802934885025024\n",
      "Loss for batch 45 = 0.030817219987511635\n",
      "Loss for batch 46 = 0.17143367230892181\n",
      "Loss for batch 47 = 0.07677340507507324\n",
      "Loss for batch 48 = 0.0178653746843338\n",
      "Loss for batch 49 = 0.035444825887680054\n",
      "Loss for batch 50 = 0.12786553800106049\n",
      "Loss for batch 51 = 0.024225154891610146\n",
      "Loss for batch 52 = 0.027359923347830772\n",
      "Loss for batch 53 = 0.024190310388803482\n",
      "Loss for batch 54 = 0.09053236246109009\n",
      "Loss for batch 55 = 0.024664314463734627\n",
      "Loss for batch 56 = 0.1181645467877388\n",
      "Loss for batch 57 = 0.02710045874118805\n",
      "Loss for batch 58 = 0.035262588411569595\n",
      "Loss for batch 59 = 0.025123095139861107\n",
      "Loss for batch 60 = 0.06226770952343941\n",
      "Loss for batch 61 = 0.24055643379688263\n",
      "Loss for batch 62 = 0.025920681655406952\n",
      "Loss for batch 63 = 0.02878594771027565\n",
      "Loss for batch 64 = 0.04355240240693092\n",
      "Loss for batch 65 = 0.07660382241010666\n",
      "Loss for batch 66 = 0.058066170662641525\n",
      "Loss for batch 67 = 0.009932917542755604\n",
      "Loss for batch 68 = 0.040567051619291306\n",
      "Loss for batch 69 = 0.023727793246507645\n",
      "Loss for batch 70 = 0.024397078901529312\n",
      "Loss for batch 71 = 0.024111904203891754\n",
      "Loss for batch 72 = 0.2655487358570099\n",
      "Loss for batch 73 = 0.012485941872000694\n",
      "Loss for batch 74 = 0.20879711210727692\n",
      "Loss for batch 75 = 0.106949083507061\n",
      "Loss for batch 76 = 0.028083398938179016\n",
      "Loss for batch 77 = 0.030321579426527023\n",
      "Loss for batch 78 = 0.021718405187129974\n",
      "Loss for batch 79 = 0.01978415995836258\n",
      "Loss for batch 80 = 0.015290721319615841\n",
      "Loss for batch 81 = 0.12785547971725464\n",
      "Loss for batch 82 = 0.03682786226272583\n",
      "Loss for batch 83 = 0.021878181025385857\n",
      "Loss for batch 84 = 0.0221556406468153\n",
      "Loss for batch 85 = 0.168785959482193\n",
      "Loss for batch 86 = 0.015864670276641846\n",
      "Loss for batch 87 = 0.16937057673931122\n",
      "Loss for batch 88 = 0.009431920945644379\n",
      "Loss for batch 89 = 0.10148528963327408\n",
      "Loss for batch 90 = 0.12107450515031815\n",
      "Loss for batch 91 = 0.10521023720502853\n",
      "Loss for batch 92 = 0.09790574759244919\n",
      "Loss for batch 93 = 0.152289479970932\n",
      "Loss for batch 94 = 0.042504165321588516\n",
      "Loss for batch 95 = 0.011148611083626747\n",
      "Loss for batch 96 = 0.14716655015945435\n",
      "Loss for batch 97 = 0.21514490246772766\n",
      "Loss for batch 98 = 0.11199897527694702\n",
      "Loss for batch 99 = 0.1381249725818634\n",
      "Loss for batch 100 = 0.023089511319994926\n",
      "Loss for batch 101 = 0.11730136722326279\n",
      "Loss for batch 102 = 0.011472685262560844\n",
      "Loss for batch 103 = 0.028595218434929848\n",
      "Loss for batch 104 = 0.038978736847639084\n",
      "Loss for batch 105 = 0.22387169301509857\n",
      "Loss for batch 106 = 0.02498655952513218\n",
      "\n",
      "Training Loss for epoch 22 = 8.84563159942627\n",
      "\n",
      "Current Validation Loss = 25.884305953979492\n",
      "Best Validation Loss = 14.9857177734375\n",
      "Epochs without Improvement = 12\n",
      "Train Accuracy: 98.21%\n",
      "Validation Accuracy: 62.63%\n",
      "\n",
      "Epoch 23\n",
      "----------\n",
      "Loss for batch 0 = 0.023614929988980293\n",
      "Loss for batch 1 = 0.01131862960755825\n",
      "Loss for batch 2 = 0.058419279754161835\n",
      "Loss for batch 3 = 0.02456774190068245\n",
      "Loss for batch 4 = 0.1613418161869049\n",
      "Loss for batch 5 = 0.01253875344991684\n",
      "Loss for batch 6 = 0.19042454659938812\n",
      "Loss for batch 7 = 0.25705647468566895\n",
      "Loss for batch 8 = 0.04639352858066559\n",
      "Loss for batch 9 = 0.03217088431119919\n",
      "Loss for batch 10 = 0.35227808356285095\n",
      "Loss for batch 11 = 0.025443580001592636\n",
      "Loss for batch 12 = 0.07267094403505325\n",
      "Loss for batch 13 = 0.05971481278538704\n",
      "Loss for batch 14 = 0.22545994818210602\n",
      "Loss for batch 15 = 0.030598154291510582\n",
      "Loss for batch 16 = 0.017938723787665367\n",
      "Loss for batch 17 = 0.0173186045140028\n",
      "Loss for batch 18 = 0.011883925646543503\n",
      "Loss for batch 19 = 0.01397156435996294\n",
      "Loss for batch 20 = 0.029579654335975647\n",
      "Loss for batch 21 = 0.1264868825674057\n",
      "Loss for batch 22 = 0.128761425614357\n",
      "Loss for batch 23 = 0.020124109461903572\n",
      "Loss for batch 24 = 0.027655068784952164\n",
      "Loss for batch 25 = 0.1484024077653885\n",
      "Loss for batch 26 = 0.027532823383808136\n",
      "Loss for batch 27 = 0.21465206146240234\n",
      "Loss for batch 28 = 0.04954097792506218\n",
      "Loss for batch 29 = 0.28536927700042725\n",
      "Loss for batch 30 = 0.01821860484778881\n",
      "Loss for batch 31 = 0.29476457834243774\n",
      "Loss for batch 32 = 0.01380868162959814\n",
      "Loss for batch 33 = 0.02035745233297348\n",
      "Loss for batch 34 = 0.014325680211186409\n",
      "Loss for batch 35 = 0.038712695240974426\n",
      "Loss for batch 36 = 0.025166114792227745\n",
      "Loss for batch 37 = 0.03104824759066105\n",
      "Loss for batch 38 = 0.15710079669952393\n",
      "Loss for batch 39 = 0.1488242894411087\n",
      "Loss for batch 40 = 0.028638852760195732\n",
      "Loss for batch 41 = 0.20142538845539093\n",
      "Loss for batch 42 = 0.040856942534446716\n",
      "Loss for batch 43 = 0.022074582055211067\n",
      "Loss for batch 44 = 0.1775672733783722\n",
      "Loss for batch 45 = 0.02464011311531067\n",
      "Loss for batch 46 = 0.12902365624904633\n",
      "Loss for batch 47 = 0.05665677785873413\n",
      "Loss for batch 48 = 0.015244034118950367\n",
      "Loss for batch 49 = 0.019686948508024216\n",
      "Loss for batch 50 = 0.134484201669693\n",
      "Loss for batch 51 = 0.018843412399291992\n",
      "Loss for batch 52 = 0.017065178602933884\n",
      "Loss for batch 53 = 0.025984490290284157\n",
      "Loss for batch 54 = 0.08407533168792725\n",
      "Loss for batch 55 = 0.01501756627112627\n",
      "Loss for batch 56 = 0.0679132416844368\n",
      "Loss for batch 57 = 0.023862093687057495\n",
      "Loss for batch 58 = 0.0243825763463974\n",
      "Loss for batch 59 = 0.023349320515990257\n",
      "Loss for batch 60 = 0.033209677785634995\n",
      "Loss for batch 61 = 0.17633678019046783\n",
      "Loss for batch 62 = 0.01709798350930214\n",
      "Loss for batch 63 = 0.01783568784594536\n",
      "Loss for batch 64 = 0.02217777818441391\n",
      "Loss for batch 65 = 0.023447934538125992\n",
      "Loss for batch 66 = 0.04449734091758728\n",
      "Loss for batch 67 = 0.007898294366896152\n",
      "Loss for batch 68 = 0.018002066761255264\n",
      "Loss for batch 69 = 0.017447154968976974\n",
      "Loss for batch 70 = 0.018708838149905205\n",
      "Loss for batch 71 = 0.028036653995513916\n",
      "Loss for batch 72 = 0.20652054250240326\n",
      "Loss for batch 73 = 0.013680757023394108\n",
      "Loss for batch 74 = 0.23891210556030273\n",
      "Loss for batch 75 = 0.10196945071220398\n",
      "Loss for batch 76 = 0.033364199101924896\n",
      "Loss for batch 77 = 0.02355695702135563\n",
      "Loss for batch 78 = 0.022734345868229866\n",
      "Loss for batch 79 = 0.015337880700826645\n",
      "Loss for batch 80 = 0.015113596804440022\n",
      "Loss for batch 81 = 0.1261553317308426\n",
      "Loss for batch 82 = 0.03454488143324852\n",
      "Loss for batch 83 = 0.017758093774318695\n",
      "Loss for batch 84 = 0.015192953869700432\n",
      "Loss for batch 85 = 0.0937279611825943\n",
      "Loss for batch 86 = 0.008758352138102055\n",
      "Loss for batch 87 = 0.1623804122209549\n",
      "Loss for batch 88 = 0.007184728514403105\n",
      "Loss for batch 89 = 0.03269549831748009\n",
      "Loss for batch 90 = 0.10492569208145142\n",
      "Loss for batch 91 = 0.016047654673457146\n",
      "Loss for batch 92 = 0.06984414160251617\n",
      "Loss for batch 93 = 0.15908291935920715\n",
      "Loss for batch 94 = 0.028992149978876114\n",
      "Loss for batch 95 = 0.007870134897530079\n",
      "Loss for batch 96 = 0.15638436377048492\n",
      "Loss for batch 97 = 0.2004483938217163\n",
      "Loss for batch 98 = 0.10726690292358398\n",
      "Loss for batch 99 = 0.14944031834602356\n",
      "Loss for batch 100 = 0.012104026973247528\n",
      "Loss for batch 101 = 0.12200739979743958\n",
      "Loss for batch 102 = 0.00909839104861021\n",
      "Loss for batch 103 = 0.020655864849686623\n",
      "Loss for batch 104 = 0.018962834030389786\n",
      "Loss for batch 105 = 0.21913942694664001\n",
      "Loss for batch 106 = 0.02311747521162033\n",
      "\n",
      "Training Loss for epoch 23 = 7.636020183563232\n",
      "\n",
      "Current Validation Loss = 27.034719467163086\n",
      "Best Validation Loss = 14.9857177734375\n",
      "Epochs without Improvement = 13\n",
      "Train Accuracy: 98.68%\n",
      "Validation Accuracy: 63.45%\n",
      "\n",
      "Epoch 24\n",
      "----------\n",
      "Loss for batch 0 = 0.016366561874747276\n",
      "Loss for batch 1 = 0.0095953568816185\n",
      "Loss for batch 2 = 0.03739098086953163\n",
      "Loss for batch 3 = 0.01597946509718895\n",
      "Loss for batch 4 = 0.03938320651650429\n",
      "Loss for batch 5 = 0.009505445137619972\n",
      "Loss for batch 6 = 0.19690994918346405\n",
      "Loss for batch 7 = 0.18279077112674713\n",
      "Loss for batch 8 = 0.03358174115419388\n",
      "Loss for batch 9 = 0.0347098745405674\n",
      "Loss for batch 10 = 0.3568311333656311\n",
      "Loss for batch 11 = 0.017589006572961807\n",
      "Loss for batch 12 = 0.052114661782979965\n",
      "Loss for batch 13 = 0.015228507108986378\n",
      "Loss for batch 14 = 0.22804142534732819\n",
      "Loss for batch 15 = 0.02389754354953766\n",
      "Loss for batch 16 = 0.01552360225468874\n",
      "Loss for batch 17 = 0.014216884970664978\n",
      "Loss for batch 18 = 0.008648606948554516\n",
      "Loss for batch 19 = 0.00905865989625454\n",
      "Loss for batch 20 = 0.015380765311419964\n",
      "Loss for batch 21 = 0.13340455293655396\n",
      "Loss for batch 22 = 0.10993892699480057\n",
      "Loss for batch 23 = 0.0110219931229949\n",
      "Loss for batch 24 = 0.016797058284282684\n",
      "Loss for batch 25 = 0.14729253947734833\n",
      "Loss for batch 26 = 0.024594122543931007\n",
      "Loss for batch 27 = 0.18074892461299896\n",
      "Loss for batch 28 = 0.027677565813064575\n",
      "Loss for batch 29 = 0.28876999020576477\n",
      "Loss for batch 30 = 0.01966308429837227\n",
      "Loss for batch 31 = 0.29834240674972534\n",
      "Loss for batch 32 = 0.010489027015864849\n",
      "Loss for batch 33 = 0.013899770565330982\n",
      "Loss for batch 34 = 0.010840141214430332\n",
      "Loss for batch 35 = 0.02956540323793888\n",
      "Loss for batch 36 = 0.012572160921990871\n",
      "Loss for batch 37 = 0.015922609716653824\n",
      "Loss for batch 38 = 0.10442464053630829\n",
      "Loss for batch 39 = 0.14503026008605957\n",
      "Loss for batch 40 = 0.022610370069742203\n",
      "Loss for batch 41 = 0.19564825296401978\n",
      "Loss for batch 42 = 0.019535277038812637\n",
      "Loss for batch 43 = 0.0209803506731987\n",
      "Loss for batch 44 = 0.17341028153896332\n",
      "Loss for batch 45 = 0.018990274518728256\n",
      "Loss for batch 46 = 0.11286816000938416\n",
      "Loss for batch 47 = 0.0344410315155983\n",
      "Loss for batch 48 = 0.014032553881406784\n",
      "Loss for batch 49 = 0.01595974713563919\n",
      "Loss for batch 50 = 0.14105576276779175\n",
      "Loss for batch 51 = 0.015660082921385765\n",
      "Loss for batch 52 = 0.014629249460995197\n",
      "Loss for batch 53 = 0.013596594333648682\n",
      "Loss for batch 54 = 0.08764097839593887\n",
      "Loss for batch 55 = 0.011932943016290665\n",
      "Loss for batch 56 = 0.0431428961455822\n",
      "Loss for batch 57 = 0.025830326601862907\n",
      "Loss for batch 58 = 0.017461318522691727\n",
      "Loss for batch 59 = 0.015585184097290039\n",
      "Loss for batch 60 = 0.02136690355837345\n",
      "Loss for batch 61 = 0.17218117415905\n",
      "Loss for batch 62 = 0.013297086581587791\n",
      "Loss for batch 63 = 0.015063690021634102\n",
      "Loss for batch 64 = 0.014037884771823883\n",
      "Loss for batch 65 = 0.01650095544755459\n",
      "Loss for batch 66 = 0.025185422971844673\n",
      "Loss for batch 67 = 0.006927379872649908\n",
      "Loss for batch 68 = 0.014603680931031704\n",
      "Loss for batch 69 = 0.02071399800479412\n",
      "Loss for batch 70 = 0.013309797272086143\n",
      "Loss for batch 71 = 0.021403303369879723\n",
      "Loss for batch 72 = 0.021888086572289467\n",
      "Loss for batch 73 = 0.008901968598365784\n",
      "Loss for batch 74 = 0.16709625720977783\n",
      "Loss for batch 75 = 0.05994989350438118\n",
      "Loss for batch 76 = 0.010685267858207226\n",
      "Loss for batch 77 = 0.01745564490556717\n",
      "Loss for batch 78 = 0.013440229929983616\n",
      "Loss for batch 79 = 0.018185341730713844\n",
      "Loss for batch 80 = 0.0115652559325099\n",
      "Loss for batch 81 = 0.12362626194953918\n",
      "Loss for batch 82 = 0.020788507536053658\n",
      "Loss for batch 83 = 0.013390855863690376\n",
      "Loss for batch 84 = 0.015214908868074417\n",
      "Loss for batch 85 = 0.1036197692155838\n",
      "Loss for batch 86 = 0.013358795084059238\n",
      "Loss for batch 87 = 0.16389313340187073\n",
      "Loss for batch 88 = 0.006136090960353613\n",
      "Loss for batch 89 = 0.01504424773156643\n",
      "Loss for batch 90 = 0.10033777356147766\n",
      "Loss for batch 91 = 0.012038579210639\n",
      "Loss for batch 92 = 0.03320256248116493\n",
      "Loss for batch 93 = 0.15760205686092377\n",
      "Loss for batch 94 = 0.028243375942111015\n",
      "Loss for batch 95 = 0.007577077951282263\n",
      "Loss for batch 96 = 0.16221992671489716\n",
      "Loss for batch 97 = 0.20408563315868378\n",
      "Loss for batch 98 = 0.10995904356241226\n",
      "Loss for batch 99 = 0.15753813087940216\n",
      "Loss for batch 100 = 0.009163130074739456\n",
      "Loss for batch 101 = 0.1069042906165123\n",
      "Loss for batch 102 = 0.007257117889821529\n",
      "Loss for batch 103 = 0.012981196865439415\n",
      "Loss for batch 104 = 0.013018801808357239\n",
      "Loss for batch 105 = 0.21448743343353271\n",
      "Loss for batch 106 = 0.02225617878139019\n",
      "\n",
      "Training Loss for epoch 24 = 6.466455459594727\n",
      "\n",
      "Current Validation Loss = 27.765235900878906\n",
      "Best Validation Loss = 14.9857177734375\n",
      "Epochs without Improvement = 14\n",
      "Train Accuracy: 98.94%\n",
      "Validation Accuracy: 64.07%\n",
      "\n",
      "Epoch 25\n",
      "----------\n",
      "Loss for batch 0 = 0.018285881727933884\n",
      "Loss for batch 1 = 0.00869357492774725\n",
      "Loss for batch 2 = 0.05645330995321274\n",
      "Loss for batch 3 = 0.015698803588747978\n",
      "Loss for batch 4 = 0.007346515543758869\n",
      "Loss for batch 5 = 0.013771840371191502\n",
      "Loss for batch 6 = 0.11355846375226974\n",
      "Loss for batch 7 = 0.09748907387256622\n",
      "Loss for batch 8 = 0.03651576116681099\n",
      "Loss for batch 9 = 0.017715413123369217\n",
      "Loss for batch 10 = 0.35138851404190063\n",
      "Loss for batch 11 = 0.011441425420343876\n",
      "Loss for batch 12 = 0.015475106425583363\n",
      "Loss for batch 13 = 0.00975180882960558\n",
      "Loss for batch 14 = 0.2302533984184265\n",
      "Loss for batch 15 = 0.02167147397994995\n",
      "Loss for batch 16 = 0.010340874083340168\n",
      "Loss for batch 17 = 0.015217520296573639\n",
      "Loss for batch 18 = 0.006853356026113033\n",
      "Loss for batch 19 = 0.008742514997720718\n",
      "Loss for batch 20 = 0.010334121994674206\n",
      "Loss for batch 21 = 0.12481458485126495\n",
      "Loss for batch 22 = 0.094081349670887\n",
      "Loss for batch 23 = 0.011375447735190392\n",
      "Loss for batch 24 = 0.017558157444000244\n",
      "Loss for batch 25 = 0.14881359040737152\n",
      "Loss for batch 26 = 0.013152508065104485\n",
      "Loss for batch 27 = 0.15954405069351196\n",
      "Loss for batch 28 = 0.01781453751027584\n",
      "Loss for batch 29 = 0.2769054174423218\n",
      "Loss for batch 30 = 0.011569635942578316\n",
      "Loss for batch 31 = 0.28874948620796204\n",
      "Loss for batch 32 = 0.010489566251635551\n",
      "Loss for batch 33 = 0.013423524796962738\n",
      "Loss for batch 34 = 0.010242040269076824\n",
      "Loss for batch 35 = 0.0280455332249403\n",
      "Loss for batch 36 = 0.014653775840997696\n",
      "Loss for batch 37 = 0.010034927166998386\n",
      "Loss for batch 38 = 0.08469515293836594\n",
      "Loss for batch 39 = 0.12764593958854675\n",
      "Loss for batch 40 = 0.01605786755681038\n",
      "Loss for batch 41 = 0.18827243149280548\n",
      "Loss for batch 42 = 0.015231379307806492\n",
      "Loss for batch 43 = 0.024013802409172058\n",
      "Loss for batch 44 = 0.16858534514904022\n",
      "Loss for batch 45 = 0.019311966374516487\n",
      "Loss for batch 46 = 0.0889652743935585\n",
      "Loss for batch 47 = 0.026277687400579453\n",
      "Loss for batch 48 = 0.015468343161046505\n",
      "Loss for batch 49 = 0.014507637359201908\n",
      "Loss for batch 50 = 0.1434032917022705\n",
      "Loss for batch 51 = 0.014413559809327126\n",
      "Loss for batch 52 = 0.010690230876207352\n",
      "Loss for batch 53 = 0.010210017673671246\n",
      "Loss for batch 54 = 0.03481367602944374\n",
      "Loss for batch 55 = 0.011693019419908524\n",
      "Loss for batch 56 = 0.022905895486474037\n",
      "Loss for batch 57 = 0.03954073786735535\n",
      "Loss for batch 58 = 0.014807647094130516\n",
      "Loss for batch 59 = 0.016118085011839867\n",
      "Loss for batch 60 = 0.020321426913142204\n",
      "Loss for batch 61 = 0.1715894490480423\n",
      "Loss for batch 62 = 0.011920373886823654\n",
      "Loss for batch 63 = 0.011652110144495964\n",
      "Loss for batch 64 = 0.012359210290014744\n",
      "Loss for batch 65 = 0.012706119567155838\n",
      "Loss for batch 66 = 0.017628997564315796\n",
      "Loss for batch 67 = 0.00598117196932435\n",
      "Loss for batch 68 = 0.01309597585350275\n",
      "Loss for batch 69 = 0.011251666583120823\n",
      "Loss for batch 70 = 0.011994331143796444\n",
      "Loss for batch 71 = 0.03004503808915615\n",
      "Loss for batch 72 = 0.01317721325904131\n",
      "Loss for batch 73 = 0.008087746798992157\n",
      "Loss for batch 74 = 0.16284897923469543\n",
      "Loss for batch 75 = 0.033829204738140106\n",
      "Loss for batch 76 = 0.007879961282014847\n",
      "Loss for batch 77 = 0.012313181534409523\n",
      "Loss for batch 78 = 0.022019751369953156\n",
      "Loss for batch 79 = 0.00751016428694129\n",
      "Loss for batch 80 = 0.008333645761013031\n",
      "Loss for batch 81 = 0.12391138821840286\n",
      "Loss for batch 82 = 0.019352076575160027\n",
      "Loss for batch 83 = 0.019958559423685074\n",
      "Loss for batch 84 = 0.01000104658305645\n",
      "Loss for batch 85 = 0.08793386816978455\n",
      "Loss for batch 86 = 0.014440984465181828\n",
      "Loss for batch 87 = 0.16829143464565277\n",
      "Loss for batch 88 = 0.0053133005276322365\n",
      "Loss for batch 89 = 0.010763881728053093\n",
      "Loss for batch 90 = 0.08115328848361969\n",
      "Loss for batch 91 = 0.00991769041866064\n",
      "Loss for batch 92 = 0.04095396772027016\n",
      "Loss for batch 93 = 0.1526564061641693\n",
      "Loss for batch 94 = 0.02621486224234104\n",
      "Loss for batch 95 = 0.004677954129874706\n",
      "Loss for batch 96 = 0.16762061417102814\n",
      "Loss for batch 97 = 0.16220879554748535\n",
      "Loss for batch 98 = 0.0875345766544342\n",
      "Loss for batch 99 = 0.16275934875011444\n",
      "Loss for batch 100 = 0.011901292949914932\n",
      "Loss for batch 101 = 0.047502048313617706\n",
      "Loss for batch 102 = 0.0072036986239254475\n",
      "Loss for batch 103 = 0.02042355202138424\n",
      "Loss for batch 104 = 0.018513338640332222\n",
      "Loss for batch 105 = 0.2190171629190445\n",
      "Loss for batch 106 = 0.02423088252544403\n",
      "\n",
      "Training Loss for epoch 25 = 5.746897220611572\n",
      "\n",
      "Current Validation Loss = 29.477392196655273\n",
      "Best Validation Loss = 14.9857177734375\n",
      "Epochs without Improvement = 15\n",
      "Train Accuracy: 99.00%\n",
      "Validation Accuracy: 62.01%\n",
      "\n",
      "Epoch 26\n",
      "----------\n",
      "Loss for batch 0 = 0.009218423627316952\n",
      "Loss for batch 1 = 0.007519204169511795\n",
      "Loss for batch 2 = 0.02137613855302334\n",
      "Loss for batch 3 = 0.00865892507135868\n",
      "Loss for batch 4 = 0.005964177194982767\n",
      "Loss for batch 5 = 0.006458704359829426\n",
      "Loss for batch 6 = 0.10950492322444916\n",
      "Loss for batch 7 = 0.02091158553957939\n",
      "Loss for batch 8 = 0.011529667302966118\n",
      "Loss for batch 9 = 0.010694511234760284\n",
      "Loss for batch 10 = 0.35795119404792786\n",
      "Loss for batch 11 = 0.013544730842113495\n",
      "Loss for batch 12 = 0.006530019920319319\n",
      "Loss for batch 13 = 0.01575656048953533\n",
      "Loss for batch 14 = 0.23564322292804718\n",
      "Loss for batch 15 = 0.014181794598698616\n",
      "Loss for batch 16 = 0.010429145768284798\n",
      "Loss for batch 17 = 0.005922092590481043\n",
      "Loss for batch 18 = 0.005573017988353968\n",
      "Loss for batch 19 = 0.007987601682543755\n",
      "Loss for batch 20 = 0.007504831999540329\n",
      "Loss for batch 21 = 0.10866114497184753\n",
      "Loss for batch 22 = 0.08348169922828674\n",
      "Loss for batch 23 = 0.006572067271918058\n",
      "Loss for batch 24 = 0.011900057084858418\n",
      "Loss for batch 25 = 0.15331150591373444\n",
      "Loss for batch 26 = 0.011301667429506779\n",
      "Loss for batch 27 = 0.16249944269657135\n",
      "Loss for batch 28 = 0.017830967903137207\n",
      "Loss for batch 29 = 0.2467648983001709\n",
      "Loss for batch 30 = 0.00904298946261406\n",
      "Loss for batch 31 = 0.2917112112045288\n",
      "Loss for batch 32 = 0.010545066557824612\n",
      "Loss for batch 33 = 0.011462440714240074\n",
      "Loss for batch 34 = 0.00878466572612524\n",
      "Loss for batch 35 = 0.017225507646799088\n",
      "Loss for batch 36 = 0.01119963638484478\n",
      "Loss for batch 37 = 0.008480442687869072\n",
      "Loss for batch 38 = 0.06334121525287628\n",
      "Loss for batch 39 = 0.038093242794275284\n",
      "Loss for batch 40 = 0.013337154872715473\n",
      "Loss for batch 41 = 0.13427790999412537\n",
      "Loss for batch 42 = 0.014376016333699226\n",
      "Loss for batch 43 = 0.023159446194767952\n",
      "Loss for batch 44 = 0.16837915778160095\n",
      "Loss for batch 45 = 0.01933654025197029\n",
      "Loss for batch 46 = 0.08559349924325943\n",
      "Loss for batch 47 = 0.0209596399217844\n",
      "Loss for batch 48 = 0.013398516923189163\n",
      "Loss for batch 49 = 0.014177069067955017\n",
      "Loss for batch 50 = 0.1334080994129181\n",
      "Loss for batch 51 = 0.017113761976361275\n",
      "Loss for batch 52 = 0.007564316503703594\n",
      "Loss for batch 53 = 0.008994526229798794\n",
      "Loss for batch 54 = 0.012213512323796749\n",
      "Loss for batch 55 = 0.007345095742493868\n",
      "Loss for batch 56 = 0.014033769257366657\n",
      "Loss for batch 57 = 0.06057088077068329\n",
      "Loss for batch 58 = 0.014699965715408325\n",
      "Loss for batch 59 = 0.015815921127796173\n",
      "Loss for batch 60 = 0.013522055931389332\n",
      "Loss for batch 61 = 0.16775605082511902\n",
      "Loss for batch 62 = 0.007874959148466587\n",
      "Loss for batch 63 = 0.009469646960496902\n",
      "Loss for batch 64 = 0.012973589822649956\n",
      "Loss for batch 65 = 0.009647018276154995\n",
      "Loss for batch 66 = 0.020083218812942505\n",
      "Loss for batch 67 = 0.005166890099644661\n",
      "Loss for batch 68 = 0.008263612166047096\n",
      "Loss for batch 69 = 0.01234433613717556\n",
      "Loss for batch 70 = 0.008174687623977661\n",
      "Loss for batch 71 = 0.019558776170015335\n",
      "Loss for batch 72 = 0.011302824132144451\n",
      "Loss for batch 73 = 0.00942017138004303\n",
      "Loss for batch 74 = 0.1589573323726654\n",
      "Loss for batch 75 = 0.020949888974428177\n",
      "Loss for batch 76 = 0.006875793449580669\n",
      "Loss for batch 77 = 0.010693862102925777\n",
      "Loss for batch 78 = 0.013339269906282425\n",
      "Loss for batch 79 = 0.005929485894739628\n",
      "Loss for batch 80 = 0.03588850796222687\n",
      "Loss for batch 81 = 0.1163753941655159\n",
      "Loss for batch 82 = 0.0087455278262496\n",
      "Loss for batch 83 = 0.00777504313737154\n",
      "Loss for batch 84 = 0.008545135147869587\n",
      "Loss for batch 85 = 0.08959520608186722\n",
      "Loss for batch 86 = 0.006314000114798546\n",
      "Loss for batch 87 = 0.17539983987808228\n",
      "Loss for batch 88 = 0.004628482274711132\n",
      "Loss for batch 89 = 0.01018193457275629\n",
      "Loss for batch 90 = 0.08175119757652283\n",
      "Loss for batch 91 = 0.007916591130197048\n",
      "Loss for batch 92 = 0.02196771651506424\n",
      "Loss for batch 93 = 0.17418155074119568\n",
      "Loss for batch 94 = 0.024732157588005066\n",
      "Loss for batch 95 = 0.006078753620386124\n",
      "Loss for batch 96 = 0.1689717322587967\n",
      "Loss for batch 97 = 0.10931169986724854\n",
      "Loss for batch 98 = 0.09271685779094696\n",
      "Loss for batch 99 = 0.16420409083366394\n",
      "Loss for batch 100 = 0.008622991852462292\n",
      "Loss for batch 101 = 0.011998445726931095\n",
      "Loss for batch 102 = 0.006513761822134256\n",
      "Loss for batch 103 = 0.012893971055746078\n",
      "Loss for batch 104 = 0.021363424137234688\n",
      "Loss for batch 105 = 0.20249059796333313\n",
      "Loss for batch 106 = 0.024624787271022797\n",
      "\n",
      "Training Loss for epoch 26 = 5.099917411804199\n",
      "\n",
      "Current Validation Loss = 30.215370178222656\n",
      "Best Validation Loss = 14.9857177734375\n",
      "Epochs without Improvement = 16\n",
      "Train Accuracy: 98.59%\n",
      "Validation Accuracy: 62.63%\n",
      "\n",
      "Epoch 27\n",
      "----------\n",
      "Loss for batch 0 = 0.008540106005966663\n",
      "Loss for batch 1 = 0.008382180705666542\n",
      "Loss for batch 2 = 0.013207283802330494\n",
      "Loss for batch 3 = 0.013782646507024765\n",
      "Loss for batch 4 = 0.005134278908371925\n",
      "Loss for batch 5 = 0.008219578303396702\n",
      "Loss for batch 6 = 0.09210751950740814\n",
      "Loss for batch 7 = 0.03289681673049927\n",
      "Loss for batch 8 = 0.00979053508490324\n",
      "Loss for batch 9 = 0.010701743885874748\n",
      "Loss for batch 10 = 0.35848820209503174\n",
      "Loss for batch 11 = 0.011456680484116077\n",
      "Loss for batch 12 = 0.008250326849520206\n",
      "Loss for batch 13 = 0.017986107617616653\n",
      "Loss for batch 14 = 0.23793436586856842\n",
      "Loss for batch 15 = 0.0760810524225235\n",
      "Loss for batch 16 = 0.006527187768369913\n",
      "Loss for batch 17 = 0.03584584966301918\n",
      "Loss for batch 18 = 0.0048659685999155045\n",
      "Loss for batch 19 = 0.005453088320791721\n",
      "Loss for batch 20 = 0.007695318665355444\n",
      "Loss for batch 21 = 0.014089529402554035\n",
      "Loss for batch 22 = 0.08077328652143478\n",
      "Loss for batch 23 = 0.005929117556661367\n",
      "Loss for batch 24 = 0.010699738748371601\n",
      "Loss for batch 25 = 0.1413974165916443\n",
      "Loss for batch 26 = 0.011781674809753895\n",
      "Loss for batch 27 = 0.15643024444580078\n",
      "Loss for batch 28 = 0.010689733549952507\n",
      "Loss for batch 29 = 0.20118758082389832\n",
      "Loss for batch 30 = 0.011548750102519989\n",
      "Loss for batch 31 = 0.29266899824142456\n",
      "Loss for batch 32 = 0.008820739574730396\n",
      "Loss for batch 33 = 0.008080539293587208\n",
      "Loss for batch 34 = 0.00965815968811512\n",
      "Loss for batch 35 = 0.019846633076667786\n",
      "Loss for batch 36 = 0.02004016935825348\n",
      "Loss for batch 37 = 0.007821349427103996\n",
      "Loss for batch 38 = 0.07389888912439346\n",
      "Loss for batch 39 = 0.018178841099143028\n",
      "Loss for batch 40 = 0.013028264977037907\n",
      "Loss for batch 41 = 0.07518322765827179\n",
      "Loss for batch 42 = 0.010576452128589153\n",
      "Loss for batch 43 = 0.016215814277529716\n",
      "Loss for batch 44 = 0.15757513046264648\n",
      "Loss for batch 45 = 0.039015695452690125\n",
      "Loss for batch 46 = 0.05741569772362709\n",
      "Loss for batch 47 = 0.06731461733579636\n",
      "Loss for batch 48 = 0.012683723121881485\n",
      "Loss for batch 49 = 0.015056976117193699\n",
      "Loss for batch 50 = 0.09903740882873535\n",
      "Loss for batch 51 = 0.017158959060907364\n",
      "Loss for batch 52 = 0.006327321752905846\n",
      "Loss for batch 53 = 0.03364456817507744\n",
      "Loss for batch 54 = 0.05299053713679314\n",
      "Loss for batch 55 = 0.006767015438526869\n",
      "Loss for batch 56 = 0.0434880405664444\n",
      "Loss for batch 57 = 0.03070068359375\n",
      "Loss for batch 58 = 0.009303557686507702\n",
      "Loss for batch 59 = 0.011128849349915981\n",
      "Loss for batch 60 = 0.02797836996614933\n",
      "Loss for batch 61 = 0.17421120405197144\n",
      "Loss for batch 62 = 0.0072861346416175365\n",
      "Loss for batch 63 = 0.006490183062851429\n",
      "Loss for batch 64 = 0.00701068714261055\n",
      "Loss for batch 65 = 0.00946776196360588\n",
      "Loss for batch 66 = 0.0157989040017128\n",
      "Loss for batch 67 = 0.004940171260386705\n",
      "Loss for batch 68 = 0.00892691407352686\n",
      "Loss for batch 69 = 0.009103701449930668\n",
      "Loss for batch 70 = 0.006037130020558834\n",
      "Loss for batch 71 = 0.008357508108019829\n",
      "Loss for batch 72 = 0.011565900407731533\n",
      "Loss for batch 73 = 0.007298503536731005\n",
      "Loss for batch 74 = 0.17046169936656952\n",
      "Loss for batch 75 = 0.01517169177532196\n",
      "Loss for batch 76 = 0.005843356251716614\n",
      "Loss for batch 77 = 0.008794404566287994\n",
      "Loss for batch 78 = 0.009811214171350002\n",
      "Loss for batch 79 = 0.0054061939008533955\n",
      "Loss for batch 80 = 0.15364950895309448\n",
      "Loss for batch 81 = 0.10888645797967911\n",
      "Loss for batch 82 = 0.012933655641973019\n",
      "Loss for batch 83 = 0.022230343893170357\n",
      "Loss for batch 84 = 0.008566769771277905\n",
      "Loss for batch 85 = 0.10553658753633499\n",
      "Loss for batch 86 = 0.0048850951716303825\n",
      "Loss for batch 87 = 0.17856673896312714\n",
      "Loss for batch 88 = 0.004145437851548195\n",
      "Loss for batch 89 = 0.009164530783891678\n",
      "Loss for batch 90 = 0.07660850882530212\n",
      "Loss for batch 91 = 0.007409804966300726\n",
      "Loss for batch 92 = 0.017239436507225037\n",
      "Loss for batch 93 = 0.13948950171470642\n",
      "Loss for batch 94 = 0.01027627568691969\n",
      "Loss for batch 95 = 0.004317617043852806\n",
      "Loss for batch 96 = 0.16972221434116364\n",
      "Loss for batch 97 = 0.08685071766376495\n",
      "Loss for batch 98 = 0.0861632227897644\n",
      "Loss for batch 99 = 0.16503316164016724\n",
      "Loss for batch 100 = 0.009584121406078339\n",
      "Loss for batch 101 = 0.008531549945473671\n",
      "Loss for batch 102 = 0.005666288547217846\n",
      "Loss for batch 103 = 0.01026151329278946\n",
      "Loss for batch 104 = 0.020024985074996948\n",
      "Loss for batch 105 = 0.19961027801036835\n",
      "Loss for batch 106 = 0.029271123930811882\n",
      "\n",
      "Training Loss for epoch 27 = 5.026088714599609\n",
      "\n",
      "Current Validation Loss = 31.45146369934082\n",
      "Best Validation Loss = 14.9857177734375\n",
      "Epochs without Improvement = 17\n",
      "Train Accuracy: 98.47%\n",
      "Validation Accuracy: 61.60%\n",
      "\n",
      "Epoch 28\n",
      "----------\n",
      "Loss for batch 0 = 0.008440318517386913\n",
      "Loss for batch 1 = 0.009438951499760151\n",
      "Loss for batch 2 = 0.012938480824232101\n",
      "Loss for batch 3 = 0.012034314684569836\n",
      "Loss for batch 4 = 0.004999035038053989\n",
      "Loss for batch 5 = 0.003994560334831476\n",
      "Loss for batch 6 = 0.09964906424283981\n",
      "Loss for batch 7 = 0.01295944582670927\n",
      "Loss for batch 8 = 0.0076634082943201065\n",
      "Loss for batch 9 = 0.007864557206630707\n",
      "Loss for batch 10 = 0.3609306514263153\n",
      "Loss for batch 11 = 0.008406171575188637\n",
      "Loss for batch 12 = 0.005208606366068125\n",
      "Loss for batch 13 = 0.008612201549112797\n",
      "Loss for batch 14 = 0.22254177927970886\n",
      "Loss for batch 15 = 0.0099671371281147\n",
      "Loss for batch 16 = 0.006298004649579525\n",
      "Loss for batch 17 = 0.00582673866301775\n",
      "Loss for batch 18 = 0.004680782090872526\n",
      "Loss for batch 19 = 0.0046743368729949\n",
      "Loss for batch 20 = 0.007242927793413401\n",
      "Loss for batch 21 = 0.017374394461512566\n",
      "Loss for batch 22 = 0.07002168893814087\n",
      "Loss for batch 23 = 0.00576012721285224\n",
      "Loss for batch 24 = 0.008558349683880806\n",
      "Loss for batch 25 = 0.14591330289840698\n",
      "Loss for batch 26 = 0.012515502981841564\n",
      "Loss for batch 27 = 0.15516141057014465\n",
      "Loss for batch 28 = 0.006881236098706722\n",
      "Loss for batch 29 = 0.15123535692691803\n",
      "Loss for batch 30 = 0.006415211595594883\n",
      "Loss for batch 31 = 0.28227972984313965\n",
      "Loss for batch 32 = 0.00885821133852005\n",
      "Loss for batch 33 = 0.008008337579667568\n",
      "Loss for batch 34 = 0.008125142194330692\n",
      "Loss for batch 35 = 0.012396073900163174\n",
      "Loss for batch 36 = 0.008248628117144108\n",
      "Loss for batch 37 = 0.008635859936475754\n",
      "Loss for batch 38 = 0.12660323083400726\n",
      "Loss for batch 39 = 0.03760076314210892\n",
      "Loss for batch 40 = 0.00938828382641077\n",
      "Loss for batch 41 = 0.14706161618232727\n",
      "Loss for batch 42 = 0.03517896309494972\n",
      "Loss for batch 43 = 0.015489316545426846\n",
      "Loss for batch 44 = 0.14764347672462463\n",
      "Loss for batch 45 = 0.05789918452501297\n",
      "Loss for batch 46 = 0.11612904071807861\n",
      "Loss for batch 47 = 0.019151730462908745\n",
      "Loss for batch 48 = 0.04762619361281395\n",
      "Loss for batch 49 = 0.013099784031510353\n",
      "Loss for batch 50 = 0.02183401957154274\n",
      "Loss for batch 51 = 0.010003474541008472\n",
      "Loss for batch 52 = 0.005703544244170189\n",
      "Loss for batch 53 = 0.01932351291179657\n",
      "Loss for batch 54 = 0.012392749078571796\n",
      "Loss for batch 55 = 0.006948899943381548\n",
      "Loss for batch 56 = 0.02481088787317276\n",
      "Loss for batch 57 = 0.01618950441479683\n",
      "Loss for batch 58 = 0.008671813644468784\n",
      "Loss for batch 59 = 0.009981252253055573\n",
      "Loss for batch 60 = 0.019212372601032257\n",
      "Loss for batch 61 = 0.15469227731227875\n",
      "Loss for batch 62 = 0.008049733005464077\n",
      "Loss for batch 63 = 0.010451333597302437\n",
      "Loss for batch 64 = 0.152134507894516\n",
      "Loss for batch 65 = 0.008874695748090744\n",
      "Loss for batch 66 = 0.0140761723741889\n",
      "Loss for batch 67 = 0.005542881786823273\n",
      "Loss for batch 68 = 0.013586118817329407\n",
      "Loss for batch 69 = 0.010773365385830402\n",
      "Loss for batch 70 = 0.011575968936085701\n",
      "Loss for batch 71 = 0.013131249696016312\n",
      "Loss for batch 72 = 0.045246340334415436\n",
      "Loss for batch 73 = 0.010601549409329891\n",
      "Loss for batch 74 = 0.14027895033359528\n",
      "Loss for batch 75 = 0.008069922216236591\n",
      "Loss for batch 76 = 0.006665276829153299\n",
      "Loss for batch 77 = 0.012710675597190857\n",
      "Loss for batch 78 = 0.026730477809906006\n",
      "Loss for batch 79 = 0.0053612105548381805\n",
      "Loss for batch 80 = 0.012302563525736332\n",
      "Loss for batch 81 = 0.10389810055494308\n",
      "Loss for batch 82 = 0.007006013300269842\n",
      "Loss for batch 83 = 0.008400053717195988\n",
      "Loss for batch 84 = 0.010618363507091999\n",
      "Loss for batch 85 = 0.1314810812473297\n",
      "Loss for batch 86 = 0.0059028323739767075\n",
      "Loss for batch 87 = 0.1815042495727539\n",
      "Loss for batch 88 = 0.004841912537813187\n",
      "Loss for batch 89 = 0.016451748088002205\n",
      "Loss for batch 90 = 0.08227039873600006\n",
      "Loss for batch 91 = 0.009098443202674389\n",
      "Loss for batch 92 = 0.005074162036180496\n",
      "Loss for batch 93 = 0.03381592035293579\n",
      "Loss for batch 94 = 0.010087711736559868\n",
      "Loss for batch 95 = 0.017928605899214745\n",
      "Loss for batch 96 = 0.09099174290895462\n",
      "Loss for batch 97 = 0.24137243628501892\n",
      "Loss for batch 98 = 0.10531982779502869\n",
      "Loss for batch 99 = 0.1728055775165558\n",
      "Loss for batch 100 = 0.15804176032543182\n",
      "Loss for batch 101 = 0.006236088462173939\n",
      "Loss for batch 102 = 0.007129437290132046\n",
      "Loss for batch 103 = 0.014360632747411728\n",
      "Loss for batch 104 = 0.016700400039553642\n",
      "Loss for batch 105 = 0.19498755037784576\n",
      "Loss for batch 106 = 0.01926877535879612\n",
      "\n",
      "Training Loss for epoch 28 = 4.99515438079834\n",
      "\n",
      "Current Validation Loss = 31.980897903442383\n",
      "Best Validation Loss = 14.9857177734375\n",
      "Epochs without Improvement = 18\n",
      "Train Accuracy: 98.86%\n",
      "Validation Accuracy: 62.42%\n",
      "\n",
      "Epoch 29\n",
      "----------\n",
      "Loss for batch 0 = 0.006874232552945614\n",
      "Loss for batch 1 = 0.006435958668589592\n",
      "Loss for batch 2 = 0.013502447865903378\n",
      "Loss for batch 3 = 0.008238674141466618\n",
      "Loss for batch 4 = 0.004228467121720314\n",
      "Loss for batch 5 = 0.003855830756947398\n",
      "Loss for batch 6 = 0.0894380435347557\n",
      "Loss for batch 7 = 0.017972303554415703\n",
      "Loss for batch 8 = 0.0074279410764575005\n",
      "Loss for batch 9 = 0.01338872965425253\n",
      "Loss for batch 10 = 0.34075671434402466\n",
      "Loss for batch 11 = 0.008309601806104183\n",
      "Loss for batch 12 = 0.010104995220899582\n",
      "Loss for batch 13 = 0.013410879299044609\n",
      "Loss for batch 14 = 0.25533193349838257\n",
      "Loss for batch 15 = 0.009705501608550549\n",
      "Loss for batch 16 = 0.005782280582934618\n",
      "Loss for batch 17 = 0.023870645090937614\n",
      "Loss for batch 18 = 0.0038885532412678003\n",
      "Loss for batch 19 = 0.00461219996213913\n",
      "Loss for batch 20 = 0.004775147885084152\n",
      "Loss for batch 21 = 0.004774198401719332\n",
      "Loss for batch 22 = 0.05105699598789215\n",
      "Loss for batch 23 = 0.0057900771498680115\n",
      "Loss for batch 24 = 0.011537319049239159\n",
      "Loss for batch 25 = 0.09943071007728577\n",
      "Loss for batch 26 = 0.007681971415877342\n",
      "Loss for batch 27 = 0.1425521820783615\n",
      "Loss for batch 28 = 0.008398156613111496\n",
      "Loss for batch 29 = 0.13409914076328278\n",
      "Loss for batch 30 = 0.005009045358747244\n",
      "Loss for batch 31 = 0.27819886803627014\n",
      "Loss for batch 32 = 0.006564089562743902\n",
      "Loss for batch 33 = 0.0056014638394117355\n",
      "Loss for batch 34 = 0.006874843966215849\n",
      "Loss for batch 35 = 0.016476167365908623\n",
      "Loss for batch 36 = 0.005972974002361298\n",
      "Loss for batch 37 = 0.0065232859924435616\n",
      "Loss for batch 38 = 0.035234592854976654\n",
      "Loss for batch 39 = 0.14905321598052979\n",
      "Loss for batch 40 = 0.00863469485193491\n",
      "Loss for batch 41 = 0.14087384939193726\n",
      "Loss for batch 42 = 0.008582462556660175\n",
      "Loss for batch 43 = 0.016063347458839417\n",
      "Loss for batch 44 = 0.1466107964515686\n",
      "Loss for batch 45 = 0.00894886814057827\n",
      "Loss for batch 46 = 0.04689919576048851\n",
      "Loss for batch 47 = 0.012925509363412857\n",
      "Loss for batch 48 = 0.009379417635500431\n",
      "Loss for batch 49 = 0.010240902192890644\n",
      "Loss for batch 50 = 0.024037396535277367\n",
      "Loss for batch 51 = 0.00839525181800127\n",
      "Loss for batch 52 = 0.005345343146473169\n",
      "Loss for batch 53 = 0.00504904193803668\n",
      "Loss for batch 54 = 0.01213182881474495\n",
      "Loss for batch 55 = 0.004804177675396204\n",
      "Loss for batch 56 = 0.009927303530275822\n",
      "Loss for batch 57 = 0.008281565271317959\n",
      "Loss for batch 58 = 0.008801707997918129\n",
      "Loss for batch 59 = 0.008025366812944412\n",
      "Loss for batch 60 = 0.005968220066279173\n",
      "Loss for batch 61 = 0.1555720418691635\n",
      "Loss for batch 62 = 0.006570149213075638\n",
      "Loss for batch 63 = 0.011911038309335709\n",
      "Loss for batch 64 = 0.010529066435992718\n",
      "Loss for batch 65 = 0.008711572736501694\n",
      "Loss for batch 66 = 0.010505488142371178\n",
      "Loss for batch 67 = 0.00427058944478631\n",
      "Loss for batch 68 = 0.011454522609710693\n",
      "Loss for batch 69 = 0.007196069695055485\n",
      "Loss for batch 70 = 0.005616798996925354\n",
      "Loss for batch 71 = 0.009177802130579948\n",
      "Loss for batch 72 = 0.007204257883131504\n",
      "Loss for batch 73 = 0.010490316897630692\n",
      "Loss for batch 74 = 0.12376253306865692\n",
      "Loss for batch 75 = 0.0133863165974617\n",
      "Loss for batch 76 = 0.007841314189136028\n",
      "Loss for batch 77 = 0.005269408691674471\n",
      "Loss for batch 78 = 0.007591426372528076\n",
      "Loss for batch 79 = 0.00682327663525939\n",
      "Loss for batch 80 = 0.005762556567788124\n",
      "Loss for batch 81 = 0.08934293687343597\n",
      "Loss for batch 82 = 0.007850585505366325\n",
      "Loss for batch 83 = 0.006115337368100882\n",
      "Loss for batch 84 = 0.007908636704087257\n",
      "Loss for batch 85 = 0.026755286380648613\n",
      "Loss for batch 86 = 0.005698833614587784\n",
      "Loss for batch 87 = 0.18625615537166595\n",
      "Loss for batch 88 = 0.005299832671880722\n",
      "Loss for batch 89 = 0.007714834529906511\n",
      "Loss for batch 90 = 0.057815808802843094\n",
      "Loss for batch 91 = 0.006769238039851189\n",
      "Loss for batch 92 = 0.009655124507844448\n",
      "Loss for batch 93 = 0.008785438723862171\n",
      "Loss for batch 94 = 0.007303354796022177\n",
      "Loss for batch 95 = 0.0034221471287310123\n",
      "Loss for batch 96 = 0.06578600406646729\n",
      "Loss for batch 97 = 0.21558791399002075\n",
      "Loss for batch 98 = 0.09736191481351852\n",
      "Loss for batch 99 = 0.17545641958713531\n",
      "Loss for batch 100 = 0.005630951840430498\n",
      "Loss for batch 101 = 0.007771690376102924\n",
      "Loss for batch 102 = 0.0059433747082948685\n",
      "Loss for batch 103 = 0.011124073527753353\n",
      "Loss for batch 104 = 0.007238287013024092\n",
      "Loss for batch 105 = 0.20174622535705566\n",
      "Loss for batch 106 = 0.014180125668644905\n",
      "\n",
      "Training Loss for epoch 29 = 4.016833782196045\n",
      "\n",
      "Current Validation Loss = 32.4627685546875\n",
      "Best Validation Loss = 14.9857177734375\n",
      "Epochs without Improvement = 19\n",
      "Train Accuracy: 99.24%\n",
      "Validation Accuracy: 63.04%\n",
      "\n",
      "Epoch 30\n",
      "----------\n",
      "Loss for batch 0 = 0.007547455374151468\n",
      "Loss for batch 1 = 0.005694450810551643\n",
      "Loss for batch 2 = 0.010459721088409424\n",
      "Loss for batch 3 = 0.006351600866764784\n",
      "Loss for batch 4 = 0.003958398476243019\n",
      "Loss for batch 5 = 0.004081840626895428\n",
      "Loss for batch 6 = 0.09337106347084045\n",
      "Loss for batch 7 = 0.013157491572201252\n",
      "Loss for batch 8 = 0.005872928071767092\n",
      "Loss for batch 9 = 0.012198603712022305\n",
      "Loss for batch 10 = 0.3338363468647003\n",
      "Loss for batch 11 = 0.008282086811959743\n",
      "Loss for batch 12 = 0.005702221300452948\n",
      "Loss for batch 13 = 0.011734169907867908\n",
      "Loss for batch 14 = 0.16787967085838318\n",
      "Loss for batch 15 = 0.010906077921390533\n",
      "Loss for batch 16 = 0.006964057683944702\n",
      "Loss for batch 17 = 0.011301799677312374\n",
      "Loss for batch 18 = 0.00420833844691515\n",
      "Loss for batch 19 = 0.005460071377456188\n",
      "Loss for batch 20 = 0.0070907073095440865\n",
      "Loss for batch 21 = 0.011461249552667141\n",
      "Loss for batch 22 = 0.057966411113739014\n",
      "Loss for batch 23 = 0.005399057641625404\n",
      "Loss for batch 24 = 0.009339520707726479\n",
      "Loss for batch 25 = 0.030438562855124474\n",
      "Loss for batch 26 = 0.007697648368775845\n",
      "Loss for batch 27 = 0.13812248408794403\n",
      "Loss for batch 28 = 0.006379805039614439\n",
      "Loss for batch 29 = 0.1291329264640808\n",
      "Loss for batch 30 = 0.004780208226293325\n",
      "Loss for batch 31 = 0.2655792534351349\n",
      "Loss for batch 32 = 0.006323119159787893\n",
      "Loss for batch 33 = 0.005481448490172625\n",
      "Loss for batch 34 = 0.0069947452284395695\n",
      "Loss for batch 35 = 0.015359852463006973\n",
      "Loss for batch 36 = 0.006184984464198351\n",
      "Loss for batch 37 = 0.006962092127650976\n",
      "Loss for batch 38 = 0.03162073343992233\n",
      "Loss for batch 39 = 0.11660843342542648\n",
      "Loss for batch 40 = 0.007302488666027784\n",
      "Loss for batch 41 = 0.14132694900035858\n",
      "Loss for batch 42 = 0.007166095543652773\n",
      "Loss for batch 43 = 0.012281006202101707\n",
      "Loss for batch 44 = 0.14251859486103058\n",
      "Loss for batch 45 = 0.008124390617012978\n",
      "Loss for batch 46 = 0.043028734624385834\n",
      "Loss for batch 47 = 0.010162821039557457\n",
      "Loss for batch 48 = 0.009119309484958649\n",
      "Loss for batch 49 = 0.010214673355221748\n",
      "Loss for batch 50 = 0.00572085939347744\n",
      "Loss for batch 51 = 0.0076099541038274765\n",
      "Loss for batch 52 = 0.005084209609776735\n",
      "Loss for batch 53 = 0.004702761769294739\n",
      "Loss for batch 54 = 0.008157863281667233\n",
      "Loss for batch 55 = 0.004310463089495897\n",
      "Loss for batch 56 = 0.01226121000945568\n",
      "Loss for batch 57 = 0.007558673620223999\n",
      "Loss for batch 58 = 0.008123945444822311\n",
      "Loss for batch 59 = 0.007751963101327419\n",
      "Loss for batch 60 = 0.005634674336761236\n",
      "Loss for batch 61 = 0.15099303424358368\n",
      "Loss for batch 62 = 0.0064194947481155396\n",
      "Loss for batch 63 = 0.008647584356367588\n",
      "Loss for batch 64 = 0.0070322477258741856\n",
      "Loss for batch 65 = 0.007923492230474949\n",
      "Loss for batch 66 = 0.008427293971180916\n",
      "Loss for batch 67 = 0.004123518709093332\n",
      "Loss for batch 68 = 0.01086723618209362\n",
      "Loss for batch 69 = 0.007125395815819502\n",
      "Loss for batch 70 = 0.005142466630786657\n",
      "Loss for batch 71 = 0.009056524373590946\n",
      "Loss for batch 72 = 0.006860086228698492\n",
      "Loss for batch 73 = 0.010334078222513199\n",
      "Loss for batch 74 = 0.11451070755720139\n",
      "Loss for batch 75 = 0.012030904181301594\n",
      "Loss for batch 76 = 0.006923370994627476\n",
      "Loss for batch 77 = 0.004853095859289169\n",
      "Loss for batch 78 = 0.007152558770030737\n",
      "Loss for batch 79 = 0.0066742198541760445\n",
      "Loss for batch 80 = 0.0055391727946698666\n",
      "Loss for batch 81 = 0.08865287154912949\n",
      "Loss for batch 82 = 0.00708123529329896\n",
      "Loss for batch 83 = 0.005878076888620853\n",
      "Loss for batch 84 = 0.007320810575038195\n",
      "Loss for batch 85 = 0.02054418995976448\n",
      "Loss for batch 86 = 0.005472979508340359\n",
      "Loss for batch 87 = 0.19022683799266815\n",
      "Loss for batch 88 = 0.005312707740813494\n",
      "Loss for batch 89 = 0.0069806817919015884\n",
      "Loss for batch 90 = 0.05645521730184555\n",
      "Loss for batch 91 = 0.007000612560659647\n",
      "Loss for batch 92 = 0.0066747162491083145\n",
      "Loss for batch 93 = 0.008180011995136738\n",
      "Loss for batch 94 = 0.007695585023611784\n",
      "Loss for batch 95 = 0.0029094850178807974\n",
      "Loss for batch 96 = 0.022132938727736473\n",
      "Loss for batch 97 = 0.1722598522901535\n",
      "Loss for batch 98 = 0.09612761437892914\n",
      "Loss for batch 99 = 0.17776277661323547\n",
      "Loss for batch 100 = 0.005988508462905884\n",
      "Loss for batch 101 = 0.006217252463102341\n",
      "Loss for batch 102 = 0.004980436526238918\n",
      "Loss for batch 103 = 0.006986496038734913\n",
      "Loss for batch 104 = 0.0073601314797997475\n",
      "Loss for batch 105 = 0.18026411533355713\n",
      "Loss for batch 106 = 0.014152472838759422\n",
      "\n",
      "Training Loss for epoch 30 = 3.5873074531555176\n",
      "\n",
      "Current Validation Loss = 33.15043640136719\n",
      "Best Validation Loss = 14.9857177734375\n",
      "Epochs without Improvement = 20\n",
      "Train Accuracy: 99.27%\n",
      "Validation Accuracy: 62.63%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TrainLoop(modelv2, optimizer_2, loss_fn_2, train_loader, val_loader, scheduler_2, 100, 20, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 60.78%\n"
     ]
    }
   ],
   "source": [
    "# test_set = CustomTextClassificationDataset(test_x, test_y, word_to_index)\n",
    "# test_loader = DataLoader(test_set, 32)\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "with torch.inference_mode():\n",
    "    for batch in test_loader:\n",
    "        caption_tokens = batch['text_indices'].to('cuda')\n",
    "        labels = batch['label'].to('cuda', dtype=torch.long)\n",
    "        preds = torch.argmax(modelv2(caption_tokens), dim=1)\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        pred_labels.extend(preds.cpu().numpy())\n",
    "\n",
    "true_labels = np.array(true_labels)\n",
    "pred_labels = np.array(pred_labels)\n",
    "\n",
    "modelv2_accuracy = accuracy_score(true_labels, pred_labels)\n",
    "\n",
    "print(f'Accuracy: {modelv2_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelv3 = LSTMmodel(3, 25, 512, bidirectionality=True)\n",
    "loss_fn_3 = torch.nn.NLLLoss()\n",
    "optimizer_3 = torch.optim.NAdam(modelv3.parameters(), 0.001)\n",
    "scheduler_3 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_3, 'min', 0.4, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44647965"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in modelv3.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eb003c29ac64031a26d41b9f652ecd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0\n",
      "----------\n",
      "Loss for batch 0 = 1.0991579294204712\n",
      "Loss for batch 1 = 1.1129205226898193\n",
      "Loss for batch 2 = 1.0859681367874146\n",
      "Loss for batch 3 = 1.1241397857666016\n",
      "Loss for batch 4 = 1.1067978143692017\n",
      "Loss for batch 5 = 1.1018072366714478\n",
      "Loss for batch 6 = 1.0994982719421387\n",
      "Loss for batch 7 = 1.086398959159851\n",
      "Loss for batch 8 = 1.1084955930709839\n",
      "Loss for batch 9 = 1.084686517715454\n",
      "Loss for batch 10 = 1.0875650644302368\n",
      "Loss for batch 11 = 1.0784155130386353\n",
      "Loss for batch 12 = 1.4079875946044922\n",
      "Loss for batch 13 = 1.0911625623703003\n",
      "Loss for batch 14 = 1.120188593864441\n",
      "Loss for batch 15 = 1.0992918014526367\n",
      "Loss for batch 16 = 1.1003555059432983\n",
      "Loss for batch 17 = 1.085225224494934\n",
      "Loss for batch 18 = 1.1184362173080444\n",
      "Loss for batch 19 = 1.0849422216415405\n",
      "Loss for batch 20 = 1.103843092918396\n",
      "Loss for batch 21 = 1.1087143421173096\n",
      "Loss for batch 22 = 1.0956099033355713\n",
      "Loss for batch 23 = 1.083115816116333\n",
      "Loss for batch 24 = 1.0610933303833008\n",
      "Loss for batch 25 = 1.1051462888717651\n",
      "Loss for batch 26 = 1.0402488708496094\n",
      "Loss for batch 27 = 1.1044822931289673\n",
      "Loss for batch 28 = 1.0869675874710083\n",
      "Loss for batch 29 = 1.1108105182647705\n",
      "Loss for batch 30 = 1.092437982559204\n",
      "Loss for batch 31 = 1.0937416553497314\n",
      "Loss for batch 32 = 1.0806978940963745\n",
      "Loss for batch 33 = 1.0681443214416504\n",
      "Loss for batch 34 = 1.00767982006073\n",
      "Loss for batch 35 = 1.0850944519042969\n",
      "Loss for batch 36 = 1.079348087310791\n",
      "Loss for batch 37 = 1.1082684993743896\n",
      "Loss for batch 38 = 1.069355845451355\n",
      "Loss for batch 39 = 1.0481795072555542\n",
      "Loss for batch 40 = 1.0741692781448364\n",
      "Loss for batch 41 = 1.1422010660171509\n",
      "Loss for batch 42 = 1.0715103149414062\n",
      "Loss for batch 43 = 1.0971211194992065\n",
      "Loss for batch 44 = 1.0951738357543945\n",
      "Loss for batch 45 = 1.057679295539856\n",
      "Loss for batch 46 = 1.0375920534133911\n",
      "Loss for batch 47 = 1.0694303512573242\n",
      "Loss for batch 48 = 1.0382064580917358\n",
      "Loss for batch 49 = 1.1081010103225708\n",
      "Loss for batch 50 = 1.1518274545669556\n",
      "Loss for batch 51 = 1.0619111061096191\n",
      "Loss for batch 52 = 1.0531134605407715\n",
      "Loss for batch 53 = 1.027241826057434\n",
      "Loss for batch 54 = 1.0444896221160889\n",
      "Loss for batch 55 = 1.1360079050064087\n",
      "Loss for batch 56 = 1.1193647384643555\n",
      "Loss for batch 57 = 1.055057168006897\n",
      "Loss for batch 58 = 1.1286615133285522\n",
      "Loss for batch 59 = 1.0518369674682617\n",
      "Loss for batch 60 = 1.0135860443115234\n",
      "Loss for batch 61 = 1.0349156856536865\n",
      "Loss for batch 62 = 1.120603322982788\n",
      "Loss for batch 63 = 1.1748329401016235\n",
      "Loss for batch 64 = 1.059207558631897\n",
      "Loss for batch 65 = 1.1454167366027832\n",
      "Loss for batch 66 = 1.0762380361557007\n",
      "Loss for batch 67 = 1.0532866716384888\n",
      "Loss for batch 68 = 1.0066964626312256\n",
      "Loss for batch 69 = 0.9918450117111206\n",
      "Loss for batch 70 = 1.0805128812789917\n",
      "Loss for batch 71 = 1.020211935043335\n",
      "Loss for batch 72 = 1.0654770135879517\n",
      "Loss for batch 73 = 0.9318588376045227\n",
      "Loss for batch 74 = 1.4600591659545898\n",
      "Loss for batch 75 = 1.1052526235580444\n",
      "Loss for batch 76 = 1.0327516794204712\n",
      "Loss for batch 77 = 1.1191009283065796\n",
      "Loss for batch 78 = 0.9964720010757446\n",
      "Loss for batch 79 = 1.0394178628921509\n",
      "Loss for batch 80 = 1.1300742626190186\n",
      "Loss for batch 81 = 1.1120712757110596\n",
      "Loss for batch 82 = 1.0230040550231934\n",
      "Loss for batch 83 = 1.0356957912445068\n",
      "Loss for batch 84 = 1.068423867225647\n",
      "Loss for batch 85 = 1.1838054656982422\n",
      "Loss for batch 86 = 1.0761102437973022\n",
      "Loss for batch 87 = 0.9984938502311707\n",
      "Loss for batch 88 = 1.0492085218429565\n",
      "Loss for batch 89 = 1.0626380443572998\n",
      "Loss for batch 90 = 1.0949552059173584\n",
      "Loss for batch 91 = 1.0682345628738403\n",
      "Loss for batch 92 = 1.117613673210144\n",
      "Loss for batch 93 = 1.0159841775894165\n",
      "Loss for batch 94 = 1.0791373252868652\n",
      "Loss for batch 95 = 1.1077828407287598\n",
      "Loss for batch 96 = 0.984218180179596\n",
      "Loss for batch 97 = 0.9790123701095581\n",
      "Loss for batch 98 = 1.1058716773986816\n",
      "Loss for batch 99 = 1.0612581968307495\n",
      "Loss for batch 100 = 1.0560988187789917\n",
      "Loss for batch 101 = 0.9857655167579651\n",
      "Loss for batch 102 = 0.9551723003387451\n",
      "Loss for batch 103 = 1.0841161012649536\n",
      "Loss for batch 104 = 0.9255419969558716\n",
      "Loss for batch 105 = 1.0222043991088867\n",
      "Loss for batch 106 = 1.1391050815582275\n",
      "\n",
      "Training Loss for epoch 0 = 115.58647155761719\n",
      "\n",
      "Current Validation Loss = 17.072711944580078\n",
      "Best Validation Loss = 17.072711944580078\n",
      "Epochs without Improvement = 0\n",
      "Train Accuracy: 42.17%\n",
      "Validation Accuracy: 41.27%\n",
      "\n",
      "Epoch 1\n",
      "----------\n",
      "Loss for batch 0 = 1.0207501649856567\n",
      "Loss for batch 1 = 0.9614378213882446\n",
      "Loss for batch 2 = 1.1031709909439087\n",
      "Loss for batch 3 = 1.0351210832595825\n",
      "Loss for batch 4 = 1.1242337226867676\n",
      "Loss for batch 5 = 1.0567766427993774\n",
      "Loss for batch 6 = 1.0988385677337646\n",
      "Loss for batch 7 = 1.104303002357483\n",
      "Loss for batch 8 = 1.0965842008590698\n",
      "Loss for batch 9 = 0.98275226354599\n",
      "Loss for batch 10 = 1.0256876945495605\n",
      "Loss for batch 11 = 1.0368958711624146\n",
      "Loss for batch 12 = 0.9648730158805847\n",
      "Loss for batch 13 = 1.040117621421814\n",
      "Loss for batch 14 = 1.0782688856124878\n",
      "Loss for batch 15 = 1.104414701461792\n",
      "Loss for batch 16 = 1.1165412664413452\n",
      "Loss for batch 17 = 1.037343144416809\n",
      "Loss for batch 18 = 1.1105002164840698\n",
      "Loss for batch 19 = 0.9931486248970032\n",
      "Loss for batch 20 = 1.0668607950210571\n",
      "Loss for batch 21 = 0.9278004765510559\n",
      "Loss for batch 22 = 1.2087981700897217\n",
      "Loss for batch 23 = 1.0104670524597168\n",
      "Loss for batch 24 = 0.9499322175979614\n",
      "Loss for batch 25 = 1.0423961877822876\n",
      "Loss for batch 26 = 1.0501621961593628\n",
      "Loss for batch 27 = 1.0489526987075806\n",
      "Loss for batch 28 = 1.0931413173675537\n",
      "Loss for batch 29 = 0.995947003364563\n",
      "Loss for batch 30 = 1.0455422401428223\n",
      "Loss for batch 31 = 1.1046231985092163\n",
      "Loss for batch 32 = 1.0833802223205566\n",
      "Loss for batch 33 = 0.9976099133491516\n",
      "Loss for batch 34 = 1.0699408054351807\n",
      "Loss for batch 35 = 1.034711241722107\n",
      "Loss for batch 36 = 1.050623893737793\n",
      "Loss for batch 37 = 1.0654218196868896\n",
      "Loss for batch 38 = 0.976026713848114\n",
      "Loss for batch 39 = 0.9300569891929626\n",
      "Loss for batch 40 = 1.0590837001800537\n",
      "Loss for batch 41 = 1.079867959022522\n",
      "Loss for batch 42 = 0.9848030805587769\n",
      "Loss for batch 43 = 1.1171168088912964\n",
      "Loss for batch 44 = 1.1075265407562256\n",
      "Loss for batch 45 = 1.0302863121032715\n",
      "Loss for batch 46 = 1.0183876752853394\n",
      "Loss for batch 47 = 1.0538852214813232\n",
      "Loss for batch 48 = 1.057815670967102\n",
      "Loss for batch 49 = 1.0849741697311401\n",
      "Loss for batch 50 = 1.0446841716766357\n",
      "Loss for batch 51 = 0.9892221093177795\n",
      "Loss for batch 52 = 1.0043214559555054\n",
      "Loss for batch 53 = 1.0958311557769775\n",
      "Loss for batch 54 = 0.974624514579773\n",
      "Loss for batch 55 = 1.171759843826294\n",
      "Loss for batch 56 = 1.0528392791748047\n",
      "Loss for batch 57 = 0.9502454996109009\n",
      "Loss for batch 58 = 1.1483330726623535\n",
      "Loss for batch 59 = 1.0243897438049316\n",
      "Loss for batch 60 = 0.990841269493103\n",
      "Loss for batch 61 = 1.0408684015274048\n",
      "Loss for batch 62 = 0.9727844595909119\n",
      "Loss for batch 63 = 1.076975703239441\n",
      "Loss for batch 64 = 1.0089036226272583\n",
      "Loss for batch 65 = 1.1101551055908203\n",
      "Loss for batch 66 = 1.1475247144699097\n",
      "Loss for batch 67 = 1.0199302434921265\n",
      "Loss for batch 68 = 0.9871958494186401\n",
      "Loss for batch 69 = 0.9441816806793213\n",
      "Loss for batch 70 = 1.0481311082839966\n",
      "Loss for batch 71 = 0.9334136247634888\n",
      "Loss for batch 72 = 1.0438096523284912\n",
      "Loss for batch 73 = 0.9031751155853271\n",
      "Loss for batch 74 = 1.0473159551620483\n",
      "Loss for batch 75 = 1.0538904666900635\n",
      "Loss for batch 76 = 0.9582581520080566\n",
      "Loss for batch 77 = 1.1148343086242676\n",
      "Loss for batch 78 = 0.9891262054443359\n",
      "Loss for batch 79 = 1.1103057861328125\n",
      "Loss for batch 80 = 1.0115617513656616\n",
      "Loss for batch 81 = 1.0844378471374512\n",
      "Loss for batch 82 = 1.0082786083221436\n",
      "Loss for batch 83 = 1.0119036436080933\n",
      "Loss for batch 84 = 1.0169111490249634\n",
      "Loss for batch 85 = 1.2084656953811646\n",
      "Loss for batch 86 = 0.9833000898361206\n",
      "Loss for batch 87 = 1.0461534261703491\n",
      "Loss for batch 88 = 1.0296897888183594\n",
      "Loss for batch 89 = 1.0502170324325562\n",
      "Loss for batch 90 = 1.0876407623291016\n",
      "Loss for batch 91 = 1.0420079231262207\n",
      "Loss for batch 92 = 1.0605612993240356\n",
      "Loss for batch 93 = 1.0805474519729614\n",
      "Loss for batch 94 = 1.0615726709365845\n",
      "Loss for batch 95 = 1.070670485496521\n",
      "Loss for batch 96 = 0.985161542892456\n",
      "Loss for batch 97 = 1.0395714044570923\n",
      "Loss for batch 98 = 1.013132095336914\n",
      "Loss for batch 99 = 1.1185450553894043\n",
      "Loss for batch 100 = 1.100469946861267\n",
      "Loss for batch 101 = 0.9273974299430847\n",
      "Loss for batch 102 = 0.9178559184074402\n",
      "Loss for batch 103 = 1.069804310798645\n",
      "Loss for batch 104 = 0.8766734600067139\n",
      "Loss for batch 105 = 0.9885179400444031\n",
      "Loss for batch 106 = 1.0849621295928955\n",
      "\n",
      "Training Loss for epoch 1 = 111.37176513671875\n",
      "\n",
      "Current Validation Loss = 16.78693962097168\n",
      "Best Validation Loss = 16.78693962097168\n",
      "Epochs without Improvement = 0\n",
      "Train Accuracy: 41.05%\n",
      "Validation Accuracy: 40.25%\n",
      "\n",
      "Epoch 2\n",
      "----------\n",
      "Loss for batch 0 = 1.0011975765228271\n",
      "Loss for batch 1 = 0.9743893146514893\n",
      "Loss for batch 2 = 1.0738452672958374\n",
      "Loss for batch 3 = 1.015524983406067\n",
      "Loss for batch 4 = 1.0369447469711304\n",
      "Loss for batch 5 = 1.081592082977295\n",
      "Loss for batch 6 = 1.0668830871582031\n",
      "Loss for batch 7 = 1.0081665515899658\n",
      "Loss for batch 8 = 1.1129857301712036\n",
      "Loss for batch 9 = 0.9778497815132141\n",
      "Loss for batch 10 = 0.985169529914856\n",
      "Loss for batch 11 = 1.0223743915557861\n",
      "Loss for batch 12 = 0.9269750118255615\n",
      "Loss for batch 13 = 1.0436296463012695\n",
      "Loss for batch 14 = 1.0914067029953003\n",
      "Loss for batch 15 = 1.1184028387069702\n",
      "Loss for batch 16 = 1.1672781705856323\n",
      "Loss for batch 17 = 1.014497995376587\n",
      "Loss for batch 18 = 1.1195663213729858\n",
      "Loss for batch 19 = 1.0987321138381958\n",
      "Loss for batch 20 = 1.100310206413269\n",
      "Loss for batch 21 = 1.0785611867904663\n",
      "Loss for batch 22 = 1.1146190166473389\n",
      "Loss for batch 23 = 1.0575261116027832\n",
      "Loss for batch 24 = 0.9665490984916687\n",
      "Loss for batch 25 = 1.2110059261322021\n",
      "Loss for batch 26 = 1.0735828876495361\n",
      "Loss for batch 27 = 1.071845293045044\n",
      "Loss for batch 28 = 1.066439151763916\n",
      "Loss for batch 29 = 1.040478229522705\n",
      "Loss for batch 30 = 1.0185339450836182\n",
      "Loss for batch 31 = 1.1137523651123047\n",
      "Loss for batch 32 = 1.1187113523483276\n",
      "Loss for batch 33 = 1.0289859771728516\n",
      "Loss for batch 34 = 1.0050212144851685\n",
      "Loss for batch 35 = 1.0447388887405396\n",
      "Loss for batch 36 = 1.0682897567749023\n",
      "Loss for batch 37 = 1.0463922023773193\n",
      "Loss for batch 38 = 1.088753581047058\n",
      "Loss for batch 39 = 0.9303789138793945\n",
      "Loss for batch 40 = 1.03799569606781\n",
      "Loss for batch 41 = 1.1721042394638062\n",
      "Loss for batch 42 = 0.9812173843383789\n",
      "Loss for batch 43 = 1.0093023777008057\n",
      "Loss for batch 44 = 1.138800024986267\n",
      "Loss for batch 45 = 1.0050426721572876\n",
      "Loss for batch 46 = 0.861386239528656\n",
      "Loss for batch 47 = 1.0144917964935303\n",
      "Loss for batch 48 = 0.9749807715415955\n",
      "Loss for batch 49 = 0.9569677710533142\n",
      "Loss for batch 50 = 1.2881152629852295\n",
      "Loss for batch 51 = 1.0950148105621338\n",
      "Loss for batch 52 = 1.028963565826416\n",
      "Loss for batch 53 = 1.062731385231018\n",
      "Loss for batch 54 = 1.0601474046707153\n",
      "Loss for batch 55 = 1.1106683015823364\n",
      "Loss for batch 56 = 1.072486400604248\n",
      "Loss for batch 57 = 1.0303003787994385\n",
      "Loss for batch 58 = 1.1212784051895142\n",
      "Loss for batch 59 = 1.0099682807922363\n",
      "Loss for batch 60 = 1.0162485837936401\n",
      "Loss for batch 61 = 1.097015142440796\n",
      "Loss for batch 62 = 1.0465774536132812\n",
      "Loss for batch 63 = 1.0487695932388306\n",
      "Loss for batch 64 = 1.05047607421875\n",
      "Loss for batch 65 = 1.1071994304656982\n",
      "Loss for batch 66 = 1.032557725906372\n",
      "Loss for batch 67 = 0.9850727319717407\n",
      "Loss for batch 68 = 0.9835726022720337\n",
      "Loss for batch 69 = 0.9718042016029358\n",
      "Loss for batch 70 = 1.0566928386688232\n",
      "Loss for batch 71 = 0.935664176940918\n",
      "Loss for batch 72 = 1.1073386669158936\n",
      "Loss for batch 73 = 1.1090949773788452\n",
      "Loss for batch 74 = 1.1625995635986328\n",
      "Loss for batch 75 = 1.0635671615600586\n",
      "Loss for batch 76 = 1.0825639963150024\n",
      "Loss for batch 77 = 1.0660344362258911\n",
      "Loss for batch 78 = 1.0568925142288208\n",
      "Loss for batch 79 = 1.0649700164794922\n",
      "Loss for batch 80 = 1.0564022064208984\n",
      "Loss for batch 81 = 1.0879161357879639\n",
      "Loss for batch 82 = 1.0999605655670166\n",
      "Loss for batch 83 = 1.0444574356079102\n",
      "Loss for batch 84 = 1.1094205379486084\n",
      "Loss for batch 85 = 1.1405653953552246\n",
      "Loss for batch 86 = 1.0477346181869507\n",
      "Loss for batch 87 = 1.0639050006866455\n",
      "Loss for batch 88 = 1.084645390510559\n",
      "Loss for batch 89 = 1.0358070135116577\n",
      "Loss for batch 90 = 1.0403482913970947\n",
      "Loss for batch 91 = 1.0381174087524414\n",
      "Loss for batch 92 = 1.1095882654190063\n",
      "Loss for batch 93 = 1.102448582649231\n",
      "Loss for batch 94 = 1.0767903327941895\n",
      "Loss for batch 95 = 1.0854864120483398\n",
      "Loss for batch 96 = 1.0646499395370483\n",
      "Loss for batch 97 = 1.0516654253005981\n",
      "Loss for batch 98 = 1.0829236507415771\n",
      "Loss for batch 99 = 1.1198747158050537\n",
      "Loss for batch 100 = 1.0989511013031006\n",
      "Loss for batch 101 = 1.0663729906082153\n",
      "Loss for batch 102 = 1.0730494260787964\n",
      "Loss for batch 103 = 1.0789470672607422\n",
      "Loss for batch 104 = 1.0511465072631836\n",
      "Loss for batch 105 = 1.1050713062286377\n",
      "Loss for batch 106 = 1.1606937646865845\n",
      "\n",
      "Training Loss for epoch 2 = 113.43549346923828\n",
      "\n",
      "Current Validation Loss = 17.274290084838867\n",
      "Best Validation Loss = 16.78693962097168\n",
      "Epochs without Improvement = 1\n",
      "Train Accuracy: 41.75%\n",
      "Validation Accuracy: 42.30%\n",
      "\n",
      "Epoch 3\n",
      "----------\n",
      "Loss for batch 0 = 1.0373784303665161\n",
      "Loss for batch 1 = 1.083505392074585\n",
      "Loss for batch 2 = 1.119014024734497\n",
      "Loss for batch 3 = 1.0760798454284668\n",
      "Loss for batch 4 = 1.1037602424621582\n",
      "Loss for batch 5 = 1.1034610271453857\n",
      "Loss for batch 6 = 1.090427279472351\n",
      "Loss for batch 7 = 1.0391721725463867\n",
      "Loss for batch 8 = 1.1221768856048584\n",
      "Loss for batch 9 = 1.0797021389007568\n",
      "Loss for batch 10 = 1.051634669303894\n",
      "Loss for batch 11 = 1.0299065113067627\n",
      "Loss for batch 12 = 1.053292155265808\n",
      "Loss for batch 13 = 1.0393449068069458\n",
      "Loss for batch 14 = 1.2463934421539307\n",
      "Loss for batch 15 = 1.1094173192977905\n",
      "Loss for batch 16 = 1.1097922325134277\n",
      "Loss for batch 17 = 1.0563232898712158\n",
      "Loss for batch 18 = 1.125366449356079\n",
      "Loss for batch 19 = 1.0691330432891846\n",
      "Loss for batch 20 = 1.0930883884429932\n",
      "Loss for batch 21 = 1.1077197790145874\n",
      "Loss for batch 22 = 1.0942111015319824\n",
      "Loss for batch 23 = 1.0495495796203613\n",
      "Loss for batch 24 = 1.0382130146026611\n",
      "Loss for batch 25 = 1.123793125152588\n",
      "Loss for batch 26 = 1.0442231893539429\n",
      "Loss for batch 27 = 1.1031113862991333\n",
      "Loss for batch 28 = 1.0890438556671143\n",
      "Loss for batch 29 = 1.0863536596298218\n",
      "Loss for batch 30 = 1.0883431434631348\n",
      "Loss for batch 31 = 1.1032358407974243\n",
      "Loss for batch 32 = 1.0807980298995972\n",
      "Loss for batch 33 = 1.0938867330551147\n",
      "Loss for batch 34 = 1.0372424125671387\n",
      "Loss for batch 35 = 1.053525447845459\n",
      "Loss for batch 36 = 1.020365595817566\n",
      "Loss for batch 37 = 1.0747584104537964\n",
      "Loss for batch 38 = 1.2417510747909546\n",
      "Loss for batch 39 = 1.0288681983947754\n",
      "Loss for batch 40 = 1.066558599472046\n",
      "Loss for batch 41 = 1.2041784524917603\n",
      "Loss for batch 42 = 1.0967075824737549\n",
      "Loss for batch 43 = 1.0939311981201172\n",
      "Loss for batch 44 = 1.1151199340820312\n",
      "Loss for batch 45 = 1.067855715751648\n",
      "Loss for batch 46 = 1.0574324131011963\n",
      "Loss for batch 47 = 1.07212233543396\n",
      "Loss for batch 48 = 1.1016623973846436\n",
      "Loss for batch 49 = 1.0583034753799438\n",
      "Loss for batch 50 = 1.0992916822433472\n",
      "Loss for batch 51 = 1.0664335489273071\n",
      "Loss for batch 52 = 1.0585857629776\n",
      "Loss for batch 53 = 1.0457468032836914\n",
      "Loss for batch 54 = 1.0613774061203003\n",
      "Loss for batch 55 = 1.1048862934112549\n",
      "Loss for batch 56 = 1.0846656560897827\n",
      "Loss for batch 57 = 1.0833622217178345\n",
      "Loss for batch 58 = 1.1105434894561768\n",
      "Loss for batch 59 = 1.072944164276123\n",
      "Loss for batch 60 = 1.0421334505081177\n",
      "Loss for batch 61 = 1.098242998123169\n",
      "Loss for batch 62 = 1.1139904260635376\n",
      "Loss for batch 63 = 1.1074340343475342\n",
      "Loss for batch 64 = 1.0628548860549927\n",
      "Loss for batch 65 = 1.0574036836624146\n",
      "Loss for batch 66 = 1.0753557682037354\n",
      "Loss for batch 67 = 1.0353977680206299\n",
      "Loss for batch 68 = 1.0416685342788696\n",
      "Loss for batch 69 = 1.0723508596420288\n",
      "Loss for batch 70 = 1.1127170324325562\n",
      "Loss for batch 71 = 1.0538561344146729\n",
      "Loss for batch 72 = 1.066501498222351\n",
      "Loss for batch 73 = 1.0779967308044434\n",
      "Loss for batch 74 = 1.1049752235412598\n",
      "Loss for batch 75 = 1.1136138439178467\n",
      "Loss for batch 76 = 1.1229090690612793\n",
      "Loss for batch 77 = 1.0441572666168213\n",
      "Loss for batch 78 = 1.1256803274154663\n",
      "Loss for batch 79 = 1.092199444770813\n",
      "Loss for batch 80 = 1.0927549600601196\n",
      "Loss for batch 81 = 1.0688363313674927\n",
      "Loss for batch 82 = 1.114298939704895\n",
      "Loss for batch 83 = 1.0795578956604004\n",
      "Loss for batch 84 = 1.0741177797317505\n",
      "Loss for batch 85 = 1.1365197896957397\n",
      "Loss for batch 86 = 1.0747764110565186\n",
      "Loss for batch 87 = 1.0541282892227173\n",
      "Loss for batch 88 = 1.0848300457000732\n",
      "Loss for batch 89 = 1.0799086093902588\n",
      "Loss for batch 90 = 1.0834953784942627\n",
      "Loss for batch 91 = 1.0667140483856201\n",
      "Loss for batch 92 = 1.111816167831421\n",
      "Loss for batch 93 = 1.0016101598739624\n",
      "Loss for batch 94 = 1.0675098896026611\n",
      "Loss for batch 95 = 1.0919737815856934\n",
      "Loss for batch 96 = 1.053480863571167\n",
      "Loss for batch 97 = 1.0423763990402222\n",
      "Loss for batch 98 = 1.0728870630264282\n",
      "Loss for batch 99 = 1.115530014038086\n",
      "Loss for batch 100 = 1.101363182067871\n",
      "Loss for batch 101 = 1.041100263595581\n",
      "Loss for batch 102 = 1.0556470155715942\n",
      "Loss for batch 103 = 1.077045202255249\n",
      "Loss for batch 104 = 1.030624508857727\n",
      "Loss for batch 105 = 1.0998293161392212\n",
      "Loss for batch 106 = 1.17192804813385\n",
      "\n",
      "Training Loss for epoch 3 = 115.88520812988281\n",
      "\n",
      "Current Validation Loss = 17.20410919189453\n",
      "Best Validation Loss = 16.78693962097168\n",
      "Epochs without Improvement = 2\n",
      "Train Accuracy: 41.70%\n",
      "Validation Accuracy: 43.12%\n",
      "\n",
      "Epoch 4\n",
      "----------\n",
      "Loss for batch 0 = 1.009005069732666\n",
      "Loss for batch 1 = 1.0553548336029053\n",
      "Loss for batch 2 = 1.0969786643981934\n",
      "Loss for batch 3 = 1.0330510139465332\n",
      "Loss for batch 4 = 1.129971981048584\n",
      "Loss for batch 5 = 1.071033239364624\n",
      "Loss for batch 6 = 1.0928821563720703\n",
      "Loss for batch 7 = 1.03455650806427\n",
      "Loss for batch 8 = 1.1249158382415771\n",
      "Loss for batch 9 = 1.0840305089950562\n",
      "Loss for batch 10 = 1.0458905696868896\n",
      "Loss for batch 11 = 1.0397586822509766\n",
      "Loss for batch 12 = 1.0413566827774048\n",
      "Loss for batch 13 = 1.0493779182434082\n",
      "Loss for batch 14 = 1.131039023399353\n",
      "Loss for batch 15 = 1.092041015625\n",
      "Loss for batch 16 = 1.0962181091308594\n",
      "Loss for batch 17 = 1.0396379232406616\n",
      "Loss for batch 18 = 1.127484679222107\n",
      "Loss for batch 19 = 1.052177906036377\n",
      "Loss for batch 20 = 1.0867996215820312\n",
      "Loss for batch 21 = 1.083357334136963\n",
      "Loss for batch 22 = 1.0684177875518799\n",
      "Loss for batch 23 = 0.9998446702957153\n",
      "Loss for batch 24 = 0.92143315076828\n",
      "Loss for batch 25 = 1.193244457244873\n",
      "Loss for batch 26 = 1.0210492610931396\n",
      "Loss for batch 27 = 1.0745313167572021\n",
      "Loss for batch 28 = 1.0953494310379028\n",
      "Loss for batch 29 = 1.0725672245025635\n",
      "Loss for batch 30 = 1.0306625366210938\n",
      "Loss for batch 31 = 1.086390733718872\n",
      "Loss for batch 32 = 1.0606573820114136\n",
      "Loss for batch 33 = 1.025165319442749\n",
      "Loss for batch 34 = 0.9880580902099609\n",
      "Loss for batch 35 = 1.1290290355682373\n",
      "Loss for batch 36 = 1.0095925331115723\n",
      "Loss for batch 37 = 1.0523147583007812\n",
      "Loss for batch 38 = 1.1725536584854126\n",
      "Loss for batch 39 = 0.9547165632247925\n",
      "Loss for batch 40 = 1.0563225746154785\n",
      "Loss for batch 41 = 1.1485613584518433\n",
      "Loss for batch 42 = 0.995481550693512\n",
      "Loss for batch 43 = 1.0390222072601318\n",
      "Loss for batch 44 = 1.0575395822525024\n",
      "Loss for batch 45 = 0.9976681470870972\n",
      "Loss for batch 46 = 0.9835668802261353\n",
      "Loss for batch 47 = 1.0702711343765259\n",
      "Loss for batch 48 = 1.0110392570495605\n",
      "Loss for batch 49 = 0.9748137593269348\n",
      "Loss for batch 50 = 1.0549665689468384\n",
      "Loss for batch 51 = 0.9635765552520752\n",
      "Loss for batch 52 = 1.017411470413208\n",
      "Loss for batch 53 = 1.0077357292175293\n",
      "Loss for batch 54 = 1.0123529434204102\n",
      "Loss for batch 55 = 1.130321979522705\n",
      "Loss for batch 56 = 1.04808509349823\n",
      "Loss for batch 57 = 1.0132790803909302\n",
      "Loss for batch 58 = 1.1048747301101685\n",
      "Loss for batch 59 = 1.010556936264038\n",
      "Loss for batch 60 = 0.9983621835708618\n",
      "Loss for batch 61 = 1.0629854202270508\n",
      "Loss for batch 62 = 1.051660418510437\n",
      "Loss for batch 63 = 1.0461275577545166\n",
      "Loss for batch 64 = 1.0444023609161377\n",
      "Loss for batch 65 = 1.0807709693908691\n",
      "Loss for batch 66 = 1.0230753421783447\n",
      "Loss for batch 67 = 1.0160092115402222\n",
      "Loss for batch 68 = 0.9429663419723511\n",
      "Loss for batch 69 = 0.9437214136123657\n",
      "Loss for batch 70 = 1.0347583293914795\n",
      "Loss for batch 71 = 0.8422788381576538\n",
      "Loss for batch 72 = 1.0159075260162354\n",
      "Loss for batch 73 = 0.836012601852417\n",
      "Loss for batch 74 = 1.3968961238861084\n",
      "Loss for batch 75 = 1.021463394165039\n",
      "Loss for batch 76 = 1.038155198097229\n",
      "Loss for batch 77 = 1.083292007446289\n",
      "Loss for batch 78 = 0.9484335780143738\n",
      "Loss for batch 79 = 1.0096486806869507\n",
      "Loss for batch 80 = 0.9702731370925903\n",
      "Loss for batch 81 = 1.0625430345535278\n",
      "Loss for batch 82 = 1.0284711122512817\n",
      "Loss for batch 83 = 0.9909904599189758\n",
      "Loss for batch 84 = 0.9990347027778625\n",
      "Loss for batch 85 = 1.1770187616348267\n",
      "Loss for batch 86 = 0.8935220241546631\n",
      "Loss for batch 87 = 0.9347987174987793\n",
      "Loss for batch 88 = 0.973646879196167\n",
      "Loss for batch 89 = 1.076567530632019\n",
      "Loss for batch 90 = 1.0242198705673218\n",
      "Loss for batch 91 = 1.022887945175171\n",
      "Loss for batch 92 = 1.0013797283172607\n",
      "Loss for batch 93 = 1.04512357711792\n",
      "Loss for batch 94 = 1.015560507774353\n",
      "Loss for batch 95 = 1.0212321281433105\n",
      "Loss for batch 96 = 0.9154999256134033\n",
      "Loss for batch 97 = 0.9711224436759949\n",
      "Loss for batch 98 = 1.0364981889724731\n",
      "Loss for batch 99 = 1.0305345058441162\n",
      "Loss for batch 100 = 1.0084747076034546\n",
      "Loss for batch 101 = 0.9141103625297546\n",
      "Loss for batch 102 = 0.8482513427734375\n",
      "Loss for batch 103 = 0.9632466435432434\n",
      "Loss for batch 104 = 0.9546194076538086\n",
      "Loss for batch 105 = 0.9836795330047607\n",
      "Loss for batch 106 = 1.0936905145645142\n",
      "\n",
      "Training Loss for epoch 4 = 110.66387939453125\n",
      "\n",
      "Current Validation Loss = 15.914377212524414\n",
      "Best Validation Loss = 15.914377212524414\n",
      "Epochs without Improvement = 0\n",
      "Train Accuracy: 51.23%\n",
      "Validation Accuracy: 48.46%\n",
      "\n",
      "Epoch 5\n",
      "----------\n",
      "Loss for batch 0 = 0.9003324508666992\n",
      "Loss for batch 1 = 0.8667345643043518\n",
      "Loss for batch 2 = 1.0112849473953247\n",
      "Loss for batch 3 = 0.9194000959396362\n",
      "Loss for batch 4 = 1.0125172138214111\n",
      "Loss for batch 5 = 1.1915205717086792\n",
      "Loss for batch 6 = 1.1075423955917358\n",
      "Loss for batch 7 = 1.0285876989364624\n",
      "Loss for batch 8 = 1.086596965789795\n",
      "Loss for batch 9 = 0.9434347152709961\n",
      "Loss for batch 10 = 0.9842533469200134\n",
      "Loss for batch 11 = 0.9714962244033813\n",
      "Loss for batch 12 = 0.9125149846076965\n",
      "Loss for batch 13 = 1.0275310277938843\n",
      "Loss for batch 14 = 1.0511035919189453\n",
      "Loss for batch 15 = 1.0804921388626099\n",
      "Loss for batch 16 = 1.0237786769866943\n",
      "Loss for batch 17 = 1.0377415418624878\n",
      "Loss for batch 18 = 1.0548908710479736\n",
      "Loss for batch 19 = 0.9625099301338196\n",
      "Loss for batch 20 = 1.0081956386566162\n",
      "Loss for batch 21 = 0.9316298365592957\n",
      "Loss for batch 22 = 1.090277075767517\n",
      "Loss for batch 23 = 0.8284883499145508\n",
      "Loss for batch 24 = 0.8323106169700623\n",
      "Loss for batch 25 = 1.041152000427246\n",
      "Loss for batch 26 = 0.9147339463233948\n",
      "Loss for batch 27 = 0.9943394064903259\n",
      "Loss for batch 28 = 1.0491186380386353\n",
      "Loss for batch 29 = 0.9025020003318787\n",
      "Loss for batch 30 = 0.8656633496284485\n",
      "Loss for batch 31 = 1.191653847694397\n",
      "Loss for batch 32 = 0.9743701815605164\n",
      "Loss for batch 33 = 0.9505831599235535\n",
      "Loss for batch 34 = 0.9001125693321228\n",
      "Loss for batch 35 = 1.0829577445983887\n",
      "Loss for batch 36 = 1.0205292701721191\n",
      "Loss for batch 37 = 0.9826121926307678\n",
      "Loss for batch 38 = 0.9966777563095093\n",
      "Loss for batch 39 = 0.8448684215545654\n",
      "Loss for batch 40 = 0.9347922205924988\n",
      "Loss for batch 41 = 0.9557580351829529\n",
      "Loss for batch 42 = 0.8653488755226135\n",
      "Loss for batch 43 = 0.9948353171348572\n",
      "Loss for batch 44 = 1.0941455364227295\n",
      "Loss for batch 45 = 0.9394537806510925\n",
      "Loss for batch 46 = 0.8447033762931824\n",
      "Loss for batch 47 = 1.064240574836731\n",
      "Loss for batch 48 = 0.8599501252174377\n",
      "Loss for batch 49 = 0.9869863986968994\n",
      "Loss for batch 50 = 0.7337691783905029\n",
      "Loss for batch 51 = 0.9961309432983398\n",
      "Loss for batch 52 = 0.958548903465271\n",
      "Loss for batch 53 = 0.9457063674926758\n",
      "Loss for batch 54 = 0.8855883479118347\n",
      "Loss for batch 55 = 1.007562518119812\n",
      "Loss for batch 56 = 0.9720030426979065\n",
      "Loss for batch 57 = 0.6883955001831055\n",
      "Loss for batch 58 = 0.8805093169212341\n",
      "Loss for batch 59 = 0.9236940145492554\n",
      "Loss for batch 60 = 0.8946952819824219\n",
      "Loss for batch 61 = 0.9825098514556885\n",
      "Loss for batch 62 = 0.9229744076728821\n",
      "Loss for batch 63 = 0.887084424495697\n",
      "Loss for batch 64 = 0.8121140003204346\n",
      "Loss for batch 65 = 0.9966022372245789\n",
      "Loss for batch 66 = 0.8939999938011169\n",
      "Loss for batch 67 = 0.7564287185668945\n",
      "Loss for batch 68 = 0.8164029717445374\n",
      "Loss for batch 69 = 0.7823598384857178\n",
      "Loss for batch 70 = 0.9677543044090271\n",
      "Loss for batch 71 = 0.7163993120193481\n",
      "Loss for batch 72 = 0.8967685699462891\n",
      "Loss for batch 73 = 0.8382063508033752\n",
      "Loss for batch 74 = 1.1044552326202393\n",
      "Loss for batch 75 = 0.9483453035354614\n",
      "Loss for batch 76 = 0.9812083840370178\n",
      "Loss for batch 77 = 1.0266335010528564\n",
      "Loss for batch 78 = 0.9574905037879944\n",
      "Loss for batch 79 = 0.8622249364852905\n",
      "Loss for batch 80 = 0.785188615322113\n",
      "Loss for batch 81 = 0.8297914266586304\n",
      "Loss for batch 82 = 0.8149763345718384\n",
      "Loss for batch 83 = 0.8343877792358398\n",
      "Loss for batch 84 = 0.8205049633979797\n",
      "Loss for batch 85 = 1.1270287036895752\n",
      "Loss for batch 86 = 0.6679849028587341\n",
      "Loss for batch 87 = 0.7377244830131531\n",
      "Loss for batch 88 = 0.8492437601089478\n",
      "Loss for batch 89 = 0.9124398827552795\n",
      "Loss for batch 90 = 0.9518982768058777\n",
      "Loss for batch 91 = 0.9360707402229309\n",
      "Loss for batch 92 = 0.961579442024231\n",
      "Loss for batch 93 = 0.8370527029037476\n",
      "Loss for batch 94 = 0.8499614596366882\n",
      "Loss for batch 95 = 0.9413199424743652\n",
      "Loss for batch 96 = 0.835080623626709\n",
      "Loss for batch 97 = 0.7403097152709961\n",
      "Loss for batch 98 = 0.9563473463058472\n",
      "Loss for batch 99 = 0.7816096544265747\n",
      "Loss for batch 100 = 0.7590198516845703\n",
      "Loss for batch 101 = 0.7597271203994751\n",
      "Loss for batch 102 = 0.6413465142250061\n",
      "Loss for batch 103 = 0.8236505389213562\n",
      "Loss for batch 104 = 0.874476969242096\n",
      "Loss for batch 105 = 0.9224487543106079\n",
      "Loss for batch 106 = 1.1297931671142578\n",
      "\n",
      "Training Loss for epoch 5 = 99.23477172851562\n",
      "\n",
      "Current Validation Loss = 13.98271656036377\n",
      "Best Validation Loss = 13.98271656036377\n",
      "Epochs without Improvement = 0\n",
      "Train Accuracy: 60.33%\n",
      "Validation Accuracy: 60.37%\n",
      "\n",
      "Epoch 6\n",
      "----------\n",
      "Loss for batch 0 = 0.7389430999755859\n",
      "Loss for batch 1 = 0.6339707374572754\n",
      "Loss for batch 2 = 0.9655359983444214\n",
      "Loss for batch 3 = 0.7405769228935242\n",
      "Loss for batch 4 = 0.7356802821159363\n",
      "Loss for batch 5 = 0.9190493226051331\n",
      "Loss for batch 6 = 1.4388372898101807\n",
      "Loss for batch 7 = 0.8759101033210754\n",
      "Loss for batch 8 = 1.0913763046264648\n",
      "Loss for batch 9 = 0.7423334717750549\n",
      "Loss for batch 10 = 0.7605192065238953\n",
      "Loss for batch 11 = 0.8242781758308411\n",
      "Loss for batch 12 = 0.8228782415390015\n",
      "Loss for batch 13 = 0.9597914814949036\n",
      "Loss for batch 14 = 0.8607677221298218\n",
      "Loss for batch 15 = 1.013060450553894\n",
      "Loss for batch 16 = 0.8676962852478027\n",
      "Loss for batch 17 = 0.9540309309959412\n",
      "Loss for batch 18 = 0.8380783200263977\n",
      "Loss for batch 19 = 0.7143002152442932\n",
      "Loss for batch 20 = 0.9736228585243225\n",
      "Loss for batch 21 = 0.7761375904083252\n",
      "Loss for batch 22 = 1.1774851083755493\n",
      "Loss for batch 23 = 0.6404104232788086\n",
      "Loss for batch 24 = 0.722375750541687\n",
      "Loss for batch 25 = 0.8023523688316345\n",
      "Loss for batch 26 = 0.7537646889686584\n",
      "Loss for batch 27 = 0.7657856941223145\n",
      "Loss for batch 28 = 0.8144306540489197\n",
      "Loss for batch 29 = 0.8327057957649231\n",
      "Loss for batch 30 = 0.818589448928833\n",
      "Loss for batch 31 = 1.0809919834136963\n",
      "Loss for batch 32 = 0.7743659615516663\n",
      "Loss for batch 33 = 0.7928382158279419\n",
      "Loss for batch 34 = 0.7845349907875061\n",
      "Loss for batch 35 = 1.0220848321914673\n",
      "Loss for batch 36 = 0.8399659395217896\n",
      "Loss for batch 37 = 0.8817950487136841\n",
      "Loss for batch 38 = 0.848320722579956\n",
      "Loss for batch 39 = 0.7373638153076172\n",
      "Loss for batch 40 = 0.8358393907546997\n",
      "Loss for batch 41 = 0.8178083300590515\n",
      "Loss for batch 42 = 0.8071195483207703\n",
      "Loss for batch 43 = 0.9663742184638977\n",
      "Loss for batch 44 = 0.8815490007400513\n",
      "Loss for batch 45 = 0.8784539699554443\n",
      "Loss for batch 46 = 0.8820856213569641\n",
      "Loss for batch 47 = 0.8036674857139587\n",
      "Loss for batch 48 = 0.6573851108551025\n",
      "Loss for batch 49 = 0.7978927493095398\n",
      "Loss for batch 50 = 0.6955586075782776\n",
      "Loss for batch 51 = 0.7423503994941711\n",
      "Loss for batch 52 = 1.1554009914398193\n",
      "Loss for batch 53 = 0.8432093262672424\n",
      "Loss for batch 54 = 0.8485509157180786\n",
      "Loss for batch 55 = 0.8113547563552856\n",
      "Loss for batch 56 = 0.7514929175376892\n",
      "Loss for batch 57 = 0.6035732626914978\n",
      "Loss for batch 58 = 0.8526257276535034\n",
      "Loss for batch 59 = 0.7084905505180359\n",
      "Loss for batch 60 = 0.7821378707885742\n",
      "Loss for batch 61 = 1.0255178213119507\n",
      "Loss for batch 62 = 0.8349208831787109\n",
      "Loss for batch 63 = 0.6472446322441101\n",
      "Loss for batch 64 = 0.6851651668548584\n",
      "Loss for batch 65 = 0.8688885569572449\n",
      "Loss for batch 66 = 0.889729380607605\n",
      "Loss for batch 67 = 0.7796461582183838\n",
      "Loss for batch 68 = 0.7566053867340088\n",
      "Loss for batch 69 = 0.6922117471694946\n",
      "Loss for batch 70 = 0.7879387736320496\n",
      "Loss for batch 71 = 0.638992428779602\n",
      "Loss for batch 72 = 0.8375797867774963\n",
      "Loss for batch 73 = 0.767241895198822\n",
      "Loss for batch 74 = 0.9724724292755127\n",
      "Loss for batch 75 = 0.9654545187950134\n",
      "Loss for batch 76 = 0.8819863796234131\n",
      "Loss for batch 77 = 0.874392569065094\n",
      "Loss for batch 78 = 0.8410755395889282\n",
      "Loss for batch 79 = 0.644798755645752\n",
      "Loss for batch 80 = 0.6576559543609619\n",
      "Loss for batch 81 = 0.6914963722229004\n",
      "Loss for batch 82 = 0.8667341470718384\n",
      "Loss for batch 83 = 0.7972904443740845\n",
      "Loss for batch 84 = 0.8155048489570618\n",
      "Loss for batch 85 = 1.1291697025299072\n",
      "Loss for batch 86 = 0.6308084726333618\n",
      "Loss for batch 87 = 0.7156183123588562\n",
      "Loss for batch 88 = 0.7975438833236694\n",
      "Loss for batch 89 = 0.9299331307411194\n",
      "Loss for batch 90 = 0.8063659071922302\n",
      "Loss for batch 91 = 0.8002349734306335\n",
      "Loss for batch 92 = 0.9330728650093079\n",
      "Loss for batch 93 = 0.8261973857879639\n",
      "Loss for batch 94 = 0.7468447685241699\n",
      "Loss for batch 95 = 0.8036729693412781\n",
      "Loss for batch 96 = 0.7534646391868591\n",
      "Loss for batch 97 = 0.7197474241256714\n",
      "Loss for batch 98 = 0.6687692999839783\n",
      "Loss for batch 99 = 0.6400761604309082\n",
      "Loss for batch 100 = 0.6488306522369385\n",
      "Loss for batch 101 = 0.6674371361732483\n",
      "Loss for batch 102 = 0.6705086827278137\n",
      "Loss for batch 103 = 0.8476633429527283\n",
      "Loss for batch 104 = 0.7958344221115112\n",
      "Loss for batch 105 = 0.9192772507667542\n",
      "Loss for batch 106 = 1.0513018369674683\n",
      "\n",
      "Training Loss for epoch 6 = 88.18338775634766\n",
      "\n",
      "Current Validation Loss = 13.499734878540039\n",
      "Best Validation Loss = 13.499734878540039\n",
      "Epochs without Improvement = 0\n",
      "Train Accuracy: 63.47%\n",
      "Validation Accuracy: 59.96%\n",
      "\n",
      "Epoch 7\n",
      "----------\n",
      "Loss for batch 0 = 0.5751024484634399\n",
      "Loss for batch 1 = 0.5838082432746887\n",
      "Loss for batch 2 = 0.7761331796646118\n",
      "Loss for batch 3 = 0.7227821946144104\n",
      "Loss for batch 4 = 0.6490731835365295\n",
      "Loss for batch 5 = 0.7178816795349121\n",
      "Loss for batch 6 = 0.8449832201004028\n",
      "Loss for batch 7 = 0.9928501844406128\n",
      "Loss for batch 8 = 0.8015822768211365\n",
      "Loss for batch 9 = 0.6978186368942261\n",
      "Loss for batch 10 = 0.6565820574760437\n",
      "Loss for batch 11 = 0.7822383642196655\n",
      "Loss for batch 12 = 0.8142284750938416\n",
      "Loss for batch 13 = 0.8604790568351746\n",
      "Loss for batch 14 = 0.7560032606124878\n",
      "Loss for batch 15 = 0.841819167137146\n",
      "Loss for batch 16 = 0.7958006262779236\n",
      "Loss for batch 17 = 0.6400084495544434\n",
      "Loss for batch 18 = 0.7204582691192627\n",
      "Loss for batch 19 = 0.5490753054618835\n",
      "Loss for batch 20 = 0.8358079195022583\n",
      "Loss for batch 21 = 0.515198826789856\n",
      "Loss for batch 22 = 1.1949074268341064\n",
      "Loss for batch 23 = 0.5985089540481567\n",
      "Loss for batch 24 = 0.653711199760437\n",
      "Loss for batch 25 = 0.6789944767951965\n",
      "Loss for batch 26 = 0.6213076114654541\n",
      "Loss for batch 27 = 0.7122839689254761\n",
      "Loss for batch 28 = 0.7202726006507874\n",
      "Loss for batch 29 = 0.8081768751144409\n",
      "Loss for batch 30 = 0.7861179113388062\n",
      "Loss for batch 31 = 1.0303442478179932\n",
      "Loss for batch 32 = 0.7041049599647522\n",
      "Loss for batch 33 = 0.6996711492538452\n",
      "Loss for batch 34 = 0.684628963470459\n",
      "Loss for batch 35 = 0.936140775680542\n",
      "Loss for batch 36 = 0.6436209678649902\n",
      "Loss for batch 37 = 0.6769877672195435\n",
      "Loss for batch 38 = 0.7268869280815125\n",
      "Loss for batch 39 = 0.6360189318656921\n",
      "Loss for batch 40 = 0.7513613700866699\n",
      "Loss for batch 41 = 0.6908729076385498\n",
      "Loss for batch 42 = 0.7447171211242676\n",
      "Loss for batch 43 = 0.9697510600090027\n",
      "Loss for batch 44 = 0.8200395107269287\n",
      "Loss for batch 45 = 0.8096856474876404\n",
      "Loss for batch 46 = 0.7372657656669617\n",
      "Loss for batch 47 = 0.6620309948921204\n",
      "Loss for batch 48 = 0.5350927114486694\n",
      "Loss for batch 49 = 0.7903861403465271\n",
      "Loss for batch 50 = 0.48331913352012634\n",
      "Loss for batch 51 = 0.6668729782104492\n",
      "Loss for batch 52 = 0.8215961456298828\n",
      "Loss for batch 53 = 0.8249285817146301\n",
      "Loss for batch 54 = 0.7489317059516907\n",
      "Loss for batch 55 = 0.7529056668281555\n",
      "Loss for batch 56 = 0.5968226194381714\n",
      "Loss for batch 57 = 0.5361632108688354\n",
      "Loss for batch 58 = 0.6565840840339661\n",
      "Loss for batch 59 = 0.6039735078811646\n",
      "Loss for batch 60 = 0.7760159373283386\n",
      "Loss for batch 61 = 0.8361228108406067\n",
      "Loss for batch 62 = 0.6741716861724854\n",
      "Loss for batch 63 = 0.5527880787849426\n",
      "Loss for batch 64 = 0.607647716999054\n",
      "Loss for batch 65 = 0.7729942798614502\n",
      "Loss for batch 66 = 0.7960147857666016\n",
      "Loss for batch 67 = 0.6724243760108948\n",
      "Loss for batch 68 = 0.7430784106254578\n",
      "Loss for batch 69 = 0.6036849617958069\n",
      "Loss for batch 70 = 0.7767950296401978\n",
      "Loss for batch 71 = 0.5379636287689209\n",
      "Loss for batch 72 = 0.7332674264907837\n",
      "Loss for batch 73 = 0.7919538617134094\n",
      "Loss for batch 74 = 0.9335101842880249\n",
      "Loss for batch 75 = 0.914608359336853\n",
      "Loss for batch 76 = 0.7564426064491272\n",
      "Loss for batch 77 = 0.8985688090324402\n",
      "Loss for batch 78 = 0.763130247592926\n",
      "Loss for batch 79 = 0.6340163946151733\n",
      "Loss for batch 80 = 0.5954453945159912\n",
      "Loss for batch 81 = 0.7336585521697998\n",
      "Loss for batch 82 = 0.8263036012649536\n",
      "Loss for batch 83 = 0.6777032017707825\n",
      "Loss for batch 84 = 0.7427490949630737\n",
      "Loss for batch 85 = 1.157568097114563\n",
      "Loss for batch 86 = 0.6576941609382629\n",
      "Loss for batch 87 = 0.7016908526420593\n",
      "Loss for batch 88 = 0.6656451225280762\n",
      "Loss for batch 89 = 0.8591172695159912\n",
      "Loss for batch 90 = 0.7057978510856628\n",
      "Loss for batch 91 = 0.7673966884613037\n",
      "Loss for batch 92 = 0.8051508665084839\n",
      "Loss for batch 93 = 0.7689321041107178\n",
      "Loss for batch 94 = 0.6077378988265991\n",
      "Loss for batch 95 = 0.690440833568573\n",
      "Loss for batch 96 = 0.6483196020126343\n",
      "Loss for batch 97 = 0.6982117891311646\n",
      "Loss for batch 98 = 0.6518015265464783\n",
      "Loss for batch 99 = 0.6127917170524597\n",
      "Loss for batch 100 = 0.5793521404266357\n",
      "Loss for batch 101 = 0.5987600684165955\n",
      "Loss for batch 102 = 0.6381890773773193\n",
      "Loss for batch 103 = 0.8268887996673584\n",
      "Loss for batch 104 = 0.5526823401451111\n",
      "Loss for batch 105 = 0.7450253367424011\n",
      "Loss for batch 106 = 0.9750030040740967\n",
      "\n",
      "Training Loss for epoch 7 = 78.11095428466797\n",
      "\n",
      "Current Validation Loss = 13.186616897583008\n",
      "Best Validation Loss = 13.186616897583008\n",
      "Epochs without Improvement = 0\n",
      "Train Accuracy: 68.08%\n",
      "Validation Accuracy: 61.60%\n",
      "\n",
      "Epoch 8\n",
      "----------\n",
      "Loss for batch 0 = 0.5821660757064819\n",
      "Loss for batch 1 = 0.5054961442947388\n",
      "Loss for batch 2 = 0.6611024141311646\n",
      "Loss for batch 3 = 0.7160729169845581\n",
      "Loss for batch 4 = 0.6867181658744812\n",
      "Loss for batch 5 = 0.4471191167831421\n",
      "Loss for batch 6 = 0.6808167695999146\n",
      "Loss for batch 7 = 0.9561718702316284\n",
      "Loss for batch 8 = 0.770675778388977\n",
      "Loss for batch 9 = 0.6411641240119934\n",
      "Loss for batch 10 = 0.5142380595207214\n",
      "Loss for batch 11 = 0.7997106909751892\n",
      "Loss for batch 12 = 0.7036197185516357\n",
      "Loss for batch 13 = 0.8681007027626038\n",
      "Loss for batch 14 = 0.6928153038024902\n",
      "Loss for batch 15 = 0.871372401714325\n",
      "Loss for batch 16 = 0.7234517931938171\n",
      "Loss for batch 17 = 0.6361873745918274\n",
      "Loss for batch 18 = 0.6978799104690552\n",
      "Loss for batch 19 = 0.5340112447738647\n",
      "Loss for batch 20 = 0.7958193421363831\n",
      "Loss for batch 21 = 0.4686128497123718\n",
      "Loss for batch 22 = 0.8230812549591064\n",
      "Loss for batch 23 = 0.5306661128997803\n",
      "Loss for batch 24 = 0.5674753189086914\n",
      "Loss for batch 25 = 0.5349181294441223\n",
      "Loss for batch 26 = 0.4974660575389862\n",
      "Loss for batch 27 = 0.6673202514648438\n",
      "Loss for batch 28 = 0.518858015537262\n",
      "Loss for batch 29 = 0.8167757987976074\n",
      "Loss for batch 30 = 0.6321792602539062\n",
      "Loss for batch 31 = 0.9812325835227966\n",
      "Loss for batch 32 = 0.7004811763763428\n",
      "Loss for batch 33 = 0.7143191695213318\n",
      "Loss for batch 34 = 0.5791097283363342\n",
      "Loss for batch 35 = 0.8527549505233765\n",
      "Loss for batch 36 = 0.5243982076644897\n",
      "Loss for batch 37 = 0.6106618046760559\n",
      "Loss for batch 38 = 0.6749592423439026\n",
      "Loss for batch 39 = 0.6336759328842163\n",
      "Loss for batch 40 = 0.7648101449012756\n",
      "Loss for batch 41 = 0.5748554468154907\n",
      "Loss for batch 42 = 0.6356188654899597\n",
      "Loss for batch 43 = 0.934851348400116\n",
      "Loss for batch 44 = 0.662522554397583\n",
      "Loss for batch 45 = 0.7845158576965332\n",
      "Loss for batch 46 = 0.7075101733207703\n",
      "Loss for batch 47 = 0.5981109142303467\n",
      "Loss for batch 48 = 0.48772159218788147\n",
      "Loss for batch 49 = 0.8173727989196777\n",
      "Loss for batch 50 = 0.44195443391799927\n",
      "Loss for batch 51 = 0.5552989840507507\n",
      "Loss for batch 52 = 0.730937123298645\n",
      "Loss for batch 53 = 0.781248927116394\n",
      "Loss for batch 54 = 0.7498776316642761\n",
      "Loss for batch 55 = 0.7239174842834473\n",
      "Loss for batch 56 = 0.5775612592697144\n",
      "Loss for batch 57 = 0.5503339767456055\n",
      "Loss for batch 58 = 0.6096771955490112\n",
      "Loss for batch 59 = 0.6204556822776794\n",
      "Loss for batch 60 = 0.7202105522155762\n",
      "Loss for batch 61 = 0.7716574668884277\n",
      "Loss for batch 62 = 0.5861110091209412\n",
      "Loss for batch 63 = 0.5150774121284485\n",
      "Loss for batch 64 = 0.6253348588943481\n",
      "Loss for batch 65 = 0.7978434562683105\n",
      "Loss for batch 66 = 0.9939031004905701\n",
      "Loss for batch 67 = 0.7532094717025757\n",
      "Loss for batch 68 = 0.7785511612892151\n",
      "Loss for batch 69 = 0.5953099727630615\n",
      "Loss for batch 70 = 0.6890442371368408\n",
      "Loss for batch 71 = 0.4694150388240814\n",
      "Loss for batch 72 = 0.7103809118270874\n",
      "Loss for batch 73 = 0.7499569058418274\n",
      "Loss for batch 74 = 0.801417887210846\n",
      "Loss for batch 75 = 0.9379337430000305\n",
      "Loss for batch 76 = 0.6815943717956543\n",
      "Loss for batch 77 = 0.898931086063385\n",
      "Loss for batch 78 = 0.7213273644447327\n",
      "Loss for batch 79 = 0.5918461084365845\n",
      "Loss for batch 80 = 0.5221096277236938\n",
      "Loss for batch 81 = 0.6992437839508057\n",
      "Loss for batch 82 = 0.6391116976737976\n",
      "Loss for batch 83 = 0.7183651328086853\n",
      "Loss for batch 84 = 0.6783567667007446\n",
      "Loss for batch 85 = 0.8352410793304443\n",
      "Loss for batch 86 = 0.45720359683036804\n",
      "Loss for batch 87 = 0.6416891813278198\n",
      "Loss for batch 88 = 0.6749166250228882\n",
      "Loss for batch 89 = 0.7249135971069336\n",
      "Loss for batch 90 = 0.6290206909179688\n",
      "Loss for batch 91 = 0.6514408588409424\n",
      "Loss for batch 92 = 0.6824343204498291\n",
      "Loss for batch 93 = 0.6503126621246338\n",
      "Loss for batch 94 = 0.5724250078201294\n",
      "Loss for batch 95 = 0.6643024682998657\n",
      "Loss for batch 96 = 0.7545082569122314\n",
      "Loss for batch 97 = 0.7137292623519897\n",
      "Loss for batch 98 = 0.6758325695991516\n",
      "Loss for batch 99 = 0.6265920996665955\n",
      "Loss for batch 100 = 0.5399520397186279\n",
      "Loss for batch 101 = 0.6866587996482849\n",
      "Loss for batch 102 = 0.6549628376960754\n",
      "Loss for batch 103 = 0.7366959452629089\n",
      "Loss for batch 104 = 0.5788984894752502\n",
      "Loss for batch 105 = 0.7899306416511536\n",
      "Loss for batch 106 = 0.8113990426063538\n",
      "\n",
      "Training Loss for epoch 8 = 72.5221176147461\n",
      "\n",
      "Current Validation Loss = 15.044881820678711\n",
      "Best Validation Loss = 13.186616897583008\n",
      "Epochs without Improvement = 1\n",
      "Train Accuracy: 69.69%\n",
      "Validation Accuracy: 61.60%\n",
      "\n",
      "Epoch 9\n",
      "----------\n",
      "Loss for batch 0 = 0.48637518286705017\n",
      "Loss for batch 1 = 0.4991048574447632\n",
      "Loss for batch 2 = 0.661399781703949\n",
      "Loss for batch 3 = 0.7599570751190186\n",
      "Loss for batch 4 = 0.7482183575630188\n",
      "Loss for batch 5 = 0.4320192337036133\n",
      "Loss for batch 6 = 0.5377981066703796\n",
      "Loss for batch 7 = 0.8608697652816772\n",
      "Loss for batch 8 = 0.65543133020401\n",
      "Loss for batch 9 = 0.5998886823654175\n",
      "Loss for batch 10 = 0.48417243361473083\n",
      "Loss for batch 11 = 0.6870700716972351\n",
      "Loss for batch 12 = 0.6830596327781677\n",
      "Loss for batch 13 = 0.7634543180465698\n",
      "Loss for batch 14 = 0.6602598428726196\n",
      "Loss for batch 15 = 0.8871337175369263\n",
      "Loss for batch 16 = 0.6112962961196899\n",
      "Loss for batch 17 = 0.6276296973228455\n",
      "Loss for batch 18 = 0.674805760383606\n",
      "Loss for batch 19 = 0.4992029666900635\n",
      "Loss for batch 20 = 0.6817428469657898\n",
      "Loss for batch 21 = 0.4418536424636841\n",
      "Loss for batch 22 = 0.7397523522377014\n",
      "Loss for batch 23 = 0.4209127724170685\n",
      "Loss for batch 24 = 0.5436782240867615\n",
      "Loss for batch 25 = 0.49831825494766235\n",
      "Loss for batch 26 = 0.40287187695503235\n",
      "Loss for batch 27 = 0.5926771759986877\n",
      "Loss for batch 28 = 0.5644137859344482\n",
      "Loss for batch 29 = 0.6699823141098022\n",
      "Loss for batch 30 = 0.5542823076248169\n",
      "Loss for batch 31 = 0.9869110584259033\n",
      "Loss for batch 32 = 0.6953284740447998\n",
      "Loss for batch 33 = 0.6446530222892761\n",
      "Loss for batch 34 = 0.4666549861431122\n",
      "Loss for batch 35 = 0.7084166407585144\n",
      "Loss for batch 36 = 0.5668343901634216\n",
      "Loss for batch 37 = 0.49134501814842224\n",
      "Loss for batch 38 = 0.6949101686477661\n",
      "Loss for batch 39 = 0.5626798272132874\n",
      "Loss for batch 40 = 0.7425701022148132\n",
      "Loss for batch 41 = 0.5216030478477478\n",
      "Loss for batch 42 = 0.5812466740608215\n",
      "Loss for batch 43 = 0.9436262249946594\n",
      "Loss for batch 44 = 0.6030423641204834\n",
      "Loss for batch 45 = 0.7586473226547241\n",
      "Loss for batch 46 = 0.6557009816169739\n",
      "Loss for batch 47 = 0.4732853174209595\n",
      "Loss for batch 48 = 0.431604266166687\n",
      "Loss for batch 49 = 0.828031063079834\n",
      "Loss for batch 50 = 0.4678456485271454\n",
      "Loss for batch 51 = 0.5643033385276794\n",
      "Loss for batch 52 = 0.7101159691810608\n",
      "Loss for batch 53 = 0.6314753890037537\n",
      "Loss for batch 54 = 0.7249637842178345\n",
      "Loss for batch 55 = 0.7409289479255676\n",
      "Loss for batch 56 = 0.5213343501091003\n",
      "Loss for batch 57 = 0.48469752073287964\n",
      "Loss for batch 58 = 0.5828590989112854\n",
      "Loss for batch 59 = 0.6328116059303284\n",
      "Loss for batch 60 = 0.6418241262435913\n",
      "Loss for batch 61 = 0.730649471282959\n",
      "Loss for batch 62 = 0.583918035030365\n",
      "Loss for batch 63 = 0.5123339891433716\n",
      "Loss for batch 64 = 0.4277920126914978\n",
      "Loss for batch 65 = 0.7187460064888\n",
      "Loss for batch 66 = 0.850353479385376\n",
      "Loss for batch 67 = 0.5346506834030151\n",
      "Loss for batch 68 = 0.6731463670730591\n",
      "Loss for batch 69 = 0.5580424666404724\n",
      "Loss for batch 70 = 0.6501930952072144\n",
      "Loss for batch 71 = 0.44516485929489136\n",
      "Loss for batch 72 = 0.7016366720199585\n",
      "Loss for batch 73 = 0.626693606376648\n",
      "Loss for batch 74 = 0.8609759211540222\n",
      "Loss for batch 75 = 0.8851092457771301\n",
      "Loss for batch 76 = 0.6448458433151245\n",
      "Loss for batch 77 = 0.7721633911132812\n",
      "Loss for batch 78 = 0.6402968168258667\n",
      "Loss for batch 79 = 0.5303127765655518\n",
      "Loss for batch 80 = 0.4739859998226166\n",
      "Loss for batch 81 = 0.6749149560928345\n",
      "Loss for batch 82 = 0.5861866474151611\n",
      "Loss for batch 83 = 0.656559944152832\n",
      "Loss for batch 84 = 0.7327660918235779\n",
      "Loss for batch 85 = 0.9254979491233826\n",
      "Loss for batch 86 = 0.4585093855857849\n",
      "Loss for batch 87 = 0.6516231298446655\n",
      "Loss for batch 88 = 0.6357021927833557\n",
      "Loss for batch 89 = 0.738405168056488\n",
      "Loss for batch 90 = 0.5630273818969727\n",
      "Loss for batch 91 = 0.5761348605155945\n",
      "Loss for batch 92 = 0.6502000689506531\n",
      "Loss for batch 93 = 0.5591614842414856\n",
      "Loss for batch 94 = 0.5585818886756897\n",
      "Loss for batch 95 = 0.5519319772720337\n",
      "Loss for batch 96 = 0.47354406118392944\n",
      "Loss for batch 97 = 0.7134992480278015\n",
      "Loss for batch 98 = 0.5453192591667175\n",
      "Loss for batch 99 = 0.46856048703193665\n",
      "Loss for batch 100 = 0.4303475320339203\n",
      "Loss for batch 101 = 0.5438272953033447\n",
      "Loss for batch 102 = 0.5389334559440613\n",
      "Loss for batch 103 = 0.7665302753448486\n",
      "Loss for batch 104 = 0.43280771374702454\n",
      "Loss for batch 105 = 0.7617856860160828\n",
      "Loss for batch 106 = 0.7598620653152466\n",
      "\n",
      "Training Loss for epoch 9 = 66.7651138305664\n",
      "\n",
      "Current Validation Loss = 13.690241813659668\n",
      "Best Validation Loss = 13.186616897583008\n",
      "Epochs without Improvement = 2\n",
      "Train Accuracy: 71.74%\n",
      "Validation Accuracy: 64.48%\n",
      "\n",
      "Epoch 10\n",
      "----------\n",
      "Loss for batch 0 = 0.5281997323036194\n",
      "Loss for batch 1 = 0.41200336813926697\n",
      "Loss for batch 2 = 0.4486459791660309\n",
      "Loss for batch 3 = 0.7498988509178162\n",
      "Loss for batch 4 = 0.5997558832168579\n",
      "Loss for batch 5 = 0.3699972927570343\n",
      "Loss for batch 6 = 0.5311592221260071\n",
      "Loss for batch 7 = 0.7516971826553345\n",
      "Loss for batch 8 = 0.6210700273513794\n",
      "Loss for batch 9 = 0.4750794768333435\n",
      "Loss for batch 10 = 0.3915071189403534\n",
      "Loss for batch 11 = 0.5625025629997253\n",
      "Loss for batch 12 = 0.6300683617591858\n",
      "Loss for batch 13 = 0.584674596786499\n",
      "Loss for batch 14 = 0.7490121722221375\n",
      "Loss for batch 15 = 0.9238840341567993\n",
      "Loss for batch 16 = 0.5083149671554565\n",
      "Loss for batch 17 = 0.5775731205940247\n",
      "Loss for batch 18 = 0.5993124842643738\n",
      "Loss for batch 19 = 0.4155837893486023\n",
      "Loss for batch 20 = 0.6686315536499023\n",
      "Loss for batch 21 = 0.4173216223716736\n",
      "Loss for batch 22 = 0.8775026202201843\n",
      "Loss for batch 23 = 0.43622028827667236\n",
      "Loss for batch 24 = 0.49211132526397705\n",
      "Loss for batch 25 = 0.44015154242515564\n",
      "Loss for batch 26 = 0.3908774256706238\n",
      "Loss for batch 27 = 0.5586271286010742\n",
      "Loss for batch 28 = 0.5167896747589111\n",
      "Loss for batch 29 = 0.6305305361747742\n",
      "Loss for batch 30 = 0.552730917930603\n",
      "Loss for batch 31 = 0.8540673851966858\n",
      "Loss for batch 32 = 0.5946338176727295\n",
      "Loss for batch 33 = 0.5400300025939941\n",
      "Loss for batch 34 = 0.4193181097507477\n",
      "Loss for batch 35 = 0.551018476486206\n",
      "Loss for batch 36 = 0.40046223998069763\n",
      "Loss for batch 37 = 0.48331961035728455\n",
      "Loss for batch 38 = 0.601054310798645\n",
      "Loss for batch 39 = 0.5248659253120422\n",
      "Loss for batch 40 = 0.5730441808700562\n",
      "Loss for batch 41 = 0.43720102310180664\n",
      "Loss for batch 42 = 0.6527045369148254\n",
      "Loss for batch 43 = 0.8595821857452393\n",
      "Loss for batch 44 = 0.6233401894569397\n",
      "Loss for batch 45 = 0.7042033076286316\n",
      "Loss for batch 46 = 0.6125794649124146\n",
      "Loss for batch 47 = 0.45781776309013367\n",
      "Loss for batch 48 = 0.378418892621994\n",
      "Loss for batch 49 = 0.835601806640625\n",
      "Loss for batch 50 = 0.39430737495422363\n",
      "Loss for batch 51 = 0.40286627411842346\n",
      "Loss for batch 52 = 0.5209075808525085\n",
      "Loss for batch 53 = 0.5868157744407654\n",
      "Loss for batch 54 = 0.6425738334655762\n",
      "Loss for batch 55 = 0.6473914980888367\n",
      "Loss for batch 56 = 0.459915429353714\n",
      "Loss for batch 57 = 0.4361894130706787\n",
      "Loss for batch 58 = 0.6022316217422485\n",
      "Loss for batch 59 = 0.556749165058136\n",
      "Loss for batch 60 = 0.5593176484107971\n",
      "Loss for batch 61 = 0.7606589794158936\n",
      "Loss for batch 62 = 0.5313134789466858\n",
      "Loss for batch 63 = 0.41540786623954773\n",
      "Loss for batch 64 = 0.3570980727672577\n",
      "Loss for batch 65 = 0.7161409854888916\n",
      "Loss for batch 66 = 0.6918621063232422\n",
      "Loss for batch 67 = 0.42413994669914246\n",
      "Loss for batch 68 = 0.5640233159065247\n",
      "Loss for batch 69 = 0.49771660566329956\n",
      "Loss for batch 70 = 0.5113933086395264\n",
      "Loss for batch 71 = 0.5927528142929077\n",
      "Loss for batch 72 = 0.5743005275726318\n",
      "Loss for batch 73 = 0.5099255442619324\n",
      "Loss for batch 74 = 0.8203845024108887\n",
      "Loss for batch 75 = 0.7470813989639282\n",
      "Loss for batch 76 = 0.595337450504303\n",
      "Loss for batch 77 = 0.7450473308563232\n",
      "Loss for batch 78 = 0.5031858086585999\n",
      "Loss for batch 79 = 0.42132487893104553\n",
      "Loss for batch 80 = 0.3901917338371277\n",
      "Loss for batch 81 = 0.5575159192085266\n",
      "Loss for batch 82 = 0.5263384580612183\n",
      "Loss for batch 83 = 0.6001962423324585\n",
      "Loss for batch 84 = 0.6606363654136658\n",
      "Loss for batch 85 = 0.9067389369010925\n",
      "Loss for batch 86 = 0.36765721440315247\n",
      "Loss for batch 87 = 0.5228376984596252\n",
      "Loss for batch 88 = 0.5615538954734802\n",
      "Loss for batch 89 = 0.7343430519104004\n",
      "Loss for batch 90 = 0.5004020929336548\n",
      "Loss for batch 91 = 0.5301293134689331\n",
      "Loss for batch 92 = 0.5612335801124573\n",
      "Loss for batch 93 = 0.4895780682563782\n",
      "Loss for batch 94 = 0.5473024249076843\n",
      "Loss for batch 95 = 0.5320351123809814\n",
      "Loss for batch 96 = 0.45453882217407227\n",
      "Loss for batch 97 = 0.7003744840621948\n",
      "Loss for batch 98 = 0.5625884532928467\n",
      "Loss for batch 99 = 0.4220060110092163\n",
      "Loss for batch 100 = 0.36361193656921387\n",
      "Loss for batch 101 = 0.40096744894981384\n",
      "Loss for batch 102 = 0.46064385771751404\n",
      "Loss for batch 103 = 0.7474557161331177\n",
      "Loss for batch 104 = 0.3738243579864502\n",
      "Loss for batch 105 = 0.727496862411499\n",
      "Loss for batch 106 = 0.5791733264923096\n",
      "\n",
      "Training Loss for epoch 10 = 60.08442306518555\n",
      "\n",
      "Current Validation Loss = 14.065515518188477\n",
      "Best Validation Loss = 13.186616897583008\n",
      "Epochs without Improvement = 3\n",
      "Train Accuracy: 76.70%\n",
      "Validation Accuracy: 63.66%\n",
      "\n",
      "Epoch 11\n",
      "----------\n",
      "Loss for batch 0 = 0.4591843783855438\n",
      "Loss for batch 1 = 0.30543339252471924\n",
      "Loss for batch 2 = 0.3782825469970703\n",
      "Loss for batch 3 = 0.6595960855484009\n",
      "Loss for batch 4 = 0.4818958640098572\n",
      "Loss for batch 5 = 0.4117661416530609\n",
      "Loss for batch 6 = 0.4837941825389862\n",
      "Loss for batch 7 = 0.8048824071884155\n",
      "Loss for batch 8 = 0.4904044568538666\n",
      "Loss for batch 9 = 0.4116496443748474\n",
      "Loss for batch 10 = 0.35919389128685\n",
      "Loss for batch 11 = 0.476345956325531\n",
      "Loss for batch 12 = 0.5317701697349548\n",
      "Loss for batch 13 = 0.48306941986083984\n",
      "Loss for batch 14 = 0.35706961154937744\n",
      "Loss for batch 15 = 0.8691781163215637\n",
      "Loss for batch 16 = 0.5255601406097412\n",
      "Loss for batch 17 = 0.576741635799408\n",
      "Loss for batch 18 = 0.5913920998573303\n",
      "Loss for batch 19 = 0.430556058883667\n",
      "Loss for batch 20 = 0.621532678604126\n",
      "Loss for batch 21 = 0.402154803276062\n",
      "Loss for batch 22 = 0.7154832482337952\n",
      "Loss for batch 23 = 0.3382736146450043\n",
      "Loss for batch 24 = 0.326008677482605\n",
      "Loss for batch 25 = 0.43486395478248596\n",
      "Loss for batch 26 = 0.3344809412956238\n",
      "Loss for batch 27 = 0.5505062937736511\n",
      "Loss for batch 28 = 0.48362624645233154\n",
      "Loss for batch 29 = 0.46896371245384216\n",
      "Loss for batch 30 = 0.3479630649089813\n",
      "Loss for batch 31 = 0.9454134702682495\n",
      "Loss for batch 32 = 0.39351996779441833\n",
      "Loss for batch 33 = 0.3927701711654663\n",
      "Loss for batch 34 = 0.3647633492946625\n",
      "Loss for batch 35 = 0.5341237187385559\n",
      "Loss for batch 36 = 0.37832486629486084\n",
      "Loss for batch 37 = 0.3649618625640869\n",
      "Loss for batch 38 = 0.4649796783924103\n",
      "Loss for batch 39 = 0.34924015402793884\n",
      "Loss for batch 40 = 0.5363704562187195\n",
      "Loss for batch 41 = 0.4249761402606964\n",
      "Loss for batch 42 = 0.524029552936554\n",
      "Loss for batch 43 = 0.8191245794296265\n",
      "Loss for batch 44 = 0.47570475935935974\n",
      "Loss for batch 45 = 0.6257686018943787\n",
      "Loss for batch 46 = 0.6024952530860901\n",
      "Loss for batch 47 = 0.3597450256347656\n",
      "Loss for batch 48 = 0.32584264874458313\n",
      "Loss for batch 49 = 0.6673199534416199\n",
      "Loss for batch 50 = 0.3484845459461212\n",
      "Loss for batch 51 = 0.3360062837600708\n",
      "Loss for batch 52 = 0.40662628412246704\n",
      "Loss for batch 53 = 0.5427204966545105\n",
      "Loss for batch 54 = 0.6505216956138611\n",
      "Loss for batch 55 = 0.6266605257987976\n",
      "Loss for batch 56 = 0.3537106513977051\n",
      "Loss for batch 57 = 0.33643674850463867\n",
      "Loss for batch 58 = 0.3544977903366089\n",
      "Loss for batch 59 = 0.5039063096046448\n",
      "Loss for batch 60 = 0.5348924994468689\n",
      "Loss for batch 61 = 0.6532472372055054\n",
      "Loss for batch 62 = 0.3577251136302948\n",
      "Loss for batch 63 = 0.26083070039749146\n",
      "Loss for batch 64 = 0.2707929015159607\n",
      "Loss for batch 65 = 0.5589337348937988\n",
      "Loss for batch 66 = 0.6560436487197876\n",
      "Loss for batch 67 = 0.341447651386261\n",
      "Loss for batch 68 = 0.4574820399284363\n",
      "Loss for batch 69 = 0.42124131321907043\n",
      "Loss for batch 70 = 0.4853387773036957\n",
      "Loss for batch 71 = 0.2833217680454254\n",
      "Loss for batch 72 = 0.4429469406604767\n",
      "Loss for batch 73 = 0.380751371383667\n",
      "Loss for batch 74 = 0.5401723980903625\n",
      "Loss for batch 75 = 0.7578146457672119\n",
      "Loss for batch 76 = 0.5957362055778503\n",
      "Loss for batch 77 = 0.7336947321891785\n",
      "Loss for batch 78 = 0.5548707246780396\n",
      "Loss for batch 79 = 0.4395466148853302\n",
      "Loss for batch 80 = 0.34428107738494873\n",
      "Loss for batch 81 = 0.46598055958747864\n",
      "Loss for batch 82 = 0.3670569956302643\n",
      "Loss for batch 83 = 0.5506914258003235\n",
      "Loss for batch 84 = 0.6554038524627686\n",
      "Loss for batch 85 = 0.8183333873748779\n",
      "Loss for batch 86 = 0.36092719435691833\n",
      "Loss for batch 87 = 0.42577916383743286\n",
      "Loss for batch 88 = 0.4919271469116211\n",
      "Loss for batch 89 = 0.6253601908683777\n",
      "Loss for batch 90 = 0.47735902667045593\n",
      "Loss for batch 91 = 0.41617661714553833\n",
      "Loss for batch 92 = 0.5540527701377869\n",
      "Loss for batch 93 = 0.4127705991268158\n",
      "Loss for batch 94 = 0.6065822243690491\n",
      "Loss for batch 95 = 0.4832450747489929\n",
      "Loss for batch 96 = 0.3701251447200775\n",
      "Loss for batch 97 = 0.6378118991851807\n",
      "Loss for batch 98 = 0.4285436272621155\n",
      "Loss for batch 99 = 0.40229594707489014\n",
      "Loss for batch 100 = 0.2588290572166443\n",
      "Loss for batch 101 = 0.34916871786117554\n",
      "Loss for batch 102 = 0.43125203251838684\n",
      "Loss for batch 103 = 0.747920036315918\n",
      "Loss for batch 104 = 0.30242618918418884\n",
      "Loss for batch 105 = 0.7615292072296143\n",
      "Loss for batch 106 = 0.33853960037231445\n",
      "\n",
      "Training Loss for epoch 11 = 51.93186569213867\n",
      "\n",
      "Current Validation Loss = 14.886439323425293\n",
      "Best Validation Loss = 13.186616897583008\n",
      "Epochs without Improvement = 4\n",
      "Train Accuracy: 77.76%\n",
      "Validation Accuracy: 64.89%\n",
      "\n",
      "Epoch 12\n",
      "----------\n",
      "Loss for batch 0 = 0.44034647941589355\n",
      "Loss for batch 1 = 0.26275792717933655\n",
      "Loss for batch 2 = 0.24344483017921448\n",
      "Loss for batch 3 = 0.5532958507537842\n",
      "Loss for batch 4 = 0.35045328736305237\n",
      "Loss for batch 5 = 0.3837636113166809\n",
      "Loss for batch 6 = 0.49046650528907776\n",
      "Loss for batch 7 = 0.6190145611763\n",
      "Loss for batch 8 = 0.3565062880516052\n",
      "Loss for batch 9 = 0.32512572407722473\n",
      "Loss for batch 10 = 0.2569987177848816\n",
      "Loss for batch 11 = 0.4430789053440094\n",
      "Loss for batch 12 = 0.5415892004966736\n",
      "Loss for batch 13 = 0.3580229580402374\n",
      "Loss for batch 14 = 0.3322491943836212\n",
      "Loss for batch 15 = 0.9312169551849365\n",
      "Loss for batch 16 = 0.5191478133201599\n",
      "Loss for batch 17 = 0.5145478248596191\n",
      "Loss for batch 18 = 0.5149439573287964\n",
      "Loss for batch 19 = 0.4039255380630493\n",
      "Loss for batch 20 = 0.5221810340881348\n",
      "Loss for batch 21 = 0.22830986976623535\n",
      "Loss for batch 22 = 0.5787535905838013\n",
      "Loss for batch 23 = 0.3479652404785156\n",
      "Loss for batch 24 = 0.3113212287425995\n",
      "Loss for batch 25 = 0.4237178564071655\n",
      "Loss for batch 26 = 0.26087725162506104\n",
      "Loss for batch 27 = 0.4364177882671356\n",
      "Loss for batch 28 = 0.39369723200798035\n",
      "Loss for batch 29 = 0.4549323618412018\n",
      "Loss for batch 30 = 0.30372971296310425\n",
      "Loss for batch 31 = 0.6955810189247131\n",
      "Loss for batch 32 = 0.3354439437389374\n",
      "Loss for batch 33 = 0.274383008480072\n",
      "Loss for batch 34 = 0.27649423480033875\n",
      "Loss for batch 35 = 0.4342741370201111\n",
      "Loss for batch 36 = 0.41909340023994446\n",
      "Loss for batch 37 = 0.42541933059692383\n",
      "Loss for batch 38 = 0.44445517659187317\n",
      "Loss for batch 39 = 0.20731379091739655\n",
      "Loss for batch 40 = 0.43533089756965637\n",
      "Loss for batch 41 = 0.3872587978839874\n",
      "Loss for batch 42 = 0.4956369400024414\n",
      "Loss for batch 43 = 0.6574804186820984\n",
      "Loss for batch 44 = 0.33941158652305603\n",
      "Loss for batch 45 = 0.4917214810848236\n",
      "Loss for batch 46 = 0.5688492059707642\n",
      "Loss for batch 47 = 0.27056801319122314\n",
      "Loss for batch 48 = 0.2570803165435791\n",
      "Loss for batch 49 = 0.5676944851875305\n",
      "Loss for batch 50 = 0.32898417115211487\n",
      "Loss for batch 51 = 0.3452768623828888\n",
      "Loss for batch 52 = 0.3095357120037079\n",
      "Loss for batch 53 = 0.5633086562156677\n",
      "Loss for batch 54 = 0.6876717805862427\n",
      "Loss for batch 55 = 0.6645491123199463\n",
      "Loss for batch 56 = 0.3933838903903961\n",
      "Loss for batch 57 = 0.34153077006340027\n",
      "Loss for batch 58 = 0.28450947999954224\n",
      "Loss for batch 59 = 0.4335024952888489\n",
      "Loss for batch 60 = 0.4477914273738861\n",
      "Loss for batch 61 = 0.5655211806297302\n",
      "Loss for batch 62 = 0.310539186000824\n",
      "Loss for batch 63 = 0.2488865703344345\n",
      "Loss for batch 64 = 0.22154587507247925\n",
      "Loss for batch 65 = 0.4178802967071533\n",
      "Loss for batch 66 = 0.46754783391952515\n",
      "Loss for batch 67 = 0.3102748394012451\n",
      "Loss for batch 68 = 0.3150140941143036\n",
      "Loss for batch 69 = 0.4000013470649719\n",
      "Loss for batch 70 = 0.37003034353256226\n",
      "Loss for batch 71 = 0.25540590286254883\n",
      "Loss for batch 72 = 0.4191341996192932\n",
      "Loss for batch 73 = 0.4207629859447479\n",
      "Loss for batch 74 = 0.5107232332229614\n",
      "Loss for batch 75 = 0.635001003742218\n",
      "Loss for batch 76 = 0.4518803060054779\n",
      "Loss for batch 77 = 0.436570405960083\n",
      "Loss for batch 78 = 0.38053300976753235\n",
      "Loss for batch 79 = 0.3305082619190216\n",
      "Loss for batch 80 = 0.26560813188552856\n",
      "Loss for batch 81 = 0.5338267087936401\n",
      "Loss for batch 82 = 0.34067216515541077\n",
      "Loss for batch 83 = 0.4973786771297455\n",
      "Loss for batch 84 = 0.49069681763648987\n",
      "Loss for batch 85 = 0.7927459478378296\n",
      "Loss for batch 86 = 0.33562058210372925\n",
      "Loss for batch 87 = 0.2958865463733673\n",
      "Loss for batch 88 = 0.2986040413379669\n",
      "Loss for batch 89 = 0.5147103667259216\n",
      "Loss for batch 90 = 0.43987953662872314\n",
      "Loss for batch 91 = 0.3049042224884033\n",
      "Loss for batch 92 = 0.48141050338745117\n",
      "Loss for batch 93 = 0.447076678276062\n",
      "Loss for batch 94 = 0.49771398305892944\n",
      "Loss for batch 95 = 0.4542195796966553\n",
      "Loss for batch 96 = 0.2838069200515747\n",
      "Loss for batch 97 = 0.5611718893051147\n",
      "Loss for batch 98 = 0.3290054202079773\n",
      "Loss for batch 99 = 0.3271147310733795\n",
      "Loss for batch 100 = 0.24284732341766357\n",
      "Loss for batch 101 = 0.2845005989074707\n",
      "Loss for batch 102 = 0.28042808175086975\n",
      "Loss for batch 103 = 0.6539638042449951\n",
      "Loss for batch 104 = 0.28837642073631287\n",
      "Loss for batch 105 = 0.8088168501853943\n",
      "Loss for batch 106 = 0.31274649500846863\n",
      "\n",
      "Training Loss for epoch 12 = 44.71782302856445\n",
      "\n",
      "Current Validation Loss = 15.624343872070312\n",
      "Best Validation Loss = 13.186616897583008\n",
      "Epochs without Improvement = 5\n",
      "Train Accuracy: 83.98%\n",
      "Validation Accuracy: 66.74%\n",
      "\n",
      "Epoch 13\n",
      "----------\n",
      "Loss for batch 0 = 0.4468434453010559\n",
      "Loss for batch 1 = 0.2635944187641144\n",
      "Loss for batch 2 = 0.20619073510169983\n",
      "Loss for batch 3 = 0.41674524545669556\n",
      "Loss for batch 4 = 0.27075809240341187\n",
      "Loss for batch 5 = 0.29830747842788696\n",
      "Loss for batch 6 = 0.48180705308914185\n",
      "Loss for batch 7 = 0.4722587466239929\n",
      "Loss for batch 8 = 0.3186635971069336\n",
      "Loss for batch 9 = 0.3143375515937805\n",
      "Loss for batch 10 = 0.20904429256916046\n",
      "Loss for batch 11 = 0.4538983702659607\n",
      "Loss for batch 12 = 0.46958404779434204\n",
      "Loss for batch 13 = 0.23417435586452484\n",
      "Loss for batch 14 = 0.30886536836624146\n",
      "Loss for batch 15 = 0.8188570141792297\n",
      "Loss for batch 16 = 0.31108564138412476\n",
      "Loss for batch 17 = 0.3402741253376007\n",
      "Loss for batch 18 = 0.345014363527298\n",
      "Loss for batch 19 = 0.3457275331020355\n",
      "Loss for batch 20 = 0.43219831585884094\n",
      "Loss for batch 21 = 0.14306436479091644\n",
      "Loss for batch 22 = 0.5604242086410522\n",
      "Loss for batch 23 = 0.30054959654808044\n",
      "Loss for batch 24 = 0.2942913770675659\n",
      "Loss for batch 25 = 0.4124777913093567\n",
      "Loss for batch 26 = 0.2582341134548187\n",
      "Loss for batch 27 = 0.347670316696167\n",
      "Loss for batch 28 = 0.31604674458503723\n",
      "Loss for batch 29 = 0.3842761516571045\n",
      "Loss for batch 30 = 0.27103668451309204\n",
      "Loss for batch 31 = 0.6837397813796997\n",
      "Loss for batch 32 = 0.20969323813915253\n",
      "Loss for batch 33 = 0.25864386558532715\n",
      "Loss for batch 34 = 0.31949135661125183\n",
      "Loss for batch 35 = 0.25897690653800964\n",
      "Loss for batch 36 = 0.25767794251441956\n",
      "Loss for batch 37 = 0.22660280764102936\n",
      "Loss for batch 38 = 0.40213844180107117\n",
      "Loss for batch 39 = 0.17284256219863892\n",
      "Loss for batch 40 = 0.3284687399864197\n",
      "Loss for batch 41 = 0.3352046012878418\n",
      "Loss for batch 42 = 0.43439972400665283\n",
      "Loss for batch 43 = 0.5857268571853638\n",
      "Loss for batch 44 = 0.29067811369895935\n",
      "Loss for batch 45 = 0.4546992778778076\n",
      "Loss for batch 46 = 0.47465091943740845\n",
      "Loss for batch 47 = 0.19198200106620789\n",
      "Loss for batch 48 = 0.20172296464443207\n",
      "Loss for batch 49 = 0.5618401169776917\n",
      "Loss for batch 50 = 0.4271150827407837\n",
      "Loss for batch 51 = 0.20805861055850983\n",
      "Loss for batch 52 = 0.24897947907447815\n",
      "Loss for batch 53 = 0.5033823251724243\n",
      "Loss for batch 54 = 0.7294341325759888\n",
      "Loss for batch 55 = 0.5749394297599792\n",
      "Loss for batch 56 = 0.35816270112991333\n",
      "Loss for batch 57 = 0.23939037322998047\n",
      "Loss for batch 58 = 0.17027181386947632\n",
      "Loss for batch 59 = 0.417954683303833\n",
      "Loss for batch 60 = 0.39912888407707214\n",
      "Loss for batch 61 = 0.6221385598182678\n",
      "Loss for batch 62 = 0.29586315155029297\n",
      "Loss for batch 63 = 0.1992465853691101\n",
      "Loss for batch 64 = 0.25371241569519043\n",
      "Loss for batch 65 = 0.49158957600593567\n",
      "Loss for batch 66 = 0.3922334611415863\n",
      "Loss for batch 67 = 0.22600291669368744\n",
      "Loss for batch 68 = 0.27779796719551086\n",
      "Loss for batch 69 = 0.34971246123313904\n",
      "Loss for batch 70 = 0.3065076172351837\n",
      "Loss for batch 71 = 0.19765299558639526\n",
      "Loss for batch 72 = 0.2753736972808838\n",
      "Loss for batch 73 = 0.3113204836845398\n",
      "Loss for batch 74 = 0.4443988502025604\n",
      "Loss for batch 75 = 0.809741199016571\n",
      "Loss for batch 76 = 0.5654610395431519\n",
      "Loss for batch 77 = 0.421490341424942\n",
      "Loss for batch 78 = 0.4461067318916321\n",
      "Loss for batch 79 = 0.3903433084487915\n",
      "Loss for batch 80 = 0.2591826021671295\n",
      "Loss for batch 81 = 0.36362123489379883\n",
      "Loss for batch 82 = 0.29293954372406006\n",
      "Loss for batch 83 = 0.4268644154071808\n",
      "Loss for batch 84 = 0.4209023714065552\n",
      "Loss for batch 85 = 0.6162975430488586\n",
      "Loss for batch 86 = 0.2841224670410156\n",
      "Loss for batch 87 = 0.3243632912635803\n",
      "Loss for batch 88 = 0.28959015011787415\n",
      "Loss for batch 89 = 0.4820472002029419\n",
      "Loss for batch 90 = 0.38471296429634094\n",
      "Loss for batch 91 = 0.3117579221725464\n",
      "Loss for batch 92 = 0.4048379361629486\n",
      "Loss for batch 93 = 0.3659568130970001\n",
      "Loss for batch 94 = 0.4877813756465912\n",
      "Loss for batch 95 = 0.394465833902359\n",
      "Loss for batch 96 = 0.2944319248199463\n",
      "Loss for batch 97 = 0.5276589393615723\n",
      "Loss for batch 98 = 0.3331708014011383\n",
      "Loss for batch 99 = 0.295787513256073\n",
      "Loss for batch 100 = 0.17543897032737732\n",
      "Loss for batch 101 = 0.31202441453933716\n",
      "Loss for batch 102 = 0.33264485001564026\n",
      "Loss for batch 103 = 0.6711682677268982\n",
      "Loss for batch 104 = 0.26198551058769226\n",
      "Loss for batch 105 = 0.7886025309562683\n",
      "Loss for batch 106 = 0.255184143781662\n",
      "\n",
      "Training Loss for epoch 13 = 39.606529235839844\n",
      "\n",
      "Current Validation Loss = 14.80561351776123\n",
      "Best Validation Loss = 13.186616897583008\n",
      "Epochs without Improvement = 6\n",
      "Train Accuracy: 85.71%\n",
      "Validation Accuracy: 69.40%\n",
      "\n",
      "Epoch 14\n",
      "----------\n",
      "Loss for batch 0 = 0.3756733238697052\n",
      "Loss for batch 1 = 0.1601123958826065\n",
      "Loss for batch 2 = 0.3097490966320038\n",
      "Loss for batch 3 = 0.42542874813079834\n",
      "Loss for batch 4 = 0.37172871828079224\n",
      "Loss for batch 5 = 0.4435019791126251\n",
      "Loss for batch 6 = 0.44314345717430115\n",
      "Loss for batch 7 = 0.38724350929260254\n",
      "Loss for batch 8 = 0.21915479004383087\n",
      "Loss for batch 9 = 0.3469574451446533\n",
      "Loss for batch 10 = 0.17813414335250854\n",
      "Loss for batch 11 = 0.41930240392684937\n",
      "Loss for batch 12 = 0.39687496423721313\n",
      "Loss for batch 13 = 0.24641750752925873\n",
      "Loss for batch 14 = 0.26759228110313416\n",
      "Loss for batch 15 = 0.7515005469322205\n",
      "Loss for batch 16 = 0.26351454854011536\n",
      "Loss for batch 17 = 0.251235693693161\n",
      "Loss for batch 18 = 0.3499417304992676\n",
      "Loss for batch 19 = 0.29247531294822693\n",
      "Loss for batch 20 = 0.3771006464958191\n",
      "Loss for batch 21 = 0.14381182193756104\n",
      "Loss for batch 22 = 0.4931352734565735\n",
      "Loss for batch 23 = 0.27376845479011536\n",
      "Loss for batch 24 = 0.24226967990398407\n",
      "Loss for batch 25 = 0.3094094395637512\n",
      "Loss for batch 26 = 0.1808418184518814\n",
      "Loss for batch 27 = 0.581457257270813\n",
      "Loss for batch 28 = 0.3330603837966919\n",
      "Loss for batch 29 = 0.33456969261169434\n",
      "Loss for batch 30 = 0.43903979659080505\n",
      "Loss for batch 31 = 0.6678711175918579\n",
      "Loss for batch 32 = 0.24708344042301178\n",
      "Loss for batch 33 = 0.2234177142381668\n",
      "Loss for batch 34 = 0.21708227694034576\n",
      "Loss for batch 35 = 0.15462838113307953\n",
      "Loss for batch 36 = 0.1390635222196579\n",
      "Loss for batch 37 = 0.2997552156448364\n",
      "Loss for batch 38 = 0.3088037669658661\n",
      "Loss for batch 39 = 0.22424495220184326\n",
      "Loss for batch 40 = 0.194158136844635\n",
      "Loss for batch 41 = 0.3045312762260437\n",
      "Loss for batch 42 = 0.2849988043308258\n",
      "Loss for batch 43 = 0.6868577003479004\n",
      "Loss for batch 44 = 0.20415827631950378\n",
      "Loss for batch 45 = 0.38340726494789124\n",
      "Loss for batch 46 = 0.34855887293815613\n",
      "Loss for batch 47 = 0.2110891044139862\n",
      "Loss for batch 48 = 0.1726057529449463\n",
      "Loss for batch 49 = 0.47479087114334106\n",
      "Loss for batch 50 = 0.2614860236644745\n",
      "Loss for batch 51 = 0.23849983513355255\n",
      "Loss for batch 52 = 0.2418283373117447\n",
      "Loss for batch 53 = 0.41902297735214233\n",
      "Loss for batch 54 = 0.6705379486083984\n",
      "Loss for batch 55 = 0.5155492424964905\n",
      "Loss for batch 56 = 0.2845393121242523\n",
      "Loss for batch 57 = 0.2900698184967041\n",
      "Loss for batch 58 = 0.19115500152111053\n",
      "Loss for batch 59 = 0.35883480310440063\n",
      "Loss for batch 60 = 0.3346896767616272\n",
      "Loss for batch 61 = 0.5792595744132996\n",
      "Loss for batch 62 = 0.2234114408493042\n",
      "Loss for batch 63 = 0.1770179271697998\n",
      "Loss for batch 64 = 0.14319531619548798\n",
      "Loss for batch 65 = 0.3185661733150482\n",
      "Loss for batch 66 = 0.3711163103580475\n",
      "Loss for batch 67 = 0.19362765550613403\n",
      "Loss for batch 68 = 0.2934546172618866\n",
      "Loss for batch 69 = 0.238450825214386\n",
      "Loss for batch 70 = 0.23118045926094055\n",
      "Loss for batch 71 = 0.13612428307533264\n",
      "Loss for batch 72 = 0.26542559266090393\n",
      "Loss for batch 73 = 0.32004231214523315\n",
      "Loss for batch 74 = 0.2767940163612366\n",
      "Loss for batch 75 = 0.4911407232284546\n",
      "Loss for batch 76 = 0.32875779271125793\n",
      "Loss for batch 77 = 0.2687378227710724\n",
      "Loss for batch 78 = 0.17912469804286957\n",
      "Loss for batch 79 = 0.2992858588695526\n",
      "Loss for batch 80 = 0.19396698474884033\n",
      "Loss for batch 81 = 0.3221104145050049\n",
      "Loss for batch 82 = 0.2787836790084839\n",
      "Loss for batch 83 = 0.4002572298049927\n",
      "Loss for batch 84 = 0.26270079612731934\n",
      "Loss for batch 85 = 0.543719470500946\n",
      "Loss for batch 86 = 0.2462511658668518\n",
      "Loss for batch 87 = 0.22770024836063385\n",
      "Loss for batch 88 = 0.16798466444015503\n",
      "Loss for batch 89 = 0.27290675044059753\n",
      "Loss for batch 90 = 0.3016989827156067\n",
      "Loss for batch 91 = 0.21544021368026733\n",
      "Loss for batch 92 = 0.4625515341758728\n",
      "Loss for batch 93 = 0.3908931612968445\n",
      "Loss for batch 94 = 0.4153502583503723\n",
      "Loss for batch 95 = 0.19522321224212646\n",
      "Loss for batch 96 = 0.18678151071071625\n",
      "Loss for batch 97 = 0.387747585773468\n",
      "Loss for batch 98 = 0.2994537949562073\n",
      "Loss for batch 99 = 0.1732286512851715\n",
      "Loss for batch 100 = 0.12760710716247559\n",
      "Loss for batch 101 = 0.15314282476902008\n",
      "Loss for batch 102 = 0.14293788373470306\n",
      "Loss for batch 103 = 0.6150836944580078\n",
      "Loss for batch 104 = 0.2827182114124298\n",
      "Loss for batch 105 = 0.7247782945632935\n",
      "Loss for batch 106 = 0.2635793685913086\n",
      "\n",
      "Training Loss for epoch 14 = 33.75078201293945\n",
      "\n",
      "Current Validation Loss = 15.975678443908691\n",
      "Best Validation Loss = 13.186616897583008\n",
      "Epochs without Improvement = 7\n",
      "Train Accuracy: 86.71%\n",
      "Validation Accuracy: 66.94%\n",
      "\n",
      "Epoch 15\n",
      "----------\n",
      "Loss for batch 0 = 0.2874532639980316\n",
      "Loss for batch 1 = 0.24079039692878723\n",
      "Loss for batch 2 = 0.23046331107616425\n",
      "Loss for batch 3 = 0.3557037115097046\n",
      "Loss for batch 4 = 0.18430650234222412\n",
      "Loss for batch 5 = 0.29476192593574524\n",
      "Loss for batch 6 = 0.3720683157444\n",
      "Loss for batch 7 = 0.33084815740585327\n",
      "Loss for batch 8 = 0.2284388542175293\n",
      "Loss for batch 9 = 0.3013071119785309\n",
      "Loss for batch 10 = 0.11563626676797867\n",
      "Loss for batch 11 = 0.3854556679725647\n",
      "Loss for batch 12 = 0.4013720154762268\n",
      "Loss for batch 13 = 0.1286015808582306\n",
      "Loss for batch 14 = 0.2530849575996399\n",
      "Loss for batch 15 = 0.8156788349151611\n",
      "Loss for batch 16 = 0.19726452231407166\n",
      "Loss for batch 17 = 0.2228401154279709\n",
      "Loss for batch 18 = 0.31244179606437683\n",
      "Loss for batch 19 = 0.24030394852161407\n",
      "Loss for batch 20 = 0.3158968687057495\n",
      "Loss for batch 21 = 0.06934061646461487\n",
      "Loss for batch 22 = 0.3523562550544739\n",
      "Loss for batch 23 = 0.18937870860099792\n",
      "Loss for batch 24 = 0.21711041033267975\n",
      "Loss for batch 25 = 0.31037428975105286\n",
      "Loss for batch 26 = 0.148258775472641\n",
      "Loss for batch 27 = 0.4686071276664734\n",
      "Loss for batch 28 = 0.28312787413597107\n",
      "Loss for batch 29 = 0.282895565032959\n",
      "Loss for batch 30 = 0.2731589376926422\n",
      "Loss for batch 31 = 0.6440705060958862\n",
      "Loss for batch 32 = 0.10829127579927444\n",
      "Loss for batch 33 = 0.1936473846435547\n",
      "Loss for batch 34 = 0.11202351748943329\n",
      "Loss for batch 35 = 0.22585944831371307\n",
      "Loss for batch 36 = 0.15993210673332214\n",
      "Loss for batch 37 = 0.23535113036632538\n",
      "Loss for batch 38 = 0.33576926589012146\n",
      "Loss for batch 39 = 0.17641712725162506\n",
      "Loss for batch 40 = 0.20727311074733734\n",
      "Loss for batch 41 = 0.1058121770620346\n",
      "Loss for batch 42 = 0.2211073338985443\n",
      "Loss for batch 43 = 0.5367472767829895\n",
      "Loss for batch 44 = 0.09316586703062057\n",
      "Loss for batch 45 = 0.2919772267341614\n",
      "Loss for batch 46 = 0.28871676325798035\n",
      "Loss for batch 47 = 0.10692895948886871\n",
      "Loss for batch 48 = 0.16186651587486267\n",
      "Loss for batch 49 = 0.512417733669281\n",
      "Loss for batch 50 = 0.1452784687280655\n",
      "Loss for batch 51 = 0.11441691964864731\n",
      "Loss for batch 52 = 0.08590405434370041\n",
      "Loss for batch 53 = 0.3171735405921936\n",
      "Loss for batch 54 = 0.6800957918167114\n",
      "Loss for batch 55 = 0.3707484006881714\n",
      "Loss for batch 56 = 0.23849867284297943\n",
      "Loss for batch 57 = 0.3828464150428772\n",
      "Loss for batch 58 = 0.14987055957317352\n",
      "Loss for batch 59 = 0.228001669049263\n",
      "Loss for batch 60 = 0.37182074785232544\n",
      "Loss for batch 61 = 0.4259832203388214\n",
      "Loss for batch 62 = 0.21693198382854462\n",
      "Loss for batch 63 = 0.21688531339168549\n",
      "Loss for batch 64 = 0.1671326458454132\n",
      "Loss for batch 65 = 0.2166822999715805\n",
      "Loss for batch 66 = 0.4070058763027191\n",
      "Loss for batch 67 = 0.10014951229095459\n",
      "Loss for batch 68 = 0.11261393874883652\n",
      "Loss for batch 69 = 0.2512511610984802\n",
      "Loss for batch 70 = 0.34375351667404175\n",
      "Loss for batch 71 = 0.11536891013383865\n",
      "Loss for batch 72 = 0.23671650886535645\n",
      "Loss for batch 73 = 0.3007446825504303\n",
      "Loss for batch 74 = 0.2092994600534439\n",
      "Loss for batch 75 = 0.40085822343826294\n",
      "Loss for batch 76 = 0.20795778930187225\n",
      "Loss for batch 77 = 0.20297139883041382\n",
      "Loss for batch 78 = 0.14661116898059845\n",
      "Loss for batch 79 = 0.19407020509243011\n",
      "Loss for batch 80 = 0.12931762635707855\n",
      "Loss for batch 81 = 0.29637882113456726\n",
      "Loss for batch 82 = 0.21260535717010498\n",
      "Loss for batch 83 = 0.29686489701271057\n",
      "Loss for batch 84 = 0.3423403799533844\n",
      "Loss for batch 85 = 0.42254993319511414\n",
      "Loss for batch 86 = 0.182549849152565\n",
      "Loss for batch 87 = 0.2088865041732788\n",
      "Loss for batch 88 = 0.15555183589458466\n",
      "Loss for batch 89 = 0.23860357701778412\n",
      "Loss for batch 90 = 0.3132348954677582\n",
      "Loss for batch 91 = 0.07759606838226318\n",
      "Loss for batch 92 = 0.3299168050289154\n",
      "Loss for batch 93 = 0.3522018790245056\n",
      "Loss for batch 94 = 0.41686704754829407\n",
      "Loss for batch 95 = 0.21248096227645874\n",
      "Loss for batch 96 = 0.16491393744945526\n",
      "Loss for batch 97 = 0.3562999367713928\n",
      "Loss for batch 98 = 0.2501915991306305\n",
      "Loss for batch 99 = 0.09104181081056595\n",
      "Loss for batch 100 = 0.08840081840753555\n",
      "Loss for batch 101 = 0.09919919073581696\n",
      "Loss for batch 102 = 0.15014778077602386\n",
      "Loss for batch 103 = 0.6536195278167725\n",
      "Loss for batch 104 = 0.22683215141296387\n",
      "Loss for batch 105 = 0.5894384384155273\n",
      "Loss for batch 106 = 0.11633027344942093\n",
      "\n",
      "Training Loss for epoch 15 = 28.0587100982666\n",
      "\n",
      "Current Validation Loss = 17.292375564575195\n",
      "Best Validation Loss = 13.186616897583008\n",
      "Epochs without Improvement = 8\n",
      "Train Accuracy: 89.11%\n",
      "Validation Accuracy: 66.94%\n",
      "\n",
      "Epoch 16\n",
      "----------\n",
      "Loss for batch 0 = 0.27379587292671204\n",
      "Loss for batch 1 = 0.21462896466255188\n",
      "Loss for batch 2 = 0.14985981583595276\n",
      "Loss for batch 3 = 0.28638550639152527\n",
      "Loss for batch 4 = 0.16099964082241058\n",
      "Loss for batch 5 = 0.3869937062263489\n",
      "Loss for batch 6 = 0.3540152609348297\n",
      "Loss for batch 7 = 0.30630195140838623\n",
      "Loss for batch 8 = 0.09959790110588074\n",
      "Loss for batch 9 = 0.2883768379688263\n",
      "Loss for batch 10 = 0.05337877199053764\n",
      "Loss for batch 11 = 0.23493492603302002\n",
      "Loss for batch 12 = 0.40209805965423584\n",
      "Loss for batch 13 = 0.07574475556612015\n",
      "Loss for batch 14 = 0.2071375697851181\n",
      "Loss for batch 15 = 0.63289874792099\n",
      "Loss for batch 16 = 0.29983827471733093\n",
      "Loss for batch 17 = 0.21056132018566132\n",
      "Loss for batch 18 = 0.14097736775875092\n",
      "Loss for batch 19 = 0.14418932795524597\n",
      "Loss for batch 20 = 0.1397862583398819\n",
      "Loss for batch 21 = 0.10466849058866501\n",
      "Loss for batch 22 = 0.22149549424648285\n",
      "Loss for batch 23 = 0.21303443610668182\n",
      "Loss for batch 24 = 0.15961457788944244\n",
      "Loss for batch 25 = 0.19607244431972504\n",
      "Loss for batch 26 = 0.12786702811717987\n",
      "Loss for batch 27 = 0.35590845346450806\n",
      "Loss for batch 28 = 0.2399602234363556\n",
      "Loss for batch 29 = 0.26456761360168457\n",
      "Loss for batch 30 = 0.16039042174816132\n",
      "Loss for batch 31 = 0.5561437606811523\n",
      "Loss for batch 32 = 0.08706662058830261\n",
      "Loss for batch 33 = 0.09393800050020218\n",
      "Loss for batch 34 = 0.13519342243671417\n",
      "Loss for batch 35 = 0.1848185509443283\n",
      "Loss for batch 36 = 0.1360277533531189\n",
      "Loss for batch 37 = 0.31360167264938354\n",
      "Loss for batch 38 = 0.3401917517185211\n",
      "Loss for batch 39 = 0.14507530629634857\n",
      "Loss for batch 40 = 0.18600134551525116\n",
      "Loss for batch 41 = 0.09010151773691177\n",
      "Loss for batch 42 = 0.1854768544435501\n",
      "Loss for batch 43 = 0.47811463475227356\n",
      "Loss for batch 44 = 0.15951448678970337\n",
      "Loss for batch 45 = 0.2912008464336395\n",
      "Loss for batch 46 = 0.24755023419857025\n",
      "Loss for batch 47 = 0.09488123655319214\n",
      "Loss for batch 48 = 0.13592937588691711\n",
      "Loss for batch 49 = 0.35349729657173157\n",
      "Loss for batch 50 = 0.12696582078933716\n",
      "Loss for batch 51 = 0.06304313987493515\n",
      "Loss for batch 52 = 0.05183655768632889\n",
      "Loss for batch 53 = 0.2849098742008209\n",
      "Loss for batch 54 = 0.4504411518573761\n",
      "Loss for batch 55 = 0.33893588185310364\n",
      "Loss for batch 56 = 0.13290216028690338\n",
      "Loss for batch 57 = 0.23353546857833862\n",
      "Loss for batch 58 = 0.04833003506064415\n",
      "Loss for batch 59 = 0.23179209232330322\n",
      "Loss for batch 60 = 0.24291382730007172\n",
      "Loss for batch 61 = 0.233424112200737\n",
      "Loss for batch 62 = 0.18070827424526215\n",
      "Loss for batch 63 = 0.0892665758728981\n",
      "Loss for batch 64 = 0.16456198692321777\n",
      "Loss for batch 65 = 0.17197926342487335\n",
      "Loss for batch 66 = 0.3022873103618622\n",
      "Loss for batch 67 = 0.12156053632497787\n",
      "Loss for batch 68 = 0.06535126268863678\n",
      "Loss for batch 69 = 0.2750387489795685\n",
      "Loss for batch 70 = 0.24867098033428192\n",
      "Loss for batch 71 = 0.05953693017363548\n",
      "Loss for batch 72 = 0.13823473453521729\n",
      "Loss for batch 73 = 0.2928011119365692\n",
      "Loss for batch 74 = 0.22209066152572632\n",
      "Loss for batch 75 = 0.43634119629859924\n",
      "Loss for batch 76 = 0.2882273495197296\n",
      "Loss for batch 77 = 0.12334737181663513\n",
      "Loss for batch 78 = 0.1344706267118454\n",
      "Loss for batch 79 = 0.09684295952320099\n",
      "Loss for batch 80 = 0.06175864115357399\n",
      "Loss for batch 81 = 0.13904546201229095\n",
      "Loss for batch 82 = 0.16951695084571838\n",
      "Loss for batch 83 = 0.28631067276000977\n",
      "Loss for batch 84 = 0.23385104537010193\n",
      "Loss for batch 85 = 0.4043287932872772\n",
      "Loss for batch 86 = 0.18621590733528137\n",
      "Loss for batch 87 = 0.15475240349769592\n",
      "Loss for batch 88 = 0.14016830921173096\n",
      "Loss for batch 89 = 0.19009920954704285\n",
      "Loss for batch 90 = 0.23527689278125763\n",
      "Loss for batch 91 = 0.07967400550842285\n",
      "Loss for batch 92 = 0.2920662462711334\n",
      "Loss for batch 93 = 0.2944968342781067\n",
      "Loss for batch 94 = 0.3332228660583496\n",
      "Loss for batch 95 = 0.16195394098758698\n",
      "Loss for batch 96 = 0.1446622610092163\n",
      "Loss for batch 97 = 0.30796611309051514\n",
      "Loss for batch 98 = 0.3074975311756134\n",
      "Loss for batch 99 = 0.044552091509103775\n",
      "Loss for batch 100 = 0.11137112975120544\n",
      "Loss for batch 101 = 0.07689393311738968\n",
      "Loss for batch 102 = 0.10108133405447006\n",
      "Loss for batch 103 = 0.6661355495452881\n",
      "Loss for batch 104 = 0.1254318654537201\n",
      "Loss for batch 105 = 0.6116992235183716\n",
      "Loss for batch 106 = 0.11234822869300842\n",
      "\n",
      "Training Loss for epoch 16 = 23.143136978149414\n",
      "\n",
      "Current Validation Loss = 17.824254989624023\n",
      "Best Validation Loss = 13.186616897583008\n",
      "Epochs without Improvement = 9\n",
      "Train Accuracy: 93.19%\n",
      "Validation Accuracy: 68.58%\n",
      "\n",
      "Epoch 17\n",
      "----------\n",
      "Loss for batch 0 = 0.5453758835792542\n",
      "Loss for batch 1 = 0.09795467555522919\n",
      "Loss for batch 2 = 0.1287502944469452\n",
      "Loss for batch 3 = 0.29537978768348694\n",
      "Loss for batch 4 = 0.07541368156671524\n",
      "Loss for batch 5 = 0.17073695361614227\n",
      "Loss for batch 6 = 0.2876982092857361\n",
      "Loss for batch 7 = 0.24966122210025787\n",
      "Loss for batch 8 = 0.04513499140739441\n",
      "Loss for batch 9 = 0.28523701429367065\n",
      "Loss for batch 10 = 0.05321892350912094\n",
      "Loss for batch 11 = 0.17918607592582703\n",
      "Loss for batch 12 = 0.28481268882751465\n",
      "Loss for batch 13 = 0.10873541980981827\n",
      "Loss for batch 14 = 0.13919469714164734\n",
      "Loss for batch 15 = 0.6027917861938477\n",
      "Loss for batch 16 = 0.07762415707111359\n",
      "Loss for batch 17 = 0.16582243144512177\n",
      "Loss for batch 18 = 0.18106180429458618\n",
      "Loss for batch 19 = 0.11125972867012024\n",
      "Loss for batch 20 = 0.11237641423940659\n",
      "Loss for batch 21 = 0.17819277942180634\n",
      "Loss for batch 22 = 0.17472000420093536\n",
      "Loss for batch 23 = 0.2146746963262558\n",
      "Loss for batch 24 = 0.16524703800678253\n",
      "Loss for batch 25 = 0.07140454649925232\n",
      "Loss for batch 26 = 0.0848626121878624\n",
      "Loss for batch 27 = 0.2581915855407715\n",
      "Loss for batch 28 = 0.11905823647975922\n",
      "Loss for batch 29 = 0.07287715375423431\n",
      "Loss for batch 30 = 0.12303885072469711\n",
      "Loss for batch 31 = 0.5435125827789307\n",
      "Loss for batch 32 = 0.050108857452869415\n",
      "Loss for batch 33 = 0.0674397423863411\n",
      "Loss for batch 34 = 0.05812005698680878\n",
      "Loss for batch 35 = 0.16379427909851074\n",
      "Loss for batch 36 = 0.13176779448986053\n",
      "Loss for batch 37 = 0.13221263885498047\n",
      "Loss for batch 38 = 0.2569291591644287\n",
      "Loss for batch 39 = 0.09732499718666077\n",
      "Loss for batch 40 = 0.1471780240535736\n",
      "Loss for batch 41 = 0.16934847831726074\n",
      "Loss for batch 42 = 0.18104003369808197\n",
      "Loss for batch 43 = 0.39422374963760376\n",
      "Loss for batch 44 = 0.08220071345567703\n",
      "Loss for batch 45 = 0.3287147283554077\n",
      "Loss for batch 46 = 0.1588769555091858\n",
      "Loss for batch 47 = 0.06285490840673447\n",
      "Loss for batch 48 = 0.04993028566241264\n",
      "Loss for batch 49 = 0.3415174186229706\n",
      "Loss for batch 50 = 0.04203781485557556\n",
      "Loss for batch 51 = 0.10253443568944931\n",
      "Loss for batch 52 = 0.03527204319834709\n",
      "Loss for batch 53 = 0.23727263510227203\n",
      "Loss for batch 54 = 0.48307687044143677\n",
      "Loss for batch 55 = 0.2494100034236908\n",
      "Loss for batch 56 = 0.050727441906929016\n",
      "Loss for batch 57 = 0.12143092602491379\n",
      "Loss for batch 58 = 0.04671930521726608\n",
      "Loss for batch 59 = 0.13111630082130432\n",
      "Loss for batch 60 = 0.24034655094146729\n",
      "Loss for batch 61 = 0.13487161695957184\n",
      "Loss for batch 62 = 0.12667258083820343\n",
      "Loss for batch 63 = 0.04980554059147835\n",
      "Loss for batch 64 = 0.14780212938785553\n",
      "Loss for batch 65 = 0.2129938006401062\n",
      "Loss for batch 66 = 0.23605962097644806\n",
      "Loss for batch 67 = 0.0760045051574707\n",
      "Loss for batch 68 = 0.09737702459096909\n",
      "Loss for batch 69 = 0.2009822279214859\n",
      "Loss for batch 70 = 0.1579103022813797\n",
      "Loss for batch 71 = 0.06606738269329071\n",
      "Loss for batch 72 = 0.06537599116563797\n",
      "Loss for batch 73 = 0.240338534116745\n",
      "Loss for batch 74 = 0.05264832079410553\n",
      "Loss for batch 75 = 0.3282354474067688\n",
      "Loss for batch 76 = 0.05141940340399742\n",
      "Loss for batch 77 = 0.05762813985347748\n",
      "Loss for batch 78 = 0.08516433089971542\n",
      "Loss for batch 79 = 0.0722372904419899\n",
      "Loss for batch 80 = 0.07938335835933685\n",
      "Loss for batch 81 = 0.08608955144882202\n",
      "Loss for batch 82 = 0.1758381575345993\n",
      "Loss for batch 83 = 0.18367978930473328\n",
      "Loss for batch 84 = 0.09195642918348312\n",
      "Loss for batch 85 = 0.2412164956331253\n",
      "Loss for batch 86 = 0.07480672746896744\n",
      "Loss for batch 87 = 0.12868666648864746\n",
      "Loss for batch 88 = 0.13071459531784058\n",
      "Loss for batch 89 = 0.32181552052497864\n",
      "Loss for batch 90 = 0.14997076988220215\n",
      "Loss for batch 91 = 0.06227482855319977\n",
      "Loss for batch 92 = 0.2273527979850769\n",
      "Loss for batch 93 = 0.2047945261001587\n",
      "Loss for batch 94 = 0.22079536318778992\n",
      "Loss for batch 95 = 0.07432004809379578\n",
      "Loss for batch 96 = 0.09970723092556\n",
      "Loss for batch 97 = 0.10612967610359192\n",
      "Loss for batch 98 = 0.24744904041290283\n",
      "Loss for batch 99 = 0.06733206659555435\n",
      "Loss for batch 100 = 0.03551105782389641\n",
      "Loss for batch 101 = 0.033650290220975876\n",
      "Loss for batch 102 = 0.03410886973142624\n",
      "Loss for batch 103 = 0.505112886428833\n",
      "Loss for batch 104 = 0.19062064588069916\n",
      "Loss for batch 105 = 0.5654690861701965\n",
      "Loss for batch 106 = 0.04677028954029083\n",
      "\n",
      "Training Loss for epoch 17 = 17.661884307861328\n",
      "\n",
      "Current Validation Loss = 17.979562759399414\n",
      "Best Validation Loss = 13.186616897583008\n",
      "Epochs without Improvement = 10\n",
      "Train Accuracy: 96.27%\n",
      "Validation Accuracy: 69.40%\n",
      "\n",
      "Epoch 18\n",
      "----------\n",
      "Loss for batch 0 = 0.3195140063762665\n",
      "Loss for batch 1 = 0.04164907708764076\n",
      "Loss for batch 2 = 0.12684550881385803\n",
      "Loss for batch 3 = 0.3435733914375305\n",
      "Loss for batch 4 = 0.03009077161550522\n",
      "Loss for batch 5 = 0.2165735960006714\n",
      "Loss for batch 6 = 0.27269303798675537\n",
      "Loss for batch 7 = 0.22080817818641663\n",
      "Loss for batch 8 = 0.034318018704652786\n",
      "Loss for batch 9 = 0.2158576250076294\n",
      "Loss for batch 10 = 0.024997863918542862\n",
      "Loss for batch 11 = 0.15371695160865784\n",
      "Loss for batch 12 = 0.29515254497528076\n",
      "Loss for batch 13 = 0.03943752869963646\n",
      "Loss for batch 14 = 0.10931510478258133\n",
      "Loss for batch 15 = 0.4824845790863037\n",
      "Loss for batch 16 = 0.06145259365439415\n",
      "Loss for batch 17 = 0.12070519477128983\n",
      "Loss for batch 18 = 0.031913161277770996\n",
      "Loss for batch 19 = 0.03152985870838165\n",
      "Loss for batch 20 = 0.06794625520706177\n",
      "Loss for batch 21 = 0.05271356180310249\n",
      "Loss for batch 22 = 0.1256110668182373\n",
      "Loss for batch 23 = 0.1298493593931198\n",
      "Loss for batch 24 = 0.1356656551361084\n",
      "Loss for batch 25 = 0.04738252982497215\n",
      "Loss for batch 26 = 0.028137482702732086\n",
      "Loss for batch 27 = 0.2514231204986572\n",
      "Loss for batch 28 = 0.031727809458971024\n",
      "Loss for batch 29 = 0.02969830483198166\n",
      "Loss for batch 30 = 0.032756250351667404\n",
      "Loss for batch 31 = 0.5224254131317139\n",
      "Loss for batch 32 = 0.0583832673728466\n",
      "Loss for batch 33 = 0.027020292356610298\n",
      "Loss for batch 34 = 0.03868960216641426\n",
      "Loss for batch 35 = 0.04703954607248306\n",
      "Loss for batch 36 = 0.12936647236347198\n",
      "Loss for batch 37 = 0.037355951964855194\n",
      "Loss for batch 38 = 0.26743507385253906\n",
      "Loss for batch 39 = 0.04361087456345558\n",
      "Loss for batch 40 = 0.033681388944387436\n",
      "Loss for batch 41 = 0.08863569796085358\n",
      "Loss for batch 42 = 0.14098918437957764\n",
      "Loss for batch 43 = 0.3429843485355377\n",
      "Loss for batch 44 = 0.19056415557861328\n",
      "Loss for batch 45 = 0.15266510844230652\n",
      "Loss for batch 46 = 0.11542234569787979\n",
      "Loss for batch 47 = 0.07163210958242416\n",
      "Loss for batch 48 = 0.022380242124199867\n",
      "Loss for batch 49 = 0.35198643803596497\n",
      "Loss for batch 50 = 0.029899751767516136\n",
      "Loss for batch 51 = 0.08452243357896805\n",
      "Loss for batch 52 = 0.020243506878614426\n",
      "Loss for batch 53 = 0.22486242651939392\n",
      "Loss for batch 54 = 0.4053812026977539\n",
      "Loss for batch 55 = 0.1709749400615692\n",
      "Loss for batch 56 = 0.037739187479019165\n",
      "Loss for batch 57 = 0.09836891293525696\n",
      "Loss for batch 58 = 0.024169933050870895\n",
      "Loss for batch 59 = 0.07772872596979141\n",
      "Loss for batch 60 = 0.21685554087162018\n",
      "Loss for batch 61 = 0.09165577590465546\n",
      "Loss for batch 62 = 0.12573695182800293\n",
      "Loss for batch 63 = 0.025545166805386543\n",
      "Loss for batch 64 = 0.19008152186870575\n",
      "Loss for batch 65 = 0.14623257517814636\n",
      "Loss for batch 66 = 0.1814182549715042\n",
      "Loss for batch 67 = 0.036686357110738754\n",
      "Loss for batch 68 = 0.05011172220110893\n",
      "Loss for batch 69 = 0.13748696446418762\n",
      "Loss for batch 70 = 0.16901026666164398\n",
      "Loss for batch 71 = 0.029301051050424576\n",
      "Loss for batch 72 = 0.04616393893957138\n",
      "Loss for batch 73 = 0.28284937143325806\n",
      "Loss for batch 74 = 0.04119843244552612\n",
      "Loss for batch 75 = 0.3356836438179016\n",
      "Loss for batch 76 = 0.026518482714891434\n",
      "Loss for batch 77 = 0.03924339637160301\n",
      "Loss for batch 78 = 0.0711660236120224\n",
      "Loss for batch 79 = 0.05329006537795067\n",
      "Loss for batch 80 = 0.035909269005060196\n",
      "Loss for batch 81 = 0.03240836039185524\n",
      "Loss for batch 82 = 0.17860718071460724\n",
      "Loss for batch 83 = 0.18393616378307343\n",
      "Loss for batch 84 = 0.06475983560085297\n",
      "Loss for batch 85 = 0.09217263013124466\n",
      "Loss for batch 86 = 0.04671748727560043\n",
      "Loss for batch 87 = 0.08289681375026703\n",
      "Loss for batch 88 = 0.12198925018310547\n",
      "Loss for batch 89 = 0.29563358426094055\n",
      "Loss for batch 90 = 0.1386023461818695\n",
      "Loss for batch 91 = 0.06176798418164253\n",
      "Loss for batch 92 = 0.22221031785011292\n",
      "Loss for batch 93 = 0.1896580010652542\n",
      "Loss for batch 94 = 0.14519913494586945\n",
      "Loss for batch 95 = 0.03283782675862312\n",
      "Loss for batch 96 = 0.08282283693552017\n",
      "Loss for batch 97 = 0.05712936073541641\n",
      "Loss for batch 98 = 0.2754543423652649\n",
      "Loss for batch 99 = 0.021517470479011536\n",
      "Loss for batch 100 = 0.02977512776851654\n",
      "Loss for batch 101 = 0.09311399608850479\n",
      "Loss for batch 102 = 0.024848245084285736\n",
      "Loss for batch 103 = 0.4853680729866028\n",
      "Loss for batch 104 = 0.09527730941772461\n",
      "Loss for batch 105 = 0.49079808592796326\n",
      "Loss for batch 106 = 0.04320109263062477\n",
      "\n",
      "Training Loss for epoch 18 = 13.882548332214355\n",
      "\n",
      "Current Validation Loss = 19.58883285522461\n",
      "Best Validation Loss = 13.186616897583008\n",
      "Epochs without Improvement = 11\n",
      "Train Accuracy: 96.16%\n",
      "Validation Accuracy: 70.23%\n",
      "\n",
      "Epoch 19\n",
      "----------\n",
      "Loss for batch 0 = 0.18037542700767517\n",
      "Loss for batch 1 = 0.030232924968004227\n",
      "Loss for batch 2 = 0.13125449419021606\n",
      "Loss for batch 3 = 0.18570011854171753\n",
      "Loss for batch 4 = 0.023465365171432495\n",
      "Loss for batch 5 = 0.19594784080982208\n",
      "Loss for batch 6 = 0.19696393609046936\n",
      "Loss for batch 7 = 0.1810527741909027\n",
      "Loss for batch 8 = 0.02389129437506199\n",
      "Loss for batch 9 = 0.16098453104496002\n",
      "Loss for batch 10 = 0.015986790880560875\n",
      "Loss for batch 11 = 0.14858487248420715\n",
      "Loss for batch 12 = 0.29182496666908264\n",
      "Loss for batch 13 = 0.03388407826423645\n",
      "Loss for batch 14 = 0.05926993489265442\n",
      "Loss for batch 15 = 0.47043532133102417\n",
      "Loss for batch 16 = 0.03578273579478264\n",
      "Loss for batch 17 = 0.11979281902313232\n",
      "Loss for batch 18 = 0.02387154847383499\n",
      "Loss for batch 19 = 0.02412557415664196\n",
      "Loss for batch 20 = 0.03562908247113228\n",
      "Loss for batch 21 = 0.04061487689614296\n",
      "Loss for batch 22 = 0.08237398415803909\n",
      "Loss for batch 23 = 0.1443924456834793\n",
      "Loss for batch 24 = 0.14422273635864258\n",
      "Loss for batch 25 = 0.030976951122283936\n",
      "Loss for batch 26 = 0.04410582780838013\n",
      "Loss for batch 27 = 0.2147126942873001\n",
      "Loss for batch 28 = 0.029150577262043953\n",
      "Loss for batch 29 = 0.021190721541643143\n",
      "Loss for batch 30 = 0.020475037395954132\n",
      "Loss for batch 31 = 0.45721596479415894\n",
      "Loss for batch 32 = 0.024766333401203156\n",
      "Loss for batch 33 = 0.017801430076360703\n",
      "Loss for batch 34 = 0.02612452767789364\n",
      "Loss for batch 35 = 0.03684350103139877\n",
      "Loss for batch 36 = 0.10665656626224518\n",
      "Loss for batch 37 = 0.01878012716770172\n",
      "Loss for batch 38 = 0.26894667744636536\n",
      "Loss for batch 39 = 0.018563173711299896\n",
      "Loss for batch 40 = 0.03476117178797722\n",
      "Loss for batch 41 = 0.015362584963440895\n",
      "Loss for batch 42 = 0.15089304745197296\n",
      "Loss for batch 43 = 0.38999274373054504\n",
      "Loss for batch 44 = 0.12947969138622284\n",
      "Loss for batch 45 = 0.1511388123035431\n",
      "Loss for batch 46 = 0.13612744212150574\n",
      "Loss for batch 47 = 0.021764855831861496\n",
      "Loss for batch 48 = 0.018919406458735466\n",
      "Loss for batch 49 = 0.2739277482032776\n",
      "Loss for batch 50 = 0.03619741275906563\n",
      "Loss for batch 51 = 0.028083909302949905\n",
      "Loss for batch 52 = 0.04571372643113136\n",
      "Loss for batch 53 = 0.16422688961029053\n",
      "Loss for batch 54 = 0.39738166332244873\n",
      "Loss for batch 55 = 0.14626123011112213\n",
      "Loss for batch 56 = 0.030368037521839142\n",
      "Loss for batch 57 = 0.041219644248485565\n",
      "Loss for batch 58 = 0.02134530246257782\n",
      "Loss for batch 59 = 0.08302632719278336\n",
      "Loss for batch 60 = 0.20669807493686676\n",
      "Loss for batch 61 = 0.08467554301023483\n",
      "Loss for batch 62 = 0.13011260330677032\n",
      "Loss for batch 63 = 0.02183854766190052\n",
      "Loss for batch 64 = 0.14844593405723572\n",
      "Loss for batch 65 = 0.1855013072490692\n",
      "Loss for batch 66 = 0.16388578712940216\n",
      "Loss for batch 67 = 0.03591293841600418\n",
      "Loss for batch 68 = 0.04494690150022507\n",
      "Loss for batch 69 = 0.1383531391620636\n",
      "Loss for batch 70 = 0.153911754488945\n",
      "Loss for batch 71 = 0.034379348158836365\n",
      "Loss for batch 72 = 0.03162006661295891\n",
      "Loss for batch 73 = 0.24916310608386993\n",
      "Loss for batch 74 = 0.04213913902640343\n",
      "Loss for batch 75 = 0.35018351674079895\n",
      "Loss for batch 76 = 0.03864394500851631\n",
      "Loss for batch 77 = 0.03840617090463638\n",
      "Loss for batch 78 = 0.05986647680401802\n",
      "Loss for batch 79 = 0.02384144626557827\n",
      "Loss for batch 80 = 0.02886994555592537\n",
      "Loss for batch 81 = 0.0289336945861578\n",
      "Loss for batch 82 = 0.20304429531097412\n",
      "Loss for batch 83 = 0.165562704205513\n",
      "Loss for batch 84 = 0.02447519078850746\n",
      "Loss for batch 85 = 0.04652218893170357\n",
      "Loss for batch 86 = 0.0453290119767189\n",
      "Loss for batch 87 = 0.04991624504327774\n",
      "Loss for batch 88 = 0.11549098789691925\n",
      "Loss for batch 89 = 0.20444798469543457\n",
      "Loss for batch 90 = 0.13507574796676636\n",
      "Loss for batch 91 = 0.04832916334271431\n",
      "Loss for batch 92 = 0.1704673320055008\n",
      "Loss for batch 93 = 0.06701329350471497\n",
      "Loss for batch 94 = 0.14621716737747192\n",
      "Loss for batch 95 = 0.023381315171718597\n",
      "Loss for batch 96 = 0.019109651446342468\n",
      "Loss for batch 97 = 0.04669056460261345\n",
      "Loss for batch 98 = 0.22725366055965424\n",
      "Loss for batch 99 = 0.013312586583197117\n",
      "Loss for batch 100 = 0.028299234807491302\n",
      "Loss for batch 101 = 0.02518574893474579\n",
      "Loss for batch 102 = 0.025330783799290657\n",
      "Loss for batch 103 = 0.4998375177383423\n",
      "Loss for batch 104 = 0.08573033660650253\n",
      "Loss for batch 105 = 0.4233175814151764\n",
      "Loss for batch 106 = 0.047737039625644684\n",
      "\n",
      "Training Loss for epoch 19 = 11.760570526123047\n",
      "\n",
      "Current Validation Loss = 20.33244514465332\n",
      "Best Validation Loss = 13.186616897583008\n",
      "Epochs without Improvement = 12\n",
      "Train Accuracy: 97.18%\n",
      "Validation Accuracy: 70.64%\n",
      "\n",
      "Epoch 20\n",
      "----------\n",
      "Loss for batch 0 = 0.1827383041381836\n",
      "Loss for batch 1 = 0.02154509536921978\n",
      "Loss for batch 2 = 0.12915772199630737\n",
      "Loss for batch 3 = 0.13832858204841614\n",
      "Loss for batch 4 = 0.016155017539858818\n",
      "Loss for batch 5 = 0.20841112732887268\n",
      "Loss for batch 6 = 0.17740443348884583\n",
      "Loss for batch 7 = 0.2047065943479538\n",
      "Loss for batch 8 = 0.07759128510951996\n",
      "Loss for batch 9 = 0.14597278833389282\n",
      "Loss for batch 10 = 0.013205434195697308\n",
      "Loss for batch 11 = 0.11813486367464066\n",
      "Loss for batch 12 = 0.2553400993347168\n",
      "Loss for batch 13 = 0.021641148254275322\n",
      "Loss for batch 14 = 0.028734462335705757\n",
      "Loss for batch 15 = 0.3621242940425873\n",
      "Loss for batch 16 = 0.02143801376223564\n",
      "Loss for batch 17 = 0.11159548908472061\n",
      "Loss for batch 18 = 0.01578902266919613\n",
      "Loss for batch 19 = 0.019034722819924355\n",
      "Loss for batch 20 = 0.022096853703260422\n",
      "Loss for batch 21 = 0.032655272632837296\n",
      "Loss for batch 22 = 0.07355281710624695\n",
      "Loss for batch 23 = 0.1368340253829956\n",
      "Loss for batch 24 = 0.13871349394321442\n",
      "Loss for batch 25 = 0.021181773394346237\n",
      "Loss for batch 26 = 0.017297759652137756\n",
      "Loss for batch 27 = 0.21024097502231598\n",
      "Loss for batch 28 = 0.02260443940758705\n",
      "Loss for batch 29 = 0.01724032312631607\n",
      "Loss for batch 30 = 0.022594735026359558\n",
      "Loss for batch 31 = 0.4144182503223419\n",
      "Loss for batch 32 = 0.024581823498010635\n",
      "Loss for batch 33 = 0.020114459097385406\n",
      "Loss for batch 34 = 0.02732258103787899\n",
      "Loss for batch 35 = 0.04214063286781311\n",
      "Loss for batch 36 = 0.02352437563240528\n",
      "Loss for batch 37 = 0.021739348769187927\n",
      "Loss for batch 38 = 0.24477535486221313\n",
      "Loss for batch 39 = 0.01643720082938671\n",
      "Loss for batch 40 = 0.03704339265823364\n",
      "Loss for batch 41 = 0.010930211283266544\n",
      "Loss for batch 42 = 0.13689038157463074\n",
      "Loss for batch 43 = 0.33615389466285706\n",
      "Loss for batch 44 = 0.024410665035247803\n",
      "Loss for batch 45 = 0.06881964951753616\n",
      "Loss for batch 46 = 0.056666839867830276\n",
      "Loss for batch 47 = 0.018131205812096596\n",
      "Loss for batch 48 = 0.01839744672179222\n",
      "Loss for batch 49 = 0.2798102796077728\n",
      "Loss for batch 50 = 0.02697543241083622\n",
      "Loss for batch 51 = 0.01835544779896736\n",
      "Loss for batch 52 = 0.04767906293272972\n",
      "Loss for batch 53 = 0.14276769757270813\n",
      "Loss for batch 54 = 0.38402432203292847\n",
      "Loss for batch 55 = 0.15977957844734192\n",
      "Loss for batch 56 = 0.031621772795915604\n",
      "Loss for batch 57 = 0.036759153008461\n",
      "Loss for batch 58 = 0.014893653802573681\n",
      "Loss for batch 59 = 0.127962127327919\n",
      "Loss for batch 60 = 0.197053924202919\n",
      "Loss for batch 61 = 0.0815858393907547\n",
      "Loss for batch 62 = 0.13099971413612366\n",
      "Loss for batch 63 = 0.01902095228433609\n",
      "Loss for batch 64 = 0.1415916383266449\n",
      "Loss for batch 65 = 0.08885554224252701\n",
      "Loss for batch 66 = 0.051655326038599014\n",
      "Loss for batch 67 = 0.020190268754959106\n",
      "Loss for batch 68 = 0.020920004695653915\n",
      "Loss for batch 69 = 0.138943612575531\n",
      "Loss for batch 70 = 0.1930011808872223\n",
      "Loss for batch 71 = 0.0160170067101717\n",
      "Loss for batch 72 = 0.07889117300510406\n",
      "Loss for batch 73 = 0.2465886026620865\n",
      "Loss for batch 74 = 0.021572336554527283\n",
      "Loss for batch 75 = 0.33904555439949036\n",
      "Loss for batch 76 = 0.01860738918185234\n",
      "Loss for batch 77 = 0.021259043365716934\n",
      "Loss for batch 78 = 0.018191656097769737\n",
      "Loss for batch 79 = 0.024603305384516716\n",
      "Loss for batch 80 = 0.01741366647183895\n",
      "Loss for batch 81 = 0.019338302314281464\n",
      "Loss for batch 82 = 0.16332878172397614\n",
      "Loss for batch 83 = 0.1419239342212677\n",
      "Loss for batch 84 = 0.021811073645949364\n",
      "Loss for batch 85 = 0.044578563421964645\n",
      "Loss for batch 86 = 0.04143204540014267\n",
      "Loss for batch 87 = 0.029252776876091957\n",
      "Loss for batch 88 = 0.09741543978452682\n",
      "Loss for batch 89 = 0.2285083830356598\n",
      "Loss for batch 90 = 0.11779306083917618\n",
      "Loss for batch 91 = 0.03383966535329819\n",
      "Loss for batch 92 = 0.1059303879737854\n",
      "Loss for batch 93 = 0.021596025675535202\n",
      "Loss for batch 94 = 0.06131526455283165\n",
      "Loss for batch 95 = 0.028637848794460297\n",
      "Loss for batch 96 = 0.14235129952430725\n",
      "Loss for batch 97 = 0.08059680461883545\n",
      "Loss for batch 98 = 0.22147928178310394\n",
      "Loss for batch 99 = 0.01290737371891737\n",
      "Loss for batch 100 = 0.014302403666079044\n",
      "Loss for batch 101 = 0.015425989404320717\n",
      "Loss for batch 102 = 0.030141469091176987\n",
      "Loss for batch 103 = 0.48744916915893555\n",
      "Loss for batch 104 = 0.09312661737203598\n",
      "Loss for batch 105 = 0.31800132989883423\n",
      "Loss for batch 106 = 0.04852912947535515\n",
      "\n",
      "Training Loss for epoch 20 = 10.304205894470215\n",
      "\n",
      "Current Validation Loss = 21.654569625854492\n",
      "Best Validation Loss = 13.186616897583008\n",
      "Epochs without Improvement = 13\n",
      "Train Accuracy: 96.89%\n",
      "Validation Accuracy: 68.79%\n",
      "\n",
      "Epoch 21\n",
      "----------\n",
      "Loss for batch 0 = 0.12668722867965698\n",
      "Loss for batch 1 = 0.04548880085349083\n",
      "Loss for batch 2 = 0.134837344288826\n",
      "Loss for batch 3 = 0.10046800225973129\n",
      "Loss for batch 4 = 0.022238709032535553\n",
      "Loss for batch 5 = 0.20267359912395477\n",
      "Loss for batch 6 = 0.1288403421640396\n",
      "Loss for batch 7 = 0.17828170955181122\n",
      "Loss for batch 8 = 0.012700500898063183\n",
      "Loss for batch 9 = 0.16194304823875427\n",
      "Loss for batch 10 = 0.010651260614395142\n",
      "Loss for batch 11 = 0.1355995386838913\n",
      "Loss for batch 12 = 0.26408523321151733\n",
      "Loss for batch 13 = 0.02288425713777542\n",
      "Loss for batch 14 = 0.03004549816250801\n",
      "Loss for batch 15 = 0.3321305215358734\n",
      "Loss for batch 16 = 0.034883927553892136\n",
      "Loss for batch 17 = 0.10811767727136612\n",
      "Loss for batch 18 = 0.016873668879270554\n",
      "Loss for batch 19 = 0.017089910805225372\n",
      "Loss for batch 20 = 0.023312486708164215\n",
      "Loss for batch 21 = 0.03013216331601143\n",
      "Loss for batch 22 = 0.07683108001947403\n",
      "Loss for batch 23 = 0.12703253328800201\n",
      "Loss for batch 24 = 0.13747788965702057\n",
      "Loss for batch 25 = 0.022595185786485672\n",
      "Loss for batch 26 = 0.018924489617347717\n",
      "Loss for batch 27 = 0.22018194198608398\n",
      "Loss for batch 28 = 0.02033560536801815\n",
      "Loss for batch 29 = 0.01750190556049347\n",
      "Loss for batch 30 = 0.013434736989438534\n",
      "Loss for batch 31 = 0.4027934670448303\n",
      "Loss for batch 32 = 0.01747322827577591\n",
      "Loss for batch 33 = 0.0129547119140625\n",
      "Loss for batch 34 = 0.02342863567173481\n",
      "Loss for batch 35 = 0.036189716309309006\n",
      "Loss for batch 36 = 0.02621425874531269\n",
      "Loss for batch 37 = 0.014541173353791237\n",
      "Loss for batch 38 = 0.24588215351104736\n",
      "Loss for batch 39 = 0.014582897536456585\n",
      "Loss for batch 40 = 0.015750067308545113\n",
      "Loss for batch 41 = 0.010072476230561733\n",
      "Loss for batch 42 = 0.11018068343400955\n",
      "Loss for batch 43 = 0.31388017535209656\n",
      "Loss for batch 44 = 0.025608140975236893\n",
      "Loss for batch 45 = 0.1423037350177765\n",
      "Loss for batch 46 = 0.025832539424300194\n",
      "Loss for batch 47 = 0.011764057911932468\n",
      "Loss for batch 48 = 0.02725222334265709\n",
      "Loss for batch 49 = 0.2851106524467468\n",
      "Loss for batch 50 = 0.019388066604733467\n",
      "Loss for batch 51 = 0.1084994226694107\n",
      "Loss for batch 52 = 0.007908616214990616\n",
      "Loss for batch 53 = 0.2669042944908142\n",
      "Loss for batch 54 = 0.37036794424057007\n",
      "Loss for batch 55 = 0.1539735198020935\n",
      "Loss for batch 56 = 0.03206321597099304\n",
      "Loss for batch 57 = 0.03259286284446716\n",
      "Loss for batch 58 = 0.020088549703359604\n",
      "Loss for batch 59 = 0.10094362497329712\n",
      "Loss for batch 60 = 0.19701606035232544\n",
      "Loss for batch 61 = 0.10712730884552002\n",
      "Loss for batch 62 = 0.13568329811096191\n",
      "Loss for batch 63 = 0.037835221737623215\n",
      "Loss for batch 64 = 0.14694158732891083\n",
      "Loss for batch 65 = 0.05793106555938721\n",
      "Loss for batch 66 = 0.14048178493976593\n",
      "Loss for batch 67 = 0.04276483505964279\n",
      "Loss for batch 68 = 0.02207774668931961\n",
      "Loss for batch 69 = 0.12033399194478989\n",
      "Loss for batch 70 = 0.13641424477100372\n",
      "Loss for batch 71 = 0.01647859439253807\n",
      "Loss for batch 72 = 0.09070684760808945\n",
      "Loss for batch 73 = 0.25548481941223145\n",
      "Loss for batch 74 = 0.022998448461294174\n",
      "Loss for batch 75 = 0.30919358134269714\n",
      "Loss for batch 76 = 0.021716607734560966\n",
      "Loss for batch 77 = 0.029000528156757355\n",
      "Loss for batch 78 = 0.0192700307816267\n",
      "Loss for batch 79 = 0.017519628629088402\n",
      "Loss for batch 80 = 0.015887727960944176\n",
      "Loss for batch 81 = 0.020507104694843292\n",
      "Loss for batch 82 = 0.1331416517496109\n",
      "Loss for batch 83 = 0.13601216673851013\n",
      "Loss for batch 84 = 0.01749776303768158\n",
      "Loss for batch 85 = 0.03434421122074127\n",
      "Loss for batch 86 = 0.02305225469172001\n",
      "Loss for batch 87 = 0.037222497165203094\n",
      "Loss for batch 88 = 0.07444640249013901\n",
      "Loss for batch 89 = 0.1851494461297989\n",
      "Loss for batch 90 = 0.12066758424043655\n",
      "Loss for batch 91 = 0.02893681265413761\n",
      "Loss for batch 92 = 0.10112719237804413\n",
      "Loss for batch 93 = 0.02650810033082962\n",
      "Loss for batch 94 = 0.11778206378221512\n",
      "Loss for batch 95 = 0.014165817759931087\n",
      "Loss for batch 96 = 0.048576854169368744\n",
      "Loss for batch 97 = 0.05283161997795105\n",
      "Loss for batch 98 = 0.22498959302902222\n",
      "Loss for batch 99 = 0.017183810472488403\n",
      "Loss for batch 100 = 0.016812045127153397\n",
      "Loss for batch 101 = 0.024721510708332062\n",
      "Loss for batch 102 = 0.01430176105350256\n",
      "Loss for batch 103 = 0.4679332971572876\n",
      "Loss for batch 104 = 0.023347152397036552\n",
      "Loss for batch 105 = 0.2732490003108978\n",
      "Loss for batch 106 = 0.033230528235435486\n",
      "\n",
      "Training Loss for epoch 21 = 9.87942886352539\n",
      "\n",
      "Current Validation Loss = 22.92034912109375\n",
      "Best Validation Loss = 13.186616897583008\n",
      "Epochs without Improvement = 14\n",
      "Train Accuracy: 97.27%\n",
      "Validation Accuracy: 69.40%\n",
      "\n",
      "Epoch 22\n",
      "----------\n",
      "Loss for batch 0 = 0.13787098228931427\n",
      "Loss for batch 1 = 0.014505747705698013\n",
      "Loss for batch 2 = 0.13859327137470245\n",
      "Loss for batch 3 = 0.038174234330654144\n",
      "Loss for batch 4 = 0.016126755625009537\n",
      "Loss for batch 5 = 0.2079915851354599\n",
      "Loss for batch 6 = 0.2052643895149231\n",
      "Loss for batch 7 = 0.15413451194763184\n",
      "Loss for batch 8 = 0.01468324288725853\n",
      "Loss for batch 9 = 0.1377926468849182\n",
      "Loss for batch 10 = 0.009817310608923435\n",
      "Loss for batch 11 = 0.21542322635650635\n",
      "Loss for batch 12 = 0.2646358013153076\n",
      "Loss for batch 13 = 0.04150186851620674\n",
      "Loss for batch 14 = 0.029528675600886345\n",
      "Loss for batch 15 = 0.3767872750759125\n",
      "Loss for batch 16 = 0.014245745725929737\n",
      "Loss for batch 17 = 0.10272325575351715\n",
      "Loss for batch 18 = 0.014739345759153366\n",
      "Loss for batch 19 = 0.018454821780323982\n",
      "Loss for batch 20 = 0.022601639851927757\n",
      "Loss for batch 21 = 0.014440677128732204\n",
      "Loss for batch 22 = 0.05413360148668289\n",
      "Loss for batch 23 = 0.13096515834331512\n",
      "Loss for batch 24 = 0.1411377638578415\n",
      "Loss for batch 25 = 0.01418166235089302\n",
      "Loss for batch 26 = 0.05134895443916321\n",
      "Loss for batch 27 = 0.2798704504966736\n",
      "Loss for batch 28 = 0.01789897307753563\n",
      "Loss for batch 29 = 0.0236260537058115\n",
      "Loss for batch 30 = 0.015927007421851158\n",
      "Loss for batch 31 = 0.4136646091938019\n",
      "Loss for batch 32 = 0.016317836940288544\n",
      "Loss for batch 33 = 0.013529762625694275\n",
      "Loss for batch 34 = 0.026822088286280632\n",
      "Loss for batch 35 = 0.05613777041435242\n",
      "Loss for batch 36 = 0.03526437655091286\n",
      "Loss for batch 37 = 0.027317076921463013\n",
      "Loss for batch 38 = 0.26266250014305115\n",
      "Loss for batch 39 = 0.012025224044919014\n",
      "Loss for batch 40 = 0.01449513342231512\n",
      "Loss for batch 41 = 0.010757742449641228\n",
      "Loss for batch 42 = 0.016466649249196053\n",
      "Loss for batch 43 = 0.22306448221206665\n",
      "Loss for batch 44 = 0.028597427532076836\n",
      "Loss for batch 45 = 0.040552522987127304\n",
      "Loss for batch 46 = 0.030257033184170723\n",
      "Loss for batch 47 = 0.011808790266513824\n",
      "Loss for batch 48 = 0.01564956270158291\n",
      "Loss for batch 49 = 0.2998006045818329\n",
      "Loss for batch 50 = 0.03319728374481201\n",
      "Loss for batch 51 = 0.011375049129128456\n",
      "Loss for batch 52 = 0.020382581278681755\n",
      "Loss for batch 53 = 0.2817654311656952\n",
      "Loss for batch 54 = 0.313648521900177\n",
      "Loss for batch 55 = 0.16814561188220978\n",
      "Loss for batch 56 = 0.019302885979413986\n",
      "Loss for batch 57 = 0.041910670697689056\n",
      "Loss for batch 58 = 0.02076169103384018\n",
      "Loss for batch 59 = 0.018909677863121033\n",
      "Loss for batch 60 = 0.19499561190605164\n",
      "Loss for batch 61 = 0.01552339643239975\n",
      "Loss for batch 62 = 0.11623463779687881\n",
      "Loss for batch 63 = 0.027743270620703697\n",
      "Loss for batch 64 = 0.18705809116363525\n",
      "Loss for batch 65 = 0.09669744223356247\n",
      "Loss for batch 66 = 0.030440308153629303\n",
      "Loss for batch 67 = 0.01783701218664646\n",
      "Loss for batch 68 = 0.02289566770195961\n",
      "Loss for batch 69 = 0.11410615593194962\n",
      "Loss for batch 70 = 0.11721928417682648\n",
      "Loss for batch 71 = 0.013585781678557396\n",
      "Loss for batch 72 = 0.013035283423960209\n",
      "Loss for batch 73 = 0.3100191354751587\n",
      "Loss for batch 74 = 0.034246060997247696\n",
      "Loss for batch 75 = 0.2989748418331146\n",
      "Loss for batch 76 = 0.01661020889878273\n",
      "Loss for batch 77 = 0.015580125153064728\n",
      "Loss for batch 78 = 0.01379153411835432\n",
      "Loss for batch 79 = 0.013374662026762962\n",
      "Loss for batch 80 = 0.01926434226334095\n",
      "Loss for batch 81 = 0.01819675974547863\n",
      "Loss for batch 82 = 0.1056690514087677\n",
      "Loss for batch 83 = 0.1371016651391983\n",
      "Loss for batch 84 = 0.016410725191235542\n",
      "Loss for batch 85 = 0.06791261583566666\n",
      "Loss for batch 86 = 0.029395494610071182\n",
      "Loss for batch 87 = 0.027228035032749176\n",
      "Loss for batch 88 = 0.06917732208967209\n",
      "Loss for batch 89 = 0.19493260979652405\n",
      "Loss for batch 90 = 0.1306561380624771\n",
      "Loss for batch 91 = 0.022006209939718246\n",
      "Loss for batch 92 = 0.04243579879403114\n",
      "Loss for batch 93 = 0.018802786245942116\n",
      "Loss for batch 94 = 0.017287081107497215\n",
      "Loss for batch 95 = 0.03901325538754463\n",
      "Loss for batch 96 = 0.014223030768334866\n",
      "Loss for batch 97 = 0.011641844175755978\n",
      "Loss for batch 98 = 0.23908650875091553\n",
      "Loss for batch 99 = 0.009166269563138485\n",
      "Loss for batch 100 = 0.017191970720887184\n",
      "Loss for batch 101 = 0.021669674664735794\n",
      "Loss for batch 102 = 0.0125505356118083\n",
      "Loss for batch 103 = 0.43382543325424194\n",
      "Loss for batch 104 = 0.06164461374282837\n",
      "Loss for batch 105 = 0.26991045475006104\n",
      "Loss for batch 106 = 0.11448298394680023\n",
      "\n",
      "Training Loss for epoch 22 = 9.234567642211914\n",
      "\n",
      "Current Validation Loss = 22.693626403808594\n",
      "Best Validation Loss = 13.186616897583008\n",
      "Epochs without Improvement = 15\n",
      "Train Accuracy: 96.95%\n",
      "Validation Accuracy: 69.20%\n",
      "\n",
      "Epoch 23\n",
      "----------\n",
      "Loss for batch 0 = 0.12536947429180145\n",
      "Loss for batch 1 = 0.015231250785291195\n",
      "Loss for batch 2 = 0.127777099609375\n",
      "Loss for batch 3 = 0.028020724654197693\n",
      "Loss for batch 4 = 0.026199625805020332\n",
      "Loss for batch 5 = 0.2045164555311203\n",
      "Loss for batch 6 = 0.12513574957847595\n",
      "Loss for batch 7 = 0.19449281692504883\n",
      "Loss for batch 8 = 0.011733336374163628\n",
      "Loss for batch 9 = 0.13513104617595673\n",
      "Loss for batch 10 = 0.010500101372599602\n",
      "Loss for batch 11 = 0.10447726398706436\n",
      "Loss for batch 12 = 0.26391223073005676\n",
      "Loss for batch 13 = 0.013376307673752308\n",
      "Loss for batch 14 = 0.03223637118935585\n",
      "Loss for batch 15 = 0.37404385209083557\n",
      "Loss for batch 16 = 0.021396074444055557\n",
      "Loss for batch 17 = 0.05480905994772911\n",
      "Loss for batch 18 = 0.014227922074496746\n",
      "Loss for batch 19 = 0.021985763683915138\n",
      "Loss for batch 20 = 0.022936232388019562\n",
      "Loss for batch 21 = 0.01620512083172798\n",
      "Loss for batch 22 = 0.0728059709072113\n",
      "Loss for batch 23 = 0.12918362021446228\n",
      "Loss for batch 24 = 0.14365404844284058\n",
      "Loss for batch 25 = 0.012969713658094406\n",
      "Loss for batch 26 = 0.01285245269536972\n",
      "Loss for batch 27 = 0.2139328420162201\n",
      "Loss for batch 28 = 0.012694944627583027\n",
      "Loss for batch 29 = 0.018763067200779915\n",
      "Loss for batch 30 = 0.01977221854031086\n",
      "Loss for batch 31 = 0.35628724098205566\n",
      "Loss for batch 32 = 0.013434523716568947\n",
      "Loss for batch 33 = 0.018689660355448723\n",
      "Loss for batch 34 = 0.020240947604179382\n",
      "Loss for batch 35 = 0.05025998502969742\n",
      "Loss for batch 36 = 0.014243684709072113\n",
      "Loss for batch 37 = 0.011903555132448673\n",
      "Loss for batch 38 = 0.2658482491970062\n",
      "Loss for batch 39 = 0.014086958952248096\n",
      "Loss for batch 40 = 0.013881590217351913\n",
      "Loss for batch 41 = 0.010524256154894829\n",
      "Loss for batch 42 = 0.03208571672439575\n",
      "Loss for batch 43 = 0.29677096009254456\n",
      "Loss for batch 44 = 0.019815940409898758\n",
      "Loss for batch 45 = 0.08789961785078049\n",
      "Loss for batch 46 = 0.04070473462343216\n",
      "Loss for batch 47 = 0.021458817645907402\n",
      "Loss for batch 48 = 0.0350957065820694\n",
      "Loss for batch 49 = 0.28026479482650757\n",
      "Loss for batch 50 = 0.013236777856945992\n",
      "Loss for batch 51 = 0.011926824226975441\n",
      "Loss for batch 52 = 0.010378691367805004\n",
      "Loss for batch 53 = 0.1626216173171997\n",
      "Loss for batch 54 = 0.46320873498916626\n",
      "Loss for batch 55 = 0.1567273736000061\n",
      "Loss for batch 56 = 0.014131101779639721\n",
      "Loss for batch 57 = 0.08352668583393097\n",
      "Loss for batch 58 = 0.017594225704669952\n",
      "Loss for batch 59 = 0.12165482342243195\n",
      "Loss for batch 60 = 0.18226473033428192\n",
      "Loss for batch 61 = 0.02155040204524994\n",
      "Loss for batch 62 = 0.10861410945653915\n",
      "Loss for batch 63 = 0.018174292519688606\n",
      "Loss for batch 64 = 0.17820774018764496\n",
      "Loss for batch 65 = 0.14409740269184113\n",
      "Loss for batch 66 = 0.01655270904302597\n",
      "Loss for batch 67 = 0.0194964949041605\n",
      "Loss for batch 68 = 0.01232751552015543\n",
      "Loss for batch 69 = 0.09771329909563065\n",
      "Loss for batch 70 = 0.16190126538276672\n",
      "Loss for batch 71 = 0.026508033275604248\n",
      "Loss for batch 72 = 0.020133215934038162\n",
      "Loss for batch 73 = 0.23508954048156738\n",
      "Loss for batch 74 = 0.06358600407838821\n",
      "Loss for batch 75 = 0.29300758242607117\n",
      "Loss for batch 76 = 0.021975554525852203\n",
      "Loss for batch 77 = 0.016259383410215378\n",
      "Loss for batch 78 = 0.01964053511619568\n",
      "Loss for batch 79 = 0.015760967507958412\n",
      "Loss for batch 80 = 0.012912912294268608\n",
      "Loss for batch 81 = 0.011955897323787212\n",
      "Loss for batch 82 = 0.33610039949417114\n",
      "Loss for batch 83 = 0.13634924590587616\n",
      "Loss for batch 84 = 0.014322330243885517\n",
      "Loss for batch 85 = 0.1317927986383438\n",
      "Loss for batch 86 = 0.01982727088034153\n",
      "Loss for batch 87 = 0.02242252416908741\n",
      "Loss for batch 88 = 0.0697454959154129\n",
      "Loss for batch 89 = 0.196326345205307\n",
      "Loss for batch 90 = 0.12113449722528458\n",
      "Loss for batch 91 = 0.022244522348046303\n",
      "Loss for batch 92 = 0.07906142622232437\n",
      "Loss for batch 93 = 0.035350438207387924\n",
      "Loss for batch 94 = 0.03763185441493988\n",
      "Loss for batch 95 = 0.0169928427785635\n",
      "Loss for batch 96 = 0.0336490273475647\n",
      "Loss for batch 97 = 0.014489702880382538\n",
      "Loss for batch 98 = 0.23646269738674164\n",
      "Loss for batch 99 = 0.009025167673826218\n",
      "Loss for batch 100 = 0.01284007541835308\n",
      "Loss for batch 101 = 0.014243131503462791\n",
      "Loss for batch 102 = 0.013796012848615646\n",
      "Loss for batch 103 = 0.44506514072418213\n",
      "Loss for batch 104 = 0.012496519833803177\n",
      "Loss for batch 105 = 0.29522040486335754\n",
      "Loss for batch 106 = 0.021916862577199936\n",
      "\n",
      "Training Loss for epoch 23 = 9.285083770751953\n",
      "\n",
      "Current Validation Loss = 21.00448226928711\n",
      "Best Validation Loss = 13.186616897583008\n",
      "Epochs without Improvement = 16\n",
      "Train Accuracy: 97.54%\n",
      "Validation Accuracy: 70.43%\n",
      "\n",
      "Epoch 24\n",
      "----------\n",
      "Loss for batch 0 = 0.13971304893493652\n",
      "Loss for batch 1 = 0.01840829849243164\n",
      "Loss for batch 2 = 0.12643937766551971\n",
      "Loss for batch 3 = 0.04423687979578972\n",
      "Loss for batch 4 = 0.014024229720234871\n",
      "Loss for batch 5 = 0.2070528268814087\n",
      "Loss for batch 6 = 0.11142634600400925\n",
      "Loss for batch 7 = 0.16500656306743622\n",
      "Loss for batch 8 = 0.011559358797967434\n",
      "Loss for batch 9 = 0.17101235687732697\n",
      "Loss for batch 10 = 0.01001695729792118\n",
      "Loss for batch 11 = 0.06677071005105972\n",
      "Loss for batch 12 = 0.2544158399105072\n",
      "Loss for batch 13 = 0.016623064875602722\n",
      "Loss for batch 14 = 0.049181245267391205\n",
      "Loss for batch 15 = 0.35765716433525085\n",
      "Loss for batch 16 = 0.016945788636803627\n",
      "Loss for batch 17 = 0.0806894451379776\n",
      "Loss for batch 18 = 0.016012150794267654\n",
      "Loss for batch 19 = 0.01835612766444683\n",
      "Loss for batch 20 = 0.01884501427412033\n",
      "Loss for batch 21 = 0.10600314289331436\n",
      "Loss for batch 22 = 0.07248680293560028\n",
      "Loss for batch 23 = 0.2600455582141876\n",
      "Loss for batch 24 = 0.14518232643604279\n",
      "Loss for batch 25 = 0.015108801424503326\n",
      "Loss for batch 26 = 0.009488348849117756\n",
      "Loss for batch 27 = 0.2123987376689911\n",
      "Loss for batch 28 = 0.015049811452627182\n",
      "Loss for batch 29 = 0.01192641444504261\n",
      "Loss for batch 30 = 0.016700051724910736\n",
      "Loss for batch 31 = 0.36336037516593933\n",
      "Loss for batch 32 = 0.020230572670698166\n",
      "Loss for batch 33 = 0.012888372875750065\n",
      "Loss for batch 34 = 0.022848976776003838\n",
      "Loss for batch 35 = 0.029814112931489944\n",
      "Loss for batch 36 = 0.017932672053575516\n",
      "Loss for batch 37 = 0.011384217999875546\n",
      "Loss for batch 38 = 0.22866985201835632\n",
      "Loss for batch 39 = 0.012992789968848228\n",
      "Loss for batch 40 = 0.015979692339897156\n",
      "Loss for batch 41 = 0.011025757528841496\n",
      "Loss for batch 42 = 0.013379187323153019\n",
      "Loss for batch 43 = 0.2201894074678421\n",
      "Loss for batch 44 = 0.03568673133850098\n",
      "Loss for batch 45 = 0.12458278983831406\n",
      "Loss for batch 46 = 0.02372814156115055\n",
      "Loss for batch 47 = 0.011755943298339844\n",
      "Loss for batch 48 = 0.18453079462051392\n",
      "Loss for batch 49 = 0.2864932119846344\n",
      "Loss for batch 50 = 0.01482531987130642\n",
      "Loss for batch 51 = 0.013586347922682762\n",
      "Loss for batch 52 = 0.013152322731912136\n",
      "Loss for batch 53 = 0.12377887964248657\n",
      "Loss for batch 54 = 0.36174672842025757\n",
      "Loss for batch 55 = 0.1819848120212555\n",
      "Loss for batch 56 = 0.01578650251030922\n",
      "Loss for batch 57 = 0.05498278886079788\n",
      "Loss for batch 58 = 0.01537875272333622\n",
      "Loss for batch 59 = 0.01761890947818756\n",
      "Loss for batch 60 = 0.18587961792945862\n",
      "Loss for batch 61 = 0.01733182556927204\n",
      "Loss for batch 62 = 0.04315255209803581\n",
      "Loss for batch 63 = 0.022058524191379547\n",
      "Loss for batch 64 = 0.06694956868886948\n",
      "Loss for batch 65 = 0.2440333515405655\n",
      "Loss for batch 66 = 0.029012039303779602\n",
      "Loss for batch 67 = 0.015855586156249046\n",
      "Loss for batch 68 = 0.015961438417434692\n",
      "Loss for batch 69 = 0.028909403830766678\n",
      "Loss for batch 70 = 0.0681244432926178\n",
      "Loss for batch 71 = 0.014724026434123516\n",
      "Loss for batch 72 = 0.014844633638858795\n",
      "Loss for batch 73 = 0.1709996610879898\n",
      "Loss for batch 74 = 0.0290804635733366\n",
      "Loss for batch 75 = 0.2938709855079651\n",
      "Loss for batch 76 = 0.013412684202194214\n",
      "Loss for batch 77 = 0.017601044848561287\n",
      "Loss for batch 78 = 0.04336937516927719\n",
      "Loss for batch 79 = 0.014501474797725677\n",
      "Loss for batch 80 = 0.048051588237285614\n",
      "Loss for batch 81 = 0.048839468508958817\n",
      "Loss for batch 82 = 0.14234524965286255\n",
      "Loss for batch 83 = 0.15442819893360138\n",
      "Loss for batch 84 = 0.02443682961165905\n",
      "Loss for batch 85 = 0.029156427830457687\n",
      "Loss for batch 86 = 0.019601106643676758\n",
      "Loss for batch 87 = 0.015044232830405235\n",
      "Loss for batch 88 = 0.10151351988315582\n",
      "Loss for batch 89 = 0.043343186378479004\n",
      "Loss for batch 90 = 0.13115255534648895\n",
      "Loss for batch 91 = 0.017147531732916832\n",
      "Loss for batch 92 = 0.037496257573366165\n",
      "Loss for batch 93 = 0.013706056401133537\n",
      "Loss for batch 94 = 0.014660441316664219\n",
      "Loss for batch 95 = 0.012386559508740902\n",
      "Loss for batch 96 = 0.00897461548447609\n",
      "Loss for batch 97 = 0.04443049430847168\n",
      "Loss for batch 98 = 0.23518957197666168\n",
      "Loss for batch 99 = 0.011763962917029858\n",
      "Loss for batch 100 = 0.01792428456246853\n",
      "Loss for batch 101 = 0.011368615552783012\n",
      "Loss for batch 102 = 0.010571751743555069\n",
      "Loss for batch 103 = 0.523199737071991\n",
      "Loss for batch 104 = 0.012739763595163822\n",
      "Loss for batch 105 = 0.28429079055786133\n",
      "Loss for batch 106 = 0.03511534631252289\n",
      "\n",
      "Training Loss for epoch 24 = 8.637648582458496\n",
      "\n",
      "Current Validation Loss = 21.79059410095215\n",
      "Best Validation Loss = 13.186616897583008\n",
      "Epochs without Improvement = 17\n",
      "Train Accuracy: 97.33%\n",
      "Validation Accuracy: 70.43%\n",
      "\n",
      "Epoch 25\n",
      "----------\n",
      "Loss for batch 0 = 0.1315235048532486\n",
      "Loss for batch 1 = 0.01788637600839138\n",
      "Loss for batch 2 = 0.13762016594409943\n",
      "Loss for batch 3 = 0.03371671959757805\n",
      "Loss for batch 4 = 0.01088595762848854\n",
      "Loss for batch 5 = 0.20060434937477112\n",
      "Loss for batch 6 = 0.10775841772556305\n",
      "Loss for batch 7 = 0.1415332555770874\n",
      "Loss for batch 8 = 0.011196499690413475\n",
      "Loss for batch 9 = 0.2409072369337082\n",
      "Loss for batch 10 = 0.00858027208596468\n",
      "Loss for batch 11 = 0.046631693840026855\n",
      "Loss for batch 12 = 0.24280907213687897\n",
      "Loss for batch 13 = 0.011701267212629318\n",
      "Loss for batch 14 = 0.016842134296894073\n",
      "Loss for batch 15 = 0.35267141461372375\n",
      "Loss for batch 16 = 0.012023687362670898\n",
      "Loss for batch 17 = 0.014639724045991898\n",
      "Loss for batch 18 = 0.011280127801001072\n",
      "Loss for batch 19 = 0.0121232308447361\n",
      "Loss for batch 20 = 0.019977228716015816\n",
      "Loss for batch 21 = 0.012414048425853252\n",
      "Loss for batch 22 = 0.05259852111339569\n",
      "Loss for batch 23 = 0.10246662050485611\n",
      "Loss for batch 24 = 0.1489146500825882\n",
      "Loss for batch 25 = 0.011609763838350773\n",
      "Loss for batch 26 = 0.009293247014284134\n",
      "Loss for batch 27 = 0.2130676656961441\n",
      "Loss for batch 28 = 0.011895479634404182\n",
      "Loss for batch 29 = 0.012697365134954453\n",
      "Loss for batch 30 = 0.01028120145201683\n",
      "Loss for batch 31 = 0.39511996507644653\n",
      "Loss for batch 32 = 0.018736260011792183\n",
      "Loss for batch 33 = 0.06029856950044632\n",
      "Loss for batch 34 = 0.03462022915482521\n",
      "Loss for batch 35 = 0.029865451157093048\n",
      "Loss for batch 36 = 0.016819903627038002\n",
      "Loss for batch 37 = 0.014784739352762699\n",
      "Loss for batch 38 = 0.15153411030769348\n",
      "Loss for batch 39 = 0.01297090109437704\n",
      "Loss for batch 40 = 0.01772049069404602\n",
      "Loss for batch 41 = 0.014917201362550259\n",
      "Loss for batch 42 = 0.01891397498548031\n",
      "Loss for batch 43 = 0.22535689175128937\n",
      "Loss for batch 44 = 0.018657634034752846\n",
      "Loss for batch 45 = 0.16246727108955383\n",
      "Loss for batch 46 = 0.01646246761083603\n",
      "Loss for batch 47 = 0.01662309654057026\n",
      "Loss for batch 48 = 0.13390231132507324\n",
      "Loss for batch 49 = 0.3934693932533264\n",
      "Loss for batch 50 = 0.01873152330517769\n",
      "Loss for batch 51 = 0.013573683798313141\n",
      "Loss for batch 52 = 0.00840386189520359\n",
      "Loss for batch 53 = 0.03527486324310303\n",
      "Loss for batch 54 = 0.5158310532569885\n",
      "Loss for batch 55 = 0.43974781036376953\n",
      "Loss for batch 56 = 0.013744530268013477\n",
      "Loss for batch 57 = 0.03019915334880352\n",
      "Loss for batch 58 = 0.021111397072672844\n",
      "Loss for batch 59 = 0.021107003092765808\n",
      "Loss for batch 60 = 0.17875757813453674\n",
      "Loss for batch 61 = 0.037028007209300995\n",
      "Loss for batch 62 = 0.052236419171094894\n",
      "Loss for batch 63 = 0.023998888209462166\n",
      "Loss for batch 64 = 0.017972644418478012\n",
      "Loss for batch 65 = 0.039733514189720154\n",
      "Loss for batch 66 = 0.0436357744038105\n",
      "Loss for batch 67 = 0.023355960845947266\n",
      "Loss for batch 68 = 0.07837110012769699\n",
      "Loss for batch 69 = 0.032809678465127945\n",
      "Loss for batch 70 = 0.167741060256958\n",
      "Loss for batch 71 = 0.011030546389520168\n",
      "Loss for batch 72 = 0.027318255975842476\n",
      "Loss for batch 73 = 0.16601580381393433\n",
      "Loss for batch 74 = 0.038541391491889954\n",
      "Loss for batch 75 = 0.2567186653614044\n",
      "Loss for batch 76 = 0.015405770391225815\n",
      "Loss for batch 77 = 0.013081477023661137\n",
      "Loss for batch 78 = 0.013950243592262268\n",
      "Loss for batch 79 = 0.015947595238685608\n",
      "Loss for batch 80 = 0.013194472528994083\n",
      "Loss for batch 81 = 0.013203153386712074\n",
      "Loss for batch 82 = 0.09755133837461472\n",
      "Loss for batch 83 = 0.14718590676784515\n",
      "Loss for batch 84 = 0.013715283013880253\n",
      "Loss for batch 85 = 0.030193675309419632\n",
      "Loss for batch 86 = 0.04307418689131737\n",
      "Loss for batch 87 = 0.01750258542597294\n",
      "Loss for batch 88 = 0.052635207772254944\n",
      "Loss for batch 89 = 0.017501458525657654\n",
      "Loss for batch 90 = 0.10230547934770584\n",
      "Loss for batch 91 = 0.015043294057250023\n",
      "Loss for batch 92 = 0.020258113741874695\n",
      "Loss for batch 93 = 0.012344524264335632\n",
      "Loss for batch 94 = 0.013877075165510178\n",
      "Loss for batch 95 = 0.009915470145642757\n",
      "Loss for batch 96 = 0.009346047416329384\n",
      "Loss for batch 97 = 0.010906021110713482\n",
      "Loss for batch 98 = 0.22884052991867065\n",
      "Loss for batch 99 = 0.008238577283918858\n",
      "Loss for batch 100 = 0.00922301597893238\n",
      "Loss for batch 101 = 0.012625165283679962\n",
      "Loss for batch 102 = 0.012836767360568047\n",
      "Loss for batch 103 = 0.49879348278045654\n",
      "Loss for batch 104 = 0.011016599833965302\n",
      "Loss for batch 105 = 0.28895968198776245\n",
      "Loss for batch 106 = 0.015624589286744595\n",
      "\n",
      "Training Loss for epoch 25 = 8.218679428100586\n",
      "\n",
      "Current Validation Loss = 22.802515029907227\n",
      "Best Validation Loss = 13.186616897583008\n",
      "Epochs without Improvement = 18\n",
      "Train Accuracy: 98.06%\n",
      "Validation Accuracy: 69.61%\n",
      "\n",
      "Epoch 26\n",
      "----------\n",
      "Loss for batch 0 = 0.13131394982337952\n",
      "Loss for batch 1 = 0.07745476812124252\n",
      "Loss for batch 2 = 0.19200177490711212\n",
      "Loss for batch 3 = 0.012002789415419102\n",
      "Loss for batch 4 = 0.011112997308373451\n",
      "Loss for batch 5 = 0.19174796342849731\n",
      "Loss for batch 6 = 0.10430178791284561\n",
      "Loss for batch 7 = 0.11527585983276367\n",
      "Loss for batch 8 = 0.00791292916983366\n",
      "Loss for batch 9 = 0.14082737267017365\n",
      "Loss for batch 10 = 0.008051962591707706\n",
      "Loss for batch 11 = 0.0862414687871933\n",
      "Loss for batch 12 = 0.2201727032661438\n",
      "Loss for batch 13 = 0.013332025147974491\n",
      "Loss for batch 14 = 0.026342496275901794\n",
      "Loss for batch 15 = 0.3357338011264801\n",
      "Loss for batch 16 = 0.01562277227640152\n",
      "Loss for batch 17 = 0.01218323316425085\n",
      "Loss for batch 18 = 0.08228131383657455\n",
      "Loss for batch 19 = 0.06298671662807465\n",
      "Loss for batch 20 = 0.014226017519831657\n",
      "Loss for batch 21 = 0.02578638307750225\n",
      "Loss for batch 22 = 0.060368895530700684\n",
      "Loss for batch 23 = 0.023215550929307938\n",
      "Loss for batch 24 = 0.12833288311958313\n",
      "Loss for batch 25 = 0.020507052540779114\n",
      "Loss for batch 26 = 0.010269414633512497\n",
      "Loss for batch 27 = 0.2046675682067871\n",
      "Loss for batch 28 = 0.017540128901600838\n",
      "Loss for batch 29 = 0.012785707600414753\n",
      "Loss for batch 30 = 0.010932255536317825\n",
      "Loss for batch 31 = 0.3663519620895386\n",
      "Loss for batch 32 = 0.013972142711281776\n",
      "Loss for batch 33 = 0.010220659896731377\n",
      "Loss for batch 34 = 0.012835554778575897\n",
      "Loss for batch 35 = 0.018573837354779243\n",
      "Loss for batch 36 = 0.0331353023648262\n",
      "Loss for batch 37 = 0.010606079362332821\n",
      "Loss for batch 38 = 0.14493407309055328\n",
      "Loss for batch 39 = 0.008752435445785522\n",
      "Loss for batch 40 = 0.009096287190914154\n",
      "Loss for batch 41 = 0.00690148351714015\n",
      "Loss for batch 42 = 0.011563032865524292\n",
      "Loss for batch 43 = 0.2014802247285843\n",
      "Loss for batch 44 = 0.04628900811076164\n",
      "Loss for batch 45 = 0.007977019064128399\n",
      "Loss for batch 46 = 0.038151293992996216\n",
      "Loss for batch 47 = 0.00895685888826847\n",
      "Loss for batch 48 = 0.012066506780683994\n",
      "Loss for batch 49 = 0.29975566267967224\n",
      "Loss for batch 50 = 0.006963851395994425\n",
      "Loss for batch 51 = 0.013168897479772568\n",
      "Loss for batch 52 = 0.005914171691983938\n",
      "Loss for batch 53 = 0.013785234652459621\n",
      "Loss for batch 54 = 0.2835830748081207\n",
      "Loss for batch 55 = 0.17143408954143524\n",
      "Loss for batch 56 = 0.010271736420691013\n",
      "Loss for batch 57 = 0.030550861731171608\n",
      "Loss for batch 58 = 0.010914967395365238\n",
      "Loss for batch 59 = 0.040456727147102356\n",
      "Loss for batch 60 = 0.16881538927555084\n",
      "Loss for batch 61 = 0.08661697804927826\n",
      "Loss for batch 62 = 0.011063167825341225\n",
      "Loss for batch 63 = 0.014516912400722504\n",
      "Loss for batch 64 = 0.10791660845279694\n",
      "Loss for batch 65 = 0.06615012139081955\n",
      "Loss for batch 66 = 0.010367065668106079\n",
      "Loss for batch 67 = 0.008465686812996864\n",
      "Loss for batch 68 = 0.009260879829525948\n",
      "Loss for batch 69 = 0.015042069368064404\n",
      "Loss for batch 70 = 0.054096147418022156\n",
      "Loss for batch 71 = 0.008746564388275146\n",
      "Loss for batch 72 = 0.009804809466004372\n",
      "Loss for batch 73 = 0.15778982639312744\n",
      "Loss for batch 74 = 0.017091894522309303\n",
      "Loss for batch 75 = 0.269596129655838\n",
      "Loss for batch 76 = 0.022660931572318077\n",
      "Loss for batch 77 = 0.017107348889112473\n",
      "Loss for batch 78 = 0.014603845775127411\n",
      "Loss for batch 79 = 0.011240455321967602\n",
      "Loss for batch 80 = 0.017514746636152267\n",
      "Loss for batch 81 = 0.03472266346216202\n",
      "Loss for batch 82 = 0.0781077891588211\n",
      "Loss for batch 83 = 0.14590853452682495\n",
      "Loss for batch 84 = 0.013288451358675957\n",
      "Loss for batch 85 = 0.09099052846431732\n",
      "Loss for batch 86 = 0.01804991066455841\n",
      "Loss for batch 87 = 0.01009149756282568\n",
      "Loss for batch 88 = 0.036337196826934814\n",
      "Loss for batch 89 = 0.009907710365951061\n",
      "Loss for batch 90 = 0.09094037860631943\n",
      "Loss for batch 91 = 0.013649458065629005\n",
      "Loss for batch 92 = 0.012494832277297974\n",
      "Loss for batch 93 = 0.009916464798152447\n",
      "Loss for batch 94 = 0.011629931628704071\n",
      "Loss for batch 95 = 0.009403850883245468\n",
      "Loss for batch 96 = 0.009087706916034222\n",
      "Loss for batch 97 = 0.008575059473514557\n",
      "Loss for batch 98 = 0.2223159521818161\n",
      "Loss for batch 99 = 0.006413005758076906\n",
      "Loss for batch 100 = 0.007533693220466375\n",
      "Loss for batch 101 = 0.010541995987296104\n",
      "Loss for batch 102 = 0.010894053615629673\n",
      "Loss for batch 103 = 0.4177270531654358\n",
      "Loss for batch 104 = 0.00935348030179739\n",
      "Loss for batch 105 = 0.25702500343322754\n",
      "Loss for batch 106 = 0.014315186999738216\n",
      "\n",
      "Training Loss for epoch 26 = 6.881883144378662\n",
      "\n",
      "Current Validation Loss = 23.167335510253906\n",
      "Best Validation Loss = 13.186616897583008\n",
      "Epochs without Improvement = 19\n",
      "Train Accuracy: 98.80%\n",
      "Validation Accuracy: 70.43%\n",
      "\n",
      "Epoch 27\n",
      "----------\n",
      "Loss for batch 0 = 0.11819934099912643\n",
      "Loss for batch 1 = 0.018131844699382782\n",
      "Loss for batch 2 = 0.13959378004074097\n",
      "Loss for batch 3 = 0.015704279765486717\n",
      "Loss for batch 4 = 0.013792083598673344\n",
      "Loss for batch 5 = 0.1992282122373581\n",
      "Loss for batch 6 = 0.09944219142198563\n",
      "Loss for batch 7 = 0.02685670740902424\n",
      "Loss for batch 8 = 0.006518187932670116\n",
      "Loss for batch 9 = 0.13389043509960175\n",
      "Loss for batch 10 = 0.006986197084188461\n",
      "Loss for batch 11 = 0.04508468508720398\n",
      "Loss for batch 12 = 0.17946411669254303\n",
      "Loss for batch 13 = 0.011037275195121765\n",
      "Loss for batch 14 = 0.012779060751199722\n",
      "Loss for batch 15 = 0.31755924224853516\n",
      "Loss for batch 16 = 0.013883479870855808\n",
      "Loss for batch 17 = 0.006052132695913315\n",
      "Loss for batch 18 = 0.014973904006183147\n",
      "Loss for batch 19 = 0.01587793603539467\n",
      "Loss for batch 20 = 0.016799231991171837\n",
      "Loss for batch 21 = 0.021981695666909218\n",
      "Loss for batch 22 = 0.05602691322565079\n",
      "Loss for batch 23 = 0.0509725883603096\n",
      "Loss for batch 24 = 0.11575325578451157\n",
      "Loss for batch 25 = 0.017656192183494568\n",
      "Loss for batch 26 = 0.009415652602910995\n",
      "Loss for batch 27 = 0.1997273862361908\n",
      "Loss for batch 28 = 0.013418750837445259\n",
      "Loss for batch 29 = 0.012971758842468262\n",
      "Loss for batch 30 = 0.009737587533891201\n",
      "Loss for batch 31 = 0.31950443983078003\n",
      "Loss for batch 32 = 0.01251685805618763\n",
      "Loss for batch 33 = 0.009974480606615543\n",
      "Loss for batch 34 = 0.015232847072184086\n",
      "Loss for batch 35 = 0.01888406090438366\n",
      "Loss for batch 36 = 0.019823020324110985\n",
      "Loss for batch 37 = 0.010657311417162418\n",
      "Loss for batch 38 = 0.1407758891582489\n",
      "Loss for batch 39 = 0.008967123925685883\n",
      "Loss for batch 40 = 0.009800780564546585\n",
      "Loss for batch 41 = 0.00618140259757638\n",
      "Loss for batch 42 = 0.009631694294512272\n",
      "Loss for batch 43 = 0.1856873631477356\n",
      "Loss for batch 44 = 0.02356286346912384\n",
      "Loss for batch 45 = 0.0064827799797058105\n",
      "Loss for batch 46 = 0.017501726746559143\n",
      "Loss for batch 47 = 0.00796808023005724\n",
      "Loss for batch 48 = 0.011723720468580723\n",
      "Loss for batch 49 = 0.3050321638584137\n",
      "Loss for batch 50 = 0.006339964456856251\n",
      "Loss for batch 51 = 0.009315558709204197\n",
      "Loss for batch 52 = 0.00546162249520421\n",
      "Loss for batch 53 = 0.019790291786193848\n",
      "Loss for batch 54 = 0.2332044541835785\n",
      "Loss for batch 55 = 0.1586735099554062\n",
      "Loss for batch 56 = 0.00961796659976244\n",
      "Loss for batch 57 = 0.029285039752721786\n",
      "Loss for batch 58 = 0.010662705637514591\n",
      "Loss for batch 59 = 0.02477378025650978\n",
      "Loss for batch 60 = 0.1693284958600998\n",
      "Loss for batch 61 = 0.08722467720508575\n",
      "Loss for batch 62 = 0.022642740979790688\n",
      "Loss for batch 63 = 0.02188568189740181\n",
      "Loss for batch 64 = 0.036385320127010345\n",
      "Loss for batch 65 = 0.06232709065079689\n",
      "Loss for batch 66 = 0.009685776196420193\n",
      "Loss for batch 67 = 0.007359349634498358\n",
      "Loss for batch 68 = 0.009424942545592785\n",
      "Loss for batch 69 = 0.025014063343405724\n",
      "Loss for batch 70 = 0.04374758526682854\n",
      "Loss for batch 71 = 0.008308417163789272\n",
      "Loss for batch 72 = 0.008759662508964539\n",
      "Loss for batch 73 = 0.15932253003120422\n",
      "Loss for batch 74 = 0.013312691822648048\n",
      "Loss for batch 75 = 0.23911833763122559\n",
      "Loss for batch 76 = 0.0124902855604887\n",
      "Loss for batch 77 = 0.022507084533572197\n",
      "Loss for batch 78 = 0.013138520531356335\n",
      "Loss for batch 79 = 0.010907898657023907\n",
      "Loss for batch 80 = 0.01509570050984621\n",
      "Loss for batch 81 = 0.01674572564661503\n",
      "Loss for batch 82 = 0.0693771243095398\n",
      "Loss for batch 83 = 0.15069063007831573\n",
      "Loss for batch 84 = 0.06323422491550446\n",
      "Loss for batch 85 = 0.07980023324489594\n",
      "Loss for batch 86 = 0.01675323024392128\n",
      "Loss for batch 87 = 0.009003202430903912\n",
      "Loss for batch 88 = 0.029892295598983765\n",
      "Loss for batch 89 = 0.007881980389356613\n",
      "Loss for batch 90 = 0.08701170235872269\n",
      "Loss for batch 91 = 0.012037797830998898\n",
      "Loss for batch 92 = 0.010650641284883022\n",
      "Loss for batch 93 = 0.008306832052767277\n",
      "Loss for batch 94 = 0.009706850163638592\n",
      "Loss for batch 95 = 0.0070852069184184074\n",
      "Loss for batch 96 = 0.008322593756020069\n",
      "Loss for batch 97 = 0.0071953777223825455\n",
      "Loss for batch 98 = 0.21654388308525085\n",
      "Loss for batch 99 = 0.005438091233372688\n",
      "Loss for batch 100 = 0.007741130422800779\n",
      "Loss for batch 101 = 0.010668054223060608\n",
      "Loss for batch 102 = 0.009568698704242706\n",
      "Loss for batch 103 = 0.40907394886016846\n",
      "Loss for batch 104 = 0.00905404333025217\n",
      "Loss for batch 105 = 0.28325846791267395\n",
      "Loss for batch 106 = 0.013892203569412231\n",
      "\n",
      "Training Loss for epoch 27 = 6.123471736907959\n",
      "\n",
      "Current Validation Loss = 23.893314361572266\n",
      "Best Validation Loss = 13.186616897583008\n",
      "Epochs without Improvement = 20\n",
      "Train Accuracy: 98.86%\n",
      "Validation Accuracy: 70.43%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIkAAAHUCAYAAAC+mnjWAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzdd1gUVxfA4d/u0ouggA3sigrSBDv23o01Ro2Kxt6iSWwxscQYW4oltqgxxmg0tsTeS4wlotgbShQ7KKBI353vj9X9QrCALizCeZ+Hx92ZO2fOLMhezt65V6UoioIQQgghhBBCCCGEyNXUpk5ACCGEEEIIIYQQQpieFImEEEIIIYQQQgghhBSJhBBCCCGEEEIIIYQUiYQQQgghhBBCCCEEUiQSQgghhBBCCCGEEEiRSAghhBBCCCGEEEIgRSIhhBBCCCGEEEIIgRSJhBBCCCGEEEIIIQRSJBJCCCGEEEIIIYQQSJFIiLdKt27d6Natm6nTeKvExsYydepUGjRogK+vLy1btmTFihXodLpU7VavXk3z5s3x9fWladOmrFixAkVRXhq7W7dulC1b9oVfHTt2zMxLe66bN29StmxZ1q1bl+XnFkIIITLbiBEjKFu2LEuWLDF1KuIVtm7dSrt27fDz86N27dqMHj2ayMjIVG0uXbpE7969qVy5MoGBgYwcOTJNm/9at27dS/tfZcuW5erVq5l5ac8l/XSRU5iZOgEhhMgsiqIwbNgwzpw5w5AhQyhZsiSHDx/miy++IDo6moEDBwKwZs0axo0bR7du3ahfvz7Hjx9n0qRJJCYmEhQU9NJzeHh48Pnnnz93n62trdGvSQghhMitHj9+zK5du3B3d+fXX3+lZ8+eqFQqU6clnmPz5s0MHz6cTp068eGHHxIZGcl3331H9+7dWbduHZaWlkRGRtK9e3cKFSrElClTSExMZMaMGXzwwQesXr0ac3Pzl55jzpw5uLi4PHefm5tbZlyWELmCFImEEDnW+fPnOXjwIN9++y1NmzYFoFq1asTExPDDDz8wYMAAVCoVa9euxd/fn08//dTQJiwsjJ9//vmVRSI7Ozt8fX0z+1KEEEKIXG/Tpk0AjB07lu7du3PkyBGqVatm4qzE88yfP5/atWszceJEw7YSJUrQsWNH9u7dS5MmTdi9ezdRUVGsXr2aokWLAmBvb0/v3r05efIklStXfuk5ypcvL8UgITKB3G4mRA506NAh3nvvPfz9/alSpQojRozgzp07hv06nY5vvvmGevXqUaFCBerVq8fMmTNJTk42tNm0aROtWrXC29ubqlWr8tFHH3Hv3r2Xnvf+/fuMHj2a2rVr4+3tTfv27dm9e7dhf1BQEG3btk1z3IABA2jVqpXh+fHjx+natSs+Pj5UrlyZkSNH8vDhQ8P+devW4eHhwZo1a6hRowaVK1cmNDT0uTl16tQpTQeyZMmSxMXF8eDBAwASExOxs7NL1cbR0ZHo6OiXXm9G1KtXj2+++YYvv/ySSpUqUaVKFT755JM053jV9w7g2rVrDBo0iMqVK1OpUiX69u2bZlh1REQEQ4YMwc/Pj8qVKzNu3DiePHli2H/27Fm6d++Ov78/fn5+9OjRg5CQEKNdrxBCCGFsa9eupVq1alStWpVixYqxatWqNG02bNjAO++8g4+PD3Xq1GHmzJkkJSUZ9oeEhBAUFETFihWpWrUqw4cPN/Rvnt3GdPPmzVQx69Wrx6hRowzPy5Yty5w5c2jbti3e3t7MmTMHgL///ptevXpRqVIlQ/9q9uzZqW5xj42NZdKkSdSsWRNfX1/atWvHvn37AJg6dSre3t48fvw41fm///57/P39iY+Pf+7rotVqWbFiBS1btsTb25s6deowY8YMEhMTAfjjjz8oW7Ysly9fTnXcrl27KFu2LOfPnwcgOjqazz77jOrVq+Pl5UXHjh05fPhwqmNedO3/ptPpqFGjRprb7kuWLAnAjRs3AAz5/bsP5ujoaMjFGGbPnk29evUMhSkfHx86duzI0aNHU7V7VR8WICkpiW+//Zb69evj7e1NixYtWL9+fao2iqKwaNEi6tSpg7e3N506deL06dOG/QkJCYwfP55atWpRoUIFmjRpwuLFi41yrUIYixSJhMhhNmzYQFBQEIUKFeLrr79m9OjRnDx5kk6dOhmKIosWLWLlypUMHDiQJUuW0LlzZxYvXsy8efMACA4O5pNPPqFRo0YsWrSI0aNHc+TIEUaMGPHC80ZGRtK+fXuOHz/Ohx9+yOzZs3F1dWXgwIH8/vvvALRq1Ypz585x/fp1w3GPHj3iwIEDtG7dGtB3sHr06IGVlRXffvstY8aM4dixY7z//vskJCQYjtNqtSxZsoTJkyczevRoSpUqlSYnT09PJk6caOhwPLNr1y7y5ctHvnz5AHj//ff5888/2bhxI48fP+bgwYOsX7/ekNPLKIpCSkrKc7/+O6fRL7/8wokTJ5gyZQojRoxg//799O3b19AuPd+7e/fu0alTJ/755x/Gjx/P9OnTDcO1/92h+u677yhUqBDff/893bt3Z/Xq1YaOXGxsLL179yZv3rzMnj2bb775hvj4eHr16pWmYyqEEEJkB1euXOHMmTO0adMGgDZt2rB79+5U89esWLGCkSNH4unpyZw5c+jTpw/Lly/niy++APQjjLt27UpiYiLTpk1jwoQJnD17ll69epGSkpKhfObPn0/Lli2ZNWsWjRs35uLFi/To0QNHR0e++eYb5s2bR0BAAHPmzGHr1q2Avu8SFBTEH3/8Qd++ffn+++8pWbIkAwcO5Pjx47Rv357ExES2bduW6lwbN26kWbNmWFtbPzeXzz77jClTptCgQQPmzZtHly5d+PnnnxkwYACKotCgQQNsbGzYvHlzquM2bdpEmTJl8PDwIDExke7du7N7924+/PBD5syZQ8GCBendu3eaQtF/r/2/1Go1o0aNokGDBqm279q1C4AyZcoA0LRpU1xcXJg4cSL3798nPDycadOm4eLiQvXq1V/5PdDpdM/tf/133smHDx8ycuRI3nvvPb777jusrKzo1asXFy5cANLXhwX46KOPWLp0KR06dGDBggUEBgYyatQowwg30Pehd+7cybhx45g+fTr379+nf//+hp+vL7/8kgMHDjBy5EgWL15M/fr1mTZtGmvXrn3l9QqRZRQhxFuja9euSteuXV+4X6vVKjVq1FCCgoJSbb9+/bri6empTJ06VVEURQkKClJ69uyZqs3y5cuVDRs2KIqiKAsWLFD8/PyUxMREw/59+/Yps2fPVnQ63XPPPW3aNMXT01O5efNmqu3du3dXatSooWi1WuXJkyeKr6+vMmfOHMP+NWvWKOXKlVPu3r2rKIqidOrUSWnRooWSkpJiaHPt2jWlfPnyys8//6woiqKsXbtWcXd3N+SbET/++KPi7u6uLFmyxLAtMTFRGTVqlOLu7m74CgoKUpKSkl4aq2vXrqmO+e/X1q1bDW3r1q2rVK5cWXn06JFh286dOxV3d3dl//796f7effXVV4q3t7dy//59Q5s7d+4oderUUfbt26eEh4cr7u7uyrBhw1LF6dy5s9KmTRtFURTl5MmTiru7uxIcHJzqPNOmTVPu3LmT3pdSCCGEyDJTpkxRKleubOib3L59WylXrpwyb948RVH0faBq1aopAwYMSHXcDz/8oLzzzjtKUlKSMnjwYKVGjRpKQkKCYf+JEyeUunXrKufPnzf0L8LDw1PFqFu3rjJy5EjDc3d3d6V79+6p2qxfv17p3bu3otVqDdu0Wq3i7++vjBs3TlEURdmzZ4/i7u6u7Ny5M1WbTp06KbNnz1YURd8P6tKli2F/cHCw4u7urpw4ceK5r8uVK1cUd3d3ZcGCBam2b9iwQXF3d1f27dunKIqijBw5UmnQoIFhf2xsrOLt7W047tdff1Xc3d2VkJAQQxudTqd06dJFadu27UuvPT2uX7+uVKlSRWndunWq12jXrl2Kt7e3oe9UqVIl5cKFCy+N9ez79KKvPn36GNrOmjVLcXd3V9avX2/YFh8fr9SoUcPQV0pPH/bSpUuKu7u78uOPP6ZqM2jQIOXTTz9VFEXfL/T29laioqIM+1evXq24u7sbrqlx48aG9s/MmTNH2bt378tfQCGykMxJJEQOEhYWRkRERJoRP0WLFsXPz49jx44BUKVKFWbOnMl7771HvXr1qFOnDl27djW0r1SpEt988w0tWrSgcePG1K5dm8DAQGrXrv3Ccx87dgw/Pz9cXV1TbW/VqhWjR4/m2rVrlC5dmgYNGrBlyxbDpNGbN2+mWrVqFChQgPj4eE6dOkWvXr0MI3QAihQpQqlSpTh06BBdunQxxC5fvnyGXp+ff/6ZKVOm0LRpU3r06GHYPmDAAIKDg/n444/x9vbm8uXLzJ49m6FDhzJ37tyXTorp6enJhAkTnrvv2f31z9SrVw97e/tUz83MzPj7779xdXVN1/cuODgYX1/fVBM1FixYkL179wIYhsgHBASkiuPm5kZwcDCg/wQvX7589OvXjyZNmlCzZk1q1KjBxx9//MLrFEIIIUwlOTmZ33//nQYNGpCQkEBCQgK2trb4+/uzevVq+vTpQ1hYGA8ePKBhw4apju3Vqxe9evUC9O+htWvXxtLS0rDfz8+PPXv2ABhGlqTHf/sgbdq0oU2bNiQmJhIWFsb169e5cOECWq3WcDt/cHAw5ubm1KtXz3CcWq1Oddtcu3btGDduHLdu3cLV1ZX169dTokQJ/Pz8npvHs/5B8+bNU21v3rw5o0eP5ujRo9SuXZvWrVuzfv16Tp8+jbe3N7t37yYpKclwu//hw4dxcXHB09Mz1aiqunXrMm3aNGJiYnBwcHjutb/K1atX6dWrF2ZmZsyaNQu1Wn8zyx9//MEnn3xCkyZNaNeuHYmJiSxZsoSgoCCWL1/+3FHi/zZv3rznTlydJ0+eVM/NzMxo0aKF4bmVlRW1atXiwIEDQPr6sM/6UI0aNUrVZvbs2amely5dOtUI9mdzJj0bqV2lShVWrVrF3bt3qV27NrVr1zb0iYXILqRIJEQO8ux2I2dn5zT7nJ2dDfec9+7dG1tbW9auXcuMGTOYPn06ZcqU4dNPP6Vq1ar4+fmxcOFCfvzxR5YuXcrChQtxdnamX79+L1zaMyYmhiJFijz3vKC/rQygdevW/P7771y8eBFnZ2eOHj3Kl19+aWij0+lYtGgRixYtShPr3506ABsbm3S9LjqdjmnTprF06VJatGjB1KlTDYWfEydOcPDgQb744gs6dOgAQOXKlSlSpAh9+vRh37591K1b94WxbW1t8fLySlceBQoUSPVcrVaTN29eYmJi0v29i46OTtckjf8dkq5Wqw23tdna2rJixQrmzZvH1q1b+fXXX7GysqJ169Z8+umnWFhYpOt6hBBCiKywb98+Hjx4wG+//cZvv/2WZv/BgwcN89o4OTm9ME50dPRL92fEf/sgCQkJTJo0iY0bN5KSkoKbmxt+fn6YmZkZ3n+jo6NxdHQ0FEmep1mzZnz55Zds3LiRXr16sXXrVvr06fPC9jExMQBpiiVmZmbkzZs3VXGiQIECbN68GW9vbzZv3kzlypUpWLCgIbeIiAg8PT2fe56IiAhDkSi9/S+Ao0ePMnjwYGxsbFi2bFmqD9DmzJmDn58f33zzjWFbjRo1aNasGd999x2zZs16aWx3d/d09YmcnZ0xM0v9Z6+Tk5Oh75WePuyztq/6+fnva/Pse/3sFrixY8dSsGBBfv/9dyZNmsSkSZPw8/Nj/PjxlCtX7pXXIkRWkCKREDnIs08u/n1//jMRERHkzZsX0L9hdenShS5duvDgwQP279/P/PnzGTx4MIcOHcLCwoKaNWtSs2ZN4uPjOXLkCD/99BNffPEFPj4+eHt7p4nv4OBARETEc88LGM5drVo1XFxc2Lp1Ky4uLlhaWho+lbG1tUWlUtGjR480n4hB2sJHeiQlJTFixAh27NhBUFAQn3zySaqRQbdv3wagYsWKqY57NhLnypUrLy0SZURUVFSq51qtlqioKPLly5fu7529vX2qSbyfOXz4MG5ubuleCrhkyZJMnz4drVbL6dOn2bhxIytXrqRo0aL07t07g1cmhBBCZJ61a9dSpEgRJk+enGq7oigMGjSIVatWMXz4cIA075FRUVGcP38ePz+/F76H7t+/n/LlyxveQ/87p82/F354kcmTJ7N9+3a+/fZbqlevbigW/HvxDHt7e6Kjo1EUJdX79fnz51EUBU9PT2xtbWnSpAlbt27F3d2duLi4l86R+KxwExERkWokTHJyMlFRUan6fi1btmTTpk3069ePQ4cOpVp5zN7enuLFizNjxoznnud1VhHbtGkTo0aNokSJEvzwww9pPiy7detWmnmLrKysqFChAleuXMnw+V7keZNgR0ZGGgo+6enDPhud9PDhQ0NhDfSjpKKjo/H3909XLhYWFvTv35/+/ftz+/Zt9u7dy/fff8+IESPSzBklhKnIxNVC5CAlSpTAxcUl1QR6AOHh4YSEhBgKIe+++65hEkcnJyfatm1Lly5dePToEbGxsUydOpV27dqhKArW1tbUrVuXkSNHAv8vqvxXpUqVOHnyJLdu3Uq1/ffff8fFxYVixYoBoNFoaNmyJXv37mXbtm2GyRRBv7qFh4cH165dw8vLy/BVpkwZZs+enWYlivQYPXo0O3fuZPTo0YwcOTJNEeXZShvHjx9Ptf3EiRMAz/1k6XUdOHAg1Qoru3fvJiUlhWrVqqX7excQEMCpU6dSdXIfPHhA79692b9/f7ry2LZtG1WrViUiIgKNRmP4BCtPnjwv/P4KIYQQphAREcHBgwdp3rw5VapUSfVVtWpVmjRpwv79+8mTJw958+Y13H79zMaNG+nTpw/JyckEBARw6NChVO/F58+fp0+fPpw7d84wGunu3buG/c+KAK8SHBxMlSpVUvVrzp49y8OHDw1Fp4CAAJKTkw23OYG+0DV69GgWLFhg2Na+fXsuX77MsmXLqF69epriyr89Wyb+vwWGzZs3o9VqUxUvWrduzd27d5k7dy4ajSbVrVOVK1fmzp07ODk5peqDHTp0iB9++AGNRvPK1+Df9u/fzyeffIKfnx8rV6587jWULFmSEydOpFroIzExkXPnzhm1/5WQkMDBgwdTPT9w4IChgJeePuyz1/HZrYnPzJgxI03x8mV5NG7cmCVLlgBQuHBhunTpQvPmzaX/JbIVGUkkxFvm7t27/Pjjj2m2u7u7U716dYYPH87o0aMZMWIErVq1Iioqijlz5uDg4EDPnj0B/ZvhkiVLcHZ2xs/Pj3v37rF06VIqV65Mvnz5qFq1KkuXLmXUqFG0atWK5ORkfvjhBxwdHalatepz8+rZsye///47PXr0YNCgQTg6OrJhwwaOHDnCl19+mWpodevWrVmyZAlqtTrNbWXDhw+nT58+hvyfrWJ26tQpBgwYkKHXateuXWzatIl69erh6+ubZol3Dw8PPDw8aNy4MV999RUxMTH4+PgQGhrK7Nmz8fT0TDO3wX/Fxsa+dOl4Ly8vQ8fqzp079O/fn/fff587d+7w9ddfU7NmTapUqWK49ld973r06MGGDRvo3bs3ffv2xdzcnHnz5lGwYEFatmyZrtXJKlasiE6nY+DAgfTp0wdbW1u2bt3K48eP09xrL4QQQpjShg0bSElJee4IY9DPBbRmzRpWr17N4MGDmThxIk5OTtSrV4+wsDBmzZpFly5dcHBwYMCAAXTq1Im+ffsaVk399ttv8fb2pkaNGiQkJGBlZcVXX33F0KFDefLkCbNmzUqzSurzeHt7s3XrVlauXEmpUqW4ePEi8+bNQ6VSGZaur1OnDn5+fowaNYphw4ZRpEgRNm7cyNWrV5k0aZIhlr+/PyVKlODYsWOpbsV6ntKlS/POO+8wa9Ys4uPjqVSpEhcuXGDOnDlUqVKFmjVrGtq6u7tTvnx5fvnlF5o2bZpq6fm2bdvy888/07NnT/r160ehQoX466+/WLRoEV27dsXc3PyVr8EziYmJjB07FltbW/r160doaGiq/QULFqRgwYIMHTqUgQMHMnToUNq3b09SUhLLli3j3r17zJw585XnuXDhwnNHYAO4urqmugVv9OjRDBs2DCcnJxYvXkxcXBz9+/cH0teHLVeuHE2aNGH69OkkJCRQvnx5Dhw4wN69ew2rx76KlZWVYeU9c3NzypYtS1hYGOvXr3/uKnFCmIoUiYR4y9y4cYMpU6ak2d6+fXuqV69O27ZtsbW1ZcGCBQwcOBA7Oztq1qzJ8OHDDW+WQ4cOxcLCgrVr1zJ37lzs7e2pV6+eYdLk2rVrM2PGDJYsWcKgQYNQqVT4+/vz008/vbCj5OLiwsqVK5k5cyZffPEFycnJlCtXju+//5769eunaluuXDnc3d2JiopKNQwbIDAwkMWLFzNnzhyGDBmCubk5np6eLF26FF9f3wy9Vjt27AD0n/r895Mf0I/kcXNzY8aMGcybN49Vq1Yxa9YsChcuTNu2bRk4cGCae9j/6/z583Tq1OmF+//++2/DEOXmzZuTJ08ehg0bho2NDe+88w4ffvihoW16vneFChXil19+Yfr06YwaNQoLCwuqVKnCN998g4ODQ7qKRPnz5+eHH37gu+++Y+zYscTHxxtGa72oCCiEEEKYwrp16yhTpgzu7u7P3e/v74+bmxtr1qxh79692NjYsHjxYn799VcKFizIBx98wAcffADoPxxavnw5M2fOZNiwYdjZ2VG7dm0++ugjLCwssLCwYPbs2cycOZOBAwfi6urKoEGD2LBhwyvzHDVqFMnJyXz77bckJSXh5uZG//79CQ0NZc+ePWi1WjQaDYsWLWLGjBl89913xMfHU7ZsWZYsWZLmVv46derw8OHDNLdjPc/kyZMpVqwYa9euZdGiReTPn5/333+fAQMGpJn/qHXr1nz11VeGCaufsbGxYcWKFcycOZPp06fz+PFjXF1dGTFiBEFBQa/M4d9OnDhhuFXreccOGjSIwYMHU79+fRYuXMj333/PoEGDsLW1xdvbm99++y1d8/MMGjTohftGjx6dapGS8ePH8+WXX/Lw4UMqVqzIypUrDaPc09uHnT59OnPmzGHZsmVERUVRqlQpZs2ala7v0TMTJ07k22+/ZcmSJURERODk5ET79u0ZOnRoumMIkdlUyr/H9wkhhMgU9erVo3Llynz11VemTkUIIYQQ2ZiiKDRv3pzAwEDGjBlj6nTearNnz2bOnDlcunTJ1KkI8daQkURCCCGEEEIIYWKxsbH8+OOPnDlzhvDw8BeuKCuEEJlJikRCCCGEEEIIYWJWVlasWrUKnU7Hl19+adTJm4UQIr3kdjMhhBBCCCGEEEIIgfrVTYQQQgghhBBCCCFETidFIiGEEEIIIYQQQgghRSIhhBBCCCGEEEIIIRNXA6DT6UhJSUGtVqNSqUydjhBCCCFeQlEUdDodZmZmqNXyeZepSP9JCCGEeDtkpO8kRSIgJSWFM2fOmDoNIYQQQmSAl5cXFhYWpk4j15L+kxBCCPF2SU/fSYpEYKikeXl5odFojBpbq9Vy5syZ1479psdnlxiSg/FiSA7Gi5EdcjBGDMnBeDEkB+PFMEYOr4oto4hMS/pP2T8HY8SQHIwXIzvkYIwYkoPxYkgOxoshOaQvbnr6TlIkAsMQaY1GY/ROzjNvGtsYuWWHGJKD8WJIDsaLkR1yMEYMycF4MSQH48XIzPdWucXJtKT/9PbkYIwYkoPxYmSHHIwRQ3IwXgzJwXgxJIeXS0/fST6CE0IIIYQQQgghhBBSJBJCCCGEEEIIIYQQJi4SJSYmMmbMGAICAggMDGTJkiUvbHv+/Hk6dOiAj48P7dq14+zZs6n2BwQEULZs2VRfT548yexLEEIIIYTIlpKSkmjRogVHjx59YZtX9a+EEEIIkbuYdE6iadOmcfbsWZYtW8bt27cZOXIkhQsXpkmTJqnaxcXF0adPH1q2bMlXX33FypUr6du3Lzt37sTGxoZ79+7x+PFjdu3ahZWVleE4Gxsbo+WqKAopKSlotdoMHfesfUJCwmtPfvUmx2eXGLkxB41Gg5mZmcyZIYQQIsslJiYyYsQIrly58sI2r+pfGUNu7j9lhxyMESMn5yB9NSGESMtkRaK4uDjWrFnDokWL8PT0xNPTkytXrrBixYo0RaItW7ZgaWnJJ598gkqlYuzYsRw4cIBt27bRtm1brl69iouLC0WKFMmUXJOSkrhz5w5xcXEZPlZRFMzMzLh+/fprvQG96fHZJUZuzcHGxoZChQrJEs1CCCGyTGhoKCNGjEBRlJe2e1X/6k3l9v5TdsjBGDFyeg7SVxNCiNRMViS6ePEiKSkp+Pn5Gbb5+/szf/58dDpdqqXZTp06hb+/v+EXukqlomLFioSEhNC2bVtCQ0MpUaJEpuSp0+kICwtDo9FQuHBhLCwsMvTmpCgK8fHxWFtbv/ab4pscn11i5LYcFEUhKSmJiIgIwsLCKFOmjCzVLIQQIkscO3aMKlWq8OGHH+Lr6/vCdq/qX70J6T9ljxyMESOn5iB9NSGEeD6TFYkiIiLImzdvqqq9s7MziYmJREdHky9fvlRtS5cunep4JycnwxDqq1evEh8fT7du3QgLC6N8+fKMGTMmw4Wj5w2FTkxMRKvV4ubm9lpDrxVFQafTYWlp+dpvim9yfHaJkRtzsLKyMnxilZCQgKWlJfD/n7OMDr1/5k2Pzyk5GCNGdsjBGDEkB+PFkByMF8MYObwqtni+9957L13tXtW/Si/pP2XfHIwRIyfn8KK+2otkh9+L2SGG5GC8GJKD8WJIDumLmx4q5VVjkTPJhg0b+O6779i7d69hW3h4OA0aNGD//v0ULFjQsL179+74+/szZMgQw7bvvvuOkydP8uOPP9KtWzfu3r3LhAkTsLOzY9GiRZw+fZrNmzdjZ2f3yly0Wi0hISEv3G9mZkaRIkVe+cYhxH8lJiYSHh5OSkqKqVMRQogcx9fX97XnJ8ktypYty08//USVKlXS7HtV/+pVpP8kcgLpqwkhcpP09J1MNpLI0tKSpKSkVNuePf/35NMva/us3eLFi0lOTsbW1haAGTNmULt2bfbu3UvLli3TnZOXl1eaFywhIYHr169jbW2dJq/0yI7Da00RI7fmoFarMTc3p3Tp0oafH61Wy5kzZ57785Yeb3p8TsnBGDGyQw7GiCE5GC+G5GC8GMbI4VWxxZt5Vf8qvaT/lH1zMEaMnJ7D8/pqL5Idfi9mhxiSg/FiSA7GiyE5pC9uepisSFSgQAGioqJISUnBzEyfRkREBFZWVuTJkydN28jIyFTbIiMjyZ8/PwAWFhapbluztLTEzc2Ne/fuZSgnjUaT5huh0WhQqVSGr9dl6uOzS4zclsOzdi/62XqT//hvenxOycEYMbJDDsaIITkYL4bkYLwYxshBZI5X9a/SS/pP2T8HY8TIqTm8rK/2Itnh92J2iCE5GC+G5GC8GJLDmzPZ7Gzly5fHzMws1TDl4OBgvLy80kwa5+Pjw8mTJw2rdCiKwokTJ/Dx8UFRFBo0aMC6desM7ePi4rh+/TolS5bMkmvJjkaNGkXZsmUpW7Ys5cqVo2LFipQrV86w7ejRoxmO+cEHHzB79ux0ta1Xr16q74mxHD9+nHLlyhk9rhBCCJHbvKx/lVtlRv+pW7duJu8/PbNu3TrKli3LmjVrMu0cQggh3m4mG0lkbW1NmzZtGD9+PF9++SX3799nyZIlTJkyBdCPKrK3t8fKyoomTZowc+ZMJk+ezLvvvsuqVauIj4+nadOmqFQq6tSpw+zZs3F1dSVfvnx89913FCxYkNq1a5vq8kxu7NixjBgxAtAvcbt48WJ+++03wycnDg4OGY45Y8aMNKO8XuS33357rYkqhRBCCJF50tu/yq0yo/80e/bsdC+vntn9p82bN1O0aFE2btxIhw4dMu08Qggh3l4mXedx9OjReHp60r17dyZMmMDgwYNp1KgRAIGBgWzZsgUAOzs7FixYQHBwMG3btuXUqVMsXLjQ8Cb68ccf07hxY0aMGEGHDh1ISUlh4cKFuXp4u729PS4uLri4uGBnZ4darTY8d3FxSXdn5d8cHBwM8z69Sr58+V5rDgIhhBBCZJ6M9K9yo8zoPzk6OmaL/tODBw84fPgwAwcO5Pjx44SHh2fKeYQQQrzdTDaSCPSjiaZOncrUqVPT7Lt06VKq597e3qxfv/65cSwtLRk1ahSjRo3KlDyfR1EU4pNfvYycoijEJWnBLOW1J+p7dryNhdkb34f9zM2bN6lfvz5Dhgzhxx9/pGXLlowbN44FCxawevVq7t+/j6OjI++++y6DBg0C9LebVa1alSFDhjBq1CgcHBy4d+8ee/fuxdHRkQ8//JA2bdoA+uHSgwYNom3btnTr1o3q1atz5MgRTp48SaFChfj000+pWbMmAFFRUYwbN45Dhw6RL18+evfuzfjx49P8DKSHTqdjyZIlrFy5koiICHx8fPj0008pW7YsANu3b2fBggXcvn2bIkWKMHz4cBo0aADATz/9xNKlS4mMjKRMmTKMGTOGgIAAI7zaQgiRyyQ+RrV3CiVunIcKv4Am9xYdTO2/76UZ6V9lhqzuP1lbK0brO0HG+k8DBw4E9LebValShcGDB5u0/7Rt2zbs7e1p1aoVX3/9NRs3bjT08UA/XcNXX33F9u3bAWjUqBGffvopFhYWPHz4kLFjx3LgwAGsra1p164dH374Ibdu3aJ+/frs3r0bNzc3QD9y6tixYyxfvpx169axevVqnJycOHLkCJ9//jn16tVj8uTJ7Nu3j8ePH+Pm5sZHH31k6I89ePCASZMmpTrXsGHDmDRpEtHR0cyfP9+Q86RJk3j06BHTp0832vdYCCFeR3ySlqPXItl1MZYDD0LRKpCs1ZGsVUjW6kjR/euxViFJqyMl1X79v8kpOso66PD1Nd21mLRI9LZSFIX28w8TfD0qS88bUCwva/pVM2pn58SJE6xduxadTseGDRtYtmwZX3/9NUWKFOHgwYOMHz+eunXr4uHhkebYFStWMHToUEaMGMFPP/3E559/Tv369bG3t0/Tdv78+YwaNYqJEyfy9ddfM27cOPbs2YNarWb48OEkJiaycuVK7t27x9ixY1/7eubOncvKlSuZNGkSxYsXZ9GiRfTu3Zvt27cTFxfHuHHjmDhxIlWrVmXbtm0MHz6cAwcOcPv2baZNm8acOXMoXbo0P/30E8OGDePAgQNp5sgSQgjxEtf2wcbBqGNu4Kg2R0mIAUspEgnT9J8yo+8E6es/1alThxIlSqQ51lT9py1btlCnTh3UajX16tVjw4YNDBw40PDafPrpp1y6dInvv/8eKysrPv74Y7799ls++eQThg8fjoWFBT///DNPnjzhww8/JH/+/NSpU+eVr9XJkyfp168f/fr1w9XVlcmTJxMWFsaSJUuwtrbmhx9+YOzYsdSqVQsLCwsGDhyIRqNJdS4XFxcaN27MkCFDiI2Nxc7ODp1Ox/bt2/niiy/S900TQggjStHqOH0rhkNXIjl0NZIT16NJ0uqe7g19o9j3YtSG+QJNQYpEr8m4XQ3T6d69O0WLFgXg7t27TJkyhWrVqgHQuXNn5s6dy5UrV55bJCpbtiwffPABAEOHDuWnn37iypUrVKxYMU3b2rVr06pVK2xsbOjfvz+tW7cmIiKCuLg4/vrrL3bt2kWRIkUoV64cgwYN4vPPP8/wtSiKws8//8zw4cOpX78+oP+EqWHDhvz+++94eXmRkpJCwYIFcXV1JSgoiLJly2JpacmtW7dQqVQULlwYNzc3hg0bRt26ddHpdFIkEkKI9Eh4BDs/g+ClACiORblSfiil7TK2UpbI2XJT/yk0NPS5RSJT9J/u3r3LiRMn6NmzJ6AfJbRy5UqCg4MJCAggJiaGbdu2sXTpUvz9/QGYOHEiFy5c4NKlS5w+fdpwLoDx48cTFxeXrtdKpVLRr18/dDodNjY2VKpUiZ49e+Lu7g5AUFAQa9as4cGDB8TExHDy5MnnnisgIAAHBwf27NlDq1atOH78OMnJydSoUSNdeQghxJtQFIXQ+7H8GRrJodAHHL32gMeJKanaFHKwolQeKFrIBQszDWZqFeZmaszVKsw1asw0asw1+sf65yosnv5r/nSfGtA9vGH0DzcyQopEr0GlUrGmX7X0D5eOi8fGxvr1h0s/Pd6Yt5s94+rqanhctWpVTp06xcyZM7l69SoXLlwgIiICnU733GOLFy9ueGxnZwdASkrKc9sWK1bsuW0vXbqEo6OjoSMA4PuaY+sePHhAdHR0qlVZzM3NqVChAlevXqVjx44EBgYSFBREiRIlqF+/Ph06dMDa2prAwEDc3d1p2bIlHh4ehn1mZvJfRAghXunqHvh9CMQ8neOk0gfo6o0j9vybfZImchZT9J+cHOwypaOdnv6TVvv86zRF/2n79u1YWloSGBgIQOXKlXFwcGD9+vUEBARw/fp1tFotnp6ehmMCAgIICAhg69atODg4GG4nAwy3ht28efOF53zGyckJKysrQ1GpTZs27Nq1i9WrV3Pt2jXOnTsHgFarJSwsLM11NWjQ4On3M46mTZuybds2WrVqxdatW2nYsCHm5uavzEEIIV7H7eh4DoVG8tfVBxwKjeT+48RU+x2szaleyonqpZ0JLO1MEUdLTp06ha+v52vPj6zVagkJefXv1swkfwG/JpVKhY3Fq18+RVEgRfPaBZ43Pf5VLC0tDY/XrFnDl19+SYcOHWjUqBEjR47k/ffff+Gxz3tTftGwuBe1NTMzM9pQun9fy79ptVp0Oh0qlYpZs2YRGhrKnj172LlzJ7/88gu//PIL5cuXZ82aNRw7doy9e/eybt06Vq5cybp16yhQoIBR8hNCiBwn4RHs+BROLNM/dywGredCiZrwgj+QRe6W1f2nzPok9m3rP23fvp2EhATDKCHQ94+2bdvGuHHjXlpoedkHZs97ff9b8Ppv/+yTTz7h5MmTtG7dms6dO+Pi4kKnTp2A51/vvzVv3pz333+f2NhYdu7cKXMRCSGMKiY+mSM3E1h/4zx/XX3AtcgnqfZbmqmpXCIf1Uvpi0IehfOgUf//9+CLPhx420iRSBisXLmSgQMH0rt3bwAePXrEgwcPMvV+yFKlShETE0N4eLjhU6OzZ8++Vix7e3ucnZ0JCQmhXLlyACQnJ3Pu3Dlq1KjBtWvXWLlyJWPHjsXHx4dhw4bRvHlzDh48SEJCAkeOHKF///5UrVqVESNGUL16dYKDg2nWrJnRrlcIIXKM0N360UOPnn7aVbkP1P8cLO1Mm5cQWSy795/CwsK4ePEiY8eOpWrVqobtoaGhfPjhh+zcuZO6deui0Wi4ePGiYdGOXbt2MXfuXKZOnUpMTAx37tyhcOHCgH6xjyNHjjB+/HgAnjz5/x9SLxtdFBsby6ZNm1i9ejXe3t4A7N+/H9AXv4oVK0Z0dDR37tyhUKFCqc41ffp0fHx8KFCgAIsWLUJRFCpXrvw6L58QQgCQkKzl+D9RHLoayaHQSM7cikH/qzsaALUKvN0cqVHaiRqlnalYNC9W5jl/BXUpEgmDvHnzcvjwYerXr8+TJ0/45ptvSE5OJikpKdPOWaJECQIDAxkzZgxjx47lwYMHzJo165XHHThwINVzS0tLqlSpQo8ePZg1axb58+enWLFiLFq0iMTERJo1a0ZKSgpr1qwhX758tGzZktDQUG7duoWHhwdWVlbMnTsXZ2dnqlWrxt9//01cXJxhVTQhhBBPJcQ8HT30k/553uL60UPFA02alhCm8qL+U3JycqadMyP9p82bN+Pg4ECnTp1Sjepxd3dn7ty5bNiwgZYtW9KmTRsmT57MhAkTUKlUfPPNN9SqVYsyZcpQqVIlxo4dy6hRo4iOjmbhwoX0798fZ2dnChUqxOLFixk8eDB///03+/bte+5clgAWFhZYW1uzY8cO8uXLR1hYGBMnTgQgKSmJMmXKULVqVcaOHcvIkSMN5+rXr58hRrNmzVi6dCkdOnR47ds5hBC5k1ancOZWDIdC9UWh49ejSEpJPbWKq72Gep6u1CzjQpWSTjhY575bWqVIJAzGjBnDmDFjaN26NU5OTjRt2hRra2suXLiQqeedMmUK48aNo2PHjhQoUIC2bdvyww8/vPSYZxM+PlOgQAEOHDhAUFAQsbGxjBs3jtjYWPz8/Fi+fDn58uVDURRmzJjBnDlzmD9/Pk5OTgwfPtxwf/7kyZP5/vvvmThxIoULF2b69OmUKlUq065bCCHeOqG7no4euqV/XqUf1P8MLGxNm5cQJvSi/tP58+dp1apVpp03vf2nLVu20KxZMywsLNLs69y5M5MnT+bevXuMGTOGyZMn07NnT8zNzWnWrBkffvghAF988QXTp0+nU6dO2NnZ0alTJ9577z1UKhWTJ09m0qRJNGvWjGrVqtGvX780H+Y9Y2FhwfTp05k6dSrLly/Hzc2N/v378+2333LhwgVKlSrF9OnTmTBhQppzxcfHA/oi0fz582WktxDilRRF4WrEE/66GsmfVyI5cu0BjxJS3xJbII8lNUo7U6OUM1VL5OVu2EV8fT1ydRFaikS5QNu2bWnSpEmqbW5ubly6dCnVtlKlSvHrr78+N4aiKCxatAgbG/0yxl999VWaNv+Ot2fPHsPj5cuXGyYc/O+54+PjOXPmDHPmzDHch75161by53/+ajgBAQFcvHjxhXMMaDQaPvzwQ0On5r+qV69OgwYNnnt869atad269XOPE0KIXC0hBraPhZPL9c/zlng6ekhWFRI515v2n571fZYvX27od5ii/7Rly5YXrkTWtWtXunbtang+ZcoUpkyZkuY6XFxcmDNnznP7TzVq1GDbtm2ptj37MK9t27a0bds21a13DRo0MEx8/Uz79u0Nj/Pnz8/cuXPT5PBMZGQkrq6uz10NTgjx9rj3KIGQ8GhOhUdz6mY0l+4+Rq3Tkv/QX+S1tSCvjQV5bcxxtLHA0cacvP/6N6+NBY625thbpp237t6jBI6ERfFnaCR/hT7g7qOEVPvtrcyoVlJ/+1iN0s6UcrE1xNBqtdzNslcg+5IikTApS0tLxowZQ+fOnWnXrh2RkZHMnTuXxo0bmzo1IYQQAKE7YfPwp6OHVFC1P9QbBxY2ps5MiFwrN/afIiIiuHDhAgsWLKB9+/YmXR5aiJxAp1O48yiBsIjHnAxPIMUxCjcnWwrYW2KmURv1XI8TkjlzM4aQm0+LQuExaYo3z9yPe5TuuGZqFY425jhYm+NoY869h4+5+Th1mcfCTE1AsbyGolCFwnmMfn05jRSJhEmp1Wrmzp3LtGnTWLp0KXZ2drRq1eqFI4GEEEJkkYQYioVMQxP+dIRAvpLQ+nsoVs20eQkhcmX/6fHjx4wZMwZfX1969uxp6nSEeCPJWh0HLkew9sRNLt2MpPiZYAo6WFMwjxUFHKwomMeKgg5WFMhjRR6r11/lOjFFS/jDeG48fML1B3FcfxDHjYdx/PPgCTcfxpOk/dd8PEeOAvrJmgvksaKwo7X+y+Ffjx2tKOxgjaON+QtzSkrRcfHuI06FRxMSHsOpm9FcjYjlv3P5q1XgXsAe3yKO+BRxpFwBOy5euoSLa3FiErRExSURHZec6t+ouGRinv4bn6wlRacQGZtEZOz/59BVqcDL1cGwAllA8dwx2bQxSZFImFxAQACrV682dRpCCCEAIq/AuQ2ojy/G+fEdFFSoqg6Aep/K6CEhspHc1n8qWbIkJ06ckBFE4q2lKArn7zxibfAtfj91K1Vh42pUxAuPszbXPC0YWaYuIj197GxjzrWoZG6fuUt4dDw3HuiLQDcexHHnUUKa4sy/mWtUuDpaY0UycYoZd2ISSNYq3IlJ4E5MAsHXo16YU2HHp8UjB2sKOVhy+fojbh85zPk7j9NMBg3gltcanyKO+Lrpi0IVXPNgY/H/coRWq0UXaYFvufzpmg8oIVn7r+JREg9jE7l54x861KmIk731K48XLyZFIiGEECK3e1oY4vwGuKdfRlsFJNi6Yd5hIRqZe0gIIYR4LfcfJbAx5DZrT9zk4t3Hhu1Otha08imECzHYuxQm4nESdx8lcPdRIvdiErj7KIGYeP2ImbDIJ4RFPnnFmR48d6uthYaiTrYUy2dDMScbijrZUCyfLcWcbCjsaA2KjpCQEHx9fVGp1ETGJnIrOp47MQncjo7nVnQ8t//1PDI2ifhkLVcjnnA14vk5OVib6wtCRRzxLeKAt5sjznaWz237uqzMNRR00BfQQF9kCkm5i6NN2kn6RcZIkUgIIYTIjSJD4fx6fXHoaWEIALUZlKyDrnwrzmvL4FOkislSFEIIId5GCcladpy/x7oTNzlwOQLd09E8Fho1DT0K0LaiK7XcXVCjPC3QFH3u6Jn4JC33HukLRvceJXA3JvXje48Suf84AWszFaUK5KG4ky1FnxaDijnZUDSfLc52Fi8dgafV/v+xWq0ifx4r8uexwu8l13YnJoE7hgJSArei4njy6CEN/ErjVzQfxZxsZNTfW0yKREIIIXKOl42pFv8qDG2Ee2f+v/1pYQiPNlCuOdjkQ9FqUUJCTJSoEEII8XZRFIXj16NYG3yTzafv8Djx/0utVyzqSDt/N1p4FcbBxtywXfvvCs1zWFtoKO5sS3Fn2xe20Wq1hpFAWbFsu5W5hhLOtpT4V06GHHwK5+ql43MKKRIJIYTIGU4sR715BJ5WLqjCqoJbABSuCIW8wTwX35v+4CqcezZi6D+FoRK1wfMdQ2FICCGEyC3O3IphT1gctzR3sLU0x9pCg42FGTYWGqzNNU+fa7Ay06BWv3hUzN3YFGbtDmV9yG1uPIwzbHd1tKZtRVfaVnRLVVARIruTIpEQQoi335Vd8MdQVIoWqyc34exv+i8AlQbye4CrH7j66wtH+cuDxvzlMd9m0eEUvLIC9bGhqQtDKo1+xJBnGyjXQgpDQgghcqUNJ28x7NcQ/ZPjp17Z3tr8acHo6b82FvoiUnySllM3Y4BIQD//T1OvQrSr6EaVEvleWlwSIruSIpEQQoi3292zsKYHKFp0Xp24auNHKasY1HdC4PYJiL2nL5TcOwMnftIfY2YFBb3BtaK+aORaEfKVMuVVGM/lHahXv49rSrz+uUoDJZ+NGJLCkBBCiNztwOUIPlqjLwyVyWdOPgc74pN1xCVpiU/SEp+sJS4phYTk/6/QFZ+s3/48KqBGaSfa+bvR2LNgqhW7hHgbyU9wDvXee+9RqFAhZs6cmWbf77//zqRJkzh06BAWFs+f/f3mzZvUr1+f3bt34+bmRrly5Vi4cCG1atVK0/bo0aO8//77XLp0KV25bd26lcqVK+Pk5MTs2bM5duwYy5cvz9gFpkO9evUYNGgQbdu2NXpsIUQ28egO/NIRkh5D8ZooLb/j0ZnzKL6+oNHo5yh6dBtuBesLRrdOwO0QSIyBm8f0X89YOqAu7IuLrRf4eANv4T31Z36D9X1R6VKIdSyPTWBf1OVbga2TqTMT4q1gzP6Tq6srFStWZNmyZVStWjVN2+zafwKIi4ujevXqeHh4sGLFikw5hxCmcPpmNP1+DiZFp9DSuxDvu+uo6Of33Hl0dDrFUByKT9ISl6QvHv2/kKQlKUWLfdwd6lf3l7l4RI4hRaIcqnnz5nzzzTckJSVhbp76loqtW7fSqFGjF3ZwnufgwYMZav8it27dYtiwYezevRuAoKAgunXr9sZxhRC5UGKsvkD06BY4u0On5aD5z+8plQocXPVfHq3023Q6eHjt/0WjW8Fw9zQkxqAK209R9qMrWhIq98r6a3oTxxbBlo8BBV2F9lwq1gdfvwB9sUwIkS7G7j/t2LGDggULvnFeWd1/2rNnDy4uLpw4cYLw8HCcnKTQLN5+YZFP6Ln0b+KStASWdmZaOy/Onz39wvZqtQpbSzNsLV/8J7N+wub7mZGuECajNnUCInM0bdqU+Ph4Dh8+nGp7bGwsf/75Jy1atMhQPBcXlzSdpdeh/GflIVtbWxwdHd84rhAil9GmwG9B+uKOrQt0WQPWedN3rFoNzqXBuyM0/Qp674TRN6HvQXRV+gOg2vUZxNzKxAswIkWB/dNgy0eAApU+QGkzXz8xtRAiQ4zdf3J2djbKh2xZ3X/atGkTDRo0wN3dnQ0bNmTaeYTIKvcfJ/D+kqM8eJJEBdc8zO/mj4WZ/CksxPPI/4zXpSiQ9CSdX3EZaPuS4zOwtHO+fPmoVq0aO3bsSLV9165dODo6UqVKFe7du8eQIUOoVKkSFSpU4J133iE4OPi58cqVK8fx48cBfUdp+PDh+Pn50bhxY86cOZOqbXBwMJ07d8bHxwdfX18++OAD7t/XV9gbNGgAQP369Vm3bh2zZ89O9UnYyZMn6dy5M76+vtSrV4+VK1ca9o0aNYqZM2fy4Ycf4uPjQ+3atd+o4/Kyc92+fZugoCD8/PyoVq0akyZNIjk5GYDLly/z7rvv4uPjQ82aNZkzZ85r5yCEeA2KAttGwZXt+rmFOq+CvMXfLKbGHAp5ozSYSKxjeVRJsbB5eIZ+75qETgfbRsPeyfrntUdCs+mgkrd3kU1ldf8pg/+Hjd1/qlixIkePHgXenv5TTEwMf/75JwEBAdStW5eNGzemKVJt3LiRJk2a4OPjw7vvvsv58+cN+5YuXUq9evXw8/OjV69ehIeHA/DBBx8we/ZsQ7ubN29StmxZbt68CUDZsmX57rvvqFKlCv369QNgzZo1NGnSBC8vL+rVq8fEiRNTLRn+vHMFBwfj4eHBw4cPDe3Onj2Lr68vT548eeF1i5zrcUIyPZb8TfjDeIo52bC0R2XsXjI6SIjcTv53vA5FgSWNIfzoK5uqgDdZ8DDV8UWqQtA2/e0T6dCiRQu++uorJkyYYNi2bds2mjVrhlqt5qOPPiJPnjysWrUKRVGYMWMG48eP548//nhp3M8//5xr167x888/8/DhQ0aNGmXY9/jxY/r27UuPHj2YNm0a9+/fZ8yYMSxcuJDhw4ezZs0aOnTowJo1a3B3d2fRokWGY69evUr37t3p0aMHkydP5tSpU0yYMAFnZ2caNmwIwK+//sqwYcMYMWIEP/30E59//jn169fH3t4+Xa9Jes81adIkbGxs2LBhAw8ePGDIkCGULFmS9957j3HjxlGpUiVmzJhBWFgYQ4YMwcvLi9q1a2coByHEazoyD/5eBKig7UL9UvfGotZw3fcjPA70Q3V5G5xdC17tjRffmLQp8PsgOPX0j8EmX0HV/qbNSYiXMUH/Sclg3wmk/7Rjxw40Gg3Vq1fHxcWF+fPnc+LECWrWrAnopyAYO3YsY8eOpXr16ixfvpy+ffuye/du1q1bx5w5c5g0aRIeHh58/fXXDB06lLVr16brtd+7dy8rV65Ep9Nx7NgxvvjiC6ZPn0758uU5ceIEn376KdWqVaNRo0asWrXqhecqUKAAO3fupFOnToD+VsHatWtjayvLkOc2iSla+i4P5vydRzjbWfBTUGVc7C1NnZYQ2Zp81Pjasv9yhg0aNCAuLo6///4b0HdA/vzzT1q2bImiKDRo0IBx48ZRqlQpSpcuTZcuXQgNDX1pzMePH7N161Y+/fRTPD09qVmzJgMGDDDsT0hIYMCAAQwcOJAiRYrg7+9Po0aNDHHz5tXfDpIvXz6srKxSxV69ejUeHh4MHz6ckiVL8s4779C1a1d++OEHQxt3d3d69+5NkSJFGDp0KAkJCVy5ciXDr82rznXr1i3s7e0pXLgwFStWZOHChYYi0J07d3B0dMTV1ZVatWqxdOlSPDw8MpyDEOI1XNgE28foHzecCB6tjX6KBPsSKIHD9U+2joQnD4x+jjeWHA+ru+kLRCoNvLNACkTiLSH9p+zef9q8eTPVq1fH2toaLy8vChYsyKZNmwz7f/31V1q0aEHnzp0pVqwYn3zyCS1atCAmJoZff/2VHj160KxZM4oXL85nn31GlSpVSEhISNdr36lTJ0qWLEnp0qWxsbFh8uTJNGrUCDc3Nxo0aED58uUNeb/oXImJiTRr1oxt27YZ4j4r8oncRadTGL76FH9dfYCthYYfe1ammJMUCoV4FRlJ9DpUKv2nUslxr2yqKApxcfHY2FijysCnWM893sI2Q5+E2dnZUadOHXbs2IG3tze7du3Czc2NChUqANC5c2e2bNnCiRMnCAsL4+zZs+h0upfG/Oeff9BqtZQrV86wzcvLy/DYxcWFNm3a8OOPP3LhwgVCQ0O5dOkSfn5+r8z36tWreHt7p9rm5+fHqlWrDM+LFi2a6voAUlJSXhk7o+fq3bs3Y8aMYefOndSqVYtmzZrh4eGBoigEBQUxe/Zsfv31V+rUqUPr1q1xcXHJcA5CiAy6FQxrewMKBARB9cGZdiol8EO4+AfcPw/bR+tHLGUXCY9gZWe4/idoLKHjMijb1NRZCfFqpug/OThlqO8EmdN/CgsLeyv6TxERERw7doxJkyYBoFKpaNCgAevXryc+Ph4bGxvCwsJ49913DcdYWFgwcuRIw3V6enoa9jk7OzNy5Mg0t6u9iKurq+FxhQoVsLKyYtasWVy5coVLly5x48YNw4imF50L9KPBfvzxR6KioggPDycqKoo6deqkulVN5GyKojBx03k2n76DuUbFgm4BVHB1MHVaQrwVZCTR61KpwMI2nV82GWj7kuNfo5PUsmVLdu/ejaIobNu2zTDhok6nIygoiCVLllC4cGF69erFtGnTXuul+PeEjPfu3aNVq1YcOXIET09PxowZQ8+ePdMVx9Iy7dBPnU6X6g39eZNnp7fjkZFztWrVir179zJixAiePHnCkCFD+OabbwDo0aMHO3fu5IMPPiA8PJzu3buzZs2aDOcghMiAqOvwy7uQEg+lG0LT6a/1OzHdNBbQajaggtO/wpWdmXeujHgSCcta6AtElnmg2zopEIm3S1b3n17z90Ru7T9t3boVrVbLuHHj8PDwwMPDg19++YUnT56wa9cuAMzMXvwZ88v2/bfY97yCzb+v5eDBg7Rt25bIyEhq1arFtGnTqFixYrrOVb58eYoWLcquXbvYvn079evXf+7rJHKu7/dd5ce//gFgZkdfAss4mzYhId4iUiTK4WrXrm0YMn348GFDJyc0NJS///6bH3/8kX79+lGnTh3D5IgvK7qUKFECc3PzVJMt/nuywp07d+Lg4MCCBQvo3r07AQEBhIeHG2K+7NPAEiVKcOrUqVTbTp48SYkSJTJ+4a/wqnN98803PHjwgM6dO7NgwQKGDRvGjh07SExMZPr06Zibm9OzZ0+WL19Ox44d2b59u9FzFEI8FR+tX+r+yX0o4AUdloImCwbCugVA1ae3g/wxDBIfZ/45XyY6HJY0gTunwMYZuv8BxQNNm5MQOZSx+08lS5Z8K/pPW7ZsoVq1amzYsCHVl5ubG+vXrwegWLFiXLx40XCMVqulXr16BAcHp9kXFRVF1apVuXnzJmZmZqkmjn42ofWLrFmzhnbt2jFx4kTat29PiRIluHHjhuE1edm5QD+aaO/evezfv5/mzZtn+LUQb6/Vx8OZvv0SAONaeNDKp7CJMxLi7SJFohzOwsKChg0b8s033+Du7k7x4sUByJMnD2q1ms2bN3Pr1i22bdtmWHEiKSnphfHs7Oxo3bo1kyZN4tSpUxw9ejTV6l6Ojo7cvn2bw4cPEx4ezsKFC9mxY4dhZTAbGxsALl68mGaFiffee48LFy7w9ddfExYWxvr16/nll1/o0qXLa1//5cuXOXDgAAcOHODgwYMcOnSIqKioV57r2rVrTJw4kYsXL3LlyhX279+Ph4cHlpaWnDx5ki+++IJr165x5swZjh8/LnMSCZFZUpJg9fsQcRHsC8F7v4JlxiaqfyP1xoJjMXh0E3ZPzLrz/lfEZf2Evw+ugEMRCNoOhX1Nl48QOVxu7D/dvHmTkydP8u677+Lu7p7qq127dhw5coR79+7RrVs3fv/9d9avX8/169eZMmUKiqLg6elJt27dWLZsGbt27SIsLIzPP/8cNzc33Nzc8PT0ZNu2bZw+fZrTp08za9asl+bj6OjIyZMnuXTpEleuXOHzzz8nIiLC8Dq/7FygLxL9+eefREREUKNGjQy9FuLttfvCPUav0xdj+9YuSa9A43/YLEROJ0WiXKB58+ZcunTJ8CkYQMGCBRk/fjyLFi2iRYsWLFy4kE8//RQzM7NUn2w9z7hx4/Dz86Nnz56MGjWKrl27GvY1bdqUVq1aMWTIENq1a8fRo0cZOXIkV69eJSkpibx589KqVSuGDRuW5hatwoULs2DBAg4ePEjLli2ZN28eo0aNol27dq997UuXLuWDDz4wfA0ePJgLFy688lzjx4/H2dmZbt260bFjR/Lnz8/YsWMB+Oqrr4iLi6N9+/b06tWLgICAVJNPCiGMRFFg04cQth/MbfUFIgfXVx9nTBa20PI7/eNji+DGkaw9P8CtE7C0CTy6Bc7u+jldnEtnfR5C5DK5rf+0ZcsW8ubNS7169dLsa9WqFWZmZmzcuJFKlSrx+eefM3fuXFq1asWFCxeYP38+VlZWtG7dmqCgICZMmEDbtm1JTEw0FIO6du2Kh4cHXbt2ZcSIEa/sOw0aNAgnJyc6depEUFAQlpaWdO7cmQsXLgC89FygH2lUunRpGjZs+Nzb7UTOE3w9ioG/nECrU2hb0ZVRTcq9+iAhRFqKUFJSUpTjx48rKSkpafbFx8cr58+fV+Lj418rtk6nU2JjYxWdTmeS47NLjNyaw/N+fl7285Yeb3p8TsnBGDGyQw7GiJFjc9g/TVE+z6Mo4x0V5dL2LMnjhcevH6DPZXaAoiS9/P3AqDlc268okwvrz72gtqLERr7ZdbxuHiY43lSxRfpJ/yn752CMGG9rDlqtVqlVq5Zy+PDhl8bIyM9qdvi9mB1iZMccrtx7pPhM2K4UG7lJ6b7kqJKUos30HIwRQ3IwXgzJwXhxZSSREEKI7OfMb7DnC/3jptPAvZFp82n8BdgVgMjLcGB61pzz0hb4uT0kxUKJWvo5iGydsubcQgjxFtu3bx9ffvklVlZWVK5c2dTpiEx2NyaB9xcfIzouGZ8ijnzfpSLmGvkzV4jXJf97hBBCZC83jsCG/vrH1QZB5Q9Mmw+AdV5o9rQ4dOhbuHvmpc3fVL7w7ajXdAdtIpRrAe+tydq5mIQQ4i22ePFitm3bxuTJk1Gr5c+dnCwmPpnuS45xOyaBki62LO1RCRuLLFjcQogcTP4HCSGEyDYsY2+i3jUUtEn64kjDSaZO6f88WkP5lnDhD9g4CHrvNv4qazotqgPTKREyVf/ct6t+TqSsWM1NCCFyiOXLl5s6BZEFErUKfX8+waV7j8lvb8lPQZXJZ2th6rSEeOtJaV0IIUT2EPeA0sdGo4qPgsIVoe0iyG6fADebAVYOcCcEjnxv3NgxN2FZK9T7pwCgqzoQWs+RApEQQgjxH1qdwndHo/n7nyjsrcxYFlQZt7w2pk5LiBwhm/W+sy9FUUydgngLyc+NEOmUkoh6dTesntxCcSiiX8nMIht29uwLQqPJ+sd7J8ODq8aJe/53mFcDrv+JYmFHmO9IlIaTQKUyTnwhTETeB0V2Jz+jb6ep2y5x9FYiFmZqFr0fQPlCeUydkhA5hhSJXuHZkplxcXEmzkS8jZ793MjSq0K8xNOl7lXhR0gxs0XX+Vewy2/qrF7MryuUqA0pCfDHUH3+ryvpCfw+BFZ3g4RoKOyH7oN9PCzS2GjpCmEK0n8Sbwvpq719/gqNZPGhfwD4uoM3VUvKog5CGJOMYX8FjUaDo6Mj9+/fB8DGxgZVBj7ZVRSFxMRE1Gp1ho4z1vHZJUZuy0FRFOLi4rh//z6Ojo5oNJrXOp8QucLhORCyAkWl5pr/Z5RyKWfqjF5OpdLPEzSvOvxzEE4sA/8eGY9z5zSs7aVfMQ0VBA6DOmNApYEbIcbNWYgsJv2n7JGDMWLk1Bykr/Z2epSQzMe/nQagUUlrmlYoaOKMhMh5pEiUDgUL6n/5POvoZISiKCQnJ2Nubv7ab4pvcnx2iZFbc3B0dDT8/AghnuPyDtj5GQBKwy94bFnJxAmlU74SUO9T2D4GdoyDMo0gT+H0HavTwdF5sGu8foJu+0LwzgIoWVu/X6vNtLSFyEq5vf+UHXIwRoycnoP01d4uX2w6z63oeIrms+Z9H1n1U4jMIEWidFCpVBQqVIj8+fOTnJycoWO1Wi0XL16kdOnSr/UJxZsen11i5MYczM3N5VMpIV4m4pJ+JI2ig4rvo1TuC6dOmTqr9KvSD86uhVvBsHkEvPvLq+cQir0PG/pD6C7987LNodVssJWh8iLnye39p+yQgzFi5OQcpK/2dtl1/h6rj99EpYJp7bwwj75h6pSEyJGkSJQBGo0mw28k2qefCFtZWb32m+KbHJ9dYkgOQohU4h7CL50g8REUrQ7NZr59kzSrNdBqDiyoBZe2wLn1UKHti9tf2akvED2JADMraPwlBAS9fdctRAbl1v5TdsjBGDEkB5EdRD1JYtS6MwB8ULMklYrnIyREikRCZAaZuFoIIUTW0ibDmh4QFQYORaHTcjCzMHVWr6eAB9Qcrn+89RN98eu/UhJh6yhY0V5fIMrvCX32QaVeUiASQggh0uHTjWeJjE2kTH47hjd0N3U6QuRoUiQSQgiRtbaNhrD9YG4LnVeCrbOpM3ozNUeASzl9AWj7mNT7Ii7Bovr6OYhAf4vaB3sgf/msz1MIIYR4C/1x6jabT99Bo1bxdUdfrMxlJJgQmUmKREIIIbLO34vh70WACtotgoIVTJ3RmzOz1M8rhApOrYSru0FRUJ34ERbUhntnwMYZ3lsNTaeCuZWpMxZCCCHeCvcfJTBu41kABtUtjZebg4kzEiLnkzmJhBBCZI2wg/pbskC/Mli55qbNx5iKVNaPEjo6D/XmDylpXRz13T/1+0rVgzbzwF5WzxFCCCHSS1EURq07Q3RcMhVc8zCoXmlTpyREriBFIiGEEJnvYRis7ga6FKjQXn+LVk5T71O4uBlVzA3yxtxEUZujajAeqg4AtQzcFUIIITJi9fFw9ly8j4WZmq87+mKukfdSIbKC/E8TQgiRuRIewcp3IT4KCleE1nNy5oTNlnbQahaKxoIEWzd0QTuh+iApEAkhhBAZFP4wjol/nAfgo0buuBewN3FGQuQeMpJICCFE5tFpYd0HEHER7AvBu7+AubWps8o8peqiG3aOc5f+wbeQt6mzEUIIId46Op3Cx7+d4kmSlkrF89IrsKSpUxIiV5GPN4UQQmSe3RPg8jYws4J3V0CeQqbOKPPZOIFKVl4RQgghXseyw/9w5NpDrM01zOjgg0adA0cfC5GNSZFICCFE5ji1Cg59p3/cei64+ps2HyGEEEJka1cjYvlq60UAxjQvTzEnWxNnJETuI0UiIYQQxhf+N/w+WP+45gjwam/afITIRRITExkzZgwBAQEEBgayZMmSF7b9888/adWqFX5+fvTo0YNr165lYaZCCPF/KVodI1afIjFFR80yznStUtTUKQmRK0mRSAghhHHF3IJV74E2Cco2h7qfmjojIXKVadOmcfbsWZYtW8bnn3/OnDlz2LZtW5p2V65coW/fvtSvX5+1a9fi4eFB9+7defLkiQmyFkLkdgsOXCMkPBp7KzOmtfdGlRMXuRDiLSBFIiGEEMaTHAerOsOT+5DfE9oulNW9hMhCcXFxrFmzhrFjx+Lp6UnDhg3p3bs3K1asSNN25cqV+Pn5MXToUEqWLMnHH3+Mvb09f/zxhwkyF0LkZudvP+LbXZcBmNDKk0IOOXiRCyGyOem5CyGEMA5FQfX7ILhzSj95c+eV+mXhhRBZ5uLFi6SkpODn52fY5u/vz6lTp9DpdKnahoeH4+39/1X4VCoV7u7uhISEZFW6QghBYoqW4atDSNYqNPIowDt+rqZOSYhczczUCQghhMgZCl75GfWlDaA2h47LIW8xU6ckRK4TERFB3rx5sbCwMGxzdnYmMTGR6Oho8uXLl2r7vXv3Uh1/9+5dHBwcMnROrVb7Zkm/JObrxn7T43NKDsaIITkYL0Z2yMEYMYydw7e7r3Lx7mPy2ZgzqbVHmoJ2VuRgqhiSg/FiSA7pi5seUiQSQgjxxlSnfsH10lL9k+YzoXgN0yYkRC4VHx+fqkAEGJ4nJSWl2t60aVMGDBhAixYtqFmzJn/88QdnzpyhSpUqGTrnmTNn3izpTIxtjNxyQg7GiCE5GC9GdsjBGDGMkcPafcEs2P8QgF4+ttwMvcDNLM4hO8SQHIwXQ3J4c1IkEkII8friHsLmEajPrQNAV7kvav/uJk5KiNzL0tIyTTHo2XMrK6tU22vVqsXAgQMZPHgwWq2WKlWq0Lp1a2JjYzN0Ti8vLzQazZsl/h9arZYzZ868duw3PT6n5GCMGJKD8WJkhxyMEcNYORw/eZqFpxLQAW18C9OvhfcrjzN2DqaOITkYL4bkkL646SFFIiGEEK/n0jb4YwjE3kNRabhTpgsFGn5h6qyEyNUKFChAVFQUKSkpmJnpu3kRERFYWVmRJ0+eNO379+9Pr169ePz4MU5OTgwdOhRX14zNB6LRaIxeJDJWbGPklhNyMEYMycF4MbJDDsaI8abH/3zmMWEP4iiYx4oJrSq8Vqzs8DoYI4bkYLwYksObk4mrhRBCZEzCI9g4EFZ2gth74FwWXdAO7pTtAWrTvJkJIfTKly+PmZlZqsmng4OD8fLyQv2flQY3bdrE5MmTsbCwwMnJiYSEBI4ePZrh282EECKjDl97wJbQOACmtvfGwcbcxBkJIZ6RIpEQQoj0u7Yf5lWHkz8DKqg2CPruh8J+rzxUCJH5rK2tadOmDePHj+f06dPs2rWLJUuW8P777wP6UUUJCQkAFC9enFWrVrFjxw7++ecfRowYQaFChahVq5YpL0EIkcM9fJLEyLX62146VypCbXcXE2ckhPg3KRIJIYR4taQ42PIJ/NQKYsIhb3HouQUaTwZza1NnJ4T4l9GjR+Pp6Un37t2ZMGECgwcPplGjRgAEBgayZcsWACpUqMD48eP56quvaNu2LQALFixIM+JICCGM5Z/IJ7Sb9xe3ohPIb6thdNOypk5JCPEfMieREEKIlws/Buv7wcOr+ucBQdBwEljamTYvIcRzWVtbM3XqVKZOnZpm36VLl1I9b9euHe3atcuq1IQQuVjw9Yf0XnacqLhkXB2t+KSKHbaW8ueoENmN/K8UQgjxfCmJsG8KHPoOFB3YF4bWs6F0A1NnJoQQQoi3yKbTtxm++hRJKTp83BxY0LUit65eMHVaQojnkCKREEKItO6c1o8eun9O/9z7XWg6FawdTZqWEEIIId4eiqIwf/81pm67CEAjjwJ8964fFhq4ZeLchBDPZ9KbzhMTExkzZgwBAQEEBgayZMmSF7Y9f/48HTp0wMfHh3bt2nH27Nnnttu6dStly8q9rUII8Vp0KbB/Oiyqqy8Q2ThDp5+h7QIpEAkhhBAi3ZK1OsasP2MoEAXVKMG8rv5YW8hKqEJkZyYtEk2bNo2zZ8+ybNkyPv/8c+bMmcO2bdvStIuLi6NPnz4EBASwbt06/Pz86Nu3L3FxcanaPXr0iMmTJ2dV+kIIkaNYPr6BemkT2PuFvlhUviUMPKr/VwghhBAinR4nJNNr2XFWHgtHrYLxLT34rKUHGrXK1KkJIV7BZEWiuLg41qxZw9ixY/H09KRhw4b07t2bFStWpGm7ZcsWLC0t+eSTTyhVqhRjx47F1tY2TUFp2rRpFClSJKsuQQghcgZFQXV0Hh4H+qC6fQKsHKDtIui4HGydTZ2dEEIIId4it6Pj6TD/MAcuR2BtrmFBtwB61Chh6rSEEOlksiLRxYsXSUlJwc/Pz7DN39+fU6dOodPpUrU9deoU/v7+qFT6yrNKpaJixYqEhIQY2hw7doxjx47Rr1+/LMlfCCFyjGMLUe8Yi1qXhFKqPgw4At4dQSWf9gkhhBAi/c7eiuGd7w9x8e5jXOwtWd23Gg09Cpg6LSFEBphs4uqIiAjy5s2LhYWFYZuzszOJiYlER0eTL1++VG1Lly6d6ngnJyeuXLkCQFJSEuPGjeOzzz7D3Nz8tXPSarWvfeyrYr5u7Dc9PrvEkByMF0NyMF6M7JCDMWK80fEPQlHv/BwVcKtsT1zaTkVjZgYZjJUdXgdjxJAcjBfDGDm8KrYQQojsY+/F+wz85QRxSVrcC9ixpEcl3PLamDotIUQGmaxIFB8fn6pABBieJyUlpavts3Zz587F09OTwMBAjh49+to5nTlz5rWPzezYxsgtO8SQHIwXQ3IwXozskIMxYmT4eEVL2UNDsUuJ55GzP3fLdOHuCxYFyLQcsmkMycF4MTLzvVUIIUT2sPzwP3z++zl0CtQo7cS8rv7ksXr9D++FEKZjsiKRpaVlmmLQs+dWVlbpamtlZcXly5dZvXo1f/zxxxvn5OXlhUZj3Nn2tVotZ86cee3Yb3p8dokhORgvhuRgvBjZIQdjxHjd41V/zUIddR7F0h6rTj/AjSh5LSUHo8UwRg6vii2EEMK0dDqFKVsvsOhgGAAd/N2Y/I4XFmYmXR9JCPEGTFYkKlCgAFFRUaSkpGBmpk8jIiICKysr8uTJk6ZtZGRkqm2RkZHkz5+fHTt2EBMTQ8OGDYH/D0H38/NjwoQJtGrVKt05aTQao3dkjRXbGLllhxiSg/FiSA7Gi5EdcjBGjAwdf+887PsSAFWTr9DkKwY3ouS1lByMHiMz31uFEEKYTnySlo9+C2HbubsAfNTInYF1SxvmkRVCvJ1MViQqX748ZmZmhISEEBAQAEBwcDBeXl6o1akrzz4+PixatAhFUVCpVCiKwokTJ+jXrx/169enZcv/L8986tQpPv74YzZs2ICTk1OWXpMQQrwVtMmwvi9ok8C9Cfh2gf8sGCCEEEII8SIxCVq6LjlGSHgMFho10zt409rX1dRpCSGMwGTjAK2trWnTpg3jx4/n9OnT7Nq1iyVLlvD+++8D+lFFCQkJADRp0oRHjx4xefJkQkNDmTx5MvHx8TRt2hRHR0eKFStm+CpQQD97frFixbCzszPV5QkhRPZ1cCbcPQ1WjtDyO1nFTAghhBDpduneY0bteUhIeAwO1uYs71VZCkRC5CAmvVl09OjReHp60r17dyZMmMDgwYNp1KgRAIGBgWzZsgUAOzs7FixYQHBwMG3btuXUqVMsXLgQGxuZLV8IITLk9kk4MF3/uPlMsC9o2nyEEEIIke1FPUni5yPXaT/vL5rNOsT9J1qK5rNm3YDqVCkpd28IkZOY7HYz0I8mmjp1KlOnTk2z79KlS6mee3t7s379+lfGrFKlSppjhRBCAMkJsL4/6FLAow1UaGfqjIQQQgiRTSUka9l94T4bQm6x79J9krUKoB+AXLGgJfN6ViN/HmsTZymEMDaTFomEEEJkoX1fQsQFsHWB5l/LbWZCCCGESEWnUzgS9oANJ2+x9cxdHiemGPZ5FMrDO36uNKtQgLthF3GytTBhpkKIzCJFIiGEyA1uHIW/Zusft/wObGVouBBCCCH0Lt59xPqTt/g95DZ3YhIM210drWnlW5g2vq6ULWgP6FeTvmuqRIUQmU6KREIIkdMlPYEN/UDRgU9nKNfc1BkJIYQQwsTuxCSw6cxdNpy8xcW7jw3b7a3MaOFdiDa+rlQqng+1WkYeC5GbSJFICCFyul0T4OE1sC8MTb4ydTZCCCGEMJGEZC0bTtzk5z8fcu63fSj6aYaw0KipW86Fd/xcqVM2P1bmGtMmKoQwGSkSCSFETnZtPxxboH/cejZYO5o0HSGEEEKYxl+hkYzdcJawyCeGbZVL5KONryvNvAriaCNzDAkhpEgkhBA5V8Ij2DhI/9i/J5RuYNp8hBBCCJHlImMTmbz5AutP3gLAxc6SRsXN6dvEn6LOdibOTgiR3UiRSAghcqodYyHmBjgWg0aTTJ2NEEIIIbKQTqew+ng4U7ZeJCY+GZUKulUtxvAGpbl68RyueWX5eiFEWlIkEkKInOjyDjjxE6CCNt+Dpb2pMxJCCCFEFrl87zFj15/h73+iAChfKA9T2nrhW8QRrVZr4uyEENmZFImEECKniXsIvw/WP67aH4oHmjYfIYQQQmSJhGQts/dcYcH+a6ToFGwsNAxv6E6P6sUx06hNnZ4Q4i0gRSIhhMhpto6E2LvgVAbqf2bqbIQQQgiRBfZfjmDchrPceBgHQIPyBZjQ2hNXR7mtTAiRflIkEkKInOT8RjizGlRqeGc+mEvHUAghhMjJ7j9OYNKmC/xx6jYABfNYMaG1J409C5o4MyHE20iKREIIkVM8iYBNH+of1xgGbgEmTUcIIYQQmUenU/jl2A2mbrvI44QU1CroUb0Ewxu5Y2cpf+YJIV6P/PYQQoicQFFQbx4BcQ8gvyfUGWXqjIQQQgiRSS7efcynG89x8kY0AF6uDkxp60UFVwfTJiaEeOtJkUgIIXKAfLd2o7q0CdRm+tvMzCxNnZIQQgghjCwuKYWfTj9m09q/0OoU7CzN+KiRO92qFUejVpk6PSFEDiBFIiGEeNs9uk2Rs7P0j2uPgkLeps1HCCGEEEYXm5hC67mHuRb5BICmFQryeUtPCjpYmTgzIUROIkUiIYR4W+h08Pg2PLgKD6/Bw6vwMAz17ZOokmNRCvmhCvzQ1FkKIYQQIhP8+nc41yKf4GipZnpHXxp6FjJ1SkKIHEiKREIIkZ3odPDo5tMi0LWnBaEwQ0EIbWKaQ1RAirkdqtZz0Wjk17oQQgiR06RodSw9FAZAJ0876pXLb+KMhBA5lfw1IYQQpnTvHKqQlZS6Foz6cCREXX9uIchAbQaOxcCpFOQrCflKonUsztkH5ni5lMu6vIUQQgiRZXacv8fNqHjy2phTu5i1qdMRQuRgUiQSQghTuH0SDsyAi5tQA47/3qc2h7zFDUUgfUGoBOQrBQ5F4L+jhbRatLEhWZW5EEIIIbLYooPXAOhSpSiWZk9MnI0QIieTIpEQQmSl8GNwYDpc2fF0gwqlXAvCzUvi6l0LjXNpyOOWthAkhBBCiFwp+HoUJ29EY6FR07VKUW5dvWDqlIQQOZj8FSKEEFnhnz9h/zQI269/rlKDVwcIHI7OqQwRISG4lvQFjcakaQohhBAie/nh6SiiNn6FcbG35JaJ8xFC5GxSJBJCiMyiKHB1j/62sht/6bepzcDnXQgcrr+NDECrNV2OQgghhMi2bjyIY/u5uwD0Cixp4myEELmBFImEEMLYFAUub9ffVnbruH6bxgL8ukGNoZC3mGnzE0IIIcRbYcmhMHQK1HJ3oWxBe7TywZIQIpNJkUgIIYxFp4OLf+iLQ3fP6LeZWUNAT6g+GPIUNm1+QgghhHhrxMQns/p4OAC9A0uYOBshRG4hRSIhhHhTihbV2bXw59cQ8XQySXNbqNwbqg0Cu/ymzU8IIYQQb51Vx24Ql6SlbAF7apZxNnU6QohcQopEuUHCIyxjw+GJK9jkAzMLU2ckRI6hOrsWz70TUD+5qd9g6QBV+kLV/vr/b0IIIYQQGZSs1fHjX/8A0KtmCVQqlWkTEkLkGlIkyskiLsORuahPraJCSgLsfbrd3AasHMHKAayf/mvl8Jxt/36e10QXIUQ2dvJn1BsHYgUo1nlRVRsIlT7Q/58RQgghhHhNW87c4U5MAs52lrT2ldvVhRBZR4pEOY2iQNgBODwXrmwHQAVoNVZotAn6Nslx+q/Ht9MdVgMULdYSfH40espCvJWuH4Y/hgFwr0RbnDt+i8bawbQ5CSGEEOKtpygKi54ue9+9WjEszTQmzkgIkZtIkSizaZNBl5L550lJgnPr4PCc/0+YiwrKNkNbpT8hD63x9fFGk/IE4qMhIQYSnv77wuf/36bE3sfl+h/oDn0DtT/O/OsRIjuLug6/dgVdMrryrblZagDOFnamzkoIIYQQOcDRsIecvfUIK3M1XarKiqhCiKwlRaLM9CQS9eyK+CYnob4UCCXrQMnakN8T1GrjnCPuIQQvhaMLIfaufpuZNfh1gaoDwKkUaLUQFQJqDVjn1X9lkHLsB1RbRqDe+wU4lwbPd4yTvxBvm8RYWNkZ4iKhkA9K67lw7rKpsxJCCCFEDvHD01FE7Sq6kc9W5hIVQmQtKRJlJo0F2OZH8+AKhO7UfwHYOEGJWlCitr5olLcEZHQyugdX4cg8CFmhv3UMwK4gVOkD/j2NPmGu4t+Texf+okDYWljfDxyKgpu/Uc8hRLan08G6PnD/HNgVgHdX6uf4EkIIIYQwgmsRsey6cB+AIFn2XghhAlIkykxWedD1P8ylA+spZ3kXddgBuP4XxD2Ac+v1X6AvuJSsBSXq6ItH9gWeH09R4MZh+GsOXNoCKPrtBbyg2kCo0C5TVy676dmP/GaxqK5sh5Xvwgd7wLFIpp1PiGxnzyS4tBk0lvDuL+Dgqh+pJ4QQQghhBEsOhQHQoHx+SrnIrexCiKwnRaLMplIT71AKxbcd1BiinzvoVjCE7Ydr++Hm3xBzA07+rP8CcCn3/1FGRaqDLgXV2bVw9Hu4ffL/scs01heHStTK+Eik17oWDbp3FqJZ1hzundUXioK2gaV95p9bCFM7vRr+/Fr/uPUccAswbT5CCCGEyFGiniTxW/BNAHoFljRxNkKI3EqKRFnNzAKKVdN/1RkFSU/0qySF7dMXje6egYiL+q9jC1Cr1PiY2aFOfvT0eCvweReqDgQX96zP39IeOq+CRfX0haLfekHnlfr5joTIqW4eh42D9I8Dh4N3R9PmI4QQQogcZ8XR6yQk6/AsnIeqJY07dYQQQqSXkWZPFq/NwhbKNIBGX0C/g/DJNeiwDAKCIF8pVIoOs+RHKLYuUHcsfHgOWn5nmgLRM45F9IUiMyu4sh12fGq6XITIbDE39RNVaxOhbHOoN87UGQkhxEslJiYyZswYAgICCAwMZMmSJS9su3PnTpo2bYqfnx+dO3fm3LlzWZipEOKZxBQtyw5fB+CDmiVRZcVdAkII8Rwykii7sckHnm30X4D24XVC/95F6TrvorG0NWlqqbj5wzvzYU0POPI9OJWGSr1MnZUQxpX0RF8genIfClSAtguNtzKhEEJkkmnTpnH27FmWLVvG7du3GTlyJIULF6ZJkyap2l25coURI0YwceJEKlasyI8//kjfvn3ZuXMn1tbWJspeiNzp95DbRDxOpGAeK5p7FzJ1OkKIXEz+2snuHNyIdfbVj9oxkbikFELCo4lP0aXe4fkO1Hs6imjLxxC6O+uTEyKz6HSwoT/cPQ02zvrbKi1lAkkhRPYWFxfHmjVrGDt2LJ6enjRs2JDevXuzYsWKNG0PHTpE6dKladOmDUWLFmX48OFEREQQGhpqgsyFyL0URWHxn/oJq3vUKI65Rv5EE0KYjvwGykTJWh3f7b7C1tAnJP63wJLNxcQns/7kTfouP07FSTtpN/8Iw7ZFcujqg9QNa34EPp1B0epHFd2/aJJ8hTC6/VPh/EZQm0Onn8GxqKkzEkKIV7p48SIpKSn4+fkZtvn7+3Pq1Cl0utR9EUdHR0JDQwkODkan07Fu3Trs7OwoWlR+3wmRlf4MjeTi3cfYWGjoXEn+/wkhTEtuN8tE9x8nMmvPVQC2/3OQkU3L0dyrULa9xzjicSI7z99j27m7/BUaSYpOMeyzNFMTGa/j/SV/071aMUY1LY+1hUa/qlrL7yDqOtz4C37pCB/sAVtnE16JEG/o7DrY/5X+ccvv9BPNCyHEWyAiIoK8efNiYWFh2Obs7ExiYiLR0dHky/f/yXCbNWvGnj17eO+999BoNKjVahYsWICDg0OGzqnVao2W/39jvm7sNz0+p+RgjBiSg/FivOj4hQeuAdDB3w07S/VL42fn68htORgjhuRgvBiSQ/ripocUiTKRq6M1M9p78cWmc4RHxTPol5P8UCSMsc3LU6l49lix4GZUHNvP3WP72bv8ff0hyv/rQpQtYE/jCgVp4lmQInkt+fjnv9h2NY5lh69z4EokMzr44F8sL5hZ6kda/FAPov6BVV2g++/67UK8bW6fhA0D9I+rDQK/LqbNRwghMiA+Pj5VgQgwPE9KSkq1PSoqioiICD777DN8fHxYuXIlo0ePZv369Tg5OaX7nGfOnHnzxDMptjFyywk5GCOG5GC8GP8+/kZMMgevPEAFVHZ8QkhISJbkYIwYkoPxYkgOxoshObw5KRJlsnf8XCmovcffMXlY9GcYIeHRdJh/mEYeBRjVtBwlXbJ+jpPQ+4/Zfu4e287e5cytmFT7fIo40sSzII09C6TKTavV8kHFPHSu5cGodWcJi3xCh/l/0bd2KYY1KIOlrRO8twZ+aADhR+D3wfDOAv1IIyHeFo/vwsr3ICUeyjSChhNNnZEQQmSIpaVlmmLQs+dWVqnnN5wxYwbu7u506aIvhk+aNImmTZuydu1a+vTpk+5zenl5odFo3jDz1LRaLWfOnHnt2G96fE7JwRgxJAfjxXje8b+uOwtAI88CNAn0e9nhRsnBGDEkB+PFkByMF0NySF/c9JAiURawNlMzpH5pulQrxjc7r/Dr3zfYcf4eey7e570qRRlavwxOdpk36kZRFK5FJbNrx2V2nL/H1Ygnhn1qFVQukY8mngVp5FmQwo4vX80ksLQz24bVYsIf51h34hbz9l1l78X7fN3RF4/C7tBxGfzcDk7/ql/xrPYnmXZdQhhVcrx+JbPHt8GlHLRbDGrj/tEjhBCZrUCBAkRFRZGSkoKZmb6bFxERgZWVFXny5EnV9ty5c3Tr1s3wXK1WU65cOW7fvp2hc2o0GqMXiYwV2xi55YQcjBFDcjBejGfHRzxOZEOI/v9bn1olMxQzO11Hbs/BGDEkB+PFkBzenBSJslB+eyumtPUiqEZxvtp6kd0X7/PT4eusO3GL/nVKEVSjhH6eHyOIjE3kUGgkB69E8ueVCO4+SgT0k06ba1TUKO1ME8+CNPAogHMGC1QO1uZ83dGXRh4FGbv+DBfvPqb13D8Z1sCdvrVqY9Z8JmwaBnsng1MpqNDOKNckRKZRFP3ot9snwDqvfiUzqzyvPk4IIbKZ8uXLY2ZmRkhICAEBAQAEBwfj5eWFWp16vZL8+fNz9erVVNvCwsLw8vLKsnyFyM2WH7lOklaHbxFHKhbNa+p0hBACkCKRSZQpYM/iHpX462okX265wNlbj5i+/RI/H7nOiEZlecfPFY06Y7dpJSRr+fufh/x5RV8YOn/nUar9lhoVdcvlp6lXIeqWy08eK/M3vo4mFQoSUDwvY9efYfu5e0zffoldF+4xs0MHSlYLhcNz9HO7OBaDQq8ePiuEqagOfQNn1oDaDDouh3wlTZ2SEEK8Fmtra9q0acP48eP58ssvuX//PkuWLGHKlCmAflSRvb09VlZWdOzYkVGjRlGhQgX8/PxYs2YNt2/f5p133jHxVQiR8yUka/n5yHUAPqhZMtsubCOEyH2kSGRC1Us58/vAQH4/dZvp2y9xKzqej9acYvGfYYxpVo6aZVxeeKxOp3Dh7iP+vBLJn6GRHAt7SGJK6qVtyxfKQ80yzlQvmQ+LR+FU8fcz+pA1ZztL5nf1Z/3JW3z++zlO3oim2ayDjG7ck/fdQ1Fd3gYr34WgXUY9rxDG4njnIOrjX+ifNJsBJWqaNiEhhHhDo0ePZvz48XTv3h07OzsGDx5Mo0aNAAgMDGTKlCm0bduWZs2a8eTJExYsWMDdu3cpX748y5Yty9Ck1UKI17PuxC0ePknC1dGaxp4FTJ2OEEIYSJHIxNRqFW38XGlSoSDL/vqHOXtDuXDnEd0WH6O2uwsjG7sb2t6JiX96+1gkh0IjefAk9cSUBfJYEljahVruzlQv5YyLvf42Mq1WS0jIzUy7BpVKRduKblQt6cTItac5eCWSzzddYl+JfixwDsci8hzqVe+i9p+WaTkI8VrunqX4Sf2n61TuCwE9TZuPEEIYgbW1NVOnTmXq1Klp9l26dCnV8w4dOtChQ4esSk0Igf7D3h/+1C97HxRYAjON+hVHCCFE1pEiUTZhZa6hb+1SdAwowqw9V/j5yHX2X47g4JUI/AtZErXvIKH/mnAawMZCQ9WSTgSWdqZmGWdK57cz6VDVwo7W/BRUmZ+PXOfLLRfZGxZHM8uB/GH1GdYRFyh5YhL4bwYTTcAlRCpR/6Be1QmVNgGlZB1Ujb80dUZCCCGEyAX2X47gWsQT7C3N6BjgZup0hBAiFSkSZTN5bS34vKUnPaoXZ9q2S2w+c4e/bycCiahV4OXmSM3SzgSWcaZi0bxYmGWvTx5UKhXdqhWnZhkXRqw5RfB16JQ0lDVWX+Bw/xi6Ez9ClfQvqytEpnh8F35qjerxHeLtS2DRdgkajfw6FEIIIUTmW3zoHwDerVwEeyPMEyqEEMYkfxVlU8WcbJnbpSI9wiJZc/AstX3KEFgmPw42b8cbSXFnW1b3rcbCA9f4ZqeayUnvMtF8GcreKeDdEawdTZ2iyK3iHsLydyDqH5S8xbkcMI0K8vMohBBCiCwQFp3M4WsP0ahV9KhRwtTpCCFEGtlrGIpIo2LRvHTytKdJhYJvTYHoGY1aRf86pfh9cA2OO7UmVFcYTcJDdAdmmDo1kVslxsKKDnD/PNgVRNdlPSlWMkGrEEIIIbLGH5f100c08yqEq6O1ibMRQoi0pEgkMl25gnlY0L0K05WuAChH5sHDaybOSpicogNdStadLyURVr0Ht46DdV54fwPkLZZ15xdCCCFErnY3JoE/byQA8EFNGUUkhMiepEgkskRhR2tKetXkgNYLjZJC7Kaxpk5JmNI/h1B/XxnvnR3h8vbMP582BX4LgrD9YGEHXdZC/vKZf14hhBBC5HqKohB8PYoxG86iVaBS8bx4uzmaOi0hhHgumZNIZJn6JW1Y+2AQNe72w+7aFrRhf6IpEWjqtERWSoqDPZPgyDxUKJgD/NoZqg+B+p+BJhNuqdTp4I8hcHETaCzh3V/Azd/45xFCCCGE+Jd7jxJYd+IWa4LDufZ0lWIV0L92SdMmJoQQLyFFIpFlVCoVAzq2YO2s+nRkFw/XfYTLh3+BWga05Qrhx2BDf3gQCoDOrxuR0bHkD1sPf82CG0eg/RJwLGK8cyoK7BgLIStApYEOS6FkbePFF0IIIYT4l6QUHbsv3GNN8E32XbqPTtFvtzbX0LRCAfwdE6jt7mLaJIUQ4iWkSCSyVGFHaywafsqjnYdweXyBe3/+SIFaQaZOS2Sm5ATY9yX8NVs/D5F9IWg1G6VkPcJDQnDyb4PmjyFw8xgsqAlt5kPZJsY59/5pcOR7/ePWc6Fcc+PEFUIIIYT4l/O3H7EmOJwNJ28RFZds2B5QLC8dAtxo7l0YazMVISEhpktSCCHSQYpEIsu1ruHL6uNd6BT9A2b7vkBbuSMaKztTpyUyw+2TsL4fRFzUP/fuBE2n6ieO1mr128q3hMI+8FtPffuVnYxz+9mR+friFEDTaeDb+c2uRQghhBDiX6LjktgYcpvVx8M5d/uRYXuBPJa0rehGe383Srn8v4+rfdb3EUKIbEyKRCLLqVQqanX7lJuzfsdNd5/jq8YT0GOGqdMSxpSSBAemw8GZoGjB1gVafAvlWzy/fb4SELQddn4GR+e/+e1nISth20j94zpjoErf174UIYQQQohntIrCvksRrDt5m53n75Gk1QFgrlHR0KMAHQKKULO0M2YamU5BCPF2kiKRMIlCTnn5y38kbidG4Bm2jLBrH1CiZFlTpyWM4e5Z2NAP7p7RP/doA82/Blunlx9nZqkfZVSsBmwc9Pq3n13YBBsH6h9XHQC1P3mtyxBCCCGEeCZZq2PO7lCWH47gYfw9w3aPQnnoGOBGa19X8tpamDBDIYQwDikSCZOp1iKIy+eW4p54ln9+HUWRT9bKpy5vM20KHPoG9k0FXTJY54PmM6FC24zF8WgFBb1e7/aza/v0xyla8O0CjSaDSvXalySEEEIIAfD93qt8t0e/+EZeG3Na+7rSIcANz8IOJs5MCCGMS/4iFyajUqvJ11Z/m1ndxD1s2LzJxBllkbtnUGmTTJ2FcUVcgsUNYc8X+gJR2eYw4EjGC0TPPLv9rEo//fO/ZsHSZhAd/uJjbh6Hle+BNkk/z1HLWbJynhBCCCHe2L1HCczffxWA7t72HBpZl/GtPKVAJITIkeQvKGFSzmWr8Y9rSwBKHP+Cy3cfveKIt9zJFWgW1abMkY/1xYy3nU4Lh2bB/Jpw+wRYOsA7C+DdFWBf4M1iP7v9rONyfdxnt59d2pa27f3z8HM7SH4CJetAu8WgkYGSQgghhHhzM3dcIj5Zi18RB1q622BpJn9CCSFyLvkNJ0yuWMevSFRZ4q++xLoV35PydALAHCc+CnaOA8D+4RlU20aZOKE3Yxl7E/Wy5vpr0iZC6QYw8Aj4vGvcW7w8WkHf/VDYT/8aruwEOz4FrX55WYsnt1GvaA8J0eBWCTqt0BeYhBBCCCHe0LnbMawJvgnA2GblUclt7EKIHM6kRaLExETGjBlDQEAAgYGBLFmy5IVtz58/T4cOHfDx8aFdu3acPXvWsE+r1TJjxgxq1KiBn58fQ4cOJTIyMisuQRiBysGN5MqDAHgv5gd+2HfBxBllkn1fQdwDFPtCKKhQn/gRjr/4Zz7bUhRUJ37EY/8HqG4eAwt7/a1dXX6DPIUz55yG28/665//NVt/+9nNv3E/8jGq2LuQ3wPeWw2Wdi+PJYQQQgiRDoqiMHnzBRQFWngXwq+oo6lTEkKITGfSItG0adM4e/Ysy5Yt4/PPP2fOnDls25b2VpK4uDj69OlDQEAA69atw8/Pj759+xIXFwfAwoUL2bJlC99++y1r1qwhJiaGTz6RFY3eJnb1RxBvlZ+i6gge7ZvDpbuPTZ2Scd07D8cWAaBrNZfb5YL027d8ol/q/W2R+BjW9ka9eThqXSJK8Vow4C/w7575E0SbWULTr1LdfqZZ2hjLuDsoeYtDt/Vgky9zcxBCCCFErrHn4n3+uvoACzM1I5uUM3U6QgiRJUxWJIqLi2PNmjWMHTsWT09PGjZsSO/evVmxYkWatlu2bMHS0pJPPvmEUqVKMXbsWGxtbQ0FJa1Wy+jRo6lUqRKlS5emW7duBAcHZ/UliTdhYYtV4/EA9Fev54vV+3PObWeKAls/0a+4Vb4llKzD3dLvoZRvpZ/k+dduEHPL1Fm+2t2zsLAOnP0NRaXhZvk+6LquA8eiWZuHRyvod0B/+xmQZOmErst6sC+YtXkIIYQQIsdK1uqYvEU/ur1njeIUyWdj4oyEECJrmKxIdPHiRVJSUvDz8zNs8/f359SpU+h0qYsDp06dwt/f33APsEqlomLFioSEhAAwaNAgGjZsCMCDBw9Ys2YNlStXzpoLEUaj8ulMcn5v7FXxNL6/hAUHrpk6JeM4vxH+OQhmVvol2QFUKnSt5kB+T3hyH37tCskJps3zRRQFgpfBD/XhQSjYF0bX/Q/ulX4XVCb6FZK3OARtR/fOIi7W/B7yFjNNHkIIIYTIkVYeu8G1iCfks7VgYN3Spk5HCCGyjMmW/4mIiCBv3rxYWFgYtjk7O5OYmEh0dDT58uVL1bZ06dS/nJ2cnLhy5UqqbbNmzWLu3Lk4ODiwcuXKDOek1WozfEx6Y75u7Dc9PrvESO/x6iZfwk8t6KzZQ8tdu6nr7kzZgvZZmoNRYyTHod4xFhWgqz4EJY/b/2NorKHjctSL66O6fQLdH8NQWs155W1bWXodSbGotnyE+sxqAJRS9dG1mY/W0hEenDHtz6XKDG35NiSfef08ssP/DWPEkByMF0NyMF4MY+TwqthCCJEZYuKT+WbnZQA+bFCGPFbmJs5ICCGyjsmKRPHx8akKRIDheVJSUrra/rdd69atqVu3Lj/88ANBQUFs3rwZO7v0T2J75syZjFxChrxpbGPklh1ivPp4G0oWrEneuwcZpV7OoJ+KMaWBM2bq/xdO3qbXstClHykcc5NE6/ycs62D8nT0279j2PuMocyRkahPr+SG1omIkm2NmsPrxrB6dI1Sxydg9SQcRaXmVtkg/eihy+FAeJbkkFUxskMOxoghORgvhuRgvBiZ+d4qhBCZ4fu9oUTFJVM6vx2dK2fxbfVCCGFiJisSWVpapinyPHtuZWWVrrb/bVesmP6Wk2nTplGrVi127NhB27bp+4MbwMvLC41Gk+726aHVajlz5sxrx37T47NLjAwdX/RblHlVqcUZij4+zpGYjgyqW+rtey2jrqPe8isAZs2n4VO+6gti+KLYJ6DaOY4i5+fhWrEhFK9puutQFFSnVqA6NBJVSjyKfSF0bX+gUNFqFMqqHLIoRnbIwRgxJAfjxZAcjBfDGDm8KrYQQhhb+MM4lh76B9AveW+mMek6P0IIkeVMViQqUKAAUVFRpKSkYGamTyMiIgIrKyvy5MmTpu1/l7SPjIwkf/78AOzduxcPDw8KFCgA6ItKRYoUISoqKkM5aTQao3dkjRXbGLllhxjpOt6lNFTpC4fn8KnZClru9aaRZ0Hc89tmXQ7GiLFrHGgToUQtNJ5t0txGlipG9cFw7yyq07+i+a0n9Nn3ynl2MuU6kp7ApuFwepX+ean6qNouRGPrnHU5mCBGdsjBGDEkB+PFkByMFyMz31uFEMLYvtp2kSStjsDSztQp62LqdIQQIsuZrDRevnx5zMzMDJNPAwQHB+Pl5YVanTotHx8fTp48iaIoACiKwokTJ/Dx8QFg6tSpbNiwwdA+NjaWf/75h1KlSmX6dYhMUutjFBsnSqtv057dfLTmFMlv02pnV/fAxU2g0kDTaa9eHl6lgpbfQSFfiH8Iq7pAUlyWpGpw7zwsrKsvEKnUUP8z6PIbvKBAJIQQQgiRkwRff8jm03dQqWBs8/KGRXOEECI3MVmRyNramjZt2jB+/HhOnz7Nrl27WLJkCe+//z6gH1WUkKBf7alJkyY8evSIyZMnExoayuTJk4mPj6dp06YAdOnShcWLF7N//36uXLnCxx9/TNGiRalVq5apLk+8KWtHVHVGAzDcfC3ht28zf/9bstqZNhm2jtQ/rtwH8pdP33Hm1vDuCrBxhntnYONA/cpiWeHkClhUDyIvgX0h6L4Jao4AtQyxFkIIIUTOpygKkzbpl7zv6F+E8oXyvOIIIYTImUz6F+Do0aPx9PSke/fuTJgwgcGDB9OoUSMAAgMD2bJlCwB2dnYsWLCA4OBg2rZty6lTp1i4cCE2NjaAvkjUu3dvxo8fT/v27VGpVMybNy/NiCTxlvHvCc5lyctjBpttYM7eq/wTnWzqrF7t2EKIvKwv9tQZlbFjHdyg03JQm8G5dXDou8zJ8ZmkJ7C+P2wcACnxUKoe9D0IxWtk7nmFEEIIIbKRP07fISQ8GhsLDSMauZs6HSGEMBmTzUkE+tFEU6dOZerUqWn2Xbp0KdVzb29v1q9f/9w4arWaPn360KdPn0zJU5iIxgwaT4YV7elptoOfExsw+5iGZoE6rLPr/Bax92HfV/rHDT4Ha8eMxyhWHZpOhc0jYNd4KFAByjQwZpYAWD3+B/Xi/vrRQyo11B0DgTJ6SAghhBC5S0KylqlbLwLQr3Yp8uexesURQgiRc8lfgyJ7K9MQStXHjBQ+s1zFPzEpzNoTauqsXmzXBEh8BIX9wLfr68cJ6AUV3wcUWBsED64aLUWSE1Cd+JFyBwegirwEdgWh+x9Q62MpEAkhhImMHDmSAwcOoNVqTZ2KELnO0kP/cCs6noJ5rPigZklTpyOEECYlfxGK7K/xZFBpqM8xqqrPs+DANYKvPzR1VmndPA4hP+sfN53+ZgUXlQqazQC3ypAQA6veg8THrx9PmwxXdsL6fjC9NOrNw9FoE1BK1oF+f0LxwNePLYQQ4o3Z2dkxduxYatSowWeffcaRI0cMC3YIITJPZGwic/fqP4D8pElZrC2y6Wh1IYTIIlIkEtlf/vLg3wOAhZazGKxey4RV+3mSmGLavP5Np4MtH+sf+7wHRSq9eUwzS/38RHYFIeKivsCjy8AKbzothB2AP4bCjDKwoj2cWglJj1HyuHKzfB90ndeAnSzvKoQQpjZu3DgOHDjArFmzMDMz46OPPqJmzZpMnjw51UqwQgjj+nbXZWITU/BydaCNr6up0xFCCJOTIpF4O9Qdg+JUhjzKIz40X8uauA84P/99/bLt2UHICrh9AizsocF448W1L6hf8UxjARc3wYHpL2+v08GNo7DlE/i6PCxrCcE/QnwU2OaHyn0haAe6Iae4V/pdUMunZUIIkV2oVCoqV67MZ599xrZt22jfvj2rV6+mc+fO1K9fnwULFpCYmGjqNIXIMa7ce8wvR28A+iXv1WpZ8l4IIf7X3n3HVVn+fxx/nXPYAgIi7j0RERRnaqW5smWW7bKv2nS0fg31a2rLr+2yYWXaMjXLhlmucpZbcQ/EheIABWWPc+7fHycpUuEABw7g+/l48NBzn3N9zvvGW7j4cN/X7dKFq0UcViUY20OrObxwCtUP/4Tf6W10SFoAHy6Axj2gy3Boco1r1tTJSLYvMA1w9bPgV8O59eu2h+vfgh+Hw/JXICQMqPX384YBx7fCju9g5/dwNu7v57wCoNVN0PoW+yVl55tCWvNCRKTcSUtLY9myZSxcuJDVq1dTo0YN/vOf/9C/f38SEhJ4/fXXWb9+PZ9++qmro4pUCq/8shubAX1a1aBz42qujiMiUi6oSSQVh8WdpDo9adD/CWZ8+y0hOz+ln2UjlgPL4MAyCG4OnR+BNneAh0/Z5VoxGdIT7e/f8aHSeY+299gbQes/xvzDQ3hdMQUSvGDXD/bm0Jl/LGzt4Qctr7M3hhpfDW4epZNJRESc5pFHHuHPP//E39+fa6+9li+++II2bdrkPd+8eXPOnTvH2LFjXZhSpPJYuS+BZXsTcDObGN0/1NVxRETKDTWJpOIxmbjzlkHcEF+LSQkHmBCymmsyFmFK3Ac/PwG/vQjth0DHB+yXa5WmU3tg3Uf2v/f7X+k2ZPq+Aid3YTq8mtCVD2Fenv33c25e0LyfvTHUrDe4e5deDhERcbrg4GA++ugjOnXqhMl08Ute2rdvz9y5c8s4mUjlY7UZvPLLbgDu69KQRsFVXJxIRKT80JpEUiF5uVt46/ZITphqMOzkQOZfsxT6ToKA+pBxBla9Dm+1ti/2fHxb6YQwDPj1GTCs0PJ6aHpN6bzPeRZ3uO1zjKp1MduyMczu0PxaGDgNnt4Pt30OrW5Ug0hEpAJ68cUXiY2NZcGCBXnbhg8fzqxZs/IeV69enSZNmrginkilMndjHHtOpFDV251R1zR1dRwRkXJFTSKpsFrXqcrjvZoBMPaXwxwL/Q+MiobbvoB6ncGWY7+b10fd4bPrYe+vYBTh7mCF2T0fDq4Aiyf0ecl5dQtSJRjb/b8SGzUe25N74a7Z0GYQePqVzfuLiEipeOutt5g6dSo+Pn9fLt2pUyc++OAD3n//fRcmE6lcUrNyeX3xPgBGXdOMAB9dli8i8k9qEkmF9vBVTWhbP4CUrFz+75ut2DDbF2oeugge+B1a3womCxxaheWbuwlbNhjTby/AoT/AmlP8N87JgEV/rQvR9TEIauScHXKEfx2Sa18F3gFl954iIlKqvvvuO9566y169uyZt+2+++7j9ddfZ86cOS5MJlK5fLzyIImpWTSs5sO9nRu4Oo6ISLmjJpFUaG4WM2/dFom3u4U1B04z489Dfz9ZJwpu/RQe3wZdH8PwqopX2jHMf74Nn/WHV5vAN/fBlq8g5USR3tf057tw9gj414VuTzh1n0RE5PKTkZGBr6/vBdsDAwNJSUlxQSKRyicx3cq01QcBeO7aUDzc9KOQiMi/6SujVHgNg6sw9jr7XSkmL9xDzMl/Taar1oXeL2B7bDsH247G1vpW8A6CrLOw60f7reXfaAFTu8NvL8DhNWDNveT7eaSfwPTnO/YHfV4s2zupiYhIpdS9e3defvll4uPj87adPHmSyZMn061bNxcmE6k8vt6RQlaujY4Ng+gbVsPVcUREyiXd3Uwqhbs71Wfp7pMs35vAE99EM++Rrhf+dsjDlzN1e1M/8mkwAcc2w/4lELMY4rfAiW32j1VvgFdVaNITmvWBpr3ANySvTN1dUzHlZkLD7hB2c9nuqIiIVErPP/88jz76KNdccw1Vq1YF4OzZs3Tu3Jnnn3/exelEKr5tR8+y4nAmAP+9PvSSdxEUEbncqUkklYLJZOLVW9rQ5+2V7Dh2jim/x/BUnxaXHmC2QL0O9o8eYyD1FOz/zd402v8bZCbDzu/tHwC1IqBZH0xVahB4fCWGyYLp2smgCYaIiDhBUFAQs2fPZs+ePRw6dAg3NzcaNmxI06a685JISSWkZDHux50ADIisTZu6Aa4NJCJSjqlJJJVGiL8XLw8IZ/jXm3l/2X56tAyhXf1Axwb7hkDknfYPay4c2/TXWUZL4Hg0HN8Kx7fmXZ9ptB+KqUZYae2KiIhchnJzcwkMDMTf3x8AwzA4ePAgu3fvpn///i5OJ1IxrY5J5PE50SSmZuHtZuKp3s1cHUlEpFwrdpMoNjaWkJAQ/Pz8WLVqFb///jutWrVi0KBBzswnUiTXtanFkl21+SE6nifnRPPLY93x8SjiYW5xg/qd7B89/wspJyH2N4hZjBH7O1nmKrhf9Vzp7ICIiFyWli5dyrhx40hOTr7guerVq6tJJFJEuVYbby+N4f3l+zEMaF7Dl0cjvagd4O3qaCIi5VqxFq6eM2cON954I7t372bXrl088sgjxMXF8c477/DOO+84O6NIkUy8qTW1qnpx6HQ6Ly/YXfKCfjUg8i4Y9Bm2/4tlZ88vdft5ERFxqjfeeIPevXuzYMEC/P39mT17NlOnTqVOnTo8/vjjro4nUqHEJ2dwx8dreW+ZvUF0Z8f6fP9IF+r56yIKEZHCFKtJNG3aNCZPnkzHjh357rvvCA0NZdq0abz11lvMnTvX2RlFiqSqtzuvD4oAYOa6Iyzbe8p5xU1mrUMkIiJOFxcXx7Bhw2jcuDGtW7cmISGBq666ivHjxzNjxgxXxxOpMJbsOkn/d1ex8XASfp5uTLmzLZMGhuPlbnF1NBGRCqFYTaKTJ08SFRUFwLJly+jVqxcANWvWJC0tzXnpRIqpa9Ng7r+iIQDPfLuNpLRs1wYSEREpgL+/PxkZGQA0atSIPXv2ANC4cWOOHj3qymgiFUJWrpWJ83fywBcbSU7PoU3dqiwY1Z0bImq7OpqISIVSrCZR48aNmT9/Pt9++y3x8fH06tWLnJwcpk+fTsuWLZ2dUaRYnru2JU2qVyEhJYv//rADwzBcHUlEROSirrrqKiZOnMj+/fvp1KkTP/74Izt37mTOnDmEhIS4Op5IuXYoMY1bPvyTGX8cAmBYt0Z8+/AV1K/m49pgIgVJPwNHN+CVcsjVSUTyKdaFuc8++yyPP/44Z8+e5a677qJJkya88MILLFmyhKlTpzo7o0ixeLlbeOv2SAZ+8CcLth/nmpbVaeDqUCIiIhcxduxYXn75ZXbs2MFNN93EokWLuPXWW/Hx8eG1114rUq2srCwmTpzI4sWL8fLyYsiQIQwZMuSC1917772sX7/+gu0DBw5k0qRJxd4XkbL0Y/Qxxn6/g9SsXAJ97EsOXBNaw9WxROyy0+B0LJzeD2di//776f2QkYQFCANsyb9Dv1fAq6qrE7tOdiqW7LP25pmlGJeHWq2YrC6+esQwsGSnFH8fAKxWMGzOzVVExWoSdenShTVr1pCSkkLVqvYD+dFHH2X06NG4u7s7NaBISbSpG8DIns14a+k+xs/fxevXBLo6koiIyAWWL1/OM888Q2Cg/fvU66+/zoQJE/D09Czy3OrVV19lx44dfP7558THx/Pss89Su3Zt+vXrl+91U6ZMIScnJ+/x1q1befzxx7nrrrtKvkMipSwj23552ewNcQB0bBjEO3dGUquq7l4mZctky4HEfZB86O8G0Om/GkIp8QWONfxqQcoJzNFfwYFlcMM70Kx32QQvDwwDDiyHtR9giVlMJMCi4pWyAJEmN0x7ukHzftC8LwQ1dlrUS8rNgkOrIWYx5n0LiUw6VOx9APt+tAgIhcjVzkpYZMVe4n/16tWEhYUB8O2337J48WJatWrFo48+ioeHh9MCipTU8B5N+H3vKbbGJfPehrP07GwUu7ErIiJSGiZOnMicOXPymkQAvr6+Ra6Tnp7O3Llz+eSTTwgLCyMsLIyYmBhmzpx5QZMoICAg7+9Wq5W33nqLYcOGER4eXuz9ECkL+06mMOLrzew7mYrJBCN7NGXUNc1wsxRrJQ2RoslOh4MrYd9CzAdX0vbMQUwUcOaHTzWo1hSCmkC1Jva/V2sKQY2xWTzZ/9sXNN/zLqYzB2DmrRB5D/R9uXLfTTk3C7bPhTUfwKmdTitrNnLtTacDy2HhcxDcHJr1sTeN6ncGi5NOaEk5ATGLYd8iiF0GOfZ1mZ11eyOre9G//ztTsZpE77//PtOmTeOzzz4jNjaW559/nkGDBrFkyRLOnj3L+PHjnZ1TpNjcLGbeui2C/u+uYvupbGb8eYgHr2rq6lgiIiJ5OnXqxM8//8zDDz9col+27dmzh9zcXNq2bZu3LSoqiqlTp2Kz2TCbL/5D9Lx58zh79iwPPPBAsd9bpLQZhsHs9UeYMH8nmTk2qvt58s7tkVzRNNjV0aSyS46DmEX2psDBlZCbCfzdFDDcq2D6ZwPo/N+DGoNP0KXrWq2kVmuD7cGVWJa/Ams/hOivIPZ3+1lFzfuU+q6VqbRE2PApbPgE0hLs29x9oO09WNs/QPSRs0RGRmIxF/03+labld1//EwrtzjM+5fA4T/tZ3gl7oM174FnVWja094watobqlRzvLjNBsej7f/++xba//5PvjWheR+sTXqzLTWQNlGdi7UP5/dj/9ZtRLrwjtrFahJ98803TJkyhYiICMaOHUuHDh2YOHEi27dvZ9iwYWoSSbnTuLovY69tybifdvHqon1ENaxGVANdeiYiIuXD6dOn+eCDD5g6dSpBQUF4enrme/63335zqE5CQgKBgYH5Gk3BwcFkZWWRnJxMUNCFP6wYhsG0adO47777qFKlSpGzW63WIo9xtGZxa5d0fGXJ4Iwa5SVDeo6Nx+ZEs2D7SQC6Nwvm9VvDCfb1dKhuedkPV2dwRo3LIoPNCsc2YIpZgilmEaZTu/I9bVSti9GsL9bGPdmd5E7L9ldjcbvEj9YFZMzLYPaE3i9Bi+sxzx+J6UwsfD0IW8SdGH0KXquo3H8uAU7txrR+KqZt32CyZgH2S+2Mjg9htL0PvAPsY+O2Y7UZYCr6DYesNoMs3/rkhl+HpfNwyDwHB363/xvuX4IpPRF2fg87v8fABHXbYzTtg9GsL9QIA5Mp/35kpcCB5Zj2L7bXSDuV7/2M2u0wmv01vmY4mMxYrVZs24u/D+f3Iy+DExWlXrGaRGfPnqVx48YYhsHy5cvzfuvk6+tbKhMFEWe4s2M9Fm45wB9xmYz4ejMLRnUnqIoujRQREde77bbbuO2220pcJyMj44Izkc4/zs6++IKe69at48SJE8V+/+3btxdrXFnUdka2ypDBGTVcmWFnQjYfbDjLiTQrFhPc1dqXG1tYOLp/N0fLKIMza5SHDM6oUdkyWLJT8E/YQNWTa6l6aj1uOefyXmNgJi2wFck1OnO2Rhcy/RqCyQQZgBds37HDKRnAC1OnKdTZO4OQA99i3jqL7L2LOdzmKc7V6OxgjZJmcFINw8A/YSMhB76lasKGvM1pVVtwssmtJNW6CsxusPeQU3PkH98QGjwA9YdQJXmv/d/25Fp8zu2HoxswHd0Ay18m26s6Z2t05mz1DoRknCRzzdP4nt5qv3ztL1aLN+eqt7e/LqQTuV5//dLlpAEntzl1H5xVo7iK1SRq2bIln376KQEBAZw5c4bevXtz8uRJ3nzzTSIjI50cUcQ5TCYTj0T5E59h5mBiOk9+E830wR0wm113Kp+IiAjAzTff7JQ6np6eFzSDzj/28vK66JhFixZx5ZVX5lujqCjCw8OxOHmxP6vVyvbt24tdu6TjK0sGZ9RwZYb45AwmLdzLL9vPAFAnwIt3bo+kbf2AMsvgzBrlIYMzalSaDLm5xKxZQAvzISyxS+HIWkzG3yc8GF5VMZpcA836YjS5Bm+fILyBWmWxH+07Y4sbgvmnkXic2U+z9WOwtbnDflbRv9YqKhefy3/WMHIwbf8G07qpmBL2ANjP3Gl5HbZOj+JVrxMNTKYL7jpd+vsRBdhvzGA9d8x+dtG+xXBwBR6ZCVQ/PJ/qh+fnG2EENvr7bKH6XfB388QfqFfsDM7Yj+I5X9cRxWoSTZgwgWeffZZjx47x5JNPUqdOHV5++WWOHTvGO++8U5ySImXC293Me3e2ZeCHa1i+N4EPV8QyvIfWJxIREde69957MRWw/sAXX3zhUJ0aNWqQlJREbm4ubn9d+pCQkICXlxf+/v4XHbNq1SpGjBhR9NB/sVgsTm8SOau2M7JVhgzOqFGWGTKyrXy0MpapK2LJzLFhNkGvRt78764rCPK9eLPT2RlKs4ZLM2yZiXnVGwTXuR5LZGTF3Y/ijj9/J6p9izDHLCIs6VD+56u3zFvo2FSvEyaLYz8ul8p+NLwCHlkNv78Ea97HvG22fUHmG96GFtcWP4NhwNmjcHQ9xG3AfHQ9EadisKwKxOQTBN5B9nWULvgz0P5xfpunn/1sqr+4ZZ3BbdWrmDfNgPRE+0YPX2h7L6ZOD0FQIxz5DJXJMRFYHzoMtX/kZPx1TCzEOLCCFKrg2+4WzC2uxVStaYHfm0uUoQxqFFexzyT68ccf8217+umndVczqRBa1vTjxZta88x323hj8V7a1Q+kS5MiLFwmIiLiZJ06dcr3ODc3l7i4OFasWMEjjzzicJ3Q0FDc3NyIjo6mffv2AGzatInw8PCLLlp95swZ4uLiiIqKKtkOiDiBYRgs2H6cSb/s4VhyBgAdGwUxrn9Lsk8doKq3k+5MdLla9xH8+gwmoMGZd7D5Ab0mwCUWtK80CrgTlc3sjqlRd0zNr7UvEh3Y0KVRL+Dubb/TWaub4IdH4XQMzLoD2twB/SYVvCj2eTkZEB8NRzfkNYZIPZH3tIm/mgLJ5yD5sOPZzO55TSOzpz/h8Vsw23Lsz1WtB50egnb3FbieUrng7g3NekOz3tisVmKio+1XR13Gt8MuVpMIYNeuXXz66accOHAAq9VKo0aNuPvuu+nYsaMz84mUikHt67Lu4Bm+23yUUbO3sGBUN0L8SvabKRERkeK61Jk88+bNY/HixQwdOtShOt7e3gwYMIAJEybwyiuvcOrUKaZPn86kSZMA+1lFfn5+eZeexcTE4OnpSd26dZ2zIyLFtDP+LBPn72L9wfOXlnkzpn8o/cNrYrPZiD5VSAEp2Oq3YOkEAIwGXTEd/gPzn+9AynG46X1wq0S/7LfZ4PgW2LfYgTtRBdGmfReXnbHhsHod4eFVsOwV+526ts2GA8vg+rehWd+/X2cYkHzkr4bQBohbDye2w/nmzXlmN6jRGup1xFY7ij1nzLRoXBdL1llIPwMZZ/71Z5L94/y23Ex7zbRTkHYKE/Zmk1EnClOXERB6Izh4FpaUP8X6l1uyZAlPPPEEffr0YeDAgVitVqKjoxkyZAhvv/02vXr1cnZOEacymUy8OCCM7ceS2XcylcdmRfPVsE5YtD6RiIiUI+fvIFsUo0ePZsKECQwePBhfX19GjhxJnz722yh369aNSZMmMXDgQMB+VzV/f/9inU4v4gynU7N4Y8k+Zq8/gs0AL3czj1zVlAevbIy3Rzn/wb0iMAxYPglWTLY/vvJpbFc+R9zPk2mw7U1M27+xn1Vy+1fl/4yPgmSl2M8Sillkbw79605U1G5nv/V5875Qs4397CmrFVt0tEviFou7N/R50d6A+fFR+63dZ9+JqfUgalgDMe97A45thNSTF46tEmJvNNXtYP+zViR4+ABgWK1kREdDvUjHz57JTs/XSLKlJrI3IZvmV99e/htuUqhiNYneeecd/u///o/7778/3/bPPvuMKVOmqEkkFYKPhxsf3B3Fje+tZs2B07y9dB9P9Wnh6lgiInIZio+Pv2BbWloan376KXXq1ClSLW9vbyZPnszkyZMveG7v3r35Hvfv35/+/fsXLayIE+RYbXy55jBvL93HuUz7HYSub1OL0f1DqRPg7eJ0lYRhwJJx8OcU++NrnofuT4HVyul6/ajXqgOWb/8DB1fCjP5w91zwr+3azEXgkXYM07q1sH8xHPoj/9kyHr7QpKe9KdS0N/jVcF1QZ6vXAR5aBctfgT+nYN4xl3zngprd7I2w802huh0goH6+9YNKzMPH/lHV/s6G1Up6RWq4SYGK1SSKi4ujR48eF2zv0aMHb775ZolDiZSVpiG+TBoYzmOzo5ny+36iGgRydYsQV8cSEZHLTM+ePTGZTBiGkXdWj2EY1KpVi1deecXF6USca+W+BF74eRf7T6UC0KqWPxNuDKNjIwfWVxHH2Gzw69OwYZr9cb//Qed/rW/W5Bq4fwF8fRuc3AHTesM930JIaNnnLYqEfZgXPkt47O/5twc1tp8t1KwPNOhauS6h+zd3L+j9AoTeiLHiVZJT0qnauhfm+p2hVoT9rCORYipWk6hJkyasXLmSe++9N9/2FStWFPm3XSKudlNkHdYfPMPMdUd4Yk40C0Z1p7Z+gyUiImXot99+y/fYZDLh7u5OcHCwLgWTSuPQ6TQm/bqPpbvtl8MEVfHg6b4tuK19PV3y70w2K/w0CqK/Akz2u2FF3X/x19aOhKFL4Ktb7IsiT+8Ld3wNDbuVXV5HZZ6Dla/C2g8x2XIxTBZocAWm5v3szaHgy/COxXXbY7tjFge02LI4UbGaRCNHjmTkyJFs3bqViIgIAKKjo1m0aBGvvvqqUwOKlIVx17ciOi6ZnfHnGDlrC7Mf7Iy7pZLf6UFERMqNOnXqMHPmTKpWrcr1118P2Bez7tq1K3feeaeL04mUTGpWLl9uS+GXeavJthq4mU3c16Uhj/VqpjuWOZs1B75/CHZ8ByYzDJgKEbcXPCawAQxdbL9rVtw6+PJmuPkjaD2wbDIXxjBg2zf2S+f+Wm/HaNaXnfXuIbTrdVoDR8TJivVTcI8ePfjkk0/Iyspi1qxZzJs3D8Mw+Prrr3Vdu1RIXu4WPri7HX5ebmw6nMSrC/e4OpKIiFxG3nrrLT788EN8fHzytnXs2JEPPviA999/34XJREomMTWL/u+u5oe9aWRbDa5sXp2Fj3fn+RtaqUHkbLlZ8M1ge4PI7Aa3zii8QXSeTxDc9yO0vB6s2fDtf2BNOfjac3wrTO8H3z9obxAFNYa75mK7YxZZVXQFi0hpKPZ96bp06UKXLl3ybcvKyiIuLo569eqVOJhIWWtQrQqv3RrBw19t4pNVB+nQMIg+YTVdHUtERC4D3333HW+//Tbt27fP23bffffRokULnn76aYYPH+7CdCLFN+mXPRxLziTYx8ykWyLp1aqmLqEsDdnpMOceiP0NLJ5w+5f2RZuLwt0bbvsCFo6G9R/BojFw9ij0edl+N7CylH4Gfn8RNs4ADHD3gSufhi7Dwc0TrNayzSNyGXHq//b169fn3WJVpCLq17omQ7s1AuCpuVs5cjrdxYlERORykJGRga+v7wXbAwMDSUlJcUEikZLbcOgM320+CsBTnQPo2TJEDaLSkJViX3w69jd7M+Xub4reIDrPbIFrJ9sXRQZY+4H9rKKcTOflLYjNal9se0o72DgdMKD1LTBiI3R/0t4gEpFSpUVXRP7l2X4taVs/gJTMXIZ/vZmsXP2mQkRESlf37t15+eWXiY+Pz9t28uRJJk+eTLdu5XABWZFC5FptjPthBwC3ta9L82qV+E5TrpSRbF9D6NAq8PCDe+ZB46tLVtNkgq6PwcBpYHaHXT/AVwMhI8kJgQtwZC18fBUseMr+XiFh9ruv3TodqurSMpGyoiaRyL94uJl57652BPi4s/3YWV76eberI4mISCX3/PPPk5OTQ8+ePencuTOdO3fmqquuwmq1Mn78eFfHEymyL9YcZs+JFAJ83Hm6T3NXx6mc0k7D5zfA0Q3gFQCDf4QGXQod5rA2g+Ce78DTHw7/YV8bKDnOefXPSzkB8x6031ntxHbwqgrXvgoPrSyfd1kTqeSKvSaRSGVWJ8Cbt26P5D8zNvDl2sN0aBTEjRG1XR1LREQqqaCgIGbPns3evXs5ePAgbm5uNGzYkKZNL8NbOkuFd+pcJm8u2QfAM31bElTFgyMuzlTppJ60n92TsBt8gu2LTtds7fz3aXwV/OdXmDkIEvbAp73hjjlOKW2y5WD6811Y9TpkpwImaHcvXDMeqgQ75T1EpOgcbhJt2LCh0Nfs3bu3RGFEypMeLUIY3qMJ7y+LZfR32wir7U/DIG9XxxIRkUooOzubt99+mzp16nD33XcDMHDgQK644goee+wx3N11FyipOF75ZTepWblE1Avgjg71MAybqyNVKu4ZpzB//gCciQW/WnDfT1C9FM/Wqtkahi2Br26FhN2YP+9PYOvHoFqmfQ2j4jgbT6vlz2NO++vMpDrtof+rUCfKeblFpFgcbhLde++9Dr1Oi9FJZfJEr+ZsPJTEuoNnGD5zM98+1NnVkUREpBJ66aWX2LRpEy+88ELetkcffZS3336bzMxM/vvf/7ownYjj1sSe5ofoeEwmePGmMMxmk25E5UxJh2jxx+OYMk5A1fr2S8yCGpf++1atC0MWwuy7MR1eTePNL8Pm4pez/PVhVKmOqddEiLiz7O+gJiIX5XCTaM+ePaWZQ6RccrOYmXJnW/q/u4o9J1KYMH8XdzZxdSoREalsFi9ezIwZMwgNDc3b1qtXL2rUqMFDDz2kJpFUCDlWG8//aF+s+u5O9WlTN8C1gSqbmCWYfxyBZ8YJjKDGmO77CQLqld37ewfAvfOwLRpL9q5f8PT0pLinBxiYORXQluBbXsNSJdCZKUWkhLQmkUghQvy9ePeOttz96Tq+3XyMGmZ/IiNdnUpERCoTwzDIysq66PacnBwXJBIpuhl/HCTmVCpBVTz4vz4tXB2n8shIhkVjIforTECGXyM87vsZS4AL7vjl5onRbzI7a95JZGQkFkvxLjezWa0cjY4m2MvfyQFFpKR0Tp+IA65oGswTvezXen+0+Rwz/jyEYRguTiUiIpVF3759GTduHBs3biQ9PZ309HQ2b97MhAkT6NWrl6vjiRTq+NkM3l4aA8Bz17YkwEe3vHeKfYvhg84Q/RVgwtbpEXZ3ex/8aro6mYhUUjqTSMRBI3o0Ze+JcyzYfoKXFuxh7YEzvHprBEFVNAkSEZGSGT16NGPHjmXw4MHYbDYMw8DNzY0BAwYwfPhwV8cTKdRLC3aTnm2lXf0Abm1X19VxKr6MJFg4BrZ+bX8c1AQGfIBRpwNGdLRLo4lI5aYmkYiDzGYT79weQS23NL7YnsbS3ae49p2VvH17W7o0qebqeCIiUoF5e3vz5ptvcu7cOQ4fPozVauXQoUPMnz+fXr16sXPnTldHFLmk1TGJLNh2HLMJXhzQGrNZN7Ipkb0LYf5jkHoCMEGX4dBjLHj4oFXARaS0qUkkUgQmk4lrm1ZhQNdwRs3ZyoGENO6atpaRPZoy6ppmuFl0BaeIiBRfTEwMP/zwAwsXLiQ1NZUmTZowZswYV8cSuaSsXCvP/2RfrPq+Lg0Jq13VxYkqsIwk+PU52Dbb/rhaU7jpA6jfybW5ROSyoiaRSDGE1vLn55HdmPDTTr7ZeJR3f9/PmgOnefuOttQJ8HZ1PBERqUCOHTvGDz/8wI8//khcXBz+/v6kpqbyxhtv0L9/f1fHEynQp6sPciAhjWBfT57o3dzVcSquvb/C/MftZw+ZzH+fPeSueaWIlC01iUSKycfDjVdvjaBr02DGfr+DDYeS6P/OKibf0oZ+rbWYoIiIFOy7777jhx9+YOPGjYSEhNCzZ0/69OlDhw4diIiIoHlz/cAt5dux5Aym/LYfgDH9W1LV293FiSqg9DOw8DnYNsf+uFozGPAB1Ovo2lwictlSk0ikhG6KrENkvQBGzdrC1qNnefirTdzbuQFjrwvFy714twUVEZHKb+zYsTRo0IDJkydz4403ujqOSJG9OH8XGTlWOjYM4ua2Lrgde0W3Z4H97KG0U/azh64YCVeP1tlDIuJSWkBFxAkaVKvC3Iev4KErGwPw5drDDHj/D/afSnFxMhERKa9eeeUV6taty+jRo+nSpQujR4/mt99+Iysry9XRRAq1fO8pFu48gcVs4oUBYZhMWqzaYeln4LthMPsue4MouDkMXQK9X1CDSERcTmcSiTiJh5uZ0f1DuaJpME99E82eEylcP2U1E24I4/YO9TR5EhGRfAYOHMjAgQM5c+YMv/76K7/88gsjRozAy8sLm83GunXraNCgAe7uuoRHypfMHCvjf7Lfce8/VzSkZU1/FyeqOKoeX4359/f+cfbQqL/OHvJydTQREUBnEok43VXNq/PLY93p1jSYzBwbz83bzshZWziXmePqaCIiUg4FBQVx9913M3PmTJYtW8bw4cMJDQ3lxRdfpHv37kyaNMnVEUXy+XjlAQ6fTifEz5PHejVzdZzyzzBg/2+Yv76Vphufx5R2Cqq3hKFLofdENYhEpFxRk0ikFIT4efHFkI48268lbmYTP287znXvrmLLkSRXRxMRkXKsZs2aDBs2jHnz5rFw4ULuueceVq1a5epYInnizqTz/jL7YtX/vb4Vfl460+2ScjJh8xfwQRf4aiCm2N8xTGZsXZ+AB1dA3ShXJxQRuYAuNxMpJWaziUeubkKnxkGMmrWFuDMZDJq6hid7N6ODn+HqeCIiUs41bNiQESNGMGLECFdHEckzcf5OsnJtdGlcjRva1HJ1HMcdWEb1g8uhnh+ElPKdA1NPwYZPYcM0SE+0b/PwxRZ5Fzt9u9Oq63Vg0c1NRKR8UpNIpJS1qx/IL491Z8y87fy87TivLtpHZA0PPm6eTYi/FicUERGRimHprpMs3X0KN7OJFyvSYtXb5mKe9wD1MWDHFKjWFJr1heZ9oX4XcPNwzvuc3AVr34dt34A1276taj3o9BC0vRfDw4/s6GjnvJeISClRk0ikDPh7uTPlzrZ0bxbM+J92En0ymxum/MGUu9rRsVGQq+OJiIiIFCgzx8rEn+2LVQ/t3oimIX4uTuSgXT/C9w9hwiDDtwFe6fGYTu+H0/vtDR0PP2jaE5r3g6a9wbd60erbbBD7G6x5Hw4s+3t7nfbQZTiE3giWv37kslqdt18iIqVETSKRMmIymbi9Q33C6/jzwIy1HEvJ4s5P1vJUn+Y8fGUTzOYK8ts4ERERuexMXXGAuDMZ1KrqxaieFWSx6r2/wrdDwLBii7iTXfWGEtmqKZZDK2HfYohZBGkJ9kbSrh8BE9SJsjeMmveBmm3gUmdL5WTA1tmw9kNI3GvfZjJD6A3QZQTU61hmuyki4kxqEomUsRY1/JjcqxpzD1j4cetxXl24lw0Hz/DmbZEEVnHS6c4iIiIiTnI8NZePVh0EYNz1rajiWQF+hNj/G3xzH9hyofWtGNe/C9u2g6c/tLrJ/mGzQfwWe7No30I4vhWObbR/LHsJ/GpDs972plHjq8DihVvmGUzLXobNn0H6aft7efhBu/vsl5UFNnDpbouIlFQF+AovUvl4u5l5Y1AbujSxX362bG8C/d9dxXt3tSWqgS4/ExERkfLBMAw+3XKO7Fwb3ZsFc23rmq6OVLiDq2D23fZ1gUJvgJunctGbOpvN9juM1Y2CHmPg3HGIWQz7FsGB5ZASD5s/t39YPDHXjiT86CbMRq59fEB96PQItL0HvPzLcg9FREqNmkQiLmIymbijY30i6gUwfOZmDiSmcftHa3mmXwse6N644iwGKSIiIpXW4l2n2HIiGw+LiYk3VoDFqo+sg69vh9wM++LUt0wHi7tj6wH514KowfaPnEw4vNreMNq3EJKPYIpbhwkw6nXC1GU4tLju7/WGREQqiYu01MtOVlYWY8aMoX379nTr1o3p06df8rW7du1i0KBBREREcMstt7Bjx4685wzD4OOPP6Znz560a9eOwYMHs3///rLYBZESC63lz08ju3FDRG1ybQav/LKHB77YSHJ6tqujiYiIyGVs/6lURn9vn3MP696IxtV9XZyoEMc2w8xbIScNGveA274o/p3L3L2gaS/o/xo8tg0eXYfturfY3e19bPf/ar9cTQ0iEamEXNokevXVV9mxYweff/4548eP57333mPhwoUXvC49PZ0HH3yQ9u3bM2/ePNq2bctDDz1Eeno6ALNnz2b69OmMGzeO7777jrp16/LAAw+QkZFR1rskUiy+nm68e0ckLw1ojYebmaW7T3Hdu6vZfCTJ1dFERETkMnTqXCaDp6/nbEYOzYLcGX51E1dHKtiJ7fDlzZB1Dhp0hTu+tjd6nMFkgpCWGO0Gkx4Y6pyaIiLllMuaROnp6cydO5exY8cSFhZG7969GTZsGDNnzrzgtb/88guenp4888wzNGnShLFjx1KlSpW8htL333/PkCFD6NGjB40aNWLChAkkJyezefPmst4tkWIzmUzc07kB8x65ggbVfDiWnMFtU9cwbdUBDMNwdTwRERG5TKRm5fKfzzZwLDmDBtV8GN0tEC93i6tjXdqpPfDFTZCZDHU7wl1zwMPH1alERCokl50juWfPHnJzc2nbtm3etqioKKZOnYrNZsNs/rt/tXXrVqKiovKugTaZTLRr147o6GgGDhzIM888Q926dfNebzKZMAyDlJSUImWyOnKtchGdr1nc2iUdX15qKIPjNUJr+vLjo1cw5vsd/LLjBC8t2M26A6eZfEs4Vb3dy8V+lIcMzqhRHjI4o4YyOK+GMjivhjMyFFZbRJwvx2rj0Zmb2Rl/jmBfD2YMbk9S3D5Xx7q007HwxY32O43VioS754Knn6tTiYhUWC5rEiUkJBAYGIiHx9/XCQcHB5OVlUVycjJBQUH5Xtu0adN846tVq0ZMTAwA7du3z/fc3Llzyc3NJSoqqkiZtm/fXtTdKLPazshWHmoog+M1hrQ0qO3ux2dbU1iy+xRb31zOU10CaBrkXmYZSnt8ealRHjI4o4YyOK+GMjivRml+bxUR5zIMg+e+287KfQl4u1v4dHAHGlTzISnO1ckuIekQfH4DpJ6EGq3h3u/BO8DVqUREKjSXNYkyMjLyNYiAvMfZ2dkOvfbfrwP7WUeTJ09m6NChVK9evUiZwsPDsViceyqt1Wpl+/btxa5d0vHlpYYyFK9G27ZwfZezjJwVTVxSBv9dfoZn+zYn0ieZNm3a6HOp/VAGJ9dQBufVcEaGwmqLiHO9uWQf320+isVs4oO72xFRL6D8nrl39qi9QXTuGAS3gHt/AJ+gQoeJiEjBXNYk8vT0vKDJc/6xl5eXQ6/99+u2bNnCAw88wJVXXsljjz1W5EwWi8XpE1ln1XZGtvJQQxmKXiOyfhA/j+rOM99uZdHOk7z0y1461/Hk41YG/h76XDqjRnnI4IwayuC8GsrgvBql+b1VRJzn63VHmPK7/e7ALw9oTY+WIS5OVICUE/D5jZB8BIIaw30/gm/RfjksIiIX57KFq2vUqEFSUhK5ubl52xISEvDy8sLf3/+C1yYmJubblpiYSEjI39+81q1bx5AhQ+jcuTNvvPFGvjWNRCq6qt7uTL0niuevb4W7xcTaY1ncOW09p85lujqaiIiIVHC/7T7Jf3+wn5036ppm3NGxvosTFSA1wd4gOhMLAfVh8Hzwr+XqVCIilYbLOimhoaG4ubkRHR2dt23Tpk2Eh4df0OCJiIhgy5YteXd4MgyDzZs3ExERAcC+fft45JFH6N69O2+//Tbu7u5lth8iZcVkMjGkWyNmDeuEv6eZnfHnuPmDP9l3smgLtIuIiIicFx2XzIivt2Az4Lb2dXmiVzNXR7q0jCT4cgAk7gW/2vYGUdW6hQ4TERHHuaxJ5O3tzYABA5gwYQLbtm1j6dKlTJ8+nfvuuw+wn1WUmWk/S6Jfv36cO3eOl19+mf379/Pyyy+TkZHBtddeC8Dzzz9PrVq1GD16NElJSSQkJOQbL1KZtK0fwKSeQTSs5sOx5Axu+fBP/oxNLHygiIiIyD8cSkxj6GcbyMixclXz6rx8c3je3YTLG3NOKuaZt8LJHVAlxN4gCmzo6lgiIpWOy9YkAhg9ejQTJkxg8ODB+Pr6MnLkSPr06QNAt27dmDRpEgMHDsTX15ePPvqI8ePH880339CiRQs+/vhjfHx8SEhIYMuWLQBcffXV+eqfHy9S2dT0dePbhzvz8Fdb2Hg4icHT1/PqrW24ua1+myYiIiKFO52axf0z1nM6LZvWdfz54O52uFtK6ffHcesIPLYak1ssFGNJCJPNRrN1b2NK2gk+1WDwTxDctPCBIiJSZC5tEnl7ezN58mQmT558wXN79+7N97hNmzZ8//33F7yuevXqF7xW5HIQ6OPBV8M68dQ3W1mw/ThPzNnKsaQMhvdoWm5/CygiIiKul56dy5DPN3LodDp1A72Zfn8HqniWwo8Fp2Nh4XNYYhbTGGBz8cqYAV/A8KqK6d4fICTUaRFFRCQ/lzaJRKRkvNwtTLmzLXUCvfl45QFeX7yPY8kZvHhTa9xK67eBIiIiUmHlWm2M/HoLW+OSCfRx5/MhHQnx8yp8YFFkpcKqN2DNe2DNxjC7kxIYhp9/VYrzaywDSM6w4n/9S1hqtXFuVhERyUdNIpEKzmw2MaZ/KHUDvZnw005mrY8jPjmT9+9uh29p/FZQREREKiTDMBj3405+23MKTzcz0wZ3oEl1X2e+Aez4DhaPg5R4+7amvbD1eYWYuFQiIyOxWCxFLmuzWjkQHU2kGkQiIqVOpxqIVBL3dWnIR/e2x8vdzIp9Cdw2dQ0nz2nxdhEREbF7f9l+Zq0/gskE797ZlqgGgc4rfmIHfHY9fDfU3iAKaAB3zIK7v4VqWj9IRKSiUJNIpBLp3aoGsx/sQrCvB7uOn+Pm9/9g74kUV8cSERERF/tu8zFeX7wPgIk3htE3rKZzCmckwS9Pw0fd4fBqcPOGHmNh+Hpo2R+0TqKISIWiJpFIJRNZL4B5j3SlcfUqxJ/N5Napf/Ln/kRXxxIRkTKSlZXFmDFjaN++Pd26dWP69OmXfO3evXu58847adOmDTfccANr164tw6RSVqJPZDHm+x0APHxVE+7r0rDkRW022PQ5TImC9R+DYYNWN8GI9XDVM+Du5HWORESkTKhJJFIJ1a/mw7xHrqBDw0BSMnMZPGM932856upYIiJSBl599VV27NjB559/zvjx43nvvfdYuHDhBa9LSUlhyJAhNG3alPnz59O7d29GjBjB6dOnXZBaSsvO+HO8tiaZXJvBgMjaPNO3RcmLHt0I03rC/FGQfhqCW8B9P8JtX0BA/ZLXFxERl9GqtiKVVICPB18O7cRTc7eyYNtxnpizlaNnMhjRU+sCiIhUVunp6cydO5dPPvmEsLAwwsLCiImJYebMmfTr1y/fa7///nt8fHyYMGECFouFUaNGsWLFCnbs2MFVV13loj0QZzp9cBu1Pr+R3yxmkqrUI7RKO8zr1tjXCKrWxN7Qsbg7XjD1FCydCNFf2R97+sPVz0HHB4tWR0REyi01iUQqMS93C1PuaEvdAG8+WnmAN5bs42hSBhNvDHV1NBERKQV79uwhNzeXtm3b5m2Liopi6tSp2Gw2zOa/TyJfv34911xzTb67TX333XdlmldK1/7fP6cTSWCCOlmnYVN0/heY3SCw4V9No6YQ1Pjvv/vVgvPHiy0X07oPYcVkyDpn3xZ5N1wzHvxqlOUuiYhIKVOTSKSSM5tNjO4fSt1Ab8b/tJM5G+OIP5vBQ2G62lREpLJJSEggMDAQDw+PvG3BwcFkZWWRnJxMUFBQ3va4uDjatGnDuHHj+P3336lTpw7PPvssUVFRRXpPq9XqtPz/rlnc2iUdX1kyeB1fD8Da4Ftp3/lKzEkHMZ3eD2cOwJkDmHIz4PR++8e/GO4+ENQYU2AjWh3bjjnlkH17rUhs/f4HdTueD1nq+1EePpfOqFEeMjijhjI4r4YyOK+GMjhW1xFqEolcJu7t0pBaVb0ZOWsLq2ISiU9047vwHAKqWAofLCIiFUJGRka+BhGQ9zg7Ozvf9vT0dD7++GPuu+8+PvnkExYsWMDQoUP59ddfqVWrlsPvuX379pIHL6XazshWUTOcSc2iW84eMIHRrB9bTY0hqDWc7xMaNtwzE/FKPYpnWhxeacfwTD2KV9pRPNPjMeWkw8kdmE7uwBvIdffnWOgwEutfC4kWSIwuk/1w5vjyUqM8ZHBGDWVwXg1lcF4NZSg5NYlELiO9WtVgzkOduX/6emKTcnjgy818MaQT3h5qFImIVAaenp4XNIPOP/byyn+3KYvFQmhoKKNGjQKgVatW/PHHH/z44488/PDDDr9neHh4vkvWnMFqtbJ9+/Zi1y7p+MqQYdHiBXibskkx+eFRrWGRatisOZB8GM4cwEiM4UT8UUL6PkVd32DqlvF+OGN8ealRHjI4o4YyOK+GMjivhjI4VtcRahKJXGba1A3gs/904I6P1rDhUBKPzNzEx/e2x8NNl5+JiFR0NWrUICkpidzcXNzc7NO8hIQEvLy88Pf3z/fa6tWr07hx43zbGjZsyPHjx4v0nhaLxelNImfVdka2iprh7L7VACQGRYLJXLQaFguEtICQFlitfTgZHU0t3+DL9nPp7BrlIYMzaiiD82oog/NqKEPJ6adCkctQWG1/xnQPxMvdzPK9CTzxTTRWm+HqWCIiUkKhoaG4ubkRHR2dt23Tpk2Eh4fnW7QaIDIykr179+bbduDAAerUqVMWUaUUpWblEnR6CwC+Tbu5OI2IiFQkahKJXKZCgz344K62uFtMLNh2nLHfb8cw1CgSEanIvL29GTBgABMmTGDbtm0sXbqU6dOnc9999wH2s4oyMzMBuOOOO9i7dy9Tpkzh8OHDvPPOO8TFxXHTTTe5chfECVbtPUU70x4AAlte6eI0IiJSkahJJHIZu6p5dd6+vS1mE8zeEMcrv+xWo0hEpIIbPXo0YWFhDB48mIkTJzJy5Ej69OkDQLdu3fjll18AqFOnDtOmTWPZsmVcf/31LFu2jI8//pgaNXRL84puy9bNVDedI9fkgal2pKvjiIhIBaI1iUQuc9e1qUVaVhue+W4bn6w6SFVvd0b0bObqWCIiUkze3t5MnjyZyZMnX/Dcvy8vi4qKYt68eWUVTcpArtVG1oE/AEiv3oYqbp4uTiQiIhWJziQSEW7rUI//XhcKwOuL9/H5n4dcG0hERESKZdPhJFrl7AK0HpGIiBSdmkQiAsCw7o0ZdY39DKLxP+3ku01HXZxIREREimrp7pO0N9vPGDM3vMLFaUREpKJRk0hE8jzRqxn3X9EQgGe+28ainSdcG0hEREQcZhgG63fuo4n5uH1DvY6uDSQiIhWOmkQiksdkMvH89a24pV1drDaDkV9v4Y/9ia6OJSIiIg6ITUijRnI0ANbgluAd6NpAIiJS4ahJJCL5mM0mJt8STt+wGmRbbTzwxUY2H0lydSwREREpxJJdJ2lv3geApUEXF6cREZGKSE0iEbmAm8XMu3e2pXuzYNKzrdw/fT27j59zdSwREREpwD/XI6K+mkQiIlJ0ahKJyEV5uln46N4o2tUP4FxmLvd+up5DiWmujiUiIiIXkZiaxa4jJ2htOmjfUL+zawOJiEiFpCaRiFySj4cbM+7vSMuafiSmZnH3tHUcP5vh6lgiIiLyL7/vOUUbDuBhsoJfLQio7+pIIiJSAalJJCIFqurjzpdDO9Gwmg/HkjO4Z9o6TqdluzqWiIiI/IN9PaLzl5p1BpPJtYFERKRCUpNIRApV3c+Tr4Z1olZVL2IT0vjPZxtJy7G5OpaIiIgAmTlWVsUk0EHrEYmISAmpSSQiDqkb6MOXQztRrYoHO+PPMWl1EhnZVlfHEhERuez9sT+R7Jxcoiwx9g1aj0hERIpJTSIRcVjTEF8+H9IRX083difmMPSLjaRm5bo6loiIyGVt6e6TNDcdxY908PCDkDBXRxIRkQpKTSIRKZLWdary6eAovN1MrDuYxN2frCVJaxSJiIi4hM1msHT3qb/XI6rbHixurg0lIiIVlppEIlJk7RsEMuGqIAJ93Nl69Cx3fLyWU+cyXR1LRETksrP1aDIJKVl0cdtn36D1iEREpATUJBKRYmka5M7XwzoS4ufJ3pMp3PbRGo4mpbs6loiIyGVl6e6TAHRx32/foPWIRESkBNQkEpFia17Dj28fvoK6gd4cOp3OoKlriE1IdXUsERGRy8bSXaeoTSJBuSfBZLFfbiYiIlJMahKJSInUr+bDtw9fQZPqVTh+NpPbpq5hV/w5V8cSERGp9I6cTmfvyRQ6nr/UrFYb8Kji2lAiIlKhqUkkIiVWs6oX3zzUhbDa/pxOy+aOj9ew6XCSq2OJiIhUakv+utSsv/9h+watRyQiIiWkJpGIOEU1X0++fqAz7RsEci4zl3s/Xccf+xNdHUtERKTSWrrL3iSKOn9nM61HJCIiJaQmkYg4TVVvd74Y2pHuzYJJz7bynxkbWPLXBFZERESc52x6DusPncGPdIJSY+wb66lJJCIiJaMmkYg4lY+HG9MGt6dvWA2yrTYe/moTP0Yfc3UsERGRSmXZ3lNYbQY3Bh3FhAGBjcCvhqtjiYhIBacmkYg4naebhffvasfAtnWw2gwenxPN1+uOuDqWiIhIpXF+PaLrA7UekYiIOI+aRCJSKtwsZl4fFMG9nRtgGDDm++18tCLW1bFEREQqvOxcGyv2JgDQ2rrLvlHrEYmIiBOoSSQipcZsNvHCTWE8enUTACb9uoc3Fu/FMAwXJxMREam41h08TWpWLrV8zfgmbrNv1JlEIiLiBGoSiUipMplMPNOvJc/0awHAlN/3M3H+Lmw2NYpERESK4/xNIe5tmIwpNwO8gyC4mYtTiYhIZeDm6gAicnl49Oqm+Hm6Me7HnXz25yFSM3O4rbEaRSIiIkVhGAZL/2oS9a5y0L6xfmcwmVyYSkREKgudSSQiZebeLg15Y1AEZhN8u/kYb6xJJiUzx9WxREREKoxdx88RfzYTb3cLjTN22DdqPSIREXESNYlEpEzdElWXD+6OwsNiYt2xLPq9+wcr9iW4OpaIiEiFsHTXKQC6N62G5eg6+0atRyQiIk6iJpGIlLl+rWvy1dCO1PS1cOJsJoOnr+fZb7dxTmcViYiIFGjpbvulZgMaZEJ6Irh5Qa0IF6cSEZHKQk0iEXGJqAaBvNk7mPuvaIDJBHM2xtH3rZWs1FlFIiIiF3X8bAbbj53FZILunvvtG2u3AzdP1wYTEZFKQ00iEXEZTzcT464LZfYDnakf5MPxs5ncN309o+dt01pFIiIi/7J0t/1Ss3b1A/E7udG+UesRiYiIE6lJJCIu16lxNRY+3p37r2gIwKz1cfR7exWrYnRWkYiIyHl5dzVrVQOOrLFv1HpEIiLiRGoSiUi54OPhxoQbw5j9oP2somPJGdz76XpGz9tOalauq+OJiIi4VGpWLmtiTwPQt4EZzsQCJqjXwbXBRESkUlGTSETKlc6Nq/HrY925r0sDAGatP0Lft1byx/5EFycTERFxnZX7Esi22mgUXIWG6dvtG0NCwTvQtcFERKRSUZNIRMqdKp5uvHBTa75+oBN1A705lpzB3dPWMfZ7nVUkIiKXp/OXmvUKDcEUt86+UesRiYiIk6lJJCLl1hVNgln0+JXc29l+VtHMdUfo9/ZK/tRZRSIichnJtdr4fa990ererWpqPSIRESk1ahKJSLlWxdONFwe05uth9rOKjiZlcNe0dYz7YQdpOqtIREQuA5uPJJOcnkOgjzvtannA8a32J3QmkYiIOJmaRCJSIVzRNJiFj1/JPZ3rA/Dl2sNcN+UPdiVkuziZiIhI6Vq6234WUY+WIbgd3wK2XPCvA1XruTiZiIhUNmoSiUiF4evpxksDwpk5rBN1AryJS8pg/IozfLzqIIZhuDqeiIiI0xmGwZK/mkS9Q2vAkbX2J+p1ApPJhclERKQyUpNIRCqcrk2DWfTElQyIrI3NgMkL9/Lgl5s4m5Hj6mgiIiJOdSzFypEz6XhYzHRvXl3rEYmISKlSk0hEKiRfTzdevzWch6L88bCYWLLrJDdMWc2OY2ddHU1ERMRpNsRnAnBF02r4upsgbr39Ca1HJCIipUBNIhGpsEwmE30a+/DNQ52pG+jNkTPpDPzwT2avP6LLz0REpFLYEJ8FQK/QGnByJ2SngIcf1AhzcTIREamM1CQSkQovvE5VFozszjUtQ8jOtfHcvO3839xtZGRbXR1NRESk2BJTs9h32n4p9TWhIRC3zv5EvQ5gtrgwmYiIVFZqEolIpVDVx51P7mvPs/1aYjbBd5uPMuD9PziQkOrqaCIiIsWybE8CBtC6tj+1qnprPSIRESl1ahKJSKVhNpt45OomzBzWmWBfT/aeTOHG9/5gwbbjro4mIiJSZEv32O9q1is0BAwDDp9vEmk9IhERKR1qEolIpdOlSTV+GdWNjo2CSM3KZfjXm5k4fyfZuTZXRxMREXFIZo6V1fsTAbimZQicjYOUeDC7QZ0oF6cTEZHKyqVNoqysLMaMGUP79u3p1q0b06dPv+Rrd+3axaBBg4iIiOCWW25hx44dF33dhx9+yHPPPVdakUWkggjx9+LrYZ14+KomAMz44xC3f7yG+OQMFycTEREp3JoDp8nMsVHN20xoLT848td6RDXbgEcV14YTEZFKy6VNoldffZUdO3bw+eefM378eN577z0WLlx4wevS09N58MEHad++PfPmzaNt27Y89NBDpKen53vdzz//zJQpU8oqvoiUc24WM89d25JP7muPn5cbW44kc/2U1azcl+DqaCIiIgVa9telZlG1PDGZTFqPSEREyoTLmkTp6enMnTuXsWPHEhYWRu/evRk2bBgzZ8684LW//PILnp6ePPPMMzRp0oSxY8dSpUqVvIZSbm4u48ePZ8yYMdSrV6+sd0VEyrnerWqwYGR3Wtfx50xaNoNnrOftpfuw2gxXRxMRcbqinKn9yCOP0KJFi3wfy5YtK8O0cjGGYfD7X02idrU87RuPrLX/qfWIRESkFLm56o337NlDbm4ubdu2zdsWFRXF1KlTsdlsmM1/96+2bt1KVFSU/bcogMlkol27dkRHRzNw4EDS09PZu3cv33zzDZ999lmxM1mtzr9d9vmaxa1d0vHlpYYyOK+GMhSvRp0AT755oBMvLNjN7A1HeXtpDBsPnmFomKVC7YcylG4NZXBeDWdkKKy2XNw/z9SOj4/n2WefpXbt2vTr1++C18bGxvLaa6/RpcvfZ6dUrVq1LOPKRcQmpHI0KQMPNzPhIR6QkQyndtmfVJNIRERKkcuaRAkJCQQGBuLh4ZG3LTg4mKysLJKTkwkKCsr32qZNm+YbX61aNWJiYgDw9/dn9uzZJc60ffv2EtcordrOyFYeaiiD82ooQ/FqDGoI1U1V+WjTWVbHnmZ3vJknUzbRMtij0LHOylBaNZTBeTWUwXk1SvN7q1zo/Jnan3zyCWFhYYSFhRETE8PMmTMvaBJlZ2dz9OhRwsPDqV69uosSy8Us22O/LLpToyC83MxwdANgQFAT8A1xbTgREanUXNYkysjIyNcgAvIeZ2dnO/Taf7+upMLDw7FYLE6tabVa2b59e7Frl3R8eamhDM6roQwlrxEZCdd2TmH4zC0cPJ3O8yuSeKp3Mx7o1giz2VQmGZxZQxmcV0MZnFfDGRkKqy0XKsqZ2gcOHMBkMulS/XLo/KVmVzcPBpIxxf21aLXOIhIRkVLmsiaRp6fnBU2e84+9vLwceu2/X1dSFovF6RNZZ9V2RrbyUEMZnFdDGUpWo1XtAH4YfgUjPlvNqiOZvLpoH+sOJvHmbRFU8/UskwzOrqEMzquhDM6rUZrfW+VCRTlT+8CBA/j6+vLMM8+wfv16atasyciRI7nqqquK9J66XN+541Myc9hw6AwAVzWrxtljyXmLVtvqdsQoYr3L+XNZ3mqUhwzOqKEMzquhDM6roQyO1XWEy5pENWrUICkpidzcXNzc7DESEhLw8vLC39//gtcmJibm25aYmEhIiE63FZHi8/V047GOVekf1ZQJ83exYl8C/d9dxTt3tKVz42qujiciUmRFOVP7wIEDZGZm0q1bNx588EGWLFnCI488wpw5cwgPD3f4PXW5vnPHrzmaSa7NoLafhbPHYjFZs+HYJgB2pVUlKzq6THI4e3xlyeCMGuUhgzNqKIPzaiiD82ooQ8m5rEkUGhqKm5sb0dHRtG/fHoBNmzYRHh6e71RogIiICD755BMMw8BkMmEYBps3b+bhhx92RXQRqURMJhO3ta9LuwZBDP96M/tPpXLXJ2t5vFdzhvdoiqWIl5+JiLhSUc7UfvTRR7n33nvzFqpu2bIlO3fu5JtvvilSk0iX6zt3/KzY7UAy/drUJTy8OQdWzMZsy8bwqUZo1xvAVLaXRVfkz2V5q1EeMjijhjI4r4YyOK+GMjhW1xEuaxJ5e3szYMAAJkyYwCuvvMKpU6eYPn06kyZNAuxnFfn5+eHl5UW/fv144403ePnll7njjjuYPXs2GRkZXHvtta6KLyKVTIuafvw0oivjftjJd5uP8uaSfaw7eJq3bo8kxM+5l7aKiJSWopypbTabL7iTWePGjdm/f3+R3lOX6ztvvM1msHyf/ez5a0JrYrFY8E3aAYCpXmcsbsWful9un8vyXKM8ZHBGDWVwXg1lcF4NZSg5c+EvKT2jR48mLCyMwYMHM3HiREaOHEmfPn0A6NatG7/88gsAvr6+fPTRR2zatImBAweydetWPv74Y3x8fFwZX0QqGR8PN964LYLXB0Xg7W7hj/2n6f/Oav7Yn1j4YBGRcuCfZ2qfd6kztZ977jlGjx6db9uePXto3LhxWUSVi9gZf47E1CyqeFjo0NC+fpTvaXuTSItWi4hIWXDZmURgP5to8uTJTJ48+YLn9u7dm+9xmzZt+P777wut+b///c9p+UTk8nRrVF0i61Vl+Mwt7D2Zwj2frmNkj6aMuqYZbhaX9tZFRApUlDO1e/bsyZNPPkmnTp1o27Yt8+fPZ9OmTbzwwgsu3ovL1/m7mnVrFoyHmxlrbi6+Z/66PKB+FxcmExGRy4V+2hERuYimIX78OKIrd3ash2HAu7/v565p6zh5LtPV0URECuTomdp9+vRh/PjxfPjhh1x//fX8/vvvTJs2jbp167oy/mXt9732JlHPln/dnOV0DG455zDcvKBWhAuTiYjI5cKlZxKJiJRnXu4WJg1sQ+fG1RgzbzvrD57h2ndW8eZtEVzdQndXFJHyqShnag8aNIhBgwaVVTQpQGJqFtuOJgPkfY8xxa21P1m7Hbh5XGKkiIiI8+hMIhGRQtwUWYf5I7vRqpY/Z9KyuX/GBiYv3EOO1ebqaCIiUkms2JuAYUBYbX9q+P91w4S4dQAY9bQekYiIlA01iUREHNC4ui/zHr2Cezs3AODD5bHc8fFa4pMzXJxMREQqgwsuNTMMTEfsZxIZ9Tu5KpaIiFxm1CQSEXGQl7uFFwe05v272uHn6camw0nc8N6frD+mdYpERKT4cq02Vu5LAP6+1IyYJZiSDmIze0BdNYlERKRsqEkkIlJE17WpxYJR3WlTtyrJGTlM/jOZx+ZEk5ia5epoIiJSAW06nERKZi5BVTyIrBcANhv8/iIApxoNAC9/l+YTEZHLh5pEIiLFUL+aD3Mf7sID3RthBn7edoJeb67gu01HMQzD1fFERKQCWbbXfhbRVc2rYzGbYPePcGIbhocvJ5rc6eJ0IiJyOVGTSESkmDzdLDzXrwX/61WN0Jp+JKfn8NTcrdw3fT1xZ9JdHU9ERCqIZXvs6xFd3aI62Kyw7BUAjM6PYvWs6spoIiJymVGTSESkhJoEuvP9o114um8LPNzMrIpJpM9bK/l09UGsNp1VJCIil3YsOYO9J1Mwm+xnErFtDiTuA+9AjM6PujqeiIhcZtQkEhFxAneLmeE9mrLwse50bBRERo6VF3/excAP/2TPiXOujiciIuXU+bOIohoEEuABLJ9kf6LbE+CptYhERKRsqUkkIuJEjav7MvuBzrxyczh+nm5sjUvm+ndX8+bivWTlWl0dT0REypm/LzULgc2fQ/IR8K0BHR5wcTIREbkcqUkkIuJkZrOJuzrVZ8mTV9G7VQ1ybQbv/r6f/u+sYuOhM66OJyIi5URmjpU/YhMBuKaJH6x83f7ElU+Dh48Lk4mIyOVKTSIRkVJSs6oXH98bxQd3tyPY15PYhDQGfbSG53/cQWpWrqvjiYiIi609cJrMHBu1qnrR4shsSD0BAfWh3WBXRxMRkcuUmkQiIqXIZDLRP7wWS5+8ktva18Uw4Is1h+n95gp+33PS1fFERMSFzl9q1rdpFUx/vGXfeNVz4ObhwlQiInI5U5NIRKQMBPh48OqtEcwc1on6QT4cP5vJkM82MmrWFk6nZbs6noiIlDHDMPh9r71JdC8/Q0YSVGsGbW53cTIREbmcqUkkIlKGujYNZtHjV/LglY0xm+CnrfH0fXsVf8ZlujqaiIiUodiENOLOZBBiSaNxzAz7xp5jweLm2mAiInJZU5NIRKSMeXtYGNM/lB+Gd6VlTT+S0nN4Y20yz83bTprWKhIRuSycv9RsfNBiTNmpUDMcQm9ycSoREbncqUkkIuIibeoGMH9kNx69ujEmYO6mY1w/ZTXbjia7OpqIiJSy3/ecIoQk+qbNt2/oOQ7MmpqLiIhr6TuRiIgLuVvMPNW7OROvDqJmVS8OJqYx8IM/mboiFpvNcHU8EREpBSmZOWw4dIYRbj/gZsuEep2gWR9XxxIREVGTSESkPAir7sGCEVfQP7wmuTaD//26h3s+XceJs1qrSESkslkdk0hN4xR3ui2zb+g5Dkwm14YSERFBTSIRkXIjwMeD9+9qx+RbwvF2t/Bn7Gn6vbOSRTtPuDqaiIg40bK9p3jM8h3u5ELjq6FRd1dHEhERAdQkEhEpV0wmE7d3qM/Po7rRuo4/yek5PPTlJsZ8v52MbKur44mISAnZbAYH92xhoGWVfUPP510bSERE5B/UJBIRKYeaVPdl3iNdeeiqxgB8ve4I109Zxc74sy5OJiIiJbEz/hyDs77GYjKwNe8PdaNcHUlERCSPmkQiIuWUh5uZ0deG8tXQToT4eRKbkMbN7//JtFUHtKi1iEgFtWPTSq63rMOGCfM1/3V1HBERkXzUJBIRKee6NQtm4eNX0iu0BtlWGy8t2M3gGes5laJFrUVEKppmO98BIK52P6gR5uI0IiIi+alJJCJSAQRV8eCT+6J4aUBrPN3MrIpJ5Nq3V/H7npOujiYiIg46u3cV7bM3kGuYqdJXaxGJiEj5oyaRiEgFYTKZuKdzA34e2Y2WNf04nZbNkM82MmH+LrKsuvxMRKRcMwyyF08EYKlXL4IbtHJxIBERkQupSSQiUsE0q+HHD8O7MqRrIwC+XHuEp5ck8uuOE1qrSESkvDqwjOqnN5BluHE4bISr04iIiFyUmkQiIhWQl7uF529oxWf/6UC1Kh4cS7EyYlY0/d5Zyfyt8VjVLBIRKT8MA+O3FwGYae1F+4g2Lg4kIiJycWoSiYhUYFe3CGHJE90Z1KoKfl5u7DuZyshZW+j79kp+jD6mZpGISHmwZwGm+M2kG57MdL+FyHoBrk4kIiJyUWoSiYhUcFW93bkjzI+V/3cVj/dqhr+XG/tPpfLY7Gh6v7WC77ccJddqc3VMEZHLk80Ky14GYIa1L+EtmmExm1wcSkRE5OLUJBIRqST8vd15vFdzVj/Xk6d6N6eqtzsHEtJ4Ys5Wer+1km83qVkkIlLWTDu/h1O7SKUKH+VeT4+WIa6OJCIicklqEomIVDL+Xu6MvKYZq5/twdN9WxDo487BxDT+b+5Wer6xgm82xJGjZpGISOmz5WJaMQmAD3OuI9Xky1XNq7s4lIiIyKWpSSQiUkn5ebkzvEdTVj/bk+eubUm1Kh4cOZPOM99to+cby5m9/gjZuWoWiYiUluC4hZiSDpLpEcgMaz/a1Q8kwMfD1bFEREQuSU0iEZFKroqnGw9f1YRVz/ZgbP9Qgn09iDuTwXPzttPj9eXMXHdYzSIREWfLzaTWvi8B+MH3DtLx0qVmIiJS7rm5OoCIiJQNHw83HriyMfd0bsDX648wdUUsx5IzGPv9Dt77fT/d6liwBibRrkEQbhb9DkFEpMgMAxL2QswizLt+xJKZgM2vNpMSrgCgp5pEIiJSzqlJJCJymfH2sDC0WyPu7lSf2euP8OGKWI6fzWTuWZi7ax3+Xm50bRrMlc2rc2Xz6tQJ8HZ1ZBGR8isnEw6vhn2LYd9CSD4MgAkwMLOr9dOcXWahVlUvWtb0c21WERGRQqhJJCJymfJyt3B/10bc0bE+P0Uf44d1Mew8beVsRg6/7jjBrztOANCkepW8hlHnRtXw9rC4OLmIiIudOw4xi2HfIjiwHHLS/n7O4gENu2Nr1oedOfWYe6Y+cISrW4RgMplclVhERMQhahKJiFzmvNwt3NKuDk3MCYS3iWDn8RRW7Etg5b4EouOSiU1IIzYhjRl/HMLDzUynRkFc2czeNGpew1c/9IhI5WezwfEt9qbQvoVwfGv+531rQvO+9o9GV4GnL4bVStaWLSzfmwDoUjMREakY1CQSEZE8FrOJtvUDaVs/kMd7Nedseg5/xCay8q+mUfzZTFbFJLIqJpGXf9lNTX8vujcLplvTalTN0uLXIlKJZKUQcHwlpiPTYP9SSDv1jydNUKcdNO8HzfpArQi4SMP8WIqVI2cy8LCYuaJJtbLLLiIiUkxqEomIyCVV9XGnf3gt+ofXwjAMYhNSWbHP3jRae+A0J85lMnfTUeZuOgpAvVUrCKtdlbDa/rSq7U9Y7arU8PfU2UYiUiR7T6SQluPCxvP2bzH//DhNslL+3ubhB016/NUY6g2+hZ8ZtPlEFgCdGgdRxVPTbhERKf/03UpERBxiMploGuJH0xA/hnZrRGaOlQ2HzrByXwIr9iWw72QqcUkZxCVlsHDnibxx1ap45DWM7H/606haFcxmNY5E5EJ/7E/k7mnr8LSYuDFuO3d1akBkvYCyaTZnp8PCZ2HzF5iALJ9auIcPwNyiH9S/Atw8ilRu83F7k0iXmomISEWhJpGIiBSLl7uF7s2q071ZdZ7rZ2XV+s1YqjVgz4lUdsafZdfxc+w/lcrptOy8S9TO8/GwEFrL3jAKq+1Pq1pVaVLdx4V7IyLlRZPqvoTW9GP3iRTmbjrG3E3HCK3lz10d63FT2zr4e7mXzhuf2g1z/wMJuwETtm5PsqNqPyLbRYGl6Av2p2TmsishG4AeLdQkEhGRikFNIhERcQo/DzORTarRvfnfPwxl5ljZcyLF3jSKP8fO+HPsOXGO9Gwrmw4nselwUt5r3cwmAr1MBK1cjZ+XO35ebvh6uePr6Yaflxt+nm74ern9/fiv53z/es7Pyx13s+GKXRcRJ6pZ1Yv5I65gzm8b2JDkxS87TrD7+DnG/biTV37Zw40RtbmzU30i6lZ1ztlFhgGbv4Bfn4XcDLK9qvN94/HMjWlCQnISfmv+xMPNjJvFjLvFhLvFjJvZjIebCTezGfe/trv99dz5x8eTM7Aa0CjYh4bBVUqeU0REpAyoSSQiIqXGy91CZL0AIusF5G3Ltdo4mJjGruP2ptHO+LPsjD9HcnoOCekGCempxX4/N7MJXw8TNVauJtjPk6AqnlSr4kGwr4f9774eVKviQTVf+9/9PN20XpJIOWQymWgZ7MEdvdow4cYw5m0+xtfrj7D/VCpzNsYxZ2McrWr5c1en+twUWRu/Yp5dlJmaRMq3I6h+6GcAVtna8ETyIyRurgr81cQ+e65E+9KjRfUSjRcRESlLahKJiEiZcrOYaVbDj2Y1/Lgpsg4AhmFw9Ewaqzdtp3aDxqRn20jJyiU1M5eUzFxSs3JIzbL/3f7Y/px9m/05mwG5NoPkTIPkzFT2niy82eRhMRNUxYOgKh55DaSMlLNUO7wTAxNgYLOBzTAw+OtPw57XZvy93TD+fp3NMMhNP0eT43uo7uf1V1PKg2p5TSpPvD2KfumKyOUqwMeDId0a8Z+uDdl4OImv1x1hwfbj7Dp+jv/+sINXftnNjRG1uatTfcLrFHx2UVaulegjyaw5cJqTe9bycMJLNDCdJNcw83rubXxkvZ5aVX0Y2KQanRoGkpZ4jPoNG2MzIMdqkGuzkZ1rI9dmkGO12bdZbXl/z7H+/Vx2jpWzSad56MrGZfjZEhERKRk1iURExOVMJhO1A7xpXs2DyKbBWIq4/odhGKRnW0lKy2Ltlu0E12lEckYuialZnEnL5nRqNqfTsjj919/PpGWTmpVLttXGiXOZnDiXmb9gbFyJ92n54UOXfM7Hw0K1v85uCs5rUnkS7OtBgLcbySezsB1JoqqPZ94ldVU83LBosW+5HCTswzzrTpq4BWNyfwBCrwd3L0wmEx0aBtGhYRDjb2jFd5uP8fW6w8QmpDF7QxyzN8QRVtt+dtH14TUByM61sTPuLGsPnGbNgdNsOpxEZo6VIZaFTHT7Gg+TleMEM6fhRBqGXcnyJtWoH+SDyWTCarUSHZ1IZIvqRf6aBPw1PppgX09nf4ZERERKjZpEIiJS4ZlMJqp4uuHlZqJRgDuRzQpvNGXmWDmdls2Z1GwS07I4k5pNQkomB44cpXatWpjNZswme22TCcwmk/0x/3r8jz9NJrDZbOw7cASPqsEkpefkNajs75NNdq6N9Gwr6WcyiDuTcemAq9ZdsKmKhyVvXSZfL3f7Ok3/WqvJ19MNHw8zGaczqd4gg3rVquiSOqlYss5B0kECjP0wby14VoXWN0PEXVCvI5hMBPh4MLRbI4Z0bciGQ0l8ve4wv+w4wc74c4z9fgcvL9hNQ38zB3/4jYwca17pAFL4yHsaVxkbAEhrfC01b/2Qx30CXbW3IiIi5YqaRCIiclnycrdQJ8CbOgHeedvsv/k/S2Rk02KdOZBXw+MMkZEtL6hhGAapWbmcScsmMTWb0+fPdErLzjvrKTEli/jTZ7Ga3UnNspKSmUOO1b4gd1q2lbRsKyfJcijLK6tX4OfpRvOafrSo6UeLGvY/W9b0I8CnaLfyFikzddtje2QNp5a8Q82TyzGdOwabPrN/BDWGiDuhze0Q2ACTyUTHRkF0bBTE+LRsvtt8lK/XH+FAQhq7Eu3NoaAqHnRuHMSNgYe5ZucE3NOOg8UD+r5ClQ7DQE1UERGRPGoSiYiIlBGTyfTXndvcaVDt4nc7On+JSmRkZF6TKSvX+o81mP61JtP5dZn+sf1cRg774s8Qn2olJSv3gjvJAdTw96RFTX9a1PClRU1/Wtb0o2mIL17uWi9JyoFqTYlvOZSQ29/BcuRP2Dobdv0IZw7AspftHw26QcQd0Oom8PInsIoHw7o3Zmi3RqyNTWTllj3ccEU4LWv4Yv7zLVg2CQwrVGsKt86AWm1cvZciIiLljppEIiIi5ZynmwVPXwvVHFzb5HyjqVXrNhxJymTPiXPsPZHCvpMp7DmRwtGkDE6ey+LkuQRW7kvIG2c2QcPgKrSo4UezkCrknkvnrE8CNav6UMPfk0AfD8xaF6ncy8rKYuLEiSxevBgvLy+GDBnCkCFDChxz9OhRbrjhBqZOnUqnTp3KKKkDTGZofJX9o/9rsOdniP4aDq6Ew6vtH788DaE32BtGja/GZLbQsVEQHmd9aOGbjnnmfXBwhb1emzvgutfB08+1+yUiIlJOqUkkIiJSSXm4me2XmdXM/wNxSmYOMadS2Xsihb0nUvKaSEnpORxISONAQhq/nn/xxk1549zMJkL8PKnu70WInyc1/D0J8fPK+7O6nyc1/O13dBPXefXVV9mxYweff/458fHxPPvss9SuXZt+/fpdcsyECRNIT08vw5TF4OlrbwRF3AFnj8K2ORA9C07HwPZv7B9+taDNbRB+O36nNmD+/XVISwB3H7juDYi8y9V7ISIiUq6pSSQiInKZ8fNyp139QNrV/3uxXsMwSEjNymsc7TuRQsyxU2TiSUKq/c5wuTaD+LOZxJ/NLKA6WMwmqvt60MAPPg+3FXt9Jym69PR05s6dyyeffEJYWBhhYWHExMQwc+bMSzaJfvrpJ9LS0so4aQlVrQvdn4JuT8KxzbD1a9j+LaQchz/ewfLHOzQ//9oare2Xl1VvXlBFERERQU0iERERwb5eUoifFyF+XnRvVv2CtZFyrDYSU7M4dS6Lk+cyOZWSxanzf6b8vS0xNQurzeDEuSwSU+BsRg5eHu6u3r3Lxp49e8jNzaVt27Z526Kiopg6dSo2mw2z2Zzv9UlJSbz22mtMnz6d66+/vqzjlpzJBHWj7B99X4F9i2DrbIyYRZhsudiihmDu9wq4exdeS0RERNQkEhERkcK5W8zUqupNraoF/7Cda7VxOi2b+KR0jh+OIdjBdZTEORISEggMDMTD4+9L/oKDg8nKyiI5OZmgoKB8r//f//7HzTffTLNmzYr9nlartfAXFbNmkWqb3KDFddDiOqwpp4iJ/oNmV9yIxWyBYmQsVgYnji8vNZTBeTXKQwZn1FAG59VQBufVUAbH6jpCTSIRERFxGjeLmRr+XgRXcceWqGlGWcvIyMjXIALyHmdnZ+fb/ueff7Jp0yZ+/vnnEr3n9u3bSzS+1Gr7NXJKtpLWKA8ZnFFDGZxXozxkcEYNZXBeDWVwXg1lKDnN3kREREQqCU9PzwuaQecfe3l55W3LzMzk+eefZ/z48fm2F0d4eLjT152yWq1s37692LVLOr6yZHBGDWVwXo3ykMEZNZTBeTWUwXk1lMGxuo5Qk0hERESkkqhRowZJSUnk5ubi5maf5iUkJODl5YW/v3/e67Zt20ZcXByjRo3KN/6BBx5gwIABvPDCCw6/p8ViKbXFyUta2xnZKkMGZ9RQBufVKA8ZnFFDGZxXQxmcV0MZSk5NIhEREZFKIjQ0FDc3N6Kjo2nfvj0AmzZtIjw8PN+i1W3atGHx4sX5xvbp04eXXnqJrl27lmlmERERKT/UJBIRERGpJLy9vRkwYAATJkzglVde4dSpU0yfPp1JkyYB9rOK/Pz88PLyokGDBheMr1GjBtWqVSvr2CIiIlJOmAt/iYiIiIhUFKNHjyYsLIzBgwczceJERo4cSZ8+fQDo1q0bv/zyi4sTioiISHmlM4lEREREKhFvb28mT57M5MmTL3hu7969lxxX0HMiIiJyeXDpmURZWVmMGTOG9u3b061bN6ZPn37J1+7atYtBgwYRERHBLbfcwo4dO/I9//PPP9OrVy8iIiIYPnw4Z86cKe34IiIiIiIiIiKVhkubRK+++io7duzg888/Z/z48bz33nssXLjwgtelp6fz4IMP0r59e+bNm0fbtm156KGHSE9PB+x36Bg7diwjRoxgzpw5nDt3jtGjR5f17oiIiIiIiIiIVFguaxKlp6czd+5cxo4dS1hYGL1792bYsGHMnDnzgtf+8ssveHp68swzz9CkSRPGjh1LlSpV8hpKX331Fddeey0DBgygZcuWvPrqq6xYsYK4uLiy3i0RERERERERkQrJZU2iPXv2kJubS9u2bfO2RUVFsXXrVmw2W77Xbt26laioKEwmEwAmk4l27doRHR2d9/z527wC1KpVi9q1a7N169bS3xERERERERERkUrAZQtXJyQkEBgYiIeHR9624OBgsrKySE5OJigoKN9rmzZtmm98tWrViImJAeDUqVOEhIRc8PyJEyeKlMlqtRZ1NxyuWdzaJR1fXmoog/NqKIPzapSHDM6ooQzOq6EMzqvhjAyF1RYRERER53JZkygjIyNfgwjIe5ydne3Qa8+/LjMzs8DnHbV9+/Yivb4sazsjW3mooQzOq6EMzqtRHjI4o4YyOK+GMjivRml+bxURERER53JZk8jT0/OCJs75x15eXg699vzrLvW8t7d3kTKFh4djsViKNKYwVquV7du3F7t2SceXlxrK4LwayuC8GuUhgzNqKIPzaiiD82o4I0NhtUVERETEuVzWJKpRowZJSUnk5ubi5maPkZCQgJeXF/7+/he8NjExMd+2xMTEvEvMLvV89erVi5TJYrE4fSLrrNrOyFYeaiiD82oog/NqlIcMzqihDM6roQzOq1Ga31tFRERExLlc1iQKDQ3Fzc2N6OjovEWnN23aRHh4OGZz/vW0IyIi+OSTTzAMA5PJhGEYbN68mYcffjjv+U2bNjFw4EAAjh8/zvHjx4mIiHAoi2EYgNYkKs0ayuC8GsrgvBrlIYMzaiiD82oog/NqlMWaROe/f4traP5U/jM4o4YyOK9GecjgjBrK4LwayuC8GsrgWF1H5k4mw4UzrOeff57NmzfzyiuvcOrUKZ599lkmTZpEnz59SEhIwM/PDy8vL1JTU+nduzfXXXcdd9xxB7Nnz2bhwoUsXrwYHx8ftmzZwr333sv48eMJDw/n5ZdfpkqVKkydOtWhHNnZ2TptXUREpIIJDw+/YE1CKTuaP4mIiFQsjsydXNokysjIYMKECSxevBhfX1+GDh3K/fffD0CLFi2YNGlS3tlB27ZtY/z48cTGxtKiRQsmTpxIq1at8mrNmzePd999l7Nnz9K1a1defPFFAgMDHcphs9nIzc3FbDZjMpmcvp8iIiLiPIZhYLPZcHNzu+DsYyk7mj+JiIhUDEWZO7m0SSQiIiIiIiIiIuWDfv0mIiIiIiIiIiJqEomIiIiIiIiIiJpEIiIiIiIiIiKCmkQiIiIiIiIiIoKaRCIiIiIiIiIigppEIiIiIiIiIiKCmkQiIiIiIiIiIoKaRKUqKyuLMWPG0L59e7p168b06dOLVSc7O5vrr7+edevWFXnsyZMnGTVqFB07dqR79+5MmjSJrKwsh8cfPnyYoUOH0rZtW66++mqmTZtW5Az/9OCDD/Lcc88VedySJUto0aJFvo9Ro0Y5PD47O5uJEyfSoUMHrrjiCt58800Mw3B4/Lx58y54/xYtWtCyZUuHaxw/fpyHHnqIdu3a0bNnTz777DOHx553+vRpRo0aRfv27enduzfz5s1zeOzFjqO4uDjuv/9+IiMj6d+/P6tXry5yDbAfJ23atClWhujoaO644w7atm1L3759mTt3bpFrrFq1ihtvvJE2bdpw4403smLFiiLvA0BKSgrdu3cv9PN6sRovvfTSBcfHV1995fD4+Ph4HnjgASIiIujduze//PJLkTI899xzFz1G77vvviLtx8aNGxk4cCCRkZHcdNNN/Pnnn0Uav2PHDm6//Xbatm3LbbfdRnR09EXHFvS1yZHj0pGvbYUdlwXVcOS4LGi8o8ekI/tR2HFZUA1HjsuCxjt6XF6qRnGOS7m8OWvuBMWfP5V07gTOnT+5au4Emj9B5Zg/lXTuVNA+wOU1fyrp3OlSNRyZP5V07lRYjfM0f9L8CUNKzQsvvGDccMMNxo4dO4zFixcbbdu2NX799dci1cjMzDSGDx9uNG/e3Fi7dm2RxtpsNuO2224zhg0bZuzbt8/YsGGD0bt3b+N///ufQ+OtVqvRp08f46mnnjIOHjxoLF++3GjXrp3x008/FSnHeT///LPRvHlz49lnny3y2A8++MB46KGHjFOnTuV9nD171uHx48aNM/r06WNs3brV+PPPP41OnToZs2bNcnh8RkZGvveOj483evfubbz88ssO17jtttuMxx9/3Dh48KCxZMkSIyIiwli8eLHD4202m3H77bcbgwYNMnbu3Gn8/vvvRocOHYxFixYVOvZix5HNZjNuuOEG46mnnjL2799vTJ061YiIiDCOHTvmcA3DMIz4+Hijb9++RvPmzYuc4dSpU0b79u2NN954wzh48KDx888/G+Hh4cayZcscrnHo0CGjTZs2xowZM4wjR44Y06dPN8LCwoy4uDiH9+G8cePGGc2bNze+++67Iu2HYRjG/fffb3z00Uf5jpP09HSHxufk5BjXX3+98fDDDxuxsbHGrFmzjLCwMGPv3r0OZzh37ly+996yZYvRunVrY8mSJQ7XSExMNKKiooxPPvnEOHLkiPHhhx8aERERxvHjx4s0/r///a+xf/9+Y8aMGUZkZOQFx1RBX5scOS4d+dpW2HFZUA1HjsuCxjt6TDr6Nbqg47KwGoUdlwWNd/S4LKhGUY9LEWfMnQyj+POnks6dDMO58ydXzp0MQ/OnyjB/KuncqaB9OO9ymT+VdO5UWI2C5k8lnTsVVuM8zZ80fzIMw1CTqJSkpaUZ4eHh+b4Ivv/++8Y999zjcI2YmBjjxhtvNG644YZiNYn2799vNG/e3EhISMjbNn/+fKNbt24OjT958qTx2GOPGSkpKXnbhg8fbowfP75IOQzDMJKSkowrr7zSuOWWW4o10XnqqaeMN954o8jjzr93q1atjHXr1uVt++ijj4znnnuuWPUMwzCmTp1q9OrVy8jKynLo9cnJyUbz5s3zfXEYMWKEMXHiRIffc9u2bUbz5s2NI0eO5G376KOPjNtuu63AcZc6jv78808jMjLSSEtLy3vt4MGDjXfffdfhGkuWLDE6d+6ct72oGb7++mujX79++V47btw448knn3S4xtq1a42XXnop32s7dOhgLFiwwKHx553/wty1a9dLTnIKqtG9e3dj1apVl/wcFDR+6dKlRlRUVL7/a4888ogxe/bsImX4pyFDhhj/93//V6QcixcvNjp27JjvtR07drzgB7RLjZ82bZpxzTXXGLm5uXmvHTp0qPH666/nG1/Q1yZHjsvCvrY5clwWVMOR47Kg8Y4ek458jS7suCysRmHHZUHjHT0ui/K9pqDjUsQZcyfDKNn8qaRzJ8Nw3vzJlXOn8++v+VPFnj+VdO5UUI3zLpf5U0nnTgXVcGT+VNK5U2E1DEPzJ82f/qbLzUrJnj17yM3NpW3btnnboqKi2Lp1KzabzaEa69evp1OnTsyZM6dYGapXr860adMIDg7Otz01NdWh8SEhIbz99tv4+vpiGAabNm1iw4YNdOzYschZJk+ezE033UTTpk2LPBYgNjaWhg0bFmvspk2b8PX1zZf7wQcfZNKkScWql5yczCeffMJTTz2Fh4eHQ2O8vLzw9vZm3rx55OTkcODAATZv3kxoaKjD7xsXF0dQUBD16tXL29aiRQt27NhBTk7OJcdd6jjaunUrrVq1wsfHJ29bVFTURU9vvVSN5cuX89hjjzF27NgCs19q/PnTKv/tYsfopWp06tQp7/1zcnKYO3cu2dnZF5wmW9D/p+zsbMaNG8fzzz9f4L/ppWqkpqZy8uTJQo/RS41fv349Xbp0wdfXN2/bBx98wO233+5wjX9as2YNGzZs4MknnyxSjoCAAJKTk1m8eDGGYbB06VLS0tJo3ry5Q+Pj4uIICwvDYrHkbWvRosUFx1RBX5scOS4L+9rmyHFZUA1HjsuCxjt6TBa2H44clwXVcOS4LGi8o8elo99rCjsuRZwxd4KSzZ9KOncC582fXDl3As2fKsP8qaRzp4JqwOU1fyrp3KmgGo7Mn0o6dyqsBmj+dL6G5k/gVmbvdJlJSEggMDAw34EZHBxMVlYWycnJBAUFFVrjrrvuKlEGf39/unfvnvfYZrPx1Vdf0blz5yLX6tmzJ/Hx8fTo0YO+ffsWaeyaNWvYuHEj8+fPZ8KECUV+b8MwOHjwIKtXr+ajjz7CarXSr18/Ro0a5dAkIy4ujjp16vDDDz8wdepUcnJyGDhwII888ghmc9H7pLNmzSIkJIR+/fo5PMbT05Pnn3+eF198kS+++AKr1crAgQMZNGiQwzWCg4NJSUkhIyMDb29vAE6cOEFubi4pKSmXPKYudRwlJCQQEhKSb1u1atU4ceKEwzVeeuklgELXe7jU+Lp161K3bt28x6dPn2bBggWMHDnS4RrnHT58mGuvvRar1cpTTz2Vr25h46dOnUqrVq3o1q1bge9xqRqxsbGYTCamTp3KypUrCQgI4D//+Q8333yzQ+PPH6Ovv/46P/74I4GBgYwaNYpevXo5nOGfPv74Y26++WZq1apVpP1o3749d999N6NGjcJsNmO1Wpk0aRKNGzd2aHxwcDB79uzJt+3EiRMkJSXl21bQ1yZHjsvCvrY5clwWVMOR49KRr6+FHZOF1XDkuCyohiPHZUHjHT0uHf1eU9hxKeKMuROUbP7kzLkTFH/+5Oq5E2j+VBnmTyWdOxVW43KaP5V07lRQDUfmTyWdOxVWAzR/0vzpbzqTqJRkZGRc8E34/OPs7GxXROK1115j165dPPHEE0Ue++677zJ16lR2795dpN8gZWVlMX78eJ5//nm8vLyK/L5gX/jr/Ofz7bff5tlnn2X+/Pm8+uqrDo1PT0/n8OHDzJ49m0mTJvHss8/y5ZdfFmvhQ8MwmDt3Lvfcc0+Rx8bGxtKjRw/mzJnDpEmTWLhwIT/99JPD4yMiIggJCeHFF1/M26cZM2YAFPibsEu51DHqquMzMzOTkSNHEhwcfNHfABUmKCiIb7/9lueff54pU6awaNEih8bt37+f2bNnM3r06CK/53kHDhzAZDLRuHFjPv74YwYNGsS4ceNYsmSJQ+PT09P5/vvvOXfuHFOnTmXAgAGMGjWK7du3FzlLXFwca9eu5d577y3y2LS0NOLi4hgxYgRz587l4Ycf5qWXXiI2Ntah8X369GHbtm1888035ObmsmrVKn777bdCj89/fm0qznFZkq9thdVw9Li82PiiHpP/rFHc4/KfNYpzXP5zfHGPy4t9LkpyXMrlo7LNnaB486fyMHcCzZ8upTLNn4o7dwLNn84r6dwJijd/Kunc6d81ikvzpwvHV4b5k84kKiWenp4X/Mc8/7i43/BL4rXXXuPzzz/nrbfeuujpj4UJDw8H7BOX//u//+OZZ55x6DdR7733Hq1bt87XJS2qOnXqsG7dOqpWrYrJZCI0NBSbzcbTTz/N6NGj852aeTFubm6kpqbyxhtvUKdOHcA+eZo1axZDhgwpUpbt27dz8uRJrrvuuiKNW7NmDd9++y0rVqzAy8uL8PBwTp48yYcffsiNN97oUA1PT0/efvttHn/8caKioqhWrRrDhg1j0qRJ+U5ndJSnpyfJycn5tmVnZ7vk+ExLS+PRRx/l0KFDfP3113m/6SsKPz8/WrVqRatWrYiNjeWrr74q9Le2hmHw3//+l1GjRl1wqmdRDBgwgB49ehAQEABAy5YtOXToELNmzaJ3796FjrdYLAQEBDBhwgTMZjNhYWFs3LiRb775Ju//nqMWLVpEaGhosS5PmDZtGoZhMGLECADCwsLYtm0bX3zxBRMnTix0fPPmzXnxxRd56aWXGD9+PKGhodx5550F/kbq31+binpclvRrW0E1HD0uLzW+KMfkP2s0a9aMO++8s8jH5b9zNGvWrEjH5b/HF+e4vNTnoiTHpVw+KtvcCYo3fyoPcyfQ/KmgepVl/lScuRNo/vRPJZ07QdHnTyWdO12sRnFo/nTx8ZVh/qQziUpJjRo1SEpKIjc3N29bQkICXl5e+Pv7l2mWF198kRkzZvDaa68V6VTnxMREli5dmm9b06ZNycnJcfja/AULFrB06VLatm1L27ZtmT9/PvPnz8+33oAjAgICMJlMeY+bNGlCVlYWZ8+eLXRs9erV8fT0zJvgADRq1Ijjx48XKQPYb8vYvn17qlatWqRxO3bsoEGDBvm+WLdq1Yr4+Pgi1WnTpg2///47K1euZPny5TRq1IjAwECqVKlSpDpgP0YTExPzbUtMTLzgdNXSlpqaytChQ4mJieHzzz8v8voJMTExbNy4Md+2Jk2aXHCJ08XEx8ezZcsWJk+enHeMxsfHM378eIYNG+ZwBpPJlPeN5LzGjRtz8uRJh8aHhITQsGHDfKfvl+QYveaaa4o8DmDnzp0X3JY4NDS0SMfpLbfcwsaNG1mxYgXz5s3DZDJd9PR1uPjXpqIcl8X92uZIDUePy4uNL+ox+e8axTkuL5ajKMflxcYX9bgs6N+jJMelXD4qw9wJSj5/Kg9zJ9D86VIqw/ypJHMn0Pzpn5wxdwLH508lnTtdqkZRaf506fGVYf6kJlEpCQ0Nxc3NLd+CYZs2bSI8PLxY13EX13vvvcfs2bN58803i/zbm6NHjzJixIh8/yF27NhBUFCQw+sCfPnll8yfP58ffviBH374gZ49e9KzZ09++OEHh3OsWrWKTp06kZGRkbdt9+7dBAQEOJQjIiKCrKwsDh48mLftwIED+SY9jtq2bRvt2rUr8riQkBAOHz6c7zekBw4cuOQPzxeTnJzMnXfeSVJSEtWrV8fNzY3ly5cXayFxsH9edu7cSWZmZt62TZs2ERERUax6xWGz2RgxYgRHjx7lyy+/pFmzZkWusWzZMv773/9iGEbetp07d170WvB/q1GjBosXL847Pn/44QdCQkIYNWoUL7/8ssMZ3nnnHe6///582/bs2eNQBrD/W8TExGC1WvO2xcbGFvkYNQyD7du3F+sYBftxun///nzbinKcrl27lieeeAKLxUJISAiGYeT9//23S31tcvS4LMnXtsJqOHpcXmp8UY7Ji9Uo6nF5qRyOHpcF/Vs4elwW9O9R0uNSLh+VYe4EJZ8/lYe5E2j+dCmVYf5UkrkTaP70TyWdO4Hj86eSzp0KqlEUmj8VPL5SzJ/K7D5ql6Fx48YZ1113nbF161ZjyZIlRrt27YxFixYVq1ZRb+FqGPbb6oWGhhpvvfWWcerUqXwfjsjNzTUGDhxoDBkyxIiJiTGWL19uXHHFFcZnn31WnF0wDMMwnn322SLfxjUlJcXo3r278eSTTxqxsbHG8uXLjW7duhkff/yxwzUefPBB4/bbbzd2795trFy50ujcubPx+eefFzW+0aNHD+Pnn38u8rhz584ZXbt2NZ5++mnjwIEDxm+//WZ07NjRmDVrVpHq3Hjjjcbo0aONI0eOGN98840RHh5ubN261eHx/zyOcnNzjf79+xuPP/64sW/fPuOjjz4yIiMjjWPHjjlc47y1a9cWeAvXS42fM2eO0bJlS2PZsmX5js+kpCSHaxw/ftxo166d8eqrrxoHDx40vvrqKyMsLMzYsWNHkfbhvB49elzyFq6XqrF161ajVatWxrRp04zDhw8bM2fONFq3bm1s3rzZofEpKSlGt27djHHjxhmHDh0yvvrqK6NVq1YF7sPF9iMuLs5o3ry5w//H/11jy5YtRmhoqDFjxgzjyJEjxowZM4ywsDBj3759Do0/ceKEERERYcycOdM4cuSIMX78eKN79+5GampqvjEFfW1y5Lh09GtbQcdlQTUcOS4LGu/oMVmUr9GXOi4LquHIcVnQeEePy8L2ozjHpVy+nDl3Moyiz59KOncyDOfPn1w1dzIMzZ/Oqwzzp5LOnS61D+ddTvOnks6d/l3DkflTSedOhdX4J82fNH9Sk6gUpaenG88884wRGRlpdOvWzZgxY0axaxWnSfTRRx8ZzZs3v+iHo06cOGEMHz7caNeundG1a1fjww8/NGw2W1Hj5ynORMcwDGPfvn3G/fffb0RGRhpdu3Y1pkyZUqQc586dM55++mkjMjLS6NKlS5HHnxceHm6sXLmyyOMMwzBiYmKM+++/32jXrp3Rq1cvY8aMGUXOEBsba9xzzz1GRESEcd111xm///57kcb/+zg6dOiQcffddxutW7c2rrvuOuOPP/4ocg3DKP4kZ8iQIRc9Pu+5554iZdiyZYsxaNAgo02bNsa1115rLF26tMj7cF5xJjmGYRhLliwxbrjhBiM8PNzo169foT/U/Ht8TExM3r9Fnz59HPqh6N81oqOjjebNmxtZWVmFjr1UjaVLlxo33nijERkZadx8882FHhP/Hr9s2TKjX79+RkREhHHfffcZ+/fvv2BMYV+bCjsuHf3aVtBxWVANR47LwjI4ckwW5Wv0pY7LwmoUdlwWNt6R47KwGsU5LuXy5cy5k2EUff7kjLmTYTh3/uSquZNhaP50XmWYP5V07nSpfTjvcpo/lXTudLEahc2fSjp3cqTGeZo/af5kMox/nM8lIiIiIiIiIiKXJa1JJCIiIiIiIiIiahKJiIiIiIiIiIiaRCIiIiIiIiIigppEIiIiIiIiIiKCmkQiIiIiIiIiIoKaRCIiIiIiIiIigppEIiIiIiIiIiKCmkQiIiIiIiIiIgK4uTqAiMi/9ezZk2PHjl30uS+++IJOnTqVyvs+99xzAPzvf/8rlfoiIiIipUXzJxFxBjWJRKRcGjNmDP37979ge9WqVV2QRkRERKT80/xJREpKTSIRKZf8/PyoXr26q2OIiIiIVBiaP4lISWlNIhGpcHr27Mlnn33GDTfcQGRkJA8++CAJCQl5z8fGxjJ06FDatWtH9+7dee+997DZbHnP//jjj/Tr14+IiAjuuOMOdu3alfdcamoqTzzxBBEREVx99dXMnz+/TPdNREREpDRo/iQijlCTSEQqpClTpjBs2DDmzJlDRkYGI0eOBODMmTPcddddhISEMHfuXMaPH89XX33FF198AcCqVasYO3YsgwcP5qeffqJ169Y89NBDZGdnA7BkyRLCwsL4+eefufbaaxkzZgwpKSku208RERERZ9H8SUQKYzIMw3B1CBGRf+rZsycJCQm4ueW/IrZ27dosWLCAnj170qtXL8aMGQNAXFwcvXr1Yv78+axdu5bp06ezdOnSvPGzZs3i/fffZ/Xq1YwYMQJfX9+8xRWzs7N56623GDJkCG+88QaHDh1i9uzZAKSkpNC+fXu++eYbIiIiyvAzICIiIlI0mj+JiDNoTSIRKZdGjRpFnz598m3756SnXbt2eX+vV68eAQEBxMbGEhsbS1hYWL7Xtm3bloSEBM6dO8fBgwe544478p7z8PDg2WefzVfrPD8/PwCysrKct2MiIiIipUTzJxEpKTWJRKRcqlatGg0aNLjk8//+LZnVasVsNuPp6XnBa89fT2+1Wi8Y928Wi+WCbTrhUkRERCoCzZ9EpKS0JpGIVEh79uzJ+/vhw4dJSUmhRYsWNGrUiJ07d5KTk5P3/JYtWwgKCiIgIIAGDRrkG2u1WunZsyebNm0q0/wiIiIiZU3zJxEpjJpEIlIupaSkkJCQcMFHeno6AF988QW//fYbe/bsYcyYMXTt2pWGDRtyww03kJ2dzfPPP09sbCxLly5lypQp3HnnnZhMJu69915++uknvv/+ew4fPsykSZMwDIOwsDAX77GIiIhIyWj+JCIlpcvNRKRceuWVV3jllVcu2P7YY48BcPPNN/Pmm28SHx/PVVddxcSJEwHw9fVl2rRpvPzyywwYMICgoCAGDx7MQw89BECHDh0YP34877//PgkJCbRu3ZqpU6fi5eVVdjsnIiIiUgo0fxKRktLdzUSkwunZsycjRoxg4MCBro4iIiIiUiFo/iQiNgYB+AAAAIFJREFUjtDlZiIiIiIiIiIioiaRiIiIiIiIiIjocjMREREREREREUFnEomIiIiIiIiICGoSiYiIiIiIiIgIahKJiIiIiIiIiAhqEomIiIiIiIiICGoSiYiIiIiIiIgIahKJiIiIiIiIiAhqEomIiIiIiIiICGoSiYiIiIiIiIgI8P/xMj6nUvkAGQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1400x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TrainLoop(modelv3, optimizer_3, loss_fn_3, train_loader, val_loader, scheduler_3, 100, 20, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 68.28%\n"
     ]
    }
   ],
   "source": [
    "# test_set = CustomTextClassificationDataset(test_x, test_y, word_to_index)\n",
    "# test_loader = DataLoader(test_set, 32)\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "with torch.inference_mode():\n",
    "    for batch in test_loader:\n",
    "        caption_tokens = batch['text_indices'].to('cuda')\n",
    "        labels = batch['label'].to('cuda', dtype=torch.long)\n",
    "        preds = torch.argmax(modelv3(caption_tokens), dim=1)\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        pred_labels.extend(preds.cpu().numpy())\n",
    "\n",
    "true_labels = np.array(true_labels)\n",
    "pred_labels = np.array(pred_labels)\n",
    "\n",
    "modelv3_accuracy = accuracy_score(true_labels, pred_labels)\n",
    "\n",
    "print(f'Accuracy: {modelv3_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMmodelv2(torch.nn.Module):\n",
    "    def __init__(self, n_layers, embed_dim, hidden_dim, embedding:str=\"twitter.27B\", bidirectionality:bool=False) -> None:\n",
    "        super().__init__()\n",
    "        glove_embeddings = torchtext.vocab.GloVe(embedding, embed_dim)\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(glove_embeddings.vectors, freeze=True)\n",
    "        self.lstm = torch.nn.LSTM(embed_dim, hidden_dim, n_layers, batch_first=True, bidirectional=bidirectionality)\n",
    "        if bidirectionality == False:\n",
    "            self.linear = torch.nn.Linear(hidden_dim, 3)\n",
    "        else:\n",
    "            self.linear = torch.nn.Linear(2*hidden_dim, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        a = self.embedding(x)\n",
    "        a, _ = self.lstm(a)\n",
    "        a =  a[:,-1,:]\n",
    "        a = self.linear(a)\n",
    "        return torch.nn.functional.log_softmax(a, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelv4 = LSTMmodelv2(4, 25, 256, bidirectionality=True)\n",
    "loss_fn_4 = torch.nn.NLLLoss()\n",
    "optimizer_4 = torch.optim.NAdam(modelv4.parameters(), 0.001)\n",
    "scheduler_4 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_4, 'min', 0.4, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7bdd1b79d9c44a2abbc5dc4faa941c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0\n",
      "----------\n",
      "Loss for batch 0 = 1.0993565320968628\n",
      "Loss for batch 1 = 1.1046375036239624\n",
      "Loss for batch 2 = 1.089618444442749\n",
      "Loss for batch 3 = 1.1192024946212769\n",
      "Loss for batch 4 = 1.107176423072815\n",
      "Loss for batch 5 = 1.095347285270691\n",
      "Loss for batch 6 = 1.101747989654541\n",
      "Loss for batch 7 = 1.090149998664856\n",
      "Loss for batch 8 = 1.1008758544921875\n",
      "Loss for batch 9 = 1.0866656303405762\n",
      "Loss for batch 10 = 1.0908020734786987\n",
      "Loss for batch 11 = 1.0980010032653809\n",
      "Loss for batch 12 = 1.0312471389770508\n",
      "Loss for batch 13 = 1.4944510459899902\n",
      "Loss for batch 14 = 1.1400336027145386\n",
      "Loss for batch 15 = 1.080434799194336\n",
      "Loss for batch 16 = 1.111159324645996\n",
      "Loss for batch 17 = 1.068657636642456\n",
      "Loss for batch 18 = 1.1230788230895996\n",
      "Loss for batch 19 = 1.0808005332946777\n",
      "Loss for batch 20 = 1.111127257347107\n",
      "Loss for batch 21 = 1.1041368246078491\n",
      "Loss for batch 22 = 1.0953290462493896\n",
      "Loss for batch 23 = 1.0785484313964844\n",
      "Loss for batch 24 = 1.0526187419891357\n",
      "Loss for batch 25 = 1.0974541902542114\n",
      "Loss for batch 26 = 1.0328007936477661\n",
      "Loss for batch 27 = 1.085588812828064\n",
      "Loss for batch 28 = 1.0872833728790283\n",
      "Loss for batch 29 = 1.0968002080917358\n",
      "Loss for batch 30 = 1.0858352184295654\n",
      "Loss for batch 31 = 1.0960074663162231\n",
      "Loss for batch 32 = 1.0720206499099731\n",
      "Loss for batch 33 = 1.0487053394317627\n",
      "Loss for batch 34 = 0.979653537273407\n",
      "Loss for batch 35 = 1.22283136844635\n",
      "Loss for batch 36 = 1.065002679824829\n",
      "Loss for batch 37 = 1.0517566204071045\n",
      "Loss for batch 38 = 1.1825090646743774\n",
      "Loss for batch 39 = 1.0538674592971802\n",
      "Loss for batch 40 = 1.0850850343704224\n",
      "Loss for batch 41 = 1.106669545173645\n",
      "Loss for batch 42 = 1.0700578689575195\n",
      "Loss for batch 43 = 1.0762581825256348\n",
      "Loss for batch 44 = 1.091948390007019\n",
      "Loss for batch 45 = 1.0289242267608643\n",
      "Loss for batch 46 = 1.0066804885864258\n",
      "Loss for batch 47 = 1.0844910144805908\n",
      "Loss for batch 48 = 1.0410397052764893\n",
      "Loss for batch 49 = 1.0574828386306763\n",
      "Loss for batch 50 = 1.1296168565750122\n",
      "Loss for batch 51 = 1.0450778007507324\n",
      "Loss for batch 52 = 1.0335309505462646\n",
      "Loss for batch 53 = 1.0758731365203857\n",
      "Loss for batch 54 = 1.0285637378692627\n",
      "Loss for batch 55 = 1.1313353776931763\n",
      "Loss for batch 56 = 1.089475393295288\n",
      "Loss for batch 57 = 1.0210270881652832\n",
      "Loss for batch 58 = 1.142494797706604\n",
      "Loss for batch 59 = 1.0346665382385254\n",
      "Loss for batch 60 = 1.0252035856246948\n",
      "Loss for batch 61 = 1.0175580978393555\n",
      "Loss for batch 62 = 1.0332682132720947\n",
      "Loss for batch 63 = 1.0356663465499878\n",
      "Loss for batch 64 = 1.0359505414962769\n",
      "Loss for batch 65 = 1.1905983686447144\n",
      "Loss for batch 66 = 1.1184462308883667\n",
      "Loss for batch 67 = 1.0626343488693237\n",
      "Loss for batch 68 = 1.0469768047332764\n",
      "Loss for batch 69 = 0.9960033297538757\n",
      "Loss for batch 70 = 1.050446629524231\n",
      "Loss for batch 71 = 0.8802184462547302\n",
      "Loss for batch 72 = 1.0334527492523193\n",
      "Loss for batch 73 = 0.8025169372558594\n",
      "Loss for batch 74 = 1.2495821714401245\n",
      "Loss for batch 75 = 1.0526208877563477\n",
      "Loss for batch 76 = 1.0470980405807495\n",
      "Loss for batch 77 = 1.0777899026870728\n",
      "Loss for batch 78 = 0.9867703914642334\n",
      "Loss for batch 79 = 1.0612335205078125\n",
      "Loss for batch 80 = 1.060880184173584\n",
      "Loss for batch 81 = 1.064895510673523\n",
      "Loss for batch 82 = 1.0121879577636719\n",
      "Loss for batch 83 = 1.0550789833068848\n",
      "Loss for batch 84 = 1.0855474472045898\n",
      "Loss for batch 85 = 1.2144373655319214\n",
      "Loss for batch 86 = 1.0885311365127563\n",
      "Loss for batch 87 = 1.0794596672058105\n",
      "Loss for batch 88 = 1.0896306037902832\n",
      "Loss for batch 89 = 1.0886578559875488\n",
      "Loss for batch 90 = 1.0899651050567627\n",
      "Loss for batch 91 = 1.0856631994247437\n",
      "Loss for batch 92 = 1.0931850671768188\n",
      "Loss for batch 93 = 1.0600001811981201\n",
      "Loss for batch 94 = 1.058629035949707\n",
      "Loss for batch 95 = 1.140811800956726\n",
      "Loss for batch 96 = 0.983787477016449\n",
      "Loss for batch 97 = 1.0194902420043945\n",
      "Loss for batch 98 = 1.051364779472351\n",
      "Loss for batch 99 = 1.0354677438735962\n",
      "Loss for batch 100 = 1.08797287940979\n",
      "Loss for batch 101 = 1.0194799900054932\n",
      "Loss for batch 102 = 1.009575366973877\n",
      "Loss for batch 103 = 1.0914596319198608\n",
      "Loss for batch 104 = 0.9783055186271667\n",
      "Loss for batch 105 = 1.0267248153686523\n",
      "Loss for batch 106 = 1.087345838546753\n",
      "\n",
      "Training Loss for epoch 0 = 114.95619201660156\n",
      "\n",
      "Current Validation Loss = 16.677337646484375\n",
      "Best Validation Loss = 16.677337646484375\n",
      "Epochs without Improvement = 0\n",
      "Train Accuracy: 43.60%\n",
      "Validation Accuracy: 43.74%\n",
      "\n",
      "Epoch 1\n",
      "----------\n",
      "Loss for batch 0 = 0.9880834221839905\n",
      "Loss for batch 1 = 0.9651703238487244\n",
      "Loss for batch 2 = 1.142892599105835\n",
      "Loss for batch 3 = 1.1146851778030396\n",
      "Loss for batch 4 = 1.0728251934051514\n",
      "Loss for batch 5 = 1.1043275594711304\n",
      "Loss for batch 6 = 1.1041028499603271\n",
      "Loss for batch 7 = 1.0957460403442383\n",
      "Loss for batch 8 = 1.0948307514190674\n",
      "Loss for batch 9 = 1.0803931951522827\n",
      "Loss for batch 10 = 1.066074252128601\n",
      "Loss for batch 11 = 1.100597620010376\n",
      "Loss for batch 12 = 1.1402519941329956\n",
      "Loss for batch 13 = 1.0885826349258423\n",
      "Loss for batch 14 = 1.1047676801681519\n",
      "Loss for batch 15 = 1.0892415046691895\n",
      "Loss for batch 16 = 1.09651517868042\n",
      "Loss for batch 17 = 1.08406662940979\n",
      "Loss for batch 18 = 1.10835599899292\n",
      "Loss for batch 19 = 1.0892049074172974\n",
      "Loss for batch 20 = 1.0966084003448486\n",
      "Loss for batch 21 = 1.0954680442810059\n",
      "Loss for batch 22 = 1.0980850458145142\n",
      "Loss for batch 23 = 1.065554141998291\n",
      "Loss for batch 24 = 1.0365091562271118\n",
      "Loss for batch 25 = 1.139471411705017\n",
      "Loss for batch 26 = 1.0397007465362549\n",
      "Loss for batch 27 = 1.0766322612762451\n",
      "Loss for batch 28 = 1.103250503540039\n",
      "Loss for batch 29 = 1.1145904064178467\n",
      "Loss for batch 30 = 1.0799148082733154\n",
      "Loss for batch 31 = 1.095141053199768\n",
      "Loss for batch 32 = 1.0887562036514282\n",
      "Loss for batch 33 = 1.0734349489212036\n",
      "Loss for batch 34 = 1.018945336341858\n",
      "Loss for batch 35 = 1.044439435005188\n",
      "Loss for batch 36 = 1.090239405632019\n",
      "Loss for batch 37 = 1.063256025314331\n",
      "Loss for batch 38 = 1.116748571395874\n",
      "Loss for batch 39 = 1.04532790184021\n",
      "Loss for batch 40 = 1.0716068744659424\n",
      "Loss for batch 41 = 1.1193257570266724\n",
      "Loss for batch 42 = 1.0487884283065796\n",
      "Loss for batch 43 = 1.0782784223556519\n",
      "Loss for batch 44 = 1.1397285461425781\n",
      "Loss for batch 45 = 1.0220646858215332\n",
      "Loss for batch 46 = 1.0118402242660522\n",
      "Loss for batch 47 = 1.0864696502685547\n",
      "Loss for batch 48 = 1.0482124090194702\n",
      "Loss for batch 49 = 1.1058205366134644\n",
      "Loss for batch 50 = 1.0821595191955566\n",
      "Loss for batch 51 = 1.0282301902770996\n",
      "Loss for batch 52 = 1.0158756971359253\n",
      "Loss for batch 53 = 1.0608093738555908\n",
      "Loss for batch 54 = 1.0105794668197632\n",
      "Loss for batch 55 = 1.160193920135498\n",
      "Loss for batch 56 = 1.1004648208618164\n",
      "Loss for batch 57 = 1.0079344511032104\n",
      "Loss for batch 58 = 1.1376019716262817\n",
      "Loss for batch 59 = 1.0496280193328857\n",
      "Loss for batch 60 = 1.0084095001220703\n",
      "Loss for batch 61 = 1.0011756420135498\n",
      "Loss for batch 62 = 1.0119205713272095\n",
      "Loss for batch 63 = 1.0315377712249756\n",
      "Loss for batch 64 = 1.0227218866348267\n",
      "Loss for batch 65 = 1.1761674880981445\n",
      "Loss for batch 66 = 1.1136445999145508\n",
      "Loss for batch 67 = 1.0145431756973267\n",
      "Loss for batch 68 = 0.9905644059181213\n",
      "Loss for batch 69 = 0.9833745956420898\n",
      "Loss for batch 70 = 1.052196979522705\n",
      "Loss for batch 71 = 0.9442678093910217\n",
      "Loss for batch 72 = 1.0646113157272339\n",
      "Loss for batch 73 = 0.9069920182228088\n",
      "Loss for batch 74 = 1.2030470371246338\n",
      "Loss for batch 75 = 1.102309226989746\n",
      "Loss for batch 76 = 1.0903129577636719\n",
      "Loss for batch 77 = 1.0628293752670288\n",
      "Loss for batch 78 = 1.0232508182525635\n",
      "Loss for batch 79 = 1.0392171144485474\n",
      "Loss for batch 80 = 1.0224727392196655\n",
      "Loss for batch 81 = 1.080980658531189\n",
      "Loss for batch 82 = 0.9968197345733643\n",
      "Loss for batch 83 = 1.041874647140503\n",
      "Loss for batch 84 = 1.0308080911636353\n",
      "Loss for batch 85 = 1.2675631046295166\n",
      "Loss for batch 86 = 1.0493706464767456\n",
      "Loss for batch 87 = 1.0210813283920288\n",
      "Loss for batch 88 = 1.0515724420547485\n",
      "Loss for batch 89 = 1.0416491031646729\n",
      "Loss for batch 90 = 1.0690312385559082\n",
      "Loss for batch 91 = 1.0409791469573975\n",
      "Loss for batch 92 = 1.1068021059036255\n",
      "Loss for batch 93 = 1.0385373830795288\n",
      "Loss for batch 94 = 1.0477795600891113\n",
      "Loss for batch 95 = 1.118801474571228\n",
      "Loss for batch 96 = 0.956024706363678\n",
      "Loss for batch 97 = 0.9998083114624023\n",
      "Loss for batch 98 = 1.052107810974121\n",
      "Loss for batch 99 = 1.0195764303207397\n",
      "Loss for batch 100 = 1.0981817245483398\n",
      "Loss for batch 101 = 1.0044450759887695\n",
      "Loss for batch 102 = 0.9914031624794006\n",
      "Loss for batch 103 = 1.102724552154541\n",
      "Loss for batch 104 = 0.9555822014808655\n",
      "Loss for batch 105 = 1.0325536727905273\n",
      "Loss for batch 106 = 1.118418574333191\n",
      "\n",
      "Training Loss for epoch 1 = 113.89845275878906\n",
      "\n",
      "Current Validation Loss = 16.614999771118164\n",
      "Best Validation Loss = 16.614999771118164\n",
      "Epochs without Improvement = 0\n",
      "Train Accuracy: 44.75%\n",
      "Validation Accuracy: 44.35%\n",
      "\n",
      "Epoch 2\n",
      "----------\n",
      "Loss for batch 0 = 0.9840359091758728\n",
      "Loss for batch 1 = 0.97391277551651\n",
      "Loss for batch 2 = 1.054922342300415\n",
      "Loss for batch 3 = 1.0224502086639404\n",
      "Loss for batch 4 = 1.0349277257919312\n",
      "Loss for batch 5 = 1.1118966341018677\n",
      "Loss for batch 6 = 1.0803261995315552\n",
      "Loss for batch 7 = 1.0172406435012817\n",
      "Loss for batch 8 = 1.1091291904449463\n",
      "Loss for batch 9 = 0.9770975708961487\n",
      "Loss for batch 10 = 0.9747307896614075\n",
      "Loss for batch 11 = 0.9073769450187683\n",
      "Loss for batch 12 = 1.3591288328170776\n",
      "Loss for batch 13 = 1.066500186920166\n",
      "Loss for batch 14 = 1.0843783617019653\n",
      "Loss for batch 15 = 1.0500811338424683\n",
      "Loss for batch 16 = 1.037827491760254\n",
      "Loss for batch 17 = 1.0401198863983154\n",
      "Loss for batch 18 = 1.111976981163025\n",
      "Loss for batch 19 = 0.9948365688323975\n",
      "Loss for batch 20 = 1.0648666620254517\n",
      "Loss for batch 21 = 0.9673995971679688\n",
      "Loss for batch 22 = 1.1277633905410767\n",
      "Loss for batch 23 = 1.0177785158157349\n",
      "Loss for batch 24 = 0.9310423135757446\n",
      "Loss for batch 25 = 1.1562408208847046\n",
      "Loss for batch 26 = 1.0386269092559814\n",
      "Loss for batch 27 = 1.034437656402588\n",
      "Loss for batch 28 = 1.0427093505859375\n",
      "Loss for batch 29 = 1.0705146789550781\n",
      "Loss for batch 30 = 0.9992712736129761\n",
      "Loss for batch 31 = 1.1122198104858398\n",
      "Loss for batch 32 = 1.1169490814208984\n",
      "Loss for batch 33 = 1.0031646490097046\n",
      "Loss for batch 34 = 1.0495284795761108\n",
      "Loss for batch 35 = 1.0371562242507935\n",
      "Loss for batch 36 = 1.0189980268478394\n",
      "Loss for batch 37 = 1.0711612701416016\n",
      "Loss for batch 38 = 1.0260168313980103\n",
      "Loss for batch 39 = 0.9132227897644043\n",
      "Loss for batch 40 = 1.0009212493896484\n",
      "Loss for batch 41 = 1.0709031820297241\n",
      "Loss for batch 42 = 1.075095534324646\n",
      "Loss for batch 43 = 1.061169147491455\n",
      "Loss for batch 44 = 1.0869030952453613\n",
      "Loss for batch 45 = 0.9576742053031921\n",
      "Loss for batch 46 = 0.862080991268158\n",
      "Loss for batch 47 = 1.0183392763137817\n",
      "Loss for batch 48 = 1.0125361680984497\n",
      "Loss for batch 49 = 0.9618486762046814\n",
      "Loss for batch 50 = 1.1035891771316528\n",
      "Loss for batch 51 = 0.9585055112838745\n",
      "Loss for batch 52 = 1.014580488204956\n",
      "Loss for batch 53 = 1.0210061073303223\n",
      "Loss for batch 54 = 0.961867094039917\n",
      "Loss for batch 55 = 1.215631127357483\n",
      "Loss for batch 56 = 1.051531195640564\n",
      "Loss for batch 57 = 0.9288191199302673\n",
      "Loss for batch 58 = 1.1454161405563354\n",
      "Loss for batch 59 = 1.0009586811065674\n",
      "Loss for batch 60 = 0.9584982395172119\n",
      "Loss for batch 61 = 1.0262726545333862\n",
      "Loss for batch 62 = 1.0861549377441406\n",
      "Loss for batch 63 = 1.1109910011291504\n",
      "Loss for batch 64 = 1.031571626663208\n",
      "Loss for batch 65 = 1.147393822669983\n",
      "Loss for batch 66 = 1.1191076040267944\n",
      "Loss for batch 67 = 1.027388095855713\n",
      "Loss for batch 68 = 1.0188179016113281\n",
      "Loss for batch 69 = 0.98591148853302\n",
      "Loss for batch 70 = 1.0511829853057861\n",
      "Loss for batch 71 = 0.9351046681404114\n",
      "Loss for batch 72 = 0.9949284791946411\n",
      "Loss for batch 73 = 0.8872400522232056\n",
      "Loss for batch 74 = 1.143288016319275\n",
      "Loss for batch 75 = 1.0354082584381104\n",
      "Loss for batch 76 = 1.0484272241592407\n",
      "Loss for batch 77 = 1.073676347732544\n",
      "Loss for batch 78 = 1.0029523372650146\n",
      "Loss for batch 79 = 0.9793188571929932\n",
      "Loss for batch 80 = 0.9952223896980286\n",
      "Loss for batch 81 = 1.0592584609985352\n",
      "Loss for batch 82 = 1.0072553157806396\n",
      "Loss for batch 83 = 1.028738260269165\n",
      "Loss for batch 84 = 1.005938172340393\n",
      "Loss for batch 85 = 1.2277772426605225\n",
      "Loss for batch 86 = 0.9519016742706299\n",
      "Loss for batch 87 = 0.9241321086883545\n",
      "Loss for batch 88 = 1.0121304988861084\n",
      "Loss for batch 89 = 1.0128675699234009\n",
      "Loss for batch 90 = 1.156770944595337\n",
      "Loss for batch 91 = 0.9819326996803284\n",
      "Loss for batch 92 = 1.0805878639221191\n",
      "Loss for batch 93 = 1.0121026039123535\n",
      "Loss for batch 94 = 1.056709885597229\n",
      "Loss for batch 95 = 1.0904462337493896\n",
      "Loss for batch 96 = 0.974943995475769\n",
      "Loss for batch 97 = 0.9584276676177979\n",
      "Loss for batch 98 = 1.0783053636550903\n",
      "Loss for batch 99 = 0.988741934299469\n",
      "Loss for batch 100 = 1.1200182437896729\n",
      "Loss for batch 101 = 1.0752171277999878\n",
      "Loss for batch 102 = 1.008021593093872\n",
      "Loss for batch 103 = 1.1187785863876343\n",
      "Loss for batch 104 = 0.9273545742034912\n",
      "Loss for batch 105 = 0.9968010783195496\n",
      "Loss for batch 106 = 1.067089319229126\n",
      "\n",
      "Training Loss for epoch 2 = 110.88245391845703\n",
      "\n",
      "Current Validation Loss = 17.002601623535156\n",
      "Best Validation Loss = 16.614999771118164\n",
      "Epochs without Improvement = 1\n",
      "Train Accuracy: 40.61%\n",
      "Validation Accuracy: 39.22%\n",
      "\n",
      "Epoch 3\n",
      "----------\n",
      "Loss for batch 0 = 1.0028470754623413\n",
      "Loss for batch 1 = 0.9469534158706665\n",
      "Loss for batch 2 = 0.9777950644493103\n",
      "Loss for batch 3 = 1.03659188747406\n",
      "Loss for batch 4 = 1.0470925569534302\n",
      "Loss for batch 5 = 1.032210350036621\n",
      "Loss for batch 6 = 1.0434123277664185\n",
      "Loss for batch 7 = 1.0340256690979004\n",
      "Loss for batch 8 = 1.0971020460128784\n",
      "Loss for batch 9 = 0.9320764541625977\n",
      "Loss for batch 10 = 0.9766757488250732\n",
      "Loss for batch 11 = 1.0408779382705688\n",
      "Loss for batch 12 = 0.9620059728622437\n",
      "Loss for batch 13 = 1.0517538785934448\n",
      "Loss for batch 14 = 1.0744255781173706\n",
      "Loss for batch 15 = 1.0200392007827759\n",
      "Loss for batch 16 = 1.2134684324264526\n",
      "Loss for batch 17 = 1.0358346700668335\n",
      "Loss for batch 18 = 1.1039469242095947\n",
      "Loss for batch 19 = 1.0552467107772827\n",
      "Loss for batch 20 = 1.0978646278381348\n",
      "Loss for batch 21 = 0.9593604803085327\n",
      "Loss for batch 22 = 1.1619315147399902\n",
      "Loss for batch 23 = 0.9840558767318726\n",
      "Loss for batch 24 = 0.9078094363212585\n",
      "Loss for batch 25 = 1.0860497951507568\n",
      "Loss for batch 26 = 1.0458868741989136\n",
      "Loss for batch 27 = 1.0891879796981812\n",
      "Loss for batch 28 = 1.04856276512146\n",
      "Loss for batch 29 = 0.9670512676239014\n",
      "Loss for batch 30 = 1.0096746683120728\n",
      "Loss for batch 31 = 1.1658425331115723\n",
      "Loss for batch 32 = 1.076053500175476\n",
      "Loss for batch 33 = 1.002915620803833\n",
      "Loss for batch 34 = 1.054445743560791\n",
      "Loss for batch 35 = 1.0213854312896729\n",
      "Loss for batch 36 = 1.0379008054733276\n",
      "Loss for batch 37 = 1.0313193798065186\n",
      "Loss for batch 38 = 1.052382469177246\n",
      "Loss for batch 39 = 0.9352070689201355\n",
      "Loss for batch 40 = 1.0490409135818481\n",
      "Loss for batch 41 = 1.1205545663833618\n",
      "Loss for batch 42 = 0.9826487898826599\n",
      "Loss for batch 43 = 1.000885009765625\n",
      "Loss for batch 44 = 1.0794094800949097\n",
      "Loss for batch 45 = 0.9674814939498901\n",
      "Loss for batch 46 = 0.9075585007667542\n",
      "Loss for batch 47 = 1.0185984373092651\n",
      "Loss for batch 48 = 1.0011835098266602\n",
      "Loss for batch 49 = 0.9264245629310608\n",
      "Loss for batch 50 = 1.1819305419921875\n",
      "Loss for batch 51 = 0.9996669292449951\n",
      "Loss for batch 52 = 1.0154684782028198\n",
      "Loss for batch 53 = 1.0301125049591064\n",
      "Loss for batch 54 = 0.9828047156333923\n",
      "Loss for batch 55 = 1.1288745403289795\n",
      "Loss for batch 56 = 1.1100701093673706\n",
      "Loss for batch 57 = 0.961047351360321\n",
      "Loss for batch 58 = 1.1609185934066772\n",
      "Loss for batch 59 = 1.0408037900924683\n",
      "Loss for batch 60 = 0.9967470169067383\n",
      "Loss for batch 61 = 1.0628597736358643\n",
      "Loss for batch 62 = 0.9940369725227356\n",
      "Loss for batch 63 = 1.1029561758041382\n",
      "Loss for batch 64 = 1.0621531009674072\n",
      "Loss for batch 65 = 1.0840023756027222\n",
      "Loss for batch 66 = 1.125773549079895\n",
      "Loss for batch 67 = 1.0806370973587036\n",
      "Loss for batch 68 = 1.1403601169586182\n",
      "Loss for batch 69 = 0.9679175019264221\n",
      "Loss for batch 70 = 1.0309890508651733\n",
      "Loss for batch 71 = 0.9639573693275452\n",
      "Loss for batch 72 = 1.019775629043579\n",
      "Loss for batch 73 = 0.9309930205345154\n",
      "Loss for batch 74 = 1.0516480207443237\n",
      "Loss for batch 75 = 1.1285665035247803\n",
      "Loss for batch 76 = 0.9817724823951721\n",
      "Loss for batch 77 = 1.1133980751037598\n",
      "Loss for batch 78 = 1.175724744796753\n",
      "Loss for batch 79 = 1.0675771236419678\n",
      "Loss for batch 80 = 1.1030848026275635\n",
      "Loss for batch 81 = 1.124964714050293\n",
      "Loss for batch 82 = 1.0594650506973267\n",
      "Loss for batch 83 = 1.060794711112976\n",
      "Loss for batch 84 = 1.0656287670135498\n",
      "Loss for batch 85 = 1.2579530477523804\n",
      "Loss for batch 86 = 1.0028797388076782\n",
      "Loss for batch 87 = 1.062815546989441\n",
      "Loss for batch 88 = 1.1064376831054688\n",
      "Loss for batch 89 = 1.0533210039138794\n",
      "Loss for batch 90 = 1.0991114377975464\n",
      "Loss for batch 91 = 1.0606474876403809\n",
      "Loss for batch 92 = 1.0834178924560547\n",
      "Loss for batch 93 = 1.096422553062439\n",
      "Loss for batch 94 = 1.0643137693405151\n",
      "Loss for batch 95 = 1.0501776933670044\n",
      "Loss for batch 96 = 0.994633138179779\n",
      "Loss for batch 97 = 1.0271062850952148\n",
      "Loss for batch 98 = 1.0482840538024902\n",
      "Loss for batch 99 = 1.010511040687561\n",
      "Loss for batch 100 = 1.0641173124313354\n",
      "Loss for batch 101 = 1.0812652111053467\n",
      "Loss for batch 102 = 0.9514651298522949\n",
      "Loss for batch 103 = 1.1097118854522705\n",
      "Loss for batch 104 = 0.9603797197341919\n",
      "Loss for batch 105 = 1.0393630266189575\n",
      "Loss for batch 106 = 1.1771891117095947\n",
      "\n",
      "Training Loss for epoch 3 = 111.98204803466797\n",
      "\n",
      "Current Validation Loss = 16.591156005859375\n",
      "Best Validation Loss = 16.591156005859375\n",
      "Epochs without Improvement = 0\n",
      "Train Accuracy: 46.24%\n",
      "Validation Accuracy: 45.79%\n",
      "\n",
      "Epoch 4\n",
      "----------\n",
      "Loss for batch 0 = 0.9688062071800232\n",
      "Loss for batch 1 = 0.9120894074440002\n",
      "Loss for batch 2 = 1.1269407272338867\n",
      "Loss for batch 3 = 0.9935540556907654\n",
      "Loss for batch 4 = 1.0554238557815552\n",
      "Loss for batch 5 = 1.0639923810958862\n",
      "Loss for batch 6 = 1.0506092309951782\n",
      "Loss for batch 7 = 1.0094696283340454\n",
      "Loss for batch 8 = 1.0779060125350952\n",
      "Loss for batch 9 = 0.9844300150871277\n",
      "Loss for batch 10 = 0.958042323589325\n",
      "Loss for batch 11 = 0.9746695756912231\n",
      "Loss for batch 12 = 0.8938283920288086\n",
      "Loss for batch 13 = 1.0424939393997192\n",
      "Loss for batch 14 = 0.9942833781242371\n",
      "Loss for batch 15 = 1.0775556564331055\n",
      "Loss for batch 16 = 0.9998303055763245\n",
      "Loss for batch 17 = 1.03629469871521\n",
      "Loss for batch 18 = 1.1411000490188599\n",
      "Loss for batch 19 = 1.0155982971191406\n",
      "Loss for batch 20 = 1.0589057207107544\n",
      "Loss for batch 21 = 0.9536970853805542\n",
      "Loss for batch 22 = 1.1593704223632812\n",
      "Loss for batch 23 = 0.9819974899291992\n",
      "Loss for batch 24 = 0.835849404335022\n",
      "Loss for batch 25 = 1.1948877573013306\n",
      "Loss for batch 26 = 0.9912201762199402\n",
      "Loss for batch 27 = 1.016959547996521\n",
      "Loss for batch 28 = 1.046447992324829\n",
      "Loss for batch 29 = 1.0400328636169434\n",
      "Loss for batch 30 = 0.9978345632553101\n",
      "Loss for batch 31 = 1.1039996147155762\n",
      "Loss for batch 32 = 1.0865137577056885\n",
      "Loss for batch 33 = 0.9803082346916199\n",
      "Loss for batch 34 = 0.9788328409194946\n",
      "Loss for batch 35 = 1.0481882095336914\n",
      "Loss for batch 36 = 0.9759026765823364\n",
      "Loss for batch 37 = 1.0549416542053223\n",
      "Loss for batch 38 = 0.934937596321106\n",
      "Loss for batch 39 = 0.8961725831031799\n",
      "Loss for batch 40 = 1.000527024269104\n",
      "Loss for batch 41 = 1.1242061853408813\n",
      "Loss for batch 42 = 0.9939802885055542\n",
      "Loss for batch 43 = 0.9661499261856079\n",
      "Loss for batch 44 = 1.0330222845077515\n",
      "Loss for batch 45 = 0.9903828501701355\n",
      "Loss for batch 46 = 0.8938117027282715\n",
      "Loss for batch 47 = 1.0170665979385376\n",
      "Loss for batch 48 = 0.9617723822593689\n",
      "Loss for batch 49 = 0.9438365697860718\n",
      "Loss for batch 50 = 1.107771873474121\n",
      "Loss for batch 51 = 0.9799694418907166\n",
      "Loss for batch 52 = 0.9976546764373779\n",
      "Loss for batch 53 = 1.0216095447540283\n",
      "Loss for batch 54 = 0.9904844760894775\n",
      "Loss for batch 55 = 1.080318808555603\n",
      "Loss for batch 56 = 1.0230494737625122\n",
      "Loss for batch 57 = 0.8804393410682678\n",
      "Loss for batch 58 = 1.022763967514038\n",
      "Loss for batch 59 = 1.0007981061935425\n",
      "Loss for batch 60 = 0.9057648181915283\n",
      "Loss for batch 61 = 1.0032538175582886\n",
      "Loss for batch 62 = 1.0144389867782593\n",
      "Loss for batch 63 = 1.1151974201202393\n",
      "Loss for batch 64 = 1.038851022720337\n",
      "Loss for batch 65 = 1.0225162506103516\n",
      "Loss for batch 66 = 1.100117802619934\n",
      "Loss for batch 67 = 0.9160208106040955\n",
      "Loss for batch 68 = 0.9766920208930969\n",
      "Loss for batch 69 = 0.9310123324394226\n",
      "Loss for batch 70 = 1.0094387531280518\n",
      "Loss for batch 71 = 0.8742545247077942\n",
      "Loss for batch 72 = 1.0156844854354858\n",
      "Loss for batch 73 = 0.9434514045715332\n",
      "Loss for batch 74 = 1.092362880706787\n",
      "Loss for batch 75 = 0.9884434938430786\n",
      "Loss for batch 76 = 1.01133131980896\n",
      "Loss for batch 77 = 1.0230368375778198\n",
      "Loss for batch 78 = 0.9618836641311646\n",
      "Loss for batch 79 = 0.9018909931182861\n",
      "Loss for batch 80 = 0.8826618194580078\n",
      "Loss for batch 81 = 1.0086414813995361\n",
      "Loss for batch 82 = 0.892598032951355\n",
      "Loss for batch 83 = 1.034044861793518\n",
      "Loss for batch 84 = 0.9470075368881226\n",
      "Loss for batch 85 = 1.189452052116394\n",
      "Loss for batch 86 = 0.8030529618263245\n",
      "Loss for batch 87 = 0.843497633934021\n",
      "Loss for batch 88 = 0.8047491312026978\n",
      "Loss for batch 89 = 1.1017998456954956\n",
      "Loss for batch 90 = 1.0183734893798828\n",
      "Loss for batch 91 = 0.9145845770835876\n",
      "Loss for batch 92 = 0.9913069009780884\n",
      "Loss for batch 93 = 0.9311980605125427\n",
      "Loss for batch 94 = 0.920595645904541\n",
      "Loss for batch 95 = 0.9862155318260193\n",
      "Loss for batch 96 = 0.8873038291931152\n",
      "Loss for batch 97 = 0.7941156625747681\n",
      "Loss for batch 98 = 1.0197833776474\n",
      "Loss for batch 99 = 0.9040581583976746\n",
      "Loss for batch 100 = 1.1158369779586792\n",
      "Loss for batch 101 = 0.7730950117111206\n",
      "Loss for batch 102 = 0.8317743539810181\n",
      "Loss for batch 103 = 1.0086778402328491\n",
      "Loss for batch 104 = 0.9586036801338196\n",
      "Loss for batch 105 = 1.0183494091033936\n",
      "Loss for batch 106 = 1.199450969696045\n",
      "\n",
      "Training Loss for epoch 4 = 106.36998748779297\n",
      "\n",
      "Current Validation Loss = 17.49909210205078\n",
      "Best Validation Loss = 16.591156005859375\n",
      "Epochs without Improvement = 1\n",
      "Train Accuracy: 33.86%\n",
      "Validation Accuracy: 35.32%\n",
      "\n",
      "Epoch 5\n",
      "----------\n",
      "Loss for batch 0 = 1.136820673942566\n",
      "Loss for batch 1 = 1.0670547485351562\n",
      "Loss for batch 2 = 1.100897192955017\n",
      "Loss for batch 3 = 1.1137568950653076\n",
      "Loss for batch 4 = 1.0967447757720947\n",
      "Loss for batch 5 = 1.0834074020385742\n",
      "Loss for batch 6 = 1.0558158159255981\n",
      "Loss for batch 7 = 1.0690550804138184\n",
      "Loss for batch 8 = 1.0842807292938232\n",
      "Loss for batch 9 = 1.0655750036239624\n",
      "Loss for batch 10 = 1.066514015197754\n",
      "Loss for batch 11 = 1.1087888479232788\n",
      "Loss for batch 12 = 0.9241092205047607\n",
      "Loss for batch 13 = 1.08854079246521\n",
      "Loss for batch 14 = 1.0142680406570435\n",
      "Loss for batch 15 = 1.0205563306808472\n",
      "Loss for batch 16 = 0.9908687472343445\n",
      "Loss for batch 17 = 1.0446343421936035\n",
      "Loss for batch 18 = 0.9866721034049988\n",
      "Loss for batch 19 = 0.9527835845947266\n",
      "Loss for batch 20 = 1.08099365234375\n",
      "Loss for batch 21 = 1.0722911357879639\n",
      "Loss for batch 22 = 1.14111328125\n",
      "Loss for batch 23 = 0.9748736619949341\n",
      "Loss for batch 24 = 1.014570951461792\n",
      "Loss for batch 25 = 1.0687495470046997\n",
      "Loss for batch 26 = 1.0795643329620361\n",
      "Loss for batch 27 = 0.8966537714004517\n",
      "Loss for batch 28 = 1.0302653312683105\n",
      "Loss for batch 29 = 0.9402378797531128\n",
      "Loss for batch 30 = 0.9283146858215332\n",
      "Loss for batch 31 = 1.1797953844070435\n",
      "Loss for batch 32 = 1.0566952228546143\n",
      "Loss for batch 33 = 1.073023796081543\n",
      "Loss for batch 34 = 0.9464436173439026\n",
      "Loss for batch 35 = 1.0648330450057983\n",
      "Loss for batch 36 = 1.0289126634597778\n",
      "Loss for batch 37 = 0.9576569199562073\n",
      "Loss for batch 38 = 1.0067299604415894\n",
      "Loss for batch 39 = 0.8523088693618774\n",
      "Loss for batch 40 = 1.0173753499984741\n",
      "Loss for batch 41 = 1.0162602663040161\n",
      "Loss for batch 42 = 0.9959224462509155\n",
      "Loss for batch 43 = 1.0410243272781372\n",
      "Loss for batch 44 = 1.0776478052139282\n",
      "Loss for batch 45 = 1.0499638319015503\n",
      "Loss for batch 46 = 0.880837619304657\n",
      "Loss for batch 47 = 1.0391119718551636\n",
      "Loss for batch 48 = 0.9044379591941833\n",
      "Loss for batch 49 = 0.9363206624984741\n",
      "Loss for batch 50 = 0.8934385776519775\n",
      "Loss for batch 51 = 0.9959568977355957\n",
      "Loss for batch 52 = 0.9289027452468872\n",
      "Loss for batch 53 = 1.1765782833099365\n",
      "Loss for batch 54 = 1.073718547821045\n",
      "Loss for batch 55 = 1.1043262481689453\n",
      "Loss for batch 56 = 1.0560940504074097\n",
      "Loss for batch 57 = 1.0382914543151855\n",
      "Loss for batch 58 = 1.0535203218460083\n",
      "Loss for batch 59 = 1.277634620666504\n",
      "Loss for batch 60 = 1.0248960256576538\n",
      "Loss for batch 61 = 1.0334796905517578\n",
      "Loss for batch 62 = 1.0042341947555542\n",
      "Loss for batch 63 = 1.081418752670288\n",
      "Loss for batch 64 = 1.1052722930908203\n",
      "Loss for batch 65 = 1.0299484729766846\n",
      "Loss for batch 66 = 1.026355504989624\n",
      "Loss for batch 67 = 1.0042877197265625\n",
      "Loss for batch 68 = 0.9328253269195557\n",
      "Loss for batch 69 = 0.9332652688026428\n",
      "Loss for batch 70 = 1.0824640989303589\n",
      "Loss for batch 71 = 1.0201730728149414\n",
      "Loss for batch 72 = 0.9839426875114441\n",
      "Loss for batch 73 = 0.951747477054596\n",
      "Loss for batch 74 = 1.206152081489563\n",
      "Loss for batch 75 = 1.0884268283843994\n",
      "Loss for batch 76 = 1.0869842767715454\n",
      "Loss for batch 77 = 1.0438810586929321\n",
      "Loss for batch 78 = 1.0451642274856567\n",
      "Loss for batch 79 = 0.9734538793563843\n",
      "Loss for batch 80 = 0.9338226914405823\n",
      "Loss for batch 81 = 0.8783360719680786\n",
      "Loss for batch 82 = 0.9105041027069092\n",
      "Loss for batch 83 = 0.9468086957931519\n",
      "Loss for batch 84 = 1.0218802690505981\n",
      "Loss for batch 85 = 1.0930861234664917\n",
      "Loss for batch 86 = 0.8423230648040771\n",
      "Loss for batch 87 = 0.8098288178443909\n",
      "Loss for batch 88 = 1.0080722570419312\n",
      "Loss for batch 89 = 1.0174236297607422\n",
      "Loss for batch 90 = 0.9856020212173462\n",
      "Loss for batch 91 = 0.8805878162384033\n",
      "Loss for batch 92 = 0.9778902530670166\n",
      "Loss for batch 93 = 0.9228124022483826\n",
      "Loss for batch 94 = 0.8424166440963745\n",
      "Loss for batch 95 = 0.9414340853691101\n",
      "Loss for batch 96 = 0.9689595699310303\n",
      "Loss for batch 97 = 0.8110330104827881\n",
      "Loss for batch 98 = 0.903926432132721\n",
      "Loss for batch 99 = 0.7554488182067871\n",
      "Loss for batch 100 = 0.9858757853507996\n",
      "Loss for batch 101 = 0.7662175893783569\n",
      "Loss for batch 102 = 0.8108994364738464\n",
      "Loss for batch 103 = 0.9192153811454773\n",
      "Loss for batch 104 = 0.8819066286087036\n",
      "Loss for batch 105 = 1.036558985710144\n",
      "Loss for batch 106 = 1.1824957132339478\n",
      "\n",
      "Training Loss for epoch 5 = 107.71627044677734\n",
      "\n",
      "Current Validation Loss = 14.799931526184082\n",
      "Best Validation Loss = 14.799931526184082\n",
      "Epochs without Improvement = 0\n",
      "Train Accuracy: 56.40%\n",
      "Validation Accuracy: 59.14%\n",
      "\n",
      "Epoch 6\n",
      "----------\n",
      "Loss for batch 0 = 0.837609052658081\n",
      "Loss for batch 1 = 0.7727121710777283\n",
      "Loss for batch 2 = 1.0445839166641235\n",
      "Loss for batch 3 = 0.7613619565963745\n",
      "Loss for batch 4 = 0.8901914954185486\n",
      "Loss for batch 5 = 0.9791852831840515\n",
      "Loss for batch 6 = 1.0643253326416016\n",
      "Loss for batch 7 = 1.0305545330047607\n",
      "Loss for batch 8 = 1.0936269760131836\n",
      "Loss for batch 9 = 0.8090957403182983\n",
      "Loss for batch 10 = 0.8810569047927856\n",
      "Loss for batch 11 = 0.9917258620262146\n",
      "Loss for batch 12 = 0.8842514157295227\n",
      "Loss for batch 13 = 1.016390323638916\n",
      "Loss for batch 14 = 0.9368897080421448\n",
      "Loss for batch 15 = 1.027163028717041\n",
      "Loss for batch 16 = 0.9690438508987427\n",
      "Loss for batch 17 = 0.8798409104347229\n",
      "Loss for batch 18 = 0.8712034225463867\n",
      "Loss for batch 19 = 0.7790685892105103\n",
      "Loss for batch 20 = 1.006055235862732\n",
      "Loss for batch 21 = 0.8238775730133057\n",
      "Loss for batch 22 = 1.2447453737258911\n",
      "Loss for batch 23 = 0.9961651563644409\n",
      "Loss for batch 24 = 0.7984553575515747\n",
      "Loss for batch 25 = 0.9350152015686035\n",
      "Loss for batch 26 = 0.8437560796737671\n",
      "Loss for batch 27 = 0.9200311303138733\n",
      "Loss for batch 28 = 1.6028951406478882\n",
      "Loss for batch 29 = 1.0551618337631226\n",
      "Loss for batch 30 = 0.9788166284561157\n",
      "Loss for batch 31 = 1.1352964639663696\n",
      "Loss for batch 32 = 0.8868803977966309\n",
      "Loss for batch 33 = 0.9529581069946289\n",
      "Loss for batch 34 = 1.0370112657546997\n",
      "Loss for batch 35 = 1.1056737899780273\n",
      "Loss for batch 36 = 0.8913872241973877\n",
      "Loss for batch 37 = 0.8731282949447632\n",
      "Loss for batch 38 = 0.9474809169769287\n",
      "Loss for batch 39 = 0.8703068494796753\n",
      "Loss for batch 40 = 0.9374088644981384\n",
      "Loss for batch 41 = 1.079990029335022\n",
      "Loss for batch 42 = 0.8596091270446777\n",
      "Loss for batch 43 = 1.0862882137298584\n",
      "Loss for batch 44 = 0.9577067494392395\n",
      "Loss for batch 45 = 0.8595502376556396\n",
      "Loss for batch 46 = 0.8652437925338745\n",
      "Loss for batch 47 = 0.9769254326820374\n",
      "Loss for batch 48 = 0.8012648820877075\n",
      "Loss for batch 49 = 0.9719098806381226\n",
      "Loss for batch 50 = 0.8688646554946899\n",
      "Loss for batch 51 = 0.8629319071769714\n",
      "Loss for batch 52 = 1.0334197282791138\n",
      "Loss for batch 53 = 1.0991207361221313\n",
      "Loss for batch 54 = 0.8901830315589905\n",
      "Loss for batch 55 = 1.009380578994751\n",
      "Loss for batch 56 = 0.930347204208374\n",
      "Loss for batch 57 = 0.7337409257888794\n",
      "Loss for batch 58 = 0.8252571225166321\n",
      "Loss for batch 59 = 0.904532790184021\n",
      "Loss for batch 60 = 0.8663139343261719\n",
      "Loss for batch 61 = 1.022352695465088\n",
      "Loss for batch 62 = 0.9213451147079468\n",
      "Loss for batch 63 = 0.9269058108329773\n",
      "Loss for batch 64 = 0.9234479069709778\n",
      "Loss for batch 65 = 1.029388427734375\n",
      "Loss for batch 66 = 0.9812800288200378\n",
      "Loss for batch 67 = 0.8653734922409058\n",
      "Loss for batch 68 = 0.8131266832351685\n",
      "Loss for batch 69 = 0.8122218251228333\n",
      "Loss for batch 70 = 0.903317391872406\n",
      "Loss for batch 71 = 0.7548547983169556\n",
      "Loss for batch 72 = 0.8578159213066101\n",
      "Loss for batch 73 = 0.8970300555229187\n",
      "Loss for batch 74 = 1.1581124067306519\n",
      "Loss for batch 75 = 1.017724871635437\n",
      "Loss for batch 76 = 0.9446516036987305\n",
      "Loss for batch 77 = 1.0410276651382446\n",
      "Loss for batch 78 = 0.8406381607055664\n",
      "Loss for batch 79 = 0.8609325885772705\n",
      "Loss for batch 80 = 0.8233209848403931\n",
      "Loss for batch 81 = 0.9150241017341614\n",
      "Loss for batch 82 = 0.759408175945282\n",
      "Loss for batch 83 = 1.0374391078948975\n",
      "Loss for batch 84 = 0.9055510759353638\n",
      "Loss for batch 85 = 1.0712789297103882\n",
      "Loss for batch 86 = 0.7212465405464172\n",
      "Loss for batch 87 = 0.8340254426002502\n",
      "Loss for batch 88 = 0.9019607305526733\n",
      "Loss for batch 89 = 0.9839079976081848\n",
      "Loss for batch 90 = 1.007735252380371\n",
      "Loss for batch 91 = 0.8453031182289124\n",
      "Loss for batch 92 = 0.9633260369300842\n",
      "Loss for batch 93 = 0.7703896760940552\n",
      "Loss for batch 94 = 0.8729032874107361\n",
      "Loss for batch 95 = 1.0159534215927124\n",
      "Loss for batch 96 = 0.9356434941291809\n",
      "Loss for batch 97 = 0.7172304391860962\n",
      "Loss for batch 98 = 0.8994185924530029\n",
      "Loss for batch 99 = 0.8890439867973328\n",
      "Loss for batch 100 = 1.0222500562667847\n",
      "Loss for batch 101 = 0.8047014474868774\n",
      "Loss for batch 102 = 0.7226147055625916\n",
      "Loss for batch 103 = 0.980136513710022\n",
      "Loss for batch 104 = 0.7952513098716736\n",
      "Loss for batch 105 = 1.0297229290008545\n",
      "Loss for batch 106 = 1.2026511430740356\n",
      "\n",
      "Training Loss for epoch 6 = 99.84396362304688\n",
      "\n",
      "Current Validation Loss = 14.042205810546875\n",
      "Best Validation Loss = 14.042205810546875\n",
      "Epochs without Improvement = 0\n",
      "Train Accuracy: 58.66%\n",
      "Validation Accuracy: 59.96%\n",
      "\n",
      "Epoch 7\n",
      "----------\n",
      "Loss for batch 0 = 0.8314206004142761\n",
      "Loss for batch 1 = 0.6834998726844788\n",
      "Loss for batch 2 = 0.9611656665802002\n",
      "Loss for batch 3 = 0.7378140687942505\n",
      "Loss for batch 4 = 0.8081074953079224\n",
      "Loss for batch 5 = 0.9147835373878479\n",
      "Loss for batch 6 = 1.102895736694336\n",
      "Loss for batch 7 = 0.9854978919029236\n",
      "Loss for batch 8 = 0.9651212692260742\n",
      "Loss for batch 9 = 0.8169441819190979\n",
      "Loss for batch 10 = 0.7612847089767456\n",
      "Loss for batch 11 = 1.0259970426559448\n",
      "Loss for batch 12 = 0.8917657732963562\n",
      "Loss for batch 13 = 0.9887558221817017\n",
      "Loss for batch 14 = 0.8765655159950256\n",
      "Loss for batch 15 = 0.9058701395988464\n",
      "Loss for batch 16 = 0.8750542402267456\n",
      "Loss for batch 17 = 0.8057453632354736\n",
      "Loss for batch 18 = 0.8407524824142456\n",
      "Loss for batch 19 = 0.6992231011390686\n",
      "Loss for batch 20 = 1.0210387706756592\n",
      "Loss for batch 21 = 0.7169883251190186\n",
      "Loss for batch 22 = 1.2724766731262207\n",
      "Loss for batch 23 = 0.7322959303855896\n",
      "Loss for batch 24 = 0.7855731248855591\n",
      "Loss for batch 25 = 0.8990039229393005\n",
      "Loss for batch 26 = 0.8470740914344788\n",
      "Loss for batch 27 = 0.8576312065124512\n",
      "Loss for batch 28 = 0.8889020681381226\n",
      "Loss for batch 29 = 0.8819395303726196\n",
      "Loss for batch 30 = 0.8539798259735107\n",
      "Loss for batch 31 = 1.2080495357513428\n",
      "Loss for batch 32 = 0.8894581198692322\n",
      "Loss for batch 33 = 0.8222721815109253\n",
      "Loss for batch 34 = 0.8240047693252563\n",
      "Loss for batch 35 = 0.9963725805282593\n",
      "Loss for batch 36 = 0.8335865139961243\n",
      "Loss for batch 37 = 0.7973580360412598\n",
      "Loss for batch 38 = 0.8215970993041992\n",
      "Loss for batch 39 = 0.7425240874290466\n",
      "Loss for batch 40 = 0.9149893522262573\n",
      "Loss for batch 41 = 0.8391201496124268\n",
      "Loss for batch 42 = 0.7745187282562256\n",
      "Loss for batch 43 = 1.090575933456421\n",
      "Loss for batch 44 = 0.9671609997749329\n",
      "Loss for batch 45 = 0.7976223230361938\n",
      "Loss for batch 46 = 0.740004301071167\n",
      "Loss for batch 47 = 0.8761457800865173\n",
      "Loss for batch 48 = 0.7340611815452576\n",
      "Loss for batch 49 = 0.9037479162216187\n",
      "Loss for batch 50 = 0.8016817569732666\n",
      "Loss for batch 51 = 0.7396404147148132\n",
      "Loss for batch 52 = 0.9698950052261353\n",
      "Loss for batch 53 = 1.0604199171066284\n",
      "Loss for batch 54 = 0.8537678122520447\n",
      "Loss for batch 55 = 0.9526207447052002\n",
      "Loss for batch 56 = 0.8671293258666992\n",
      "Loss for batch 57 = 0.6922467350959778\n",
      "Loss for batch 58 = 0.7436139583587646\n",
      "Loss for batch 59 = 0.8812002539634705\n",
      "Loss for batch 60 = 0.8292890191078186\n",
      "Loss for batch 61 = 0.9341351985931396\n",
      "Loss for batch 62 = 0.7788146138191223\n",
      "Loss for batch 63 = 0.7749879360198975\n",
      "Loss for batch 64 = 0.8446054458618164\n",
      "Loss for batch 65 = 0.8869478106498718\n",
      "Loss for batch 66 = 1.0056694746017456\n",
      "Loss for batch 67 = 0.870415210723877\n",
      "Loss for batch 68 = 0.8950128555297852\n",
      "Loss for batch 69 = 0.743281900882721\n",
      "Loss for batch 70 = 0.8734570741653442\n",
      "Loss for batch 71 = 0.6896170973777771\n",
      "Loss for batch 72 = 0.9363645315170288\n",
      "Loss for batch 73 = 0.940056324005127\n",
      "Loss for batch 74 = 1.0496633052825928\n",
      "Loss for batch 75 = 1.009483814239502\n",
      "Loss for batch 76 = 0.931634247303009\n",
      "Loss for batch 77 = 1.0262315273284912\n",
      "Loss for batch 78 = 0.8198948502540588\n",
      "Loss for batch 79 = 0.7814741134643555\n",
      "Loss for batch 80 = 0.7783868908882141\n",
      "Loss for batch 81 = 0.8580240607261658\n",
      "Loss for batch 82 = 0.7843149900436401\n",
      "Loss for batch 83 = 0.8768578171730042\n",
      "Loss for batch 84 = 0.8518943786621094\n",
      "Loss for batch 85 = 1.112221598625183\n",
      "Loss for batch 86 = 0.695988118648529\n",
      "Loss for batch 87 = 0.7730401158332825\n",
      "Loss for batch 88 = 0.9144084453582764\n",
      "Loss for batch 89 = 1.0194543600082397\n",
      "Loss for batch 90 = 0.9639180898666382\n",
      "Loss for batch 91 = 0.8078887462615967\n",
      "Loss for batch 92 = 0.8966124653816223\n",
      "Loss for batch 93 = 0.7751985788345337\n",
      "Loss for batch 94 = 0.8168404698371887\n",
      "Loss for batch 95 = 0.9998857975006104\n",
      "Loss for batch 96 = 0.865234375\n",
      "Loss for batch 97 = 0.6958059072494507\n",
      "Loss for batch 98 = 0.8261315822601318\n",
      "Loss for batch 99 = 0.7830168008804321\n",
      "Loss for batch 100 = 0.8552843332290649\n",
      "Loss for batch 101 = 0.7631527781486511\n",
      "Loss for batch 102 = 0.7516206502914429\n",
      "Loss for batch 103 = 0.9773920774459839\n",
      "Loss for batch 104 = 0.7693894505500793\n",
      "Loss for batch 105 = 0.9515019655227661\n",
      "Loss for batch 106 = 1.1115772724151611\n",
      "\n",
      "Training Loss for epoch 7 = 93.26663208007812\n",
      "\n",
      "Current Validation Loss = 13.333395957946777\n",
      "Best Validation Loss = 13.333395957946777\n",
      "Epochs without Improvement = 0\n",
      "Train Accuracy: 62.15%\n",
      "Validation Accuracy: 64.48%\n",
      "\n",
      "Epoch 8\n",
      "----------\n",
      "Loss for batch 0 = 0.7699192762374878\n",
      "Loss for batch 1 = 0.710295557975769\n",
      "Loss for batch 2 = 0.9131126403808594\n",
      "Loss for batch 3 = 0.743216335773468\n",
      "Loss for batch 4 = 0.7267484068870544\n",
      "Loss for batch 5 = 0.8599869012832642\n",
      "Loss for batch 6 = 1.065371036529541\n",
      "Loss for batch 7 = 1.0360078811645508\n",
      "Loss for batch 8 = 0.9861370325088501\n",
      "Loss for batch 9 = 0.7404054999351501\n",
      "Loss for batch 10 = 0.6930596828460693\n",
      "Loss for batch 11 = 0.9988042116165161\n",
      "Loss for batch 12 = 0.8741867542266846\n",
      "Loss for batch 13 = 0.9508985280990601\n",
      "Loss for batch 14 = 0.8532876968383789\n",
      "Loss for batch 15 = 0.9481909275054932\n",
      "Loss for batch 16 = 0.8654313087463379\n",
      "Loss for batch 17 = 0.8057028651237488\n",
      "Loss for batch 18 = 0.8305715322494507\n",
      "Loss for batch 19 = 0.6817432641983032\n",
      "Loss for batch 20 = 0.9287654161453247\n",
      "Loss for batch 21 = 0.6977047920227051\n",
      "Loss for batch 22 = 1.2594083547592163\n",
      "Loss for batch 23 = 0.7161067128181458\n",
      "Loss for batch 24 = 0.7722001075744629\n",
      "Loss for batch 25 = 0.8306834697723389\n",
      "Loss for batch 26 = 0.7931060194969177\n",
      "Loss for batch 27 = 0.798677921295166\n",
      "Loss for batch 28 = 0.850953221321106\n",
      "Loss for batch 29 = 0.8447337746620178\n",
      "Loss for batch 30 = 0.8635382652282715\n",
      "Loss for batch 31 = 1.1879607439041138\n",
      "Loss for batch 32 = 0.8164172172546387\n",
      "Loss for batch 33 = 0.7887953519821167\n",
      "Loss for batch 34 = 0.8179753422737122\n",
      "Loss for batch 35 = 0.9277464151382446\n",
      "Loss for batch 36 = 0.7753128409385681\n",
      "Loss for batch 37 = 0.7594302296638489\n",
      "Loss for batch 38 = 0.8776446580886841\n",
      "Loss for batch 39 = 0.8326359987258911\n",
      "Loss for batch 40 = 0.8583883047103882\n",
      "Loss for batch 41 = 0.7862038016319275\n",
      "Loss for batch 42 = 0.7259752154350281\n",
      "Loss for batch 43 = 1.0207256078720093\n",
      "Loss for batch 44 = 0.9164398908615112\n",
      "Loss for batch 45 = 0.808836817741394\n",
      "Loss for batch 46 = 0.7408918142318726\n",
      "Loss for batch 47 = 0.7940171360969543\n",
      "Loss for batch 48 = 0.6906036734580994\n",
      "Loss for batch 49 = 0.8355729579925537\n",
      "Loss for batch 50 = 0.8339611887931824\n",
      "Loss for batch 51 = 0.6592391729354858\n",
      "Loss for batch 52 = 0.9527106881141663\n",
      "Loss for batch 53 = 0.9906207323074341\n",
      "Loss for batch 54 = 0.8046568036079407\n",
      "Loss for batch 55 = 0.9112073183059692\n",
      "Loss for batch 56 = 0.8428661227226257\n",
      "Loss for batch 57 = 0.6937756538391113\n",
      "Loss for batch 58 = 0.6991497278213501\n",
      "Loss for batch 59 = 0.8960865139961243\n",
      "Loss for batch 60 = 0.7532307505607605\n",
      "Loss for batch 61 = 0.9232484698295593\n",
      "Loss for batch 62 = 0.775201678276062\n",
      "Loss for batch 63 = 0.7604101896286011\n",
      "Loss for batch 64 = 0.7406213879585266\n",
      "Loss for batch 65 = 0.85001140832901\n",
      "Loss for batch 66 = 0.848021388053894\n",
      "Loss for batch 67 = 0.780964732170105\n",
      "Loss for batch 68 = 0.7715378999710083\n",
      "Loss for batch 69 = 0.7184079885482788\n",
      "Loss for batch 70 = 0.829876720905304\n",
      "Loss for batch 71 = 0.5894566774368286\n",
      "Loss for batch 72 = 0.88103848695755\n",
      "Loss for batch 73 = 0.8579000234603882\n",
      "Loss for batch 74 = 1.0727415084838867\n",
      "Loss for batch 75 = 1.1401411294937134\n",
      "Loss for batch 76 = 0.9377765655517578\n",
      "Loss for batch 77 = 0.9801576137542725\n",
      "Loss for batch 78 = 0.7780265212059021\n",
      "Loss for batch 79 = 0.7391366362571716\n",
      "Loss for batch 80 = 0.7544206380844116\n",
      "Loss for batch 81 = 0.8216561675071716\n",
      "Loss for batch 82 = 0.6981088519096375\n",
      "Loss for batch 83 = 0.9425948262214661\n",
      "Loss for batch 84 = 0.8436376452445984\n",
      "Loss for batch 85 = 1.0233885049819946\n",
      "Loss for batch 86 = 0.692647397518158\n",
      "Loss for batch 87 = 0.7927669882774353\n",
      "Loss for batch 88 = 0.850594699382782\n",
      "Loss for batch 89 = 1.0072613954544067\n",
      "Loss for batch 90 = 0.9864072799682617\n",
      "Loss for batch 91 = 0.827192485332489\n",
      "Loss for batch 92 = 0.945902407169342\n",
      "Loss for batch 93 = 0.751082181930542\n",
      "Loss for batch 94 = 0.7985575795173645\n",
      "Loss for batch 95 = 0.9449900984764099\n",
      "Loss for batch 96 = 0.7454614639282227\n",
      "Loss for batch 97 = 0.7051090002059937\n",
      "Loss for batch 98 = 0.8566441535949707\n",
      "Loss for batch 99 = 0.8015193939208984\n",
      "Loss for batch 100 = 0.7831519842147827\n",
      "Loss for batch 101 = 0.8169466257095337\n",
      "Loss for batch 102 = 0.7345242500305176\n",
      "Loss for batch 103 = 0.968586802482605\n",
      "Loss for batch 104 = 0.7479727268218994\n",
      "Loss for batch 105 = 0.8820497393608093\n",
      "Loss for batch 106 = 1.0384024381637573\n",
      "\n",
      "Training Loss for epoch 8 = 90.144287109375\n",
      "\n",
      "Current Validation Loss = 13.025218963623047\n",
      "Best Validation Loss = 13.025218963623047\n",
      "Epochs without Improvement = 0\n",
      "Train Accuracy: 64.44%\n",
      "Validation Accuracy: 62.63%\n",
      "\n",
      "Epoch 9\n",
      "----------\n",
      "Loss for batch 0 = 0.712102472782135\n",
      "Loss for batch 1 = 0.6081842184066772\n",
      "Loss for batch 2 = 0.877646267414093\n",
      "Loss for batch 3 = 0.6819596886634827\n",
      "Loss for batch 4 = 0.6796944737434387\n",
      "Loss for batch 5 = 0.7155935168266296\n",
      "Loss for batch 6 = 1.0849727392196655\n",
      "Loss for batch 7 = 1.1257752180099487\n",
      "Loss for batch 8 = 0.7473756074905396\n",
      "Loss for batch 9 = 0.6824711561203003\n",
      "Loss for batch 10 = 0.6221126317977905\n",
      "Loss for batch 11 = 0.9558122158050537\n",
      "Loss for batch 12 = 0.7852111458778381\n",
      "Loss for batch 13 = 0.8473482131958008\n",
      "Loss for batch 14 = 0.752687394618988\n",
      "Loss for batch 15 = 0.8036180138587952\n",
      "Loss for batch 16 = 0.9080213308334351\n",
      "Loss for batch 17 = 0.8026468753814697\n",
      "Loss for batch 18 = 0.7638471126556396\n",
      "Loss for batch 19 = 0.5834792256355286\n",
      "Loss for batch 20 = 0.8537057638168335\n",
      "Loss for batch 21 = 0.6798537373542786\n",
      "Loss for batch 22 = 1.4492881298065186\n",
      "Loss for batch 23 = 0.6813012361526489\n",
      "Loss for batch 24 = 0.7849159836769104\n",
      "Loss for batch 25 = 0.7935377359390259\n",
      "Loss for batch 26 = 0.7191503047943115\n",
      "Loss for batch 27 = 0.8132352828979492\n",
      "Loss for batch 28 = 0.846280038356781\n",
      "Loss for batch 29 = 0.8947464227676392\n",
      "Loss for batch 30 = 0.8295966386795044\n",
      "Loss for batch 31 = 1.0942412614822388\n",
      "Loss for batch 32 = 0.7167848944664001\n",
      "Loss for batch 33 = 0.7221360802650452\n",
      "Loss for batch 34 = 0.7659235596656799\n",
      "Loss for batch 35 = 0.955663800239563\n",
      "Loss for batch 36 = 0.8188818097114563\n",
      "Loss for batch 37 = 0.7228241562843323\n",
      "Loss for batch 38 = 0.9456958174705505\n",
      "Loss for batch 39 = 0.6637342572212219\n",
      "Loss for batch 40 = 0.8079478144645691\n",
      "Loss for batch 41 = 0.7742002010345459\n",
      "Loss for batch 42 = 0.778843104839325\n",
      "Loss for batch 43 = 0.9556559920310974\n",
      "Loss for batch 44 = 0.8264802694320679\n",
      "Loss for batch 45 = 0.7775543928146362\n",
      "Loss for batch 46 = 0.7985999584197998\n",
      "Loss for batch 47 = 0.7459104657173157\n",
      "Loss for batch 48 = 0.6417126655578613\n",
      "Loss for batch 49 = 0.8003174662590027\n",
      "Loss for batch 50 = 0.8145185112953186\n",
      "Loss for batch 51 = 0.65338534116745\n",
      "Loss for batch 52 = 0.9133516550064087\n",
      "Loss for batch 53 = 0.9410392045974731\n",
      "Loss for batch 54 = 0.8287351131439209\n",
      "Loss for batch 55 = 0.8596339821815491\n",
      "Loss for batch 56 = 0.7306888699531555\n",
      "Loss for batch 57 = 0.7104030847549438\n",
      "Loss for batch 58 = 0.6789107918739319\n",
      "Loss for batch 59 = 0.8150752186775208\n",
      "Loss for batch 60 = 0.7064908742904663\n",
      "Loss for batch 61 = 0.9208388328552246\n",
      "Loss for batch 62 = 0.8059501647949219\n",
      "Loss for batch 63 = 0.6447665691375732\n",
      "Loss for batch 64 = 0.6822247505187988\n",
      "Loss for batch 65 = 0.8263415694236755\n",
      "Loss for batch 66 = 0.9209277033805847\n",
      "Loss for batch 67 = 0.6782026886940002\n",
      "Loss for batch 68 = 0.7510005831718445\n",
      "Loss for batch 69 = 0.6247267127037048\n",
      "Loss for batch 70 = 0.8154054880142212\n",
      "Loss for batch 71 = 0.6380603909492493\n",
      "Loss for batch 72 = 0.7836563587188721\n",
      "Loss for batch 73 = 0.8821921944618225\n",
      "Loss for batch 74 = 0.9915713667869568\n",
      "Loss for batch 75 = 1.1217825412750244\n",
      "Loss for batch 76 = 0.9507955312728882\n",
      "Loss for batch 77 = 0.9645445346832275\n",
      "Loss for batch 78 = 0.7509095668792725\n",
      "Loss for batch 79 = 0.6483045816421509\n",
      "Loss for batch 80 = 0.6791672706604004\n",
      "Loss for batch 81 = 0.7967833876609802\n",
      "Loss for batch 82 = 0.6611722707748413\n",
      "Loss for batch 83 = 0.8999914526939392\n",
      "Loss for batch 84 = 0.8539808392524719\n",
      "Loss for batch 85 = 1.1005117893218994\n",
      "Loss for batch 86 = 0.732954740524292\n",
      "Loss for batch 87 = 0.7009184956550598\n",
      "Loss for batch 88 = 0.841882586479187\n",
      "Loss for batch 89 = 0.9561004638671875\n",
      "Loss for batch 90 = 0.9677159786224365\n",
      "Loss for batch 91 = 0.7716489434242249\n",
      "Loss for batch 92 = 0.8414087295532227\n",
      "Loss for batch 93 = 0.7125235795974731\n",
      "Loss for batch 94 = 0.818313479423523\n",
      "Loss for batch 95 = 0.931106448173523\n",
      "Loss for batch 96 = 0.7262383699417114\n",
      "Loss for batch 97 = 0.7442200183868408\n",
      "Loss for batch 98 = 0.8865370750427246\n",
      "Loss for batch 99 = 0.8346616625785828\n",
      "Loss for batch 100 = 0.734226405620575\n",
      "Loss for batch 101 = 0.7380901575088501\n",
      "Loss for batch 102 = 0.7161542177200317\n",
      "Loss for batch 103 = 0.9507764577865601\n",
      "Loss for batch 104 = 0.8242360353469849\n",
      "Loss for batch 105 = 0.841372549533844\n",
      "Loss for batch 106 = 0.9924874305725098\n",
      "\n",
      "Training Loss for epoch 9 = 86.7088851928711\n",
      "\n",
      "Current Validation Loss = 13.528534889221191\n",
      "Best Validation Loss = 13.025218963623047\n",
      "Epochs without Improvement = 1\n",
      "Train Accuracy: 60.30%\n",
      "Validation Accuracy: 58.52%\n",
      "\n",
      "Epoch 10\n",
      "----------\n",
      "Loss for batch 0 = 0.736808717250824\n",
      "Loss for batch 1 = 0.5921908617019653\n",
      "Loss for batch 2 = 0.9043055176734924\n",
      "Loss for batch 3 = 0.7164741158485413\n",
      "Loss for batch 4 = 0.6427891850471497\n",
      "Loss for batch 5 = 0.7348377108573914\n",
      "Loss for batch 6 = 0.8667078018188477\n",
      "Loss for batch 7 = 0.9858819842338562\n",
      "Loss for batch 8 = 0.7481821179389954\n",
      "Loss for batch 9 = 0.6455880999565125\n",
      "Loss for batch 10 = 0.6159992218017578\n",
      "Loss for batch 11 = 0.910499095916748\n",
      "Loss for batch 12 = 0.7705800533294678\n",
      "Loss for batch 13 = 0.8335059881210327\n",
      "Loss for batch 14 = 0.717971920967102\n",
      "Loss for batch 15 = 0.8624703288078308\n",
      "Loss for batch 16 = 0.8498597741127014\n",
      "Loss for batch 17 = 0.783195972442627\n",
      "Loss for batch 18 = 0.7717669010162354\n",
      "Loss for batch 19 = 0.6002321839332581\n",
      "Loss for batch 20 = 0.8720779418945312\n",
      "Loss for batch 21 = 0.5962526798248291\n",
      "Loss for batch 22 = 1.4374003410339355\n",
      "Loss for batch 23 = 0.6335040330886841\n",
      "Loss for batch 24 = 0.701732337474823\n",
      "Loss for batch 25 = 0.7044429183006287\n",
      "Loss for batch 26 = 0.7520250082015991\n",
      "Loss for batch 27 = 0.761518120765686\n",
      "Loss for batch 28 = 0.7651529908180237\n",
      "Loss for batch 29 = 0.8653740882873535\n",
      "Loss for batch 30 = 0.7517607808113098\n",
      "Loss for batch 31 = 1.1269954442977905\n",
      "Loss for batch 32 = 0.7287933230400085\n",
      "Loss for batch 33 = 0.702642560005188\n",
      "Loss for batch 34 = 0.7660084962844849\n",
      "Loss for batch 35 = 0.9215688109397888\n",
      "Loss for batch 36 = 0.7476295828819275\n",
      "Loss for batch 37 = 0.7326033711433411\n",
      "Loss for batch 38 = 0.8205812573432922\n",
      "Loss for batch 39 = 0.7182575464248657\n",
      "Loss for batch 40 = 0.7980325222015381\n",
      "Loss for batch 41 = 0.7933883666992188\n",
      "Loss for batch 42 = 0.7769794464111328\n",
      "Loss for batch 43 = 0.9486947059631348\n",
      "Loss for batch 44 = 0.8480199575424194\n",
      "Loss for batch 45 = 0.7651707530021667\n",
      "Loss for batch 46 = 0.784945011138916\n",
      "Loss for batch 47 = 0.7153723835945129\n",
      "Loss for batch 48 = 0.5613937973976135\n",
      "Loss for batch 49 = 0.7568280100822449\n",
      "Loss for batch 50 = 0.7481570243835449\n",
      "Loss for batch 51 = 0.7820526361465454\n",
      "Loss for batch 52 = 0.9368584156036377\n",
      "Loss for batch 53 = 0.9522148966789246\n",
      "Loss for batch 54 = 0.7541539669036865\n",
      "Loss for batch 55 = 0.8442787528038025\n",
      "Loss for batch 56 = 0.8343955874443054\n",
      "Loss for batch 57 = 0.567842960357666\n",
      "Loss for batch 58 = 0.6599595546722412\n",
      "Loss for batch 59 = 0.6860710382461548\n",
      "Loss for batch 60 = 0.6625733971595764\n",
      "Loss for batch 61 = 0.7469474077224731\n",
      "Loss for batch 62 = 0.7454484701156616\n",
      "Loss for batch 63 = 0.6401079893112183\n",
      "Loss for batch 64 = 0.6293119192123413\n",
      "Loss for batch 65 = 0.8515642881393433\n",
      "Loss for batch 66 = 0.8917070031166077\n",
      "Loss for batch 67 = 0.6223829388618469\n",
      "Loss for batch 68 = 0.7017730474472046\n",
      "Loss for batch 69 = 0.7589066624641418\n",
      "Loss for batch 70 = 0.8226924538612366\n",
      "Loss for batch 71 = 0.6218245625495911\n",
      "Loss for batch 72 = 1.071372628211975\n",
      "Loss for batch 73 = 0.7953739166259766\n",
      "Loss for batch 74 = 0.9647899270057678\n",
      "Loss for batch 75 = 1.0906424522399902\n",
      "Loss for batch 76 = 0.9030437469482422\n",
      "Loss for batch 77 = 1.0011742115020752\n",
      "Loss for batch 78 = 0.7303764820098877\n",
      "Loss for batch 79 = 0.7373334765434265\n",
      "Loss for batch 80 = 0.7270655632019043\n",
      "Loss for batch 81 = 0.7522374391555786\n",
      "Loss for batch 82 = 0.7064899206161499\n",
      "Loss for batch 83 = 0.7907896041870117\n",
      "Loss for batch 84 = 0.8592289090156555\n",
      "Loss for batch 85 = 1.007522463798523\n",
      "Loss for batch 86 = 0.7272177934646606\n",
      "Loss for batch 87 = 0.7925782799720764\n",
      "Loss for batch 88 = 0.8692031502723694\n",
      "Loss for batch 89 = 0.91092449426651\n",
      "Loss for batch 90 = 0.9423360824584961\n",
      "Loss for batch 91 = 0.8403358459472656\n",
      "Loss for batch 92 = 0.7843602895736694\n",
      "Loss for batch 93 = 0.7307473421096802\n",
      "Loss for batch 94 = 0.839854896068573\n",
      "Loss for batch 95 = 0.9039556384086609\n",
      "Loss for batch 96 = 0.7629660964012146\n",
      "Loss for batch 97 = 0.7037339210510254\n",
      "Loss for batch 98 = 0.7841087579727173\n",
      "Loss for batch 99 = 0.7887837886810303\n",
      "Loss for batch 100 = 0.7200398445129395\n",
      "Loss for batch 101 = 0.825071394443512\n",
      "Loss for batch 102 = 0.7639734745025635\n",
      "Loss for batch 103 = 0.9762372970581055\n",
      "Loss for batch 104 = 0.6721301674842834\n",
      "Loss for batch 105 = 0.8266969323158264\n",
      "Loss for batch 106 = 0.8339241743087769\n",
      "\n",
      "Training Loss for epoch 10 = 84.74284362792969\n",
      "\n",
      "Current Validation Loss = 13.181449890136719\n",
      "Best Validation Loss = 13.025218963623047\n",
      "Epochs without Improvement = 2\n",
      "Train Accuracy: 65.23%\n",
      "Validation Accuracy: 61.81%\n",
      "\n",
      "Epoch 11\n",
      "----------\n",
      "Loss for batch 0 = 0.7457773089408875\n",
      "Loss for batch 1 = 0.5682965517044067\n",
      "Loss for batch 2 = 0.8356825113296509\n",
      "Loss for batch 3 = 0.6879743337631226\n",
      "Loss for batch 4 = 0.6630989909172058\n",
      "Loss for batch 5 = 0.6325172781944275\n",
      "Loss for batch 6 = 0.8703382611274719\n",
      "Loss for batch 7 = 1.0643045902252197\n",
      "Loss for batch 8 = 0.7433851957321167\n",
      "Loss for batch 9 = 0.6065954566001892\n",
      "Loss for batch 10 = 0.5586646795272827\n",
      "Loss for batch 11 = 0.8836572170257568\n",
      "Loss for batch 12 = 0.7670465111732483\n",
      "Loss for batch 13 = 0.7216674089431763\n",
      "Loss for batch 14 = 0.6895226836204529\n",
      "Loss for batch 15 = 0.7950044870376587\n",
      "Loss for batch 16 = 1.187780499458313\n",
      "Loss for batch 17 = 0.7372739911079407\n",
      "Loss for batch 18 = 0.8848533034324646\n",
      "Loss for batch 19 = 0.6669188737869263\n",
      "Loss for batch 20 = 0.813786506652832\n",
      "Loss for batch 21 = 0.5814075469970703\n",
      "Loss for batch 22 = 1.4359266757965088\n",
      "Loss for batch 23 = 0.7320520877838135\n",
      "Loss for batch 24 = 0.8045465350151062\n",
      "Loss for batch 25 = 0.7360795140266418\n",
      "Loss for batch 26 = 0.8280205726623535\n",
      "Loss for batch 27 = 0.8028988838195801\n",
      "Loss for batch 28 = 0.9234257340431213\n",
      "Loss for batch 29 = 0.899002194404602\n",
      "Loss for batch 30 = 0.8560686707496643\n",
      "Loss for batch 31 = 1.020956039428711\n",
      "Loss for batch 32 = 0.7349112033843994\n",
      "Loss for batch 33 = 0.6685637831687927\n",
      "Loss for batch 34 = 0.8706295490264893\n",
      "Loss for batch 35 = 1.0414879322052002\n",
      "Loss for batch 36 = 0.719230592250824\n",
      "Loss for batch 37 = 0.7312822937965393\n",
      "Loss for batch 38 = 0.8423891663551331\n",
      "Loss for batch 39 = 0.6263931393623352\n",
      "Loss for batch 40 = 0.8619893193244934\n",
      "Loss for batch 41 = 0.639290988445282\n",
      "Loss for batch 42 = 0.7305516600608826\n",
      "Loss for batch 43 = 0.9775556921958923\n",
      "Loss for batch 44 = 0.8032355308532715\n",
      "Loss for batch 45 = 0.8126365542411804\n",
      "Loss for batch 46 = 0.7541776895523071\n",
      "Loss for batch 47 = 0.7280756235122681\n",
      "Loss for batch 48 = 0.701036274433136\n",
      "Loss for batch 49 = 0.7443411946296692\n",
      "Loss for batch 50 = 0.8014212846755981\n",
      "Loss for batch 51 = 0.66483473777771\n",
      "Loss for batch 52 = 0.9036917090415955\n",
      "Loss for batch 53 = 0.8900638818740845\n",
      "Loss for batch 54 = 0.7333732843399048\n",
      "Loss for batch 55 = 0.7910369634628296\n",
      "Loss for batch 56 = 0.8238284587860107\n",
      "Loss for batch 57 = 0.677030086517334\n",
      "Loss for batch 58 = 0.6771083474159241\n",
      "Loss for batch 59 = 0.8004354238510132\n",
      "Loss for batch 60 = 0.683789074420929\n",
      "Loss for batch 61 = 0.8819341659545898\n",
      "Loss for batch 62 = 1.1265827417373657\n",
      "Loss for batch 63 = 0.6540812253952026\n",
      "Loss for batch 64 = 0.6310802102088928\n",
      "Loss for batch 65 = 0.8017191886901855\n",
      "Loss for batch 66 = 0.902606725692749\n",
      "Loss for batch 67 = 0.6395972967147827\n",
      "Loss for batch 68 = 0.7582487463951111\n",
      "Loss for batch 69 = 0.6909236311912537\n",
      "Loss for batch 70 = 0.7666446566581726\n",
      "Loss for batch 71 = 0.5795180797576904\n",
      "Loss for batch 72 = 0.7811054587364197\n",
      "Loss for batch 73 = 0.9382458925247192\n",
      "Loss for batch 74 = 0.9159384965896606\n",
      "Loss for batch 75 = 0.9726396799087524\n",
      "Loss for batch 76 = 0.9036029577255249\n",
      "Loss for batch 77 = 0.9505672454833984\n",
      "Loss for batch 78 = 0.7102411389350891\n",
      "Loss for batch 79 = 0.6246652603149414\n",
      "Loss for batch 80 = 0.6639475226402283\n",
      "Loss for batch 81 = 0.7759101986885071\n",
      "Loss for batch 82 = 0.7678149938583374\n",
      "Loss for batch 83 = 0.8394942283630371\n",
      "Loss for batch 84 = 0.8469588756561279\n",
      "Loss for batch 85 = 1.0072518587112427\n",
      "Loss for batch 86 = 0.8102525472640991\n",
      "Loss for batch 87 = 0.9065912365913391\n",
      "Loss for batch 88 = 0.9518881440162659\n",
      "Loss for batch 89 = 0.9919476509094238\n",
      "Loss for batch 90 = 0.952254593372345\n",
      "Loss for batch 91 = 0.9403761625289917\n",
      "Loss for batch 92 = 0.8224523663520813\n",
      "Loss for batch 93 = 0.8476063013076782\n",
      "Loss for batch 94 = 0.76634681224823\n",
      "Loss for batch 95 = 0.8754362463951111\n",
      "Loss for batch 96 = 0.7588619589805603\n",
      "Loss for batch 97 = 0.7529593706130981\n",
      "Loss for batch 98 = 0.8028110265731812\n",
      "Loss for batch 99 = 0.7701838612556458\n",
      "Loss for batch 100 = 0.7765139937400818\n",
      "Loss for batch 101 = 0.7486388683319092\n",
      "Loss for batch 102 = 0.7453440427780151\n",
      "Loss for batch 103 = 0.9881635308265686\n",
      "Loss for batch 104 = 0.7886627912521362\n",
      "Loss for batch 105 = 0.8885522484779358\n",
      "Loss for batch 106 = 0.8169151544570923\n",
      "\n",
      "Training Loss for epoch 11 = 86.01301574707031\n",
      "\n",
      "Current Validation Loss = 13.452672004699707\n",
      "Best Validation Loss = 13.025218963623047\n",
      "Epochs without Improvement = 3\n",
      "Train Accuracy: 65.43%\n",
      "Validation Accuracy: 62.42%\n",
      "\n",
      "Epoch 12\n",
      "----------\n",
      "Loss for batch 0 = 0.6397561430931091\n",
      "Loss for batch 1 = 0.621781587600708\n",
      "Loss for batch 2 = 0.8080161213874817\n",
      "Loss for batch 3 = 0.6985547542572021\n",
      "Loss for batch 4 = 0.624464750289917\n",
      "Loss for batch 5 = 0.5259259343147278\n",
      "Loss for batch 6 = 0.9206448793411255\n",
      "Loss for batch 7 = 1.174678921699524\n",
      "Loss for batch 8 = 0.7477172613143921\n",
      "Loss for batch 9 = 0.6589377522468567\n",
      "Loss for batch 10 = 0.611373245716095\n",
      "Loss for batch 11 = 0.9805884957313538\n",
      "Loss for batch 12 = 0.7636045217514038\n",
      "Loss for batch 13 = 0.77928626537323\n",
      "Loss for batch 14 = 0.6626260280609131\n",
      "Loss for batch 15 = 0.7684418559074402\n",
      "Loss for batch 16 = 0.7370980978012085\n",
      "Loss for batch 17 = 0.7877935767173767\n",
      "Loss for batch 18 = 0.8245691061019897\n",
      "Loss for batch 19 = 0.579919159412384\n",
      "Loss for batch 20 = 0.8209234476089478\n",
      "Loss for batch 21 = 0.5511227250099182\n",
      "Loss for batch 22 = 1.424919605255127\n",
      "Loss for batch 23 = 0.7340049147605896\n",
      "Loss for batch 24 = 0.772350013256073\n",
      "Loss for batch 25 = 0.7147274613380432\n",
      "Loss for batch 26 = 0.7415984869003296\n",
      "Loss for batch 27 = 0.8531679511070251\n",
      "Loss for batch 28 = 0.7434692978858948\n",
      "Loss for batch 29 = 0.8365349173545837\n",
      "Loss for batch 30 = 0.7904678583145142\n",
      "Loss for batch 31 = 1.0487438440322876\n",
      "Loss for batch 32 = 0.7100581526756287\n",
      "Loss for batch 33 = 0.6823316812515259\n",
      "Loss for batch 34 = 0.7944427728652954\n",
      "Loss for batch 35 = 0.9373818635940552\n",
      "Loss for batch 36 = 0.6963380575180054\n",
      "Loss for batch 37 = 0.7578886151313782\n",
      "Loss for batch 38 = 0.8873981237411499\n",
      "Loss for batch 39 = 0.6542148590087891\n",
      "Loss for batch 40 = 0.9174116253852844\n",
      "Loss for batch 41 = 0.7446398138999939\n",
      "Loss for batch 42 = 0.7593808770179749\n",
      "Loss for batch 43 = 0.9574854373931885\n",
      "Loss for batch 44 = 0.7398326992988586\n",
      "Loss for batch 45 = 0.829649031162262\n",
      "Loss for batch 46 = 0.8075403571128845\n",
      "Loss for batch 47 = 0.6904867887496948\n",
      "Loss for batch 48 = 0.6142078042030334\n",
      "Loss for batch 49 = 0.7451037764549255\n",
      "Loss for batch 50 = 0.7883015871047974\n",
      "Loss for batch 51 = 0.6971244215965271\n",
      "Loss for batch 52 = 0.7521318793296814\n",
      "Loss for batch 53 = 0.8824338316917419\n",
      "Loss for batch 54 = 0.6989465355873108\n",
      "Loss for batch 55 = 0.8190543055534363\n",
      "Loss for batch 56 = 0.8016117811203003\n",
      "Loss for batch 57 = 0.643078088760376\n",
      "Loss for batch 58 = 0.6661862134933472\n",
      "Loss for batch 59 = 0.8403292298316956\n",
      "Loss for batch 60 = 0.7042784690856934\n",
      "Loss for batch 61 = 0.8135164380073547\n",
      "Loss for batch 62 = 0.8279902935028076\n",
      "Loss for batch 63 = 0.5776860117912292\n",
      "Loss for batch 64 = 0.5704017281532288\n",
      "Loss for batch 65 = 0.779059648513794\n",
      "Loss for batch 66 = 0.8405895829200745\n",
      "Loss for batch 67 = 0.6393944621086121\n",
      "Loss for batch 68 = 0.6721263527870178\n",
      "Loss for batch 69 = 0.6329070925712585\n",
      "Loss for batch 70 = 0.7928686738014221\n",
      "Loss for batch 71 = 0.5531073212623596\n",
      "Loss for batch 72 = 0.7462766766548157\n",
      "Loss for batch 73 = 0.8732264041900635\n",
      "Loss for batch 74 = 0.8547165393829346\n",
      "Loss for batch 75 = 1.0103517770767212\n",
      "Loss for batch 76 = 0.9245588183403015\n",
      "Loss for batch 77 = 0.9667068123817444\n",
      "Loss for batch 78 = 0.8019432425498962\n",
      "Loss for batch 79 = 0.598511815071106\n",
      "Loss for batch 80 = 0.6983697414398193\n",
      "Loss for batch 81 = 0.7358497381210327\n",
      "Loss for batch 82 = 0.6592194437980652\n",
      "Loss for batch 83 = 0.8955112099647522\n",
      "Loss for batch 84 = 0.8563851118087769\n",
      "Loss for batch 85 = 0.9988214373588562\n",
      "Loss for batch 86 = 0.7126260995864868\n",
      "Loss for batch 87 = 0.7374797463417053\n",
      "Loss for batch 88 = 0.8187322020530701\n",
      "Loss for batch 89 = 0.8728978037834167\n",
      "Loss for batch 90 = 0.9098755717277527\n",
      "Loss for batch 91 = 0.7710155844688416\n",
      "Loss for batch 92 = 0.7893984317779541\n",
      "Loss for batch 93 = 0.6625932455062866\n",
      "Loss for batch 94 = 0.7645241022109985\n",
      "Loss for batch 95 = 0.8248783349990845\n",
      "Loss for batch 96 = 0.5981124639511108\n",
      "Loss for batch 97 = 0.7249033451080322\n",
      "Loss for batch 98 = 0.8249345421791077\n",
      "Loss for batch 99 = 0.8505662679672241\n",
      "Loss for batch 100 = 0.6979202032089233\n",
      "Loss for batch 101 = 0.6919749975204468\n",
      "Loss for batch 102 = 0.6852396130561829\n",
      "Loss for batch 103 = 0.9399086833000183\n",
      "Loss for batch 104 = 0.637579619884491\n",
      "Loss for batch 105 = 0.8801894783973694\n",
      "Loss for batch 106 = 0.7347148656845093\n",
      "\n",
      "Training Loss for epoch 12 = 82.61725616455078\n",
      "\n",
      "Current Validation Loss = 13.267621994018555\n",
      "Best Validation Loss = 13.025218963623047\n",
      "Epochs without Improvement = 4\n",
      "Train Accuracy: 64.67%\n",
      "Validation Accuracy: 60.78%\n",
      "\n",
      "Epoch 13\n",
      "----------\n",
      "Loss for batch 0 = 0.6869879364967346\n",
      "Loss for batch 1 = 0.5101702809333801\n",
      "Loss for batch 2 = 0.8957764506340027\n",
      "Loss for batch 3 = 0.595719039440155\n",
      "Loss for batch 4 = 0.7531261444091797\n",
      "Loss for batch 5 = 0.59766685962677\n",
      "Loss for batch 6 = 0.904447078704834\n",
      "Loss for batch 7 = 1.1004068851470947\n",
      "Loss for batch 8 = 0.7400978207588196\n",
      "Loss for batch 9 = 0.63127201795578\n",
      "Loss for batch 10 = 0.5631880760192871\n",
      "Loss for batch 11 = 0.928871214389801\n",
      "Loss for batch 12 = 0.7470245361328125\n",
      "Loss for batch 13 = 0.8092204928398132\n",
      "Loss for batch 14 = 0.6372290849685669\n",
      "Loss for batch 15 = 0.8413665890693665\n",
      "Loss for batch 16 = 0.8581536412239075\n",
      "Loss for batch 17 = 0.7301384210586548\n",
      "Loss for batch 18 = 0.8593572378158569\n",
      "Loss for batch 19 = 0.6054850220680237\n",
      "Loss for batch 20 = 0.7127988338470459\n",
      "Loss for batch 21 = 0.5097086429595947\n",
      "Loss for batch 22 = 1.3304959535598755\n",
      "Loss for batch 23 = 0.6753637194633484\n",
      "Loss for batch 24 = 0.6680334806442261\n",
      "Loss for batch 25 = 0.679443359375\n",
      "Loss for batch 26 = 0.7785571217536926\n",
      "Loss for batch 27 = 0.7759647965431213\n",
      "Loss for batch 28 = 0.8127695918083191\n",
      "Loss for batch 29 = 0.8566839098930359\n",
      "Loss for batch 30 = 0.7079887986183167\n",
      "Loss for batch 31 = 1.1310049295425415\n",
      "Loss for batch 32 = 0.6864317059516907\n",
      "Loss for batch 33 = 0.7077705264091492\n",
      "Loss for batch 34 = 0.7631994485855103\n",
      "Loss for batch 35 = 0.9191195964813232\n",
      "Loss for batch 36 = 0.677193820476532\n",
      "Loss for batch 37 = 0.7474244236946106\n",
      "Loss for batch 38 = 0.7592613697052002\n",
      "Loss for batch 39 = 0.5683539509773254\n",
      "Loss for batch 40 = 0.8018030524253845\n",
      "Loss for batch 41 = 0.7356841564178467\n",
      "Loss for batch 42 = 0.7550288438796997\n",
      "Loss for batch 43 = 0.8841760754585266\n",
      "Loss for batch 44 = 0.7087782621383667\n",
      "Loss for batch 45 = 0.7047253847122192\n",
      "Loss for batch 46 = 0.8479992151260376\n",
      "Loss for batch 47 = 0.7208964228630066\n",
      "Loss for batch 48 = 0.4906576871871948\n",
      "Loss for batch 49 = 0.7897108793258667\n",
      "Loss for batch 50 = 0.7414501905441284\n",
      "Loss for batch 51 = 0.5820431113243103\n",
      "Loss for batch 52 = 0.808022677898407\n",
      "Loss for batch 53 = 0.8766645193099976\n",
      "Loss for batch 54 = 0.7383856773376465\n",
      "Loss for batch 55 = 0.9044458866119385\n",
      "Loss for batch 56 = 0.7651903629302979\n",
      "Loss for batch 57 = 0.6579956412315369\n",
      "Loss for batch 58 = 0.6194077730178833\n",
      "Loss for batch 59 = 0.7945902943611145\n",
      "Loss for batch 60 = 0.6289075613021851\n",
      "Loss for batch 61 = 0.7643514275550842\n",
      "Loss for batch 62 = 0.7290052771568298\n",
      "Loss for batch 63 = 0.6059171557426453\n",
      "Loss for batch 64 = 0.6732968688011169\n",
      "Loss for batch 65 = 1.2378956079483032\n",
      "Loss for batch 66 = 1.3263270854949951\n",
      "Loss for batch 67 = 0.9777272343635559\n",
      "Loss for batch 68 = 1.1804167032241821\n",
      "Loss for batch 69 = 0.8395706415176392\n",
      "Loss for batch 70 = 0.9703609943389893\n",
      "Loss for batch 71 = 0.9028143882751465\n",
      "Loss for batch 72 = 0.9876240491867065\n",
      "Loss for batch 73 = 0.8948685526847839\n",
      "Loss for batch 74 = 1.0092861652374268\n",
      "Loss for batch 75 = 1.0161192417144775\n",
      "Loss for batch 76 = 1.0972976684570312\n",
      "Loss for batch 77 = 1.0628225803375244\n",
      "Loss for batch 78 = 0.8970398306846619\n",
      "Loss for batch 79 = 0.8423463106155396\n",
      "Loss for batch 80 = 0.8726974725723267\n",
      "Loss for batch 81 = 0.9082657098770142\n",
      "Loss for batch 82 = 0.8107923269271851\n",
      "Loss for batch 83 = 1.1011277437210083\n",
      "Loss for batch 84 = 0.9005985260009766\n",
      "Loss for batch 85 = 1.019096851348877\n",
      "Loss for batch 86 = 0.8996874094009399\n",
      "Loss for batch 87 = 0.9402168989181519\n",
      "Loss for batch 88 = 0.8799955248832703\n",
      "Loss for batch 89 = 1.011698603630066\n",
      "Loss for batch 90 = 0.9594722986221313\n",
      "Loss for batch 91 = 0.8409332036972046\n",
      "Loss for batch 92 = 0.9986852407455444\n",
      "Loss for batch 93 = 0.8026115298271179\n",
      "Loss for batch 94 = 0.7371019124984741\n",
      "Loss for batch 95 = 0.9499976634979248\n",
      "Loss for batch 96 = 0.9272085428237915\n",
      "Loss for batch 97 = 0.8550384044647217\n",
      "Loss for batch 98 = 0.8036478757858276\n",
      "Loss for batch 99 = 0.9585793018341064\n",
      "Loss for batch 100 = 0.9076895117759705\n",
      "Loss for batch 101 = 0.9185115694999695\n",
      "Loss for batch 102 = 0.9068359136581421\n",
      "Loss for batch 103 = 1.0848699808120728\n",
      "Loss for batch 104 = 0.8850904703140259\n",
      "Loss for batch 105 = 1.005369782447815\n",
      "Loss for batch 106 = 0.9936391115188599\n",
      "\n",
      "Training Loss for epoch 13 = 88.91438293457031\n",
      "\n",
      "Current Validation Loss = 13.72598934173584\n",
      "Best Validation Loss = 13.025218963623047\n",
      "Epochs without Improvement = 5\n",
      "Train Accuracy: 64.64%\n",
      "Validation Accuracy: 63.24%\n",
      "\n",
      "Epoch 14\n",
      "----------\n",
      "Loss for batch 0 = 0.8594924211502075\n",
      "Loss for batch 1 = 0.698986291885376\n",
      "Loss for batch 2 = 0.817906379699707\n",
      "Loss for batch 3 = 0.8250797986984253\n",
      "Loss for batch 4 = 0.7724965214729309\n",
      "Loss for batch 5 = 0.7549430727958679\n",
      "Loss for batch 6 = 0.9825029373168945\n",
      "Loss for batch 7 = 1.033432960510254\n",
      "Loss for batch 8 = 0.8309394121170044\n",
      "Loss for batch 9 = 0.749228835105896\n",
      "Loss for batch 10 = 0.6721518635749817\n",
      "Loss for batch 11 = 0.9311021566390991\n",
      "Loss for batch 12 = 0.8125901222229004\n",
      "Loss for batch 13 = 0.8529922366142273\n",
      "Loss for batch 14 = 0.8690105676651001\n",
      "Loss for batch 15 = 0.779078483581543\n",
      "Loss for batch 16 = 0.9088270664215088\n",
      "Loss for batch 17 = 0.7220755815505981\n",
      "Loss for batch 18 = 0.8228329420089722\n",
      "Loss for batch 19 = 0.6274987459182739\n",
      "Loss for batch 20 = 0.8519951105117798\n",
      "Loss for batch 21 = 0.6525413393974304\n",
      "Loss for batch 22 = 1.288638710975647\n",
      "Loss for batch 23 = 0.6573531627655029\n",
      "Loss for batch 24 = 0.9193953275680542\n",
      "Loss for batch 25 = 0.701793372631073\n",
      "Loss for batch 26 = 0.7729363441467285\n",
      "Loss for batch 27 = 0.7979813814163208\n",
      "Loss for batch 28 = 0.8530083298683167\n",
      "Loss for batch 29 = 0.8850023150444031\n",
      "Loss for batch 30 = 0.7035052180290222\n",
      "Loss for batch 31 = 1.1082661151885986\n",
      "Loss for batch 32 = 0.7958908081054688\n",
      "Loss for batch 33 = 0.7952783703804016\n",
      "Loss for batch 34 = 0.8725232481956482\n",
      "Loss for batch 35 = 0.926754355430603\n",
      "Loss for batch 36 = 0.741921067237854\n",
      "Loss for batch 37 = 0.7457712888717651\n",
      "Loss for batch 38 = 0.8749607801437378\n",
      "Loss for batch 39 = 0.6524719595909119\n",
      "Loss for batch 40 = 0.7098278403282166\n",
      "Loss for batch 41 = 0.7209172248840332\n",
      "Loss for batch 42 = 0.8711000680923462\n",
      "Loss for batch 43 = 0.9908658266067505\n",
      "Loss for batch 44 = 0.7671167850494385\n",
      "Loss for batch 45 = 0.8759686946868896\n",
      "Loss for batch 46 = 0.7617331743240356\n",
      "Loss for batch 47 = 0.7456181645393372\n",
      "Loss for batch 48 = 0.6195295453071594\n",
      "Loss for batch 49 = 0.8485110402107239\n",
      "Loss for batch 50 = 0.7308928966522217\n",
      "Loss for batch 51 = 0.6330651044845581\n",
      "Loss for batch 52 = 0.8594515919685364\n",
      "Loss for batch 53 = 1.1382249593734741\n",
      "Loss for batch 54 = 0.8223326206207275\n",
      "Loss for batch 55 = 0.7595211863517761\n",
      "Loss for batch 56 = 1.0392738580703735\n",
      "Loss for batch 57 = 0.6585749983787537\n",
      "Loss for batch 58 = 0.738023042678833\n",
      "Loss for batch 59 = 0.8874119520187378\n",
      "Loss for batch 60 = 0.7762231826782227\n",
      "Loss for batch 61 = 0.7777189612388611\n",
      "Loss for batch 62 = 0.8031970858573914\n",
      "Loss for batch 63 = 0.6627973318099976\n",
      "Loss for batch 64 = 0.8698323965072632\n",
      "Loss for batch 65 = 0.8025745153427124\n",
      "Loss for batch 66 = 0.9847493171691895\n",
      "Loss for batch 67 = 0.7609940767288208\n",
      "Loss for batch 68 = 0.7594553232192993\n",
      "Loss for batch 69 = 0.6725028157234192\n",
      "Loss for batch 70 = 0.8583642244338989\n",
      "Loss for batch 71 = 0.6493845582008362\n",
      "Loss for batch 72 = 0.9690661430358887\n",
      "Loss for batch 73 = 0.8028145432472229\n",
      "Loss for batch 74 = 0.886162281036377\n",
      "Loss for batch 75 = 1.0123013257980347\n",
      "Loss for batch 76 = 0.9052696824073792\n",
      "Loss for batch 77 = 0.997869610786438\n",
      "Loss for batch 78 = 0.7290520071983337\n",
      "Loss for batch 79 = 0.6900656819343567\n",
      "Loss for batch 80 = 0.5721548795700073\n",
      "Loss for batch 81 = 0.7679896354675293\n",
      "Loss for batch 82 = 0.646729052066803\n",
      "Loss for batch 83 = 0.977831244468689\n",
      "Loss for batch 84 = 0.9243875741958618\n",
      "Loss for batch 85 = 1.1317503452301025\n",
      "Loss for batch 86 = 0.7682853937149048\n",
      "Loss for batch 87 = 0.8449552059173584\n",
      "Loss for batch 88 = 0.8956080079078674\n",
      "Loss for batch 89 = 0.9437126517295837\n",
      "Loss for batch 90 = 0.9169564247131348\n",
      "Loss for batch 91 = 0.819237232208252\n",
      "Loss for batch 92 = 0.8540663719177246\n",
      "Loss for batch 93 = 0.7145053148269653\n",
      "Loss for batch 94 = 0.7187727093696594\n",
      "Loss for batch 95 = 0.9269659519195557\n",
      "Loss for batch 96 = 0.7215450406074524\n",
      "Loss for batch 97 = 0.7413493990898132\n",
      "Loss for batch 98 = 0.7832394242286682\n",
      "Loss for batch 99 = 0.752212643623352\n",
      "Loss for batch 100 = 0.6947815418243408\n",
      "Loss for batch 101 = 0.9122880101203918\n",
      "Loss for batch 102 = 0.6568589210510254\n",
      "Loss for batch 103 = 1.0271284580230713\n",
      "Loss for batch 104 = 0.653790295124054\n",
      "Loss for batch 105 = 0.8175495862960815\n",
      "Loss for batch 106 = 0.8482034206390381\n",
      "\n",
      "Training Loss for epoch 14 = 87.50234985351562\n",
      "\n",
      "Current Validation Loss = 13.04908275604248\n",
      "Best Validation Loss = 13.025218963623047\n",
      "Epochs without Improvement = 6\n",
      "Train Accuracy: 65.96%\n",
      "Validation Accuracy: 62.01%\n",
      "\n",
      "Epoch 15\n",
      "----------\n",
      "Loss for batch 0 = 0.7085513472557068\n",
      "Loss for batch 1 = 0.5436820387840271\n",
      "Loss for batch 2 = 0.767855167388916\n",
      "Loss for batch 3 = 0.6684840321540833\n",
      "Loss for batch 4 = 0.7291454672813416\n",
      "Loss for batch 5 = 0.638780415058136\n",
      "Loss for batch 6 = 0.9961246848106384\n",
      "Loss for batch 7 = 1.0069293975830078\n",
      "Loss for batch 8 = 1.023857593536377\n",
      "Loss for batch 9 = 0.6640017628669739\n",
      "Loss for batch 10 = 0.8247450590133667\n",
      "Loss for batch 11 = 1.043049693107605\n",
      "Loss for batch 12 = 0.8112221360206604\n",
      "Loss for batch 13 = 0.8558493256568909\n",
      "Loss for batch 14 = 0.8691127300262451\n",
      "Loss for batch 15 = 0.898906409740448\n",
      "Loss for batch 16 = 0.7855105400085449\n",
      "Loss for batch 17 = 0.805562436580658\n",
      "Loss for batch 18 = 0.7469683885574341\n",
      "Loss for batch 19 = 0.6280103921890259\n",
      "Loss for batch 20 = 0.8114112019538879\n",
      "Loss for batch 21 = 0.5388409495353699\n",
      "Loss for batch 22 = 1.2742500305175781\n",
      "Loss for batch 23 = 0.7441928386688232\n",
      "Loss for batch 24 = 0.7406884431838989\n",
      "Loss for batch 25 = 0.6192185878753662\n",
      "Loss for batch 26 = 0.8544043302536011\n",
      "Loss for batch 27 = 0.8273873329162598\n",
      "Loss for batch 28 = 0.8324146866798401\n",
      "Loss for batch 29 = 0.8768543004989624\n",
      "Loss for batch 30 = 0.7512192130088806\n",
      "Loss for batch 31 = 1.1235357522964478\n",
      "Loss for batch 32 = 0.6281665563583374\n",
      "Loss for batch 33 = 0.7242366075515747\n",
      "Loss for batch 34 = 0.7919630408287048\n",
      "Loss for batch 35 = 0.9277812838554382\n",
      "Loss for batch 36 = 0.6517276763916016\n",
      "Loss for batch 37 = 0.7405619025230408\n",
      "Loss for batch 38 = 0.8138816356658936\n",
      "Loss for batch 39 = 0.558578372001648\n",
      "Loss for batch 40 = 0.7079392671585083\n",
      "Loss for batch 41 = 0.674383282661438\n",
      "Loss for batch 42 = 0.8441881537437439\n",
      "Loss for batch 43 = 0.9174190163612366\n",
      "Loss for batch 44 = 0.7373955249786377\n",
      "Loss for batch 45 = 0.7496404051780701\n",
      "Loss for batch 46 = 0.8129575848579407\n",
      "Loss for batch 47 = 0.7349902391433716\n",
      "Loss for batch 48 = 0.5666400790214539\n",
      "Loss for batch 49 = 0.7458622455596924\n",
      "Loss for batch 50 = 0.7227625250816345\n",
      "Loss for batch 51 = 0.6056819558143616\n",
      "Loss for batch 52 = 0.7068485021591187\n",
      "Loss for batch 53 = 0.9650931358337402\n",
      "Loss for batch 54 = 0.7056118845939636\n",
      "Loss for batch 55 = 0.8314696550369263\n",
      "Loss for batch 56 = 1.0158995389938354\n",
      "Loss for batch 57 = 0.5776046514511108\n",
      "Loss for batch 58 = 0.6588067412376404\n",
      "Loss for batch 59 = 0.7751291990280151\n",
      "Loss for batch 60 = 0.6597795486450195\n",
      "Loss for batch 61 = 0.7978821396827698\n",
      "Loss for batch 62 = 0.8088515400886536\n",
      "Loss for batch 63 = 0.6794463396072388\n",
      "Loss for batch 64 = 0.7714864611625671\n",
      "Loss for batch 65 = 0.7845509052276611\n",
      "Loss for batch 66 = 0.9349648952484131\n",
      "Loss for batch 67 = 0.6674606800079346\n",
      "Loss for batch 68 = 0.6584912538528442\n",
      "Loss for batch 69 = 0.5687538385391235\n",
      "Loss for batch 70 = 0.792311429977417\n",
      "Loss for batch 71 = 0.6545878052711487\n",
      "Loss for batch 72 = 0.9024670720100403\n",
      "Loss for batch 73 = 0.8400657773017883\n",
      "Loss for batch 74 = 0.8561838269233704\n",
      "Loss for batch 75 = 0.9478335976600647\n",
      "Loss for batch 76 = 0.8744633793830872\n",
      "Loss for batch 77 = 0.9596964120864868\n",
      "Loss for batch 78 = 0.6937269568443298\n",
      "Loss for batch 79 = 0.6651257276535034\n",
      "Loss for batch 80 = 0.59656822681427\n",
      "Loss for batch 81 = 0.6801950335502625\n",
      "Loss for batch 82 = 0.5817105174064636\n",
      "Loss for batch 83 = 0.8455790877342224\n",
      "Loss for batch 84 = 0.812221348285675\n",
      "Loss for batch 85 = 1.0115506649017334\n",
      "Loss for batch 86 = 0.7052054405212402\n",
      "Loss for batch 87 = 0.7864536643028259\n",
      "Loss for batch 88 = 0.8119087815284729\n",
      "Loss for batch 89 = 0.9370658993721008\n",
      "Loss for batch 90 = 0.8686190843582153\n",
      "Loss for batch 91 = 0.7422657012939453\n",
      "Loss for batch 92 = 0.7987043261528015\n",
      "Loss for batch 93 = 0.7316105365753174\n",
      "Loss for batch 94 = 0.7358930110931396\n",
      "Loss for batch 95 = 0.9231087565422058\n",
      "Loss for batch 96 = 0.640995442867279\n",
      "Loss for batch 97 = 0.7036860585212708\n",
      "Loss for batch 98 = 0.7917311191558838\n",
      "Loss for batch 99 = 0.8430127501487732\n",
      "Loss for batch 100 = 0.6288628578186035\n",
      "Loss for batch 101 = 0.7391676306724548\n",
      "Loss for batch 102 = 0.599661648273468\n",
      "Loss for batch 103 = 1.0970489978790283\n",
      "Loss for batch 104 = 0.5726715326309204\n",
      "Loss for batch 105 = 0.8047332167625427\n",
      "Loss for batch 106 = 0.654457151889801\n",
      "\n",
      "Training Loss for epoch 15 = 83.06681060791016\n",
      "\n",
      "Current Validation Loss = 12.916242599487305\n",
      "Best Validation Loss = 12.916242599487305\n",
      "Epochs without Improvement = 0\n",
      "Train Accuracy: 68.19%\n",
      "Validation Accuracy: 62.83%\n",
      "\n",
      "Epoch 16\n",
      "----------\n",
      "Loss for batch 0 = 0.7235940098762512\n",
      "Loss for batch 1 = 0.48685210943222046\n",
      "Loss for batch 2 = 0.6881297826766968\n",
      "Loss for batch 3 = 0.5865769386291504\n",
      "Loss for batch 4 = 0.6891034841537476\n",
      "Loss for batch 5 = 0.6595456600189209\n",
      "Loss for batch 6 = 0.8948698043823242\n",
      "Loss for batch 7 = 1.1301733255386353\n",
      "Loss for batch 8 = 0.7293773293495178\n",
      "Loss for batch 9 = 0.5105380415916443\n",
      "Loss for batch 10 = 0.5999976992607117\n",
      "Loss for batch 11 = 0.9498772621154785\n",
      "Loss for batch 12 = 0.8089280128479004\n",
      "Loss for batch 13 = 0.7736640572547913\n",
      "Loss for batch 14 = 0.7746657729148865\n",
      "Loss for batch 15 = 0.8146525025367737\n",
      "Loss for batch 16 = 0.8738893866539001\n",
      "Loss for batch 17 = 0.7486838102340698\n",
      "Loss for batch 18 = 0.7744590044021606\n",
      "Loss for batch 19 = 0.5403991937637329\n",
      "Loss for batch 20 = 0.7671244740486145\n",
      "Loss for batch 21 = 0.49821245670318604\n",
      "Loss for batch 22 = 1.3186191320419312\n",
      "Loss for batch 23 = 0.6689939498901367\n",
      "Loss for batch 24 = 0.7026445865631104\n",
      "Loss for batch 25 = 0.617574155330658\n",
      "Loss for batch 26 = 0.8223137259483337\n",
      "Loss for batch 27 = 0.7408450245857239\n",
      "Loss for batch 28 = 0.7655748128890991\n",
      "Loss for batch 29 = 0.8981095552444458\n",
      "Loss for batch 30 = 0.7242909073829651\n",
      "Loss for batch 31 = 1.066582202911377\n",
      "Loss for batch 32 = 0.6560648679733276\n",
      "Loss for batch 33 = 0.6106097102165222\n",
      "Loss for batch 34 = 0.8111106157302856\n",
      "Loss for batch 35 = 0.8918238282203674\n",
      "Loss for batch 36 = 0.6975404024124146\n",
      "Loss for batch 37 = 0.7144737839698792\n",
      "Loss for batch 38 = 0.8001009225845337\n",
      "Loss for batch 39 = 0.5989735126495361\n",
      "Loss for batch 40 = 0.7185764312744141\n",
      "Loss for batch 41 = 0.6903898119926453\n",
      "Loss for batch 42 = 0.7535120248794556\n",
      "Loss for batch 43 = 0.8429704308509827\n",
      "Loss for batch 44 = 0.6582887172698975\n",
      "Loss for batch 45 = 0.7636285424232483\n",
      "Loss for batch 46 = 0.7308371067047119\n",
      "Loss for batch 47 = 0.6240971684455872\n",
      "Loss for batch 48 = 0.533081591129303\n",
      "Loss for batch 49 = 0.7074207663536072\n",
      "Loss for batch 50 = 0.7248905897140503\n",
      "Loss for batch 51 = 0.589388370513916\n",
      "Loss for batch 52 = 0.6757969260215759\n",
      "Loss for batch 53 = 0.928763747215271\n",
      "Loss for batch 54 = 0.6656656861305237\n",
      "Loss for batch 55 = 0.8015302419662476\n",
      "Loss for batch 56 = 0.7509545683860779\n",
      "Loss for batch 57 = 0.6094425320625305\n",
      "Loss for batch 58 = 0.6191648244857788\n",
      "Loss for batch 59 = 0.8168867230415344\n",
      "Loss for batch 60 = 0.5128374099731445\n",
      "Loss for batch 61 = 0.795844554901123\n",
      "Loss for batch 62 = 0.7125421166419983\n",
      "Loss for batch 63 = 0.6328639388084412\n",
      "Loss for batch 64 = 0.6905748844146729\n",
      "Loss for batch 65 = 0.758403480052948\n",
      "Loss for batch 66 = 0.9113914370536804\n",
      "Loss for batch 67 = 0.5483719706535339\n",
      "Loss for batch 68 = 0.6655547618865967\n",
      "Loss for batch 69 = 0.5531418323516846\n",
      "Loss for batch 70 = 0.7773081660270691\n",
      "Loss for batch 71 = 0.5389129519462585\n",
      "Loss for batch 72 = 0.9251764416694641\n",
      "Loss for batch 73 = 0.7517214417457581\n",
      "Loss for batch 74 = 0.8509355783462524\n",
      "Loss for batch 75 = 1.0193290710449219\n",
      "Loss for batch 76 = 0.8336094617843628\n",
      "Loss for batch 77 = 0.9794501662254333\n",
      "Loss for batch 78 = 0.6580649614334106\n",
      "Loss for batch 79 = 0.614691436290741\n",
      "Loss for batch 80 = 0.5913472175598145\n",
      "Loss for batch 81 = 0.6884812712669373\n",
      "Loss for batch 82 = 0.5479992628097534\n",
      "Loss for batch 83 = 0.7857572436332703\n",
      "Loss for batch 84 = 0.8054147958755493\n",
      "Loss for batch 85 = 1.0119731426239014\n",
      "Loss for batch 86 = 0.6464706659317017\n",
      "Loss for batch 87 = 0.72926265001297\n",
      "Loss for batch 88 = 1.1118667125701904\n",
      "Loss for batch 89 = 0.9693164229393005\n",
      "Loss for batch 90 = 1.0117344856262207\n",
      "Loss for batch 91 = 0.8546969890594482\n",
      "Loss for batch 92 = 1.104455828666687\n",
      "Loss for batch 93 = 0.771198034286499\n",
      "Loss for batch 94 = 0.9050191044807434\n",
      "Loss for batch 95 = 1.0291334390640259\n",
      "Loss for batch 96 = 0.8290284276008606\n",
      "Loss for batch 97 = 0.832958996295929\n",
      "Loss for batch 98 = 0.8813631534576416\n",
      "Loss for batch 99 = 0.8731440305709839\n",
      "Loss for batch 100 = 0.9957978129386902\n",
      "Loss for batch 101 = 0.8089557886123657\n",
      "Loss for batch 102 = 0.6370985507965088\n",
      "Loss for batch 103 = 1.013612985610962\n",
      "Loss for batch 104 = 0.7405856251716614\n",
      "Loss for batch 105 = 0.9189208745956421\n",
      "Loss for batch 106 = 0.9997405409812927\n",
      "\n",
      "Training Loss for epoch 16 = 82.1609115600586\n",
      "\n",
      "Current Validation Loss = 14.281675338745117\n",
      "Best Validation Loss = 12.916242599487305\n",
      "Epochs without Improvement = 1\n",
      "Train Accuracy: 62.76%\n",
      "Validation Accuracy: 57.70%\n",
      "\n",
      "Epoch 17\n",
      "----------\n",
      "Loss for batch 0 = 0.7492440342903137\n",
      "Loss for batch 1 = 0.6337237358093262\n",
      "Loss for batch 2 = 0.8150078654289246\n",
      "Loss for batch 3 = 0.7389135360717773\n",
      "Loss for batch 4 = 0.7661227583885193\n",
      "Loss for batch 5 = 0.7692310810089111\n",
      "Loss for batch 6 = 0.8927399516105652\n",
      "Loss for batch 7 = 1.0058482885360718\n",
      "Loss for batch 8 = 0.7985166311264038\n",
      "Loss for batch 9 = 0.6455800533294678\n",
      "Loss for batch 10 = 0.6161966919898987\n",
      "Loss for batch 11 = 1.0521320104599\n",
      "Loss for batch 12 = 0.7652320265769958\n",
      "Loss for batch 13 = 0.7251757979393005\n",
      "Loss for batch 14 = 0.7720603942871094\n",
      "Loss for batch 15 = 0.7749897837638855\n",
      "Loss for batch 16 = 0.8588627576828003\n",
      "Loss for batch 17 = 0.7745664715766907\n",
      "Loss for batch 18 = 0.7076378464698792\n",
      "Loss for batch 19 = 0.664986252784729\n",
      "Loss for batch 20 = 0.7727490663528442\n",
      "Loss for batch 21 = 0.5922349095344543\n",
      "Loss for batch 22 = 1.2657285928726196\n",
      "Loss for batch 23 = 0.8149181604385376\n",
      "Loss for batch 24 = 0.7904003262519836\n",
      "Loss for batch 25 = 0.6294679045677185\n",
      "Loss for batch 26 = 0.7462984919548035\n",
      "Loss for batch 27 = 0.7598916888237\n",
      "Loss for batch 28 = 0.6882990598678589\n",
      "Loss for batch 29 = 0.8335393071174622\n",
      "Loss for batch 30 = 0.7595566511154175\n",
      "Loss for batch 31 = 1.1327569484710693\n",
      "Loss for batch 32 = 0.759043276309967\n",
      "Loss for batch 33 = 0.6414235830307007\n",
      "Loss for batch 34 = 0.8679345846176147\n",
      "Loss for batch 35 = 0.8207669854164124\n",
      "Loss for batch 36 = 0.7415465712547302\n",
      "Loss for batch 37 = 0.7227869033813477\n",
      "Loss for batch 38 = 0.7580585479736328\n",
      "Loss for batch 39 = 0.5803748369216919\n",
      "Loss for batch 40 = 0.6939006447792053\n",
      "Loss for batch 41 = 0.7539786100387573\n",
      "Loss for batch 42 = 0.7733545899391174\n",
      "Loss for batch 43 = 0.8226313591003418\n",
      "Loss for batch 44 = 0.681300938129425\n",
      "Loss for batch 45 = 0.7651004195213318\n",
      "Loss for batch 46 = 0.762829601764679\n",
      "Loss for batch 47 = 0.6776995062828064\n",
      "Loss for batch 48 = 0.4949340224266052\n",
      "Loss for batch 49 = 0.7144825458526611\n",
      "Loss for batch 50 = 0.6694972515106201\n",
      "Loss for batch 51 = 0.6664965152740479\n",
      "Loss for batch 52 = 0.7626702189445496\n",
      "Loss for batch 53 = 0.9652855396270752\n",
      "Loss for batch 54 = 0.6695092916488647\n",
      "Loss for batch 55 = 0.797347366809845\n",
      "Loss for batch 56 = 0.8637233376502991\n",
      "Loss for batch 57 = 0.5997774600982666\n",
      "Loss for batch 58 = 0.6457095146179199\n",
      "Loss for batch 59 = 0.8408425450325012\n",
      "Loss for batch 60 = 0.5353891849517822\n",
      "Loss for batch 61 = 0.7917572855949402\n",
      "Loss for batch 62 = 0.8091916441917419\n",
      "Loss for batch 63 = 0.5700911283493042\n",
      "Loss for batch 64 = 0.732451856136322\n",
      "Loss for batch 65 = 0.7419473528862\n",
      "Loss for batch 66 = 0.9059918522834778\n",
      "Loss for batch 67 = 0.4738805890083313\n",
      "Loss for batch 68 = 0.6419488787651062\n",
      "Loss for batch 69 = 0.5881111025810242\n",
      "Loss for batch 70 = 0.7713807225227356\n",
      "Loss for batch 71 = 0.5213027000427246\n",
      "Loss for batch 72 = 0.9935957193374634\n",
      "Loss for batch 73 = 0.7440091371536255\n",
      "Loss for batch 74 = 0.8815370202064514\n",
      "Loss for batch 75 = 1.0974996089935303\n",
      "Loss for batch 76 = 0.9680300951004028\n",
      "Loss for batch 77 = 0.9829233884811401\n",
      "Loss for batch 78 = 0.7226627469062805\n",
      "Loss for batch 79 = 0.661710262298584\n",
      "Loss for batch 80 = 0.6724727153778076\n",
      "Loss for batch 81 = 0.6917365789413452\n",
      "Loss for batch 82 = 0.5893439650535583\n",
      "Loss for batch 83 = 0.8708887696266174\n",
      "Loss for batch 84 = 0.8440961837768555\n",
      "Loss for batch 85 = 1.0247522592544556\n",
      "Loss for batch 86 = 0.6178897619247437\n",
      "Loss for batch 87 = 0.7449430823326111\n",
      "Loss for batch 88 = 0.8165475726127625\n",
      "Loss for batch 89 = 0.9061461687088013\n",
      "Loss for batch 90 = 0.8131836652755737\n",
      "Loss for batch 91 = 0.7275351881980896\n",
      "Loss for batch 92 = 0.8957940340042114\n",
      "Loss for batch 93 = 0.7786564230918884\n",
      "Loss for batch 94 = 0.757036566734314\n",
      "Loss for batch 95 = 0.8940237760543823\n",
      "Loss for batch 96 = 0.6296178698539734\n",
      "Loss for batch 97 = 0.6787970662117004\n",
      "Loss for batch 98 = 0.799764096736908\n",
      "Loss for batch 99 = 0.7841235399246216\n",
      "Loss for batch 100 = 0.6824944019317627\n",
      "Loss for batch 101 = 0.6954220533370972\n",
      "Loss for batch 102 = 0.5736445188522339\n",
      "Loss for batch 103 = 1.0631482601165771\n",
      "Loss for batch 104 = 0.6938858032226562\n",
      "Loss for batch 105 = 0.7899793386459351\n",
      "Loss for batch 106 = 0.8788217306137085\n",
      "\n",
      "Training Loss for epoch 17 = 81.87576293945312\n",
      "\n",
      "Current Validation Loss = 12.45102310180664\n",
      "Best Validation Loss = 12.45102310180664\n",
      "Epochs without Improvement = 0\n",
      "Train Accuracy: 69.16%\n",
      "Validation Accuracy: 63.86%\n",
      "\n",
      "Epoch 18\n",
      "----------\n",
      "Loss for batch 0 = 0.7678584456443787\n",
      "Loss for batch 1 = 0.5381462574005127\n",
      "Loss for batch 2 = 0.6535547375679016\n",
      "Loss for batch 3 = 0.6165215969085693\n",
      "Loss for batch 4 = 0.5996506810188293\n",
      "Loss for batch 5 = 0.666266679763794\n",
      "Loss for batch 6 = 0.8294740319252014\n",
      "Loss for batch 7 = 0.9621904492378235\n",
      "Loss for batch 8 = 0.7317277193069458\n",
      "Loss for batch 9 = 0.5632864832878113\n",
      "Loss for batch 10 = 0.5170206427574158\n",
      "Loss for batch 11 = 0.9615100026130676\n",
      "Loss for batch 12 = 0.7800487279891968\n",
      "Loss for batch 13 = 0.7238163948059082\n",
      "Loss for batch 14 = 0.6730221509933472\n",
      "Loss for batch 15 = 0.8115576505661011\n",
      "Loss for batch 16 = 0.8957539796829224\n",
      "Loss for batch 17 = 0.6863604187965393\n",
      "Loss for batch 18 = 0.855526328086853\n",
      "Loss for batch 19 = 0.5956912636756897\n",
      "Loss for batch 20 = 0.6971508264541626\n",
      "Loss for batch 21 = 0.5322266221046448\n",
      "Loss for batch 22 = 1.3032182455062866\n",
      "Loss for batch 23 = 0.6326891183853149\n",
      "Loss for batch 24 = 0.771217942237854\n",
      "Loss for batch 25 = 0.6494285464286804\n",
      "Loss for batch 26 = 0.7977976202964783\n",
      "Loss for batch 27 = 0.6260635256767273\n",
      "Loss for batch 28 = 0.6954919099807739\n",
      "Loss for batch 29 = 0.7632759213447571\n",
      "Loss for batch 30 = 0.8335643410682678\n",
      "Loss for batch 31 = 1.1414856910705566\n",
      "Loss for batch 32 = 0.5903795957565308\n",
      "Loss for batch 33 = 0.5633091926574707\n",
      "Loss for batch 34 = 0.8753349184989929\n",
      "Loss for batch 35 = 0.8394662737846375\n",
      "Loss for batch 36 = 0.6771983504295349\n",
      "Loss for batch 37 = 0.8288024663925171\n",
      "Loss for batch 38 = 0.7963969707489014\n",
      "Loss for batch 39 = 0.5565599203109741\n",
      "Loss for batch 40 = 0.6166619658470154\n",
      "Loss for batch 41 = 0.6839160323143005\n",
      "Loss for batch 42 = 0.7804286479949951\n",
      "Loss for batch 43 = 0.786806583404541\n",
      "Loss for batch 44 = 0.6911628246307373\n",
      "Loss for batch 45 = 0.7423631548881531\n",
      "Loss for batch 46 = 0.7714200019836426\n",
      "Loss for batch 47 = 0.5924969911575317\n",
      "Loss for batch 48 = 0.48389825224876404\n",
      "Loss for batch 49 = 0.654305636882782\n",
      "Loss for batch 50 = 0.5920047163963318\n",
      "Loss for batch 51 = 0.5924567580223083\n",
      "Loss for batch 52 = 0.6760283708572388\n",
      "Loss for batch 53 = 0.9447264671325684\n",
      "Loss for batch 54 = 0.6935740113258362\n",
      "Loss for batch 55 = 0.7371515035629272\n",
      "Loss for batch 56 = 0.6412789225578308\n",
      "Loss for batch 57 = 0.5274192094802856\n",
      "Loss for batch 58 = 0.6160593032836914\n",
      "Loss for batch 59 = 0.7513452172279358\n",
      "Loss for batch 60 = 0.4374741315841675\n",
      "Loss for batch 61 = 0.7322139739990234\n",
      "Loss for batch 62 = 0.7461473941802979\n",
      "Loss for batch 63 = 0.5371824502944946\n",
      "Loss for batch 64 = 0.735691487789154\n",
      "Loss for batch 65 = 0.6911548972129822\n",
      "Loss for batch 66 = 0.92512047290802\n",
      "Loss for batch 67 = 0.5436030030250549\n",
      "Loss for batch 68 = 0.6634456515312195\n",
      "Loss for batch 69 = 0.5452583432197571\n",
      "Loss for batch 70 = 0.781705379486084\n",
      "Loss for batch 71 = 0.4818052351474762\n",
      "Loss for batch 72 = 0.8837437033653259\n",
      "Loss for batch 73 = 0.6850235462188721\n",
      "Loss for batch 74 = 0.8591306209564209\n",
      "Loss for batch 75 = 0.9325692057609558\n",
      "Loss for batch 76 = 0.8532277345657349\n",
      "Loss for batch 77 = 0.9661572575569153\n",
      "Loss for batch 78 = 0.6266857981681824\n",
      "Loss for batch 79 = 0.6257531642913818\n",
      "Loss for batch 80 = 0.6104401350021362\n",
      "Loss for batch 81 = 0.6741979718208313\n",
      "Loss for batch 82 = 0.5398666262626648\n",
      "Loss for batch 83 = 0.8309197425842285\n",
      "Loss for batch 84 = 0.8102763295173645\n",
      "Loss for batch 85 = 0.9958563446998596\n",
      "Loss for batch 86 = 0.6596292853355408\n",
      "Loss for batch 87 = 0.6900210380554199\n",
      "Loss for batch 88 = 0.7609747648239136\n",
      "Loss for batch 89 = 0.9145364165306091\n",
      "Loss for batch 90 = 0.8094050288200378\n",
      "Loss for batch 91 = 0.7323169112205505\n",
      "Loss for batch 92 = 0.811019241809845\n",
      "Loss for batch 93 = 0.6722766160964966\n",
      "Loss for batch 94 = 0.6836448907852173\n",
      "Loss for batch 95 = 0.8589317202568054\n",
      "Loss for batch 96 = 0.6239769458770752\n",
      "Loss for batch 97 = 0.6348331570625305\n",
      "Loss for batch 98 = 0.7883898615837097\n",
      "Loss for batch 99 = 0.7632008790969849\n",
      "Loss for batch 100 = 0.7068683505058289\n",
      "Loss for batch 101 = 0.6719229221343994\n",
      "Loss for batch 102 = 0.5980105996131897\n",
      "Loss for batch 103 = 0.9343810677528381\n",
      "Loss for batch 104 = 0.5770825743675232\n",
      "Loss for batch 105 = 0.7859572172164917\n",
      "Loss for batch 106 = 0.6886850595474243\n",
      "\n",
      "Training Loss for epoch 18 = 77.25624084472656\n",
      "\n",
      "Current Validation Loss = 12.825881004333496\n",
      "Best Validation Loss = 12.45102310180664\n",
      "Epochs without Improvement = 1\n",
      "Train Accuracy: 69.98%\n",
      "Validation Accuracy: 63.04%\n",
      "\n",
      "Epoch 19\n",
      "----------\n",
      "Loss for batch 0 = 0.6666889786720276\n",
      "Loss for batch 1 = 0.4275010824203491\n",
      "Loss for batch 2 = 0.7711027264595032\n",
      "Loss for batch 3 = 0.627312421798706\n",
      "Loss for batch 4 = 0.6689362525939941\n",
      "Loss for batch 5 = 0.642973780632019\n",
      "Loss for batch 6 = 0.785141110420227\n",
      "Loss for batch 7 = 0.9808927178382874\n",
      "Loss for batch 8 = 0.6260987520217896\n",
      "Loss for batch 9 = 0.5442341566085815\n",
      "Loss for batch 10 = 0.5198289752006531\n",
      "Loss for batch 11 = 1.082655429840088\n",
      "Loss for batch 12 = 0.8013579249382019\n",
      "Loss for batch 13 = 0.8115401268005371\n",
      "Loss for batch 14 = 0.7173977494239807\n",
      "Loss for batch 15 = 0.8040956854820251\n",
      "Loss for batch 16 = 0.7978922724723816\n",
      "Loss for batch 17 = 0.6818791627883911\n",
      "Loss for batch 18 = 0.8150767683982849\n",
      "Loss for batch 19 = 0.5898203253746033\n",
      "Loss for batch 20 = 0.7172842621803284\n",
      "Loss for batch 21 = 0.5528638958930969\n",
      "Loss for batch 22 = 1.1871627569198608\n",
      "Loss for batch 23 = 0.5957582592964172\n",
      "Loss for batch 24 = 0.6391600370407104\n",
      "Loss for batch 25 = 0.5509289503097534\n",
      "Loss for batch 26 = 0.7456912398338318\n",
      "Loss for batch 27 = 0.6765937805175781\n",
      "Loss for batch 28 = 0.6678630709648132\n",
      "Loss for batch 29 = 0.8323373198509216\n",
      "Loss for batch 30 = 0.7418842911720276\n",
      "Loss for batch 31 = 1.0627822875976562\n",
      "Loss for batch 32 = 0.6022245287895203\n",
      "Loss for batch 33 = 0.5668148994445801\n",
      "Loss for batch 34 = 0.7832831740379333\n",
      "Loss for batch 35 = 0.8614563941955566\n",
      "Loss for batch 36 = 0.6117183566093445\n",
      "Loss for batch 37 = 0.7208666801452637\n",
      "Loss for batch 38 = 0.7172942757606506\n",
      "Loss for batch 39 = 0.5577453970909119\n",
      "Loss for batch 40 = 0.6171318292617798\n",
      "Loss for batch 41 = 0.6722726225852966\n",
      "Loss for batch 42 = 0.7055912017822266\n",
      "Loss for batch 43 = 0.8289749622344971\n",
      "Loss for batch 44 = 0.629813015460968\n",
      "Loss for batch 45 = 0.7755003571510315\n",
      "Loss for batch 46 = 0.8433058261871338\n",
      "Loss for batch 47 = 0.5717201828956604\n",
      "Loss for batch 48 = 0.4635128080844879\n",
      "Loss for batch 49 = 0.6842877268791199\n",
      "Loss for batch 50 = 0.7025781869888306\n",
      "Loss for batch 51 = 0.5777668356895447\n",
      "Loss for batch 52 = 0.6400609016418457\n",
      "Loss for batch 53 = 0.8818003535270691\n",
      "Loss for batch 54 = 0.6722182035446167\n",
      "Loss for batch 55 = 0.7574411034584045\n",
      "Loss for batch 56 = 0.6375992298126221\n",
      "Loss for batch 57 = 0.49716079235076904\n",
      "Loss for batch 58 = 0.5811941623687744\n",
      "Loss for batch 59 = 0.785515546798706\n",
      "Loss for batch 60 = 0.4049767851829529\n",
      "Loss for batch 61 = 0.7429746389389038\n",
      "Loss for batch 62 = 0.6881275177001953\n",
      "Loss for batch 63 = 0.46537405252456665\n",
      "Loss for batch 64 = 0.6365625858306885\n",
      "Loss for batch 65 = 0.6892532706260681\n",
      "Loss for batch 66 = 0.8571011424064636\n",
      "Loss for batch 67 = 0.49581754207611084\n",
      "Loss for batch 68 = 0.5406946539878845\n",
      "Loss for batch 69 = 0.5159581303596497\n",
      "Loss for batch 70 = 0.6975065469741821\n",
      "Loss for batch 71 = 0.5138870477676392\n",
      "Loss for batch 72 = 0.8392308950424194\n",
      "Loss for batch 73 = 0.6968293190002441\n",
      "Loss for batch 74 = 0.7987255454063416\n",
      "Loss for batch 75 = 1.0149219036102295\n",
      "Loss for batch 76 = 0.9051955342292786\n",
      "Loss for batch 77 = 0.9048457145690918\n",
      "Loss for batch 78 = 0.6405168175697327\n",
      "Loss for batch 79 = 0.6210099458694458\n",
      "Loss for batch 80 = 0.5820664167404175\n",
      "Loss for batch 81 = 0.64108806848526\n",
      "Loss for batch 82 = 0.574080765247345\n",
      "Loss for batch 83 = 0.774586021900177\n",
      "Loss for batch 84 = 0.7988557815551758\n",
      "Loss for batch 85 = 0.9525582194328308\n",
      "Loss for batch 86 = 0.6374342441558838\n",
      "Loss for batch 87 = 0.6429172158241272\n",
      "Loss for batch 88 = 0.7319368720054626\n",
      "Loss for batch 89 = 0.8140584826469421\n",
      "Loss for batch 90 = 0.7275775671005249\n",
      "Loss for batch 91 = 0.6986780166625977\n",
      "Loss for batch 92 = 0.8009935021400452\n",
      "Loss for batch 93 = 0.6665251851081848\n",
      "Loss for batch 94 = 0.6846387982368469\n",
      "Loss for batch 95 = 0.834052324295044\n",
      "Loss for batch 96 = 0.5875129699707031\n",
      "Loss for batch 97 = 0.5508500933647156\n",
      "Loss for batch 98 = 0.7732905745506287\n",
      "Loss for batch 99 = 0.7512331008911133\n",
      "Loss for batch 100 = 0.6584609746932983\n",
      "Loss for batch 101 = 0.6143710613250732\n",
      "Loss for batch 102 = 0.5528958439826965\n",
      "Loss for batch 103 = 0.985792338848114\n",
      "Loss for batch 104 = 0.48559650778770447\n",
      "Loss for batch 105 = 0.8316262364387512\n",
      "Loss for batch 106 = 0.6496381759643555\n",
      "\n",
      "Training Loss for epoch 19 = 74.95242309570312\n",
      "\n",
      "Current Validation Loss = 12.593426704406738\n",
      "Best Validation Loss = 12.45102310180664\n",
      "Epochs without Improvement = 2\n",
      "Train Accuracy: 71.51%\n",
      "Validation Accuracy: 64.89%\n",
      "\n",
      "Epoch 20\n",
      "----------\n",
      "Loss for batch 0 = 0.6665558218955994\n",
      "Loss for batch 1 = 0.39468997716903687\n",
      "Loss for batch 2 = 0.6761811971664429\n",
      "Loss for batch 3 = 0.555728018283844\n",
      "Loss for batch 4 = 0.5285549163818359\n",
      "Loss for batch 5 = 0.6117256283760071\n",
      "Loss for batch 6 = 0.8008163571357727\n",
      "Loss for batch 7 = 0.9558363556861877\n",
      "Loss for batch 8 = 0.5680351257324219\n",
      "Loss for batch 9 = 0.5984687209129333\n",
      "Loss for batch 10 = 0.5049837827682495\n",
      "Loss for batch 11 = 0.9610944390296936\n",
      "Loss for batch 12 = 0.7931517362594604\n",
      "Loss for batch 13 = 0.763364315032959\n",
      "Loss for batch 14 = 0.5986307263374329\n",
      "Loss for batch 15 = 0.7885769605636597\n",
      "Loss for batch 16 = 0.8265783190727234\n",
      "Loss for batch 17 = 0.6372120380401611\n",
      "Loss for batch 18 = 0.7457204461097717\n",
      "Loss for batch 19 = 0.5662243962287903\n",
      "Loss for batch 20 = 0.6182430982589722\n",
      "Loss for batch 21 = 0.49781376123428345\n",
      "Loss for batch 22 = 1.2759863138198853\n",
      "Loss for batch 23 = 0.5995454788208008\n",
      "Loss for batch 24 = 0.6497778296470642\n",
      "Loss for batch 25 = 0.5249230265617371\n",
      "Loss for batch 26 = 0.7595422267913818\n",
      "Loss for batch 27 = 0.6389904022216797\n",
      "Loss for batch 28 = 0.6312080025672913\n",
      "Loss for batch 29 = 0.8014047145843506\n",
      "Loss for batch 30 = 0.8018044829368591\n",
      "Loss for batch 31 = 1.0126407146453857\n",
      "Loss for batch 32 = 0.5716136693954468\n",
      "Loss for batch 33 = 0.5974021553993225\n",
      "Loss for batch 34 = 0.7920102477073669\n",
      "Loss for batch 35 = 0.8253211379051208\n",
      "Loss for batch 36 = 0.5905391573905945\n",
      "Loss for batch 37 = 0.7646263241767883\n",
      "Loss for batch 38 = 0.7539682388305664\n",
      "Loss for batch 39 = 0.47932013869285583\n",
      "Loss for batch 40 = 0.572772204875946\n",
      "Loss for batch 41 = 0.5878873467445374\n",
      "Loss for batch 42 = 0.660399317741394\n",
      "Loss for batch 43 = 0.7803412079811096\n",
      "Loss for batch 44 = 0.5867903232574463\n",
      "Loss for batch 45 = 0.7187843322753906\n",
      "Loss for batch 46 = 0.8004777431488037\n",
      "Loss for batch 47 = 0.49672943353652954\n",
      "Loss for batch 48 = 0.4233189821243286\n",
      "Loss for batch 49 = 0.6342242360115051\n",
      "Loss for batch 50 = 0.6004941463470459\n",
      "Loss for batch 51 = 0.5376391410827637\n",
      "Loss for batch 52 = 0.5955424904823303\n",
      "Loss for batch 53 = 0.9185158014297485\n",
      "Loss for batch 54 = 0.6436545252799988\n",
      "Loss for batch 55 = 0.713910698890686\n",
      "Loss for batch 56 = 0.6059284210205078\n",
      "Loss for batch 57 = 0.4710291028022766\n",
      "Loss for batch 58 = 0.5625885128974915\n",
      "Loss for batch 59 = 0.7545574307441711\n",
      "Loss for batch 60 = 0.4030042588710785\n",
      "Loss for batch 61 = 0.6841166019439697\n",
      "Loss for batch 62 = 0.6801840662956238\n",
      "Loss for batch 63 = 0.46982553601264954\n",
      "Loss for batch 64 = 0.6040757894515991\n",
      "Loss for batch 65 = 0.6680974960327148\n",
      "Loss for batch 66 = 0.8603068590164185\n",
      "Loss for batch 67 = 0.45611560344696045\n",
      "Loss for batch 68 = 0.5620648860931396\n",
      "Loss for batch 69 = 0.5245344638824463\n",
      "Loss for batch 70 = 0.6420878767967224\n",
      "Loss for batch 71 = 0.5036244988441467\n",
      "Loss for batch 72 = 0.7591264247894287\n",
      "Loss for batch 73 = 0.6373634338378906\n",
      "Loss for batch 74 = 0.8109772205352783\n",
      "Loss for batch 75 = 0.9189644455909729\n",
      "Loss for batch 76 = 0.8794201612472534\n",
      "Loss for batch 77 = 0.9205548167228699\n",
      "Loss for batch 78 = 0.6225650906562805\n",
      "Loss for batch 79 = 0.5881788730621338\n",
      "Loss for batch 80 = 0.5452302694320679\n",
      "Loss for batch 81 = 0.6530317664146423\n",
      "Loss for batch 82 = 0.5428684949874878\n",
      "Loss for batch 83 = 0.7740315198898315\n",
      "Loss for batch 84 = 0.7556244134902954\n",
      "Loss for batch 85 = 0.9512506127357483\n",
      "Loss for batch 86 = 0.6136916875839233\n",
      "Loss for batch 87 = 0.600172221660614\n",
      "Loss for batch 88 = 0.6949033141136169\n",
      "Loss for batch 89 = 0.8241145014762878\n",
      "Loss for batch 90 = 0.6972547173500061\n",
      "Loss for batch 91 = 0.6568915247917175\n",
      "Loss for batch 92 = 0.7626373767852783\n",
      "Loss for batch 93 = 0.6379973888397217\n",
      "Loss for batch 94 = 0.6787806749343872\n",
      "Loss for batch 95 = 0.8102250099182129\n",
      "Loss for batch 96 = 0.5643289089202881\n",
      "Loss for batch 97 = 0.5399248600006104\n",
      "Loss for batch 98 = 0.737758994102478\n",
      "Loss for batch 99 = 0.7253363132476807\n",
      "Loss for batch 100 = 0.6617411971092224\n",
      "Loss for batch 101 = 0.617078959941864\n",
      "Loss for batch 102 = 0.5327349901199341\n",
      "Loss for batch 103 = 0.9301996231079102\n",
      "Loss for batch 104 = 0.4696466028690338\n",
      "Loss for batch 105 = 0.7920582294464111\n",
      "Loss for batch 106 = 0.6549687385559082\n",
      "\n",
      "Training Loss for epoch 20 = 72.01408386230469\n",
      "\n",
      "Current Validation Loss = 12.740901947021484\n",
      "Best Validation Loss = 12.45102310180664\n",
      "Epochs without Improvement = 3\n",
      "Train Accuracy: 72.80%\n",
      "Validation Accuracy: 65.50%\n",
      "\n",
      "Epoch 21\n",
      "----------\n",
      "Loss for batch 0 = 0.6592514514923096\n",
      "Loss for batch 1 = 0.3871302008628845\n",
      "Loss for batch 2 = 0.531252384185791\n",
      "Loss for batch 3 = 0.5191878080368042\n",
      "Loss for batch 4 = 0.5121626257896423\n",
      "Loss for batch 5 = 0.5820326805114746\n",
      "Loss for batch 6 = 0.7152005434036255\n",
      "Loss for batch 7 = 0.9476763010025024\n",
      "Loss for batch 8 = 0.49399667978286743\n",
      "Loss for batch 9 = 0.5624216198921204\n",
      "Loss for batch 10 = 0.42401251196861267\n",
      "Loss for batch 11 = 1.0000004768371582\n",
      "Loss for batch 12 = 0.7633005380630493\n",
      "Loss for batch 13 = 0.7936697602272034\n",
      "Loss for batch 14 = 0.5534870624542236\n",
      "Loss for batch 15 = 0.7901740670204163\n",
      "Loss for batch 16 = 0.7700441479682922\n",
      "Loss for batch 17 = 0.607155442237854\n",
      "Loss for batch 18 = 0.7185236215591431\n",
      "Loss for batch 19 = 0.5625684261322021\n",
      "Loss for batch 20 = 0.625972330570221\n",
      "Loss for batch 21 = 0.5012181401252747\n",
      "Loss for batch 22 = 1.2026762962341309\n",
      "Loss for batch 23 = 0.579627513885498\n",
      "Loss for batch 24 = 0.6507217884063721\n",
      "Loss for batch 25 = 0.49145182967185974\n",
      "Loss for batch 26 = 0.7659210562705994\n",
      "Loss for batch 27 = 0.6499758958816528\n",
      "Loss for batch 28 = 0.6018853783607483\n",
      "Loss for batch 29 = 0.7446590662002563\n",
      "Loss for batch 30 = 0.7413102388381958\n",
      "Loss for batch 31 = 1.030930757522583\n",
      "Loss for batch 32 = 0.5781521797180176\n",
      "Loss for batch 33 = 0.606804609298706\n",
      "Loss for batch 34 = 0.7844393253326416\n",
      "Loss for batch 35 = 0.8117647171020508\n",
      "Loss for batch 36 = 0.5934648513793945\n",
      "Loss for batch 37 = 0.7376661896705627\n",
      "Loss for batch 38 = 0.6219567656517029\n",
      "Loss for batch 39 = 0.5155609846115112\n",
      "Loss for batch 40 = 0.5438738465309143\n",
      "Loss for batch 41 = 0.5423123240470886\n",
      "Loss for batch 42 = 0.7112990021705627\n",
      "Loss for batch 43 = 0.7303979992866516\n",
      "Loss for batch 44 = 0.6003816723823547\n",
      "Loss for batch 45 = 0.6725175380706787\n",
      "Loss for batch 46 = 0.7769888043403625\n",
      "Loss for batch 47 = 0.4535747170448303\n",
      "Loss for batch 48 = 0.348990261554718\n",
      "Loss for batch 49 = 0.5558285713195801\n",
      "Loss for batch 50 = 0.5296117663383484\n",
      "Loss for batch 51 = 0.5499730706214905\n",
      "Loss for batch 52 = 0.6762566566467285\n",
      "Loss for batch 53 = 0.929071843624115\n",
      "Loss for batch 54 = 0.6331694722175598\n",
      "Loss for batch 55 = 0.642887532711029\n",
      "Loss for batch 56 = 0.5580016374588013\n",
      "Loss for batch 57 = 0.481623113155365\n",
      "Loss for batch 58 = 0.5415024161338806\n",
      "Loss for batch 59 = 0.8143447637557983\n",
      "Loss for batch 60 = 0.33501186966896057\n",
      "Loss for batch 61 = 0.6735802292823792\n",
      "Loss for batch 62 = 0.6658709645271301\n",
      "Loss for batch 63 = 0.4386277496814728\n",
      "Loss for batch 64 = 0.6189450621604919\n",
      "Loss for batch 65 = 0.5825693607330322\n",
      "Loss for batch 66 = 0.8613552451133728\n",
      "Loss for batch 67 = 0.45221206545829773\n",
      "Loss for batch 68 = 0.5540859699249268\n",
      "Loss for batch 69 = 0.4795151352882385\n",
      "Loss for batch 70 = 0.6198930144309998\n",
      "Loss for batch 71 = 0.5810273885726929\n",
      "Loss for batch 72 = 0.6955791711807251\n",
      "Loss for batch 73 = 0.6625373959541321\n",
      "Loss for batch 74 = 0.783184289932251\n",
      "Loss for batch 75 = 0.9449065923690796\n",
      "Loss for batch 76 = 0.8123090863227844\n",
      "Loss for batch 77 = 0.8626677989959717\n",
      "Loss for batch 78 = 0.6150290966033936\n",
      "Loss for batch 79 = 0.5739656090736389\n",
      "Loss for batch 80 = 0.5641977787017822\n",
      "Loss for batch 81 = 0.6088530421257019\n",
      "Loss for batch 82 = 0.5000805258750916\n",
      "Loss for batch 83 = 0.7222686409950256\n",
      "Loss for batch 84 = 0.7489944696426392\n",
      "Loss for batch 85 = 0.9307000637054443\n",
      "Loss for batch 86 = 0.6126742362976074\n",
      "Loss for batch 87 = 0.573944628238678\n",
      "Loss for batch 88 = 0.6462599635124207\n",
      "Loss for batch 89 = 0.8059741854667664\n",
      "Loss for batch 90 = 0.7508018612861633\n",
      "Loss for batch 91 = 0.6300522685050964\n",
      "Loss for batch 92 = 0.7504310607910156\n",
      "Loss for batch 93 = 0.6037254929542542\n",
      "Loss for batch 94 = 0.6996110677719116\n",
      "Loss for batch 95 = 0.7580581903457642\n",
      "Loss for batch 96 = 0.5331863164901733\n",
      "Loss for batch 97 = 0.5484728813171387\n",
      "Loss for batch 98 = 0.6699730157852173\n",
      "Loss for batch 99 = 0.7172969579696655\n",
      "Loss for batch 100 = 0.6241046190261841\n",
      "Loss for batch 101 = 0.5914108753204346\n",
      "Loss for batch 102 = 0.5332678556442261\n",
      "Loss for batch 103 = 0.9266700148582458\n",
      "Loss for batch 104 = 0.40613359212875366\n",
      "Loss for batch 105 = 0.7524058222770691\n",
      "Loss for batch 106 = 0.6064053177833557\n",
      "\n",
      "Training Loss for epoch 21 = 69.65084838867188\n",
      "\n",
      "Current Validation Loss = 12.922507286071777\n",
      "Best Validation Loss = 12.45102310180664\n",
      "Epochs without Improvement = 4\n",
      "Train Accuracy: 72.45%\n",
      "Validation Accuracy: 65.09%\n",
      "\n",
      "Epoch 22\n",
      "----------\n",
      "Loss for batch 0 = 0.5811530947685242\n",
      "Loss for batch 1 = 0.3743807375431061\n",
      "Loss for batch 2 = 0.5326215028762817\n",
      "Loss for batch 3 = 0.4927913546562195\n",
      "Loss for batch 4 = 0.4754394292831421\n",
      "Loss for batch 5 = 0.5467783212661743\n",
      "Loss for batch 6 = 0.7270419597625732\n",
      "Loss for batch 7 = 0.8410680890083313\n",
      "Loss for batch 8 = 0.4931323230266571\n",
      "Loss for batch 9 = 0.5763540863990784\n",
      "Loss for batch 10 = 0.3660741150379181\n",
      "Loss for batch 11 = 0.8908734917640686\n",
      "Loss for batch 12 = 0.834534764289856\n",
      "Loss for batch 13 = 0.794867753982544\n",
      "Loss for batch 14 = 0.5501826405525208\n",
      "Loss for batch 15 = 0.8232675790786743\n",
      "Loss for batch 16 = 0.7918712496757507\n",
      "Loss for batch 17 = 0.5941312909126282\n",
      "Loss for batch 18 = 0.6789838671684265\n",
      "Loss for batch 19 = 0.5418787598609924\n",
      "Loss for batch 20 = 0.6222332119941711\n",
      "Loss for batch 21 = 0.6033125519752502\n",
      "Loss for batch 22 = 1.1717867851257324\n",
      "Loss for batch 23 = 0.5951808094978333\n",
      "Loss for batch 24 = 0.6571866869926453\n",
      "Loss for batch 25 = 0.5414270758628845\n",
      "Loss for batch 26 = 0.7448251843452454\n",
      "Loss for batch 27 = 0.6348319053649902\n",
      "Loss for batch 28 = 0.5706542134284973\n",
      "Loss for batch 29 = 0.8098753690719604\n",
      "Loss for batch 30 = 0.6610039472579956\n",
      "Loss for batch 31 = 0.9740159511566162\n",
      "Loss for batch 32 = 0.5969268679618835\n",
      "Loss for batch 33 = 0.560555636882782\n",
      "Loss for batch 34 = 0.7021477818489075\n",
      "Loss for batch 35 = 0.7597952485084534\n",
      "Loss for batch 36 = 0.6134325861930847\n",
      "Loss for batch 37 = 0.7210041284561157\n",
      "Loss for batch 38 = 0.6357461810112\n",
      "Loss for batch 39 = 0.5598366260528564\n",
      "Loss for batch 40 = 0.5438395738601685\n",
      "Loss for batch 41 = 0.5368044376373291\n",
      "Loss for batch 42 = 0.6064299941062927\n",
      "Loss for batch 43 = 0.8267333507537842\n",
      "Loss for batch 44 = 0.6049200296401978\n",
      "Loss for batch 45 = 0.7011072635650635\n",
      "Loss for batch 46 = 0.748759925365448\n",
      "Loss for batch 47 = 0.5498262047767639\n",
      "Loss for batch 48 = 0.36781543493270874\n",
      "Loss for batch 49 = 0.6272323727607727\n",
      "Loss for batch 50 = 0.5556113123893738\n",
      "Loss for batch 51 = 0.5107690095901489\n",
      "Loss for batch 52 = 0.7915982007980347\n",
      "Loss for batch 53 = 0.874572217464447\n",
      "Loss for batch 54 = 0.6659736633300781\n",
      "Loss for batch 55 = 0.6448686122894287\n",
      "Loss for batch 56 = 0.6410501003265381\n",
      "Loss for batch 57 = 0.42808881402015686\n",
      "Loss for batch 58 = 0.5037515163421631\n",
      "Loss for batch 59 = 0.7746303081512451\n",
      "Loss for batch 60 = 0.43179094791412354\n",
      "Loss for batch 61 = 0.7176719903945923\n",
      "Loss for batch 62 = 0.6470842361450195\n",
      "Loss for batch 63 = 0.4255363345146179\n",
      "Loss for batch 64 = 0.6014196276664734\n",
      "Loss for batch 65 = 0.5746771693229675\n",
      "Loss for batch 66 = 0.769018828868866\n",
      "Loss for batch 67 = 0.43194901943206787\n",
      "Loss for batch 68 = 0.5622566342353821\n",
      "Loss for batch 69 = 0.4605247676372528\n",
      "Loss for batch 70 = 0.635941743850708\n",
      "Loss for batch 71 = 0.4970565140247345\n",
      "Loss for batch 72 = 0.6329714059829712\n",
      "Loss for batch 73 = 0.544413149356842\n",
      "Loss for batch 74 = 0.7679061889648438\n",
      "Loss for batch 75 = 0.9865137934684753\n",
      "Loss for batch 76 = 0.8395868539810181\n",
      "Loss for batch 77 = 0.8665840029716492\n",
      "Loss for batch 78 = 0.6389644145965576\n",
      "Loss for batch 79 = 0.6072431802749634\n",
      "Loss for batch 80 = 0.564612090587616\n",
      "Loss for batch 81 = 0.5702789425849915\n",
      "Loss for batch 82 = 0.47039559483528137\n",
      "Loss for batch 83 = 0.675556480884552\n",
      "Loss for batch 84 = 0.73576420545578\n",
      "Loss for batch 85 = 0.9398529529571533\n",
      "Loss for batch 86 = 0.5958740711212158\n",
      "Loss for batch 87 = 0.6353632211685181\n",
      "Loss for batch 88 = 0.6453523635864258\n",
      "Loss for batch 89 = 0.7707175612449646\n",
      "Loss for batch 90 = 0.7147300243377686\n",
      "Loss for batch 91 = 0.6286328434944153\n",
      "Loss for batch 92 = 0.7475234866142273\n",
      "Loss for batch 93 = 0.5981835722923279\n",
      "Loss for batch 94 = 0.6793403625488281\n",
      "Loss for batch 95 = 0.8373284935951233\n",
      "Loss for batch 96 = 0.5257093906402588\n",
      "Loss for batch 97 = 0.5416907668113708\n",
      "Loss for batch 98 = 0.6399195790290833\n",
      "Loss for batch 99 = 0.663284182548523\n",
      "Loss for batch 100 = 0.5989997982978821\n",
      "Loss for batch 101 = 0.6086472868919373\n",
      "Loss for batch 102 = 0.4975338578224182\n",
      "Loss for batch 103 = 0.8219240307807922\n",
      "Loss for batch 104 = 0.48113900423049927\n",
      "Loss for batch 105 = 0.7461534142494202\n",
      "Loss for batch 106 = 0.7343617677688599\n",
      "\n",
      "Training Loss for epoch 22 = 69.05557250976562\n",
      "\n",
      "Current Validation Loss = 12.787240982055664\n",
      "Best Validation Loss = 12.45102310180664\n",
      "Epochs without Improvement = 5\n",
      "Train Accuracy: 72.65%\n",
      "Validation Accuracy: 65.09%\n",
      "\n",
      "Epoch 23\n",
      "----------\n",
      "Loss for batch 0 = 0.6403039693832397\n",
      "Loss for batch 1 = 0.3678230345249176\n",
      "Loss for batch 2 = 0.5691590309143066\n",
      "Loss for batch 3 = 0.425757497549057\n",
      "Loss for batch 4 = 0.4450953006744385\n",
      "Loss for batch 5 = 0.628954291343689\n",
      "Loss for batch 6 = 0.7061527371406555\n",
      "Loss for batch 7 = 0.8932614326477051\n",
      "Loss for batch 8 = 0.5788789987564087\n",
      "Loss for batch 9 = 0.48003777861595154\n",
      "Loss for batch 10 = 0.3377804756164551\n",
      "Loss for batch 11 = 0.9225229620933533\n",
      "Loss for batch 12 = 0.7774526476860046\n",
      "Loss for batch 13 = 0.6190207004547119\n",
      "Loss for batch 14 = 0.4881269335746765\n",
      "Loss for batch 15 = 0.8151283264160156\n",
      "Loss for batch 16 = 0.7832616567611694\n",
      "Loss for batch 17 = 0.6506513953208923\n",
      "Loss for batch 18 = 0.6516188383102417\n",
      "Loss for batch 19 = 0.4894886910915375\n",
      "Loss for batch 20 = 0.5561110973358154\n",
      "Loss for batch 21 = 0.48937493562698364\n",
      "Loss for batch 22 = 1.1323362588882446\n",
      "Loss for batch 23 = 0.5309478640556335\n",
      "Loss for batch 24 = 0.6821868419647217\n",
      "Loss for batch 25 = 0.4701487421989441\n",
      "Loss for batch 26 = 0.7106836438179016\n",
      "Loss for batch 27 = 0.5340436697006226\n",
      "Loss for batch 28 = 0.5784462094306946\n",
      "Loss for batch 29 = 0.7328968644142151\n",
      "Loss for batch 30 = 0.6712251305580139\n",
      "Loss for batch 31 = 0.9977059364318848\n",
      "Loss for batch 32 = 0.5450398325920105\n",
      "Loss for batch 33 = 0.5938190221786499\n",
      "Loss for batch 34 = 0.722771167755127\n",
      "Loss for batch 35 = 0.7889465093612671\n",
      "Loss for batch 36 = 0.4991086423397064\n",
      "Loss for batch 37 = 0.6648385524749756\n",
      "Loss for batch 38 = 0.643036961555481\n",
      "Loss for batch 39 = 0.4094353914260864\n",
      "Loss for batch 40 = 0.5080060958862305\n",
      "Loss for batch 41 = 0.5436131954193115\n",
      "Loss for batch 42 = 0.5574212074279785\n",
      "Loss for batch 43 = 0.7577944397926331\n",
      "Loss for batch 44 = 0.6146413683891296\n",
      "Loss for batch 45 = 0.6505259275436401\n",
      "Loss for batch 46 = 0.7488055229187012\n",
      "Loss for batch 47 = 0.46582168340682983\n",
      "Loss for batch 48 = 0.3390754163265228\n",
      "Loss for batch 49 = 0.6160940527915955\n",
      "Loss for batch 50 = 0.5225023627281189\n",
      "Loss for batch 51 = 0.5450092554092407\n",
      "Loss for batch 52 = 0.5744196176528931\n",
      "Loss for batch 53 = 0.7744265794754028\n",
      "Loss for batch 54 = 0.6150938272476196\n",
      "Loss for batch 55 = 0.5888208746910095\n",
      "Loss for batch 56 = 0.5762189030647278\n",
      "Loss for batch 57 = 0.4135299324989319\n",
      "Loss for batch 58 = 0.49261757731437683\n",
      "Loss for batch 59 = 0.7215779423713684\n",
      "Loss for batch 60 = 0.29810774326324463\n",
      "Loss for batch 61 = 0.6935467720031738\n",
      "Loss for batch 62 = 0.6412621736526489\n",
      "Loss for batch 63 = 0.42725327610969543\n",
      "Loss for batch 64 = 0.5656207203865051\n",
      "Loss for batch 65 = 0.5532207489013672\n",
      "Loss for batch 66 = 0.7932187914848328\n",
      "Loss for batch 67 = 0.40188533067703247\n",
      "Loss for batch 68 = 0.5503473877906799\n",
      "Loss for batch 69 = 0.4157666265964508\n",
      "Loss for batch 70 = 0.5689670443534851\n",
      "Loss for batch 71 = 0.4484891891479492\n",
      "Loss for batch 72 = 0.6336776614189148\n",
      "Loss for batch 73 = 0.5229532122612\n",
      "Loss for batch 74 = 0.7521318793296814\n",
      "Loss for batch 75 = 0.8537843227386475\n",
      "Loss for batch 76 = 0.809897780418396\n",
      "Loss for batch 77 = 0.8626145720481873\n",
      "Loss for batch 78 = 0.6300097107887268\n",
      "Loss for batch 79 = 0.5353711843490601\n",
      "Loss for batch 80 = 0.5315107107162476\n",
      "Loss for batch 81 = 0.5893999934196472\n",
      "Loss for batch 82 = 0.4725376069545746\n",
      "Loss for batch 83 = 0.5977087020874023\n",
      "Loss for batch 84 = 0.6825199127197266\n",
      "Loss for batch 85 = 0.8911760449409485\n",
      "Loss for batch 86 = 0.535485029220581\n",
      "Loss for batch 87 = 0.5411193370819092\n",
      "Loss for batch 88 = 0.5567536354064941\n",
      "Loss for batch 89 = 0.7317618727684021\n",
      "Loss for batch 90 = 0.6024156808853149\n",
      "Loss for batch 91 = 0.5461174845695496\n",
      "Loss for batch 92 = 0.7055312395095825\n",
      "Loss for batch 93 = 0.5769103765487671\n",
      "Loss for batch 94 = 0.6574965715408325\n",
      "Loss for batch 95 = 0.7949886322021484\n",
      "Loss for batch 96 = 0.5510934591293335\n",
      "Loss for batch 97 = 0.5166480541229248\n",
      "Loss for batch 98 = 0.6494723558425903\n",
      "Loss for batch 99 = 0.7055636048316956\n",
      "Loss for batch 100 = 0.5855501890182495\n",
      "Loss for batch 101 = 0.5708796381950378\n",
      "Loss for batch 102 = 0.5437542200088501\n",
      "Loss for batch 103 = 0.8163142204284668\n",
      "Loss for batch 104 = 0.4615958631038666\n",
      "Loss for batch 105 = 0.7266441583633423\n",
      "Loss for batch 106 = 0.5714205503463745\n",
      "\n",
      "Training Loss for epoch 23 = 65.52008819580078\n",
      "\n",
      "Current Validation Loss = 12.92271900177002\n",
      "Best Validation Loss = 12.45102310180664\n",
      "Epochs without Improvement = 6\n",
      "Train Accuracy: 73.86%\n",
      "Validation Accuracy: 65.09%\n",
      "\n",
      "Epoch 24\n",
      "----------\n",
      "Loss for batch 0 = 0.5512705445289612\n",
      "Loss for batch 1 = 0.3942609131336212\n",
      "Loss for batch 2 = 0.5364160537719727\n",
      "Loss for batch 3 = 0.40908128023147583\n",
      "Loss for batch 4 = 0.4124259948730469\n",
      "Loss for batch 5 = 0.506592869758606\n",
      "Loss for batch 6 = 0.6498324871063232\n",
      "Loss for batch 7 = 0.7543371915817261\n",
      "Loss for batch 8 = 0.47510474920272827\n",
      "Loss for batch 9 = 0.45017799735069275\n",
      "Loss for batch 10 = 0.3003951609134674\n",
      "Loss for batch 11 = 0.8657230138778687\n",
      "Loss for batch 12 = 0.8091357350349426\n",
      "Loss for batch 13 = 0.6971656680107117\n",
      "Loss for batch 14 = 0.48127326369285583\n",
      "Loss for batch 15 = 0.7911952137947083\n",
      "Loss for batch 16 = 0.7717183232307434\n",
      "Loss for batch 17 = 0.5332778096199036\n",
      "Loss for batch 18 = 0.6782147884368896\n",
      "Loss for batch 19 = 0.4865291118621826\n",
      "Loss for batch 20 = 0.5304677486419678\n",
      "Loss for batch 21 = 0.37321737408638\n",
      "Loss for batch 22 = 1.1382708549499512\n",
      "Loss for batch 23 = 0.5131109952926636\n",
      "Loss for batch 24 = 0.6299842596054077\n",
      "Loss for batch 25 = 0.44244158267974854\n",
      "Loss for batch 26 = 0.5713788270950317\n",
      "Loss for batch 27 = 0.5931317806243896\n",
      "Loss for batch 28 = 0.4896034896373749\n",
      "Loss for batch 29 = 0.6751017570495605\n",
      "Loss for batch 30 = 0.6054607033729553\n",
      "Loss for batch 31 = 1.0040420293807983\n",
      "Loss for batch 32 = 0.5524824857711792\n",
      "Loss for batch 33 = 0.5763918161392212\n",
      "Loss for batch 34 = 0.7261036038398743\n",
      "Loss for batch 35 = 0.7331496477127075\n",
      "Loss for batch 36 = 0.49475544691085815\n",
      "Loss for batch 37 = 0.6375546455383301\n",
      "Loss for batch 38 = 0.5950033068656921\n",
      "Loss for batch 39 = 0.4298630654811859\n",
      "Loss for batch 40 = 0.5138810873031616\n",
      "Loss for batch 41 = 0.557526707649231\n",
      "Loss for batch 42 = 0.620467483997345\n",
      "Loss for batch 43 = 0.6976857781410217\n",
      "Loss for batch 44 = 0.5886828303337097\n",
      "Loss for batch 45 = 0.6783868074417114\n",
      "Loss for batch 46 = 0.7470860481262207\n",
      "Loss for batch 47 = 0.43603387475013733\n",
      "Loss for batch 48 = 0.4331693947315216\n",
      "Loss for batch 49 = 0.612549364566803\n",
      "Loss for batch 50 = 0.5052348971366882\n",
      "Loss for batch 51 = 0.4910094439983368\n",
      "Loss for batch 52 = 0.6749723553657532\n",
      "Loss for batch 53 = 0.903468132019043\n",
      "Loss for batch 54 = 0.6365689039230347\n",
      "Loss for batch 55 = 0.6037038564682007\n",
      "Loss for batch 56 = 0.7000218033790588\n",
      "Loss for batch 57 = 0.484924852848053\n",
      "Loss for batch 58 = 0.44561392068862915\n",
      "Loss for batch 59 = 0.8980466723442078\n",
      "Loss for batch 60 = 0.37244150042533875\n",
      "Loss for batch 61 = 0.7834764719009399\n",
      "Loss for batch 62 = 0.6657339334487915\n",
      "Loss for batch 63 = 0.41723260283470154\n",
      "Loss for batch 64 = 0.6390640139579773\n",
      "Loss for batch 65 = 0.5900724530220032\n",
      "Loss for batch 66 = 0.7893539071083069\n",
      "Loss for batch 67 = 0.46002501249313354\n",
      "Loss for batch 68 = 0.5405999422073364\n",
      "Loss for batch 69 = 0.461005836725235\n",
      "Loss for batch 70 = 0.6175985932350159\n",
      "Loss for batch 71 = 0.47564205527305603\n",
      "Loss for batch 72 = 0.6083192229270935\n",
      "Loss for batch 73 = 0.5233644247055054\n",
      "Loss for batch 74 = 0.673159122467041\n",
      "Loss for batch 75 = 0.8565872311592102\n",
      "Loss for batch 76 = 0.8340945243835449\n",
      "Loss for batch 77 = 0.8291701078414917\n",
      "Loss for batch 78 = 0.5918199419975281\n",
      "Loss for batch 79 = 0.5007728934288025\n",
      "Loss for batch 80 = 0.5306511521339417\n",
      "Loss for batch 81 = 0.5570490956306458\n",
      "Loss for batch 82 = 0.47705498337745667\n",
      "Loss for batch 83 = 0.6226222515106201\n",
      "Loss for batch 84 = 0.6181436777114868\n",
      "Loss for batch 85 = 0.9348273873329163\n",
      "Loss for batch 86 = 0.6089423298835754\n",
      "Loss for batch 87 = 0.6224374175071716\n",
      "Loss for batch 88 = 0.5378558039665222\n",
      "Loss for batch 89 = 0.7711092233657837\n",
      "Loss for batch 90 = 0.561501145362854\n",
      "Loss for batch 91 = 0.5468361377716064\n",
      "Loss for batch 92 = 0.6217767000198364\n",
      "Loss for batch 93 = 0.6212683320045471\n",
      "Loss for batch 94 = 0.5923510789871216\n",
      "Loss for batch 95 = 0.8715540766716003\n",
      "Loss for batch 96 = 0.554361879825592\n",
      "Loss for batch 97 = 0.4916507601737976\n",
      "Loss for batch 98 = 0.6195407509803772\n",
      "Loss for batch 99 = 0.6791931390762329\n",
      "Loss for batch 100 = 0.5648786425590515\n",
      "Loss for batch 101 = 0.6444962024688721\n",
      "Loss for batch 102 = 0.5017572641372681\n",
      "Loss for batch 103 = 0.7767030596733093\n",
      "Loss for batch 104 = 0.38260290026664734\n",
      "Loss for batch 105 = 0.6942464113235474\n",
      "Loss for batch 106 = 0.6547797322273254\n",
      "\n",
      "Training Loss for epoch 24 = 64.98441314697266\n",
      "\n",
      "Current Validation Loss = 12.672895431518555\n",
      "Best Validation Loss = 12.45102310180664\n",
      "Epochs without Improvement = 7\n",
      "Train Accuracy: 75.32%\n",
      "Validation Accuracy: 65.30%\n",
      "\n",
      "Epoch 25\n",
      "----------\n",
      "Loss for batch 0 = 0.6047667860984802\n",
      "Loss for batch 1 = 0.4816895127296448\n",
      "Loss for batch 2 = 0.4773360788822174\n",
      "Loss for batch 3 = 0.4305545687675476\n",
      "Loss for batch 4 = 0.4555564522743225\n",
      "Loss for batch 5 = 0.6274290680885315\n",
      "Loss for batch 6 = 0.7089176177978516\n",
      "Loss for batch 7 = 0.7561704516410828\n",
      "Loss for batch 8 = 0.6351293921470642\n",
      "Loss for batch 9 = 0.43570375442504883\n",
      "Loss for batch 10 = 0.45634037256240845\n",
      "Loss for batch 11 = 0.7961951494216919\n",
      "Loss for batch 12 = 0.8519616723060608\n",
      "Loss for batch 13 = 0.745324432849884\n",
      "Loss for batch 14 = 0.4594249129295349\n",
      "Loss for batch 15 = 0.7242671251296997\n",
      "Loss for batch 16 = 0.6873860955238342\n",
      "Loss for batch 17 = 0.5063338279724121\n",
      "Loss for batch 18 = 0.6034384369850159\n",
      "Loss for batch 19 = 0.48235735297203064\n",
      "Loss for batch 20 = 0.6143225431442261\n",
      "Loss for batch 21 = 0.5386967658996582\n",
      "Loss for batch 22 = 0.9666268825531006\n",
      "Loss for batch 23 = 0.46597912907600403\n",
      "Loss for batch 24 = 0.6647223234176636\n",
      "Loss for batch 25 = 0.3858213424682617\n",
      "Loss for batch 26 = 0.5881044864654541\n",
      "Loss for batch 27 = 0.4841603636741638\n",
      "Loss for batch 28 = 0.4291227161884308\n",
      "Loss for batch 29 = 0.7986699342727661\n",
      "Loss for batch 30 = 0.5617079138755798\n",
      "Loss for batch 31 = 0.8989483714103699\n",
      "Loss for batch 32 = 0.5233756899833679\n",
      "Loss for batch 33 = 0.5804726481437683\n",
      "Loss for batch 34 = 0.5986410975456238\n",
      "Loss for batch 35 = 0.7308876514434814\n",
      "Loss for batch 36 = 0.43696147203445435\n",
      "Loss for batch 37 = 0.7371814250946045\n",
      "Loss for batch 38 = 0.5836125612258911\n",
      "Loss for batch 39 = 0.3817683160305023\n",
      "Loss for batch 40 = 0.47181186079978943\n",
      "Loss for batch 41 = 0.5316166281700134\n",
      "Loss for batch 42 = 0.5483318567276001\n",
      "Loss for batch 43 = 0.6317693591117859\n",
      "Loss for batch 44 = 0.5625275373458862\n",
      "Loss for batch 45 = 0.5833311676979065\n",
      "Loss for batch 46 = 0.7924428582191467\n",
      "Loss for batch 47 = 0.34798887372016907\n",
      "Loss for batch 48 = 0.28043463826179504\n",
      "Loss for batch 49 = 0.6693232655525208\n",
      "Loss for batch 50 = 0.5078972578048706\n",
      "Loss for batch 51 = 0.544230580329895\n",
      "Loss for batch 52 = 0.6685999631881714\n",
      "Loss for batch 53 = 0.7417477965354919\n",
      "Loss for batch 54 = 0.5973857641220093\n",
      "Loss for batch 55 = 0.5805870890617371\n",
      "Loss for batch 56 = 0.5543346405029297\n",
      "Loss for batch 57 = 0.41971415281295776\n",
      "Loss for batch 58 = 0.3941112458705902\n",
      "Loss for batch 59 = 0.7170847058296204\n",
      "Loss for batch 60 = 0.31696203351020813\n",
      "Loss for batch 61 = 0.7012249231338501\n",
      "Loss for batch 62 = 0.6161066889762878\n",
      "Loss for batch 63 = 0.451867014169693\n",
      "Loss for batch 64 = 0.6148741841316223\n",
      "Loss for batch 65 = 0.5506588220596313\n",
      "Loss for batch 66 = 0.8723955154418945\n",
      "Loss for batch 67 = 0.4430278539657593\n",
      "Loss for batch 68 = 0.5654042959213257\n",
      "Loss for batch 69 = 0.39303624629974365\n",
      "Loss for batch 70 = 0.559665858745575\n",
      "Loss for batch 71 = 0.502611517906189\n",
      "Loss for batch 72 = 0.5630791783332825\n",
      "Loss for batch 73 = 0.5186862945556641\n",
      "Loss for batch 74 = 0.7118847370147705\n",
      "Loss for batch 75 = 0.8260877728462219\n",
      "Loss for batch 76 = 0.8258445858955383\n",
      "Loss for batch 77 = 0.8528835773468018\n",
      "Loss for batch 78 = 0.5747718214988708\n",
      "Loss for batch 79 = 0.5001363754272461\n",
      "Loss for batch 80 = 0.5942219495773315\n",
      "Loss for batch 81 = 0.599256157875061\n",
      "Loss for batch 82 = 0.4446059763431549\n",
      "Loss for batch 83 = 0.6233813762664795\n",
      "Loss for batch 84 = 0.7083823680877686\n",
      "Loss for batch 85 = 0.8766930103302002\n",
      "Loss for batch 86 = 0.5787721276283264\n",
      "Loss for batch 87 = 0.5663413405418396\n",
      "Loss for batch 88 = 0.5587647557258606\n",
      "Loss for batch 89 = 0.7239577174186707\n",
      "Loss for batch 90 = 0.545336127281189\n",
      "Loss for batch 91 = 0.6305713653564453\n",
      "Loss for batch 92 = 0.6776571273803711\n",
      "Loss for batch 93 = 0.5747849941253662\n",
      "Loss for batch 94 = 0.6219455003738403\n",
      "Loss for batch 95 = 0.8173301219940186\n",
      "Loss for batch 96 = 0.54781174659729\n",
      "Loss for batch 97 = 0.49520498514175415\n",
      "Loss for batch 98 = 0.7204265594482422\n",
      "Loss for batch 99 = 0.7188405394554138\n",
      "Loss for batch 100 = 0.5593537092208862\n",
      "Loss for batch 101 = 0.5862146019935608\n",
      "Loss for batch 102 = 0.5322120189666748\n",
      "Loss for batch 103 = 0.8302717208862305\n",
      "Loss for batch 104 = 0.31895455718040466\n",
      "Loss for batch 105 = 0.6903048753738403\n",
      "Loss for batch 106 = 0.536500871181488\n",
      "\n",
      "Training Loss for epoch 25 = 63.68065643310547\n",
      "\n",
      "Current Validation Loss = 13.157685279846191\n",
      "Best Validation Loss = 12.45102310180664\n",
      "Epochs without Improvement = 8\n",
      "Train Accuracy: 77.11%\n",
      "Validation Accuracy: 65.30%\n",
      "\n",
      "Epoch 26\n",
      "----------\n",
      "Loss for batch 0 = 0.633922278881073\n",
      "Loss for batch 1 = 0.2842097282409668\n",
      "Loss for batch 2 = 0.43145090341567993\n",
      "Loss for batch 3 = 0.46917158365249634\n",
      "Loss for batch 4 = 0.4725538194179535\n",
      "Loss for batch 5 = 0.5775749087333679\n",
      "Loss for batch 6 = 0.5178393721580505\n",
      "Loss for batch 7 = 0.6920617818832397\n",
      "Loss for batch 8 = 0.3940057158470154\n",
      "Loss for batch 9 = 0.5123624801635742\n",
      "Loss for batch 10 = 0.28048157691955566\n",
      "Loss for batch 11 = 0.7768787145614624\n",
      "Loss for batch 12 = 0.7756981253623962\n",
      "Loss for batch 13 = 0.6116435527801514\n",
      "Loss for batch 14 = 0.5146292448043823\n",
      "Loss for batch 15 = 0.7125495672225952\n",
      "Loss for batch 16 = 0.7596063613891602\n",
      "Loss for batch 17 = 0.4879618287086487\n",
      "Loss for batch 18 = 0.6187223792076111\n",
      "Loss for batch 19 = 0.4570249319076538\n",
      "Loss for batch 20 = 0.47231000661849976\n",
      "Loss for batch 21 = 0.4956691265106201\n",
      "Loss for batch 22 = 0.9620038866996765\n",
      "Loss for batch 23 = 0.38100892305374146\n",
      "Loss for batch 24 = 0.5401490330696106\n",
      "Loss for batch 25 = 0.3790687620639801\n",
      "Loss for batch 26 = 0.5442666411399841\n",
      "Loss for batch 27 = 0.45239123702049255\n",
      "Loss for batch 28 = 0.3760937452316284\n",
      "Loss for batch 29 = 0.7854101061820984\n",
      "Loss for batch 30 = 0.5682170391082764\n",
      "Loss for batch 31 = 0.8752671480178833\n",
      "Loss for batch 32 = 0.3631412386894226\n",
      "Loss for batch 33 = 0.6100671887397766\n",
      "Loss for batch 34 = 0.5700712203979492\n",
      "Loss for batch 35 = 0.645349383354187\n",
      "Loss for batch 36 = 0.4512753188610077\n",
      "Loss for batch 37 = 0.6405913233757019\n",
      "Loss for batch 38 = 0.5553784370422363\n",
      "Loss for batch 39 = 0.3645668923854828\n",
      "Loss for batch 40 = 0.4233849346637726\n",
      "Loss for batch 41 = 0.49036654829978943\n",
      "Loss for batch 42 = 0.48470598459243774\n",
      "Loss for batch 43 = 0.5499625205993652\n",
      "Loss for batch 44 = 0.5193148851394653\n",
      "Loss for batch 45 = 0.5810744166374207\n",
      "Loss for batch 46 = 1.0160950422286987\n",
      "Loss for batch 47 = 0.42508456110954285\n",
      "Loss for batch 48 = 0.2895456552505493\n",
      "Loss for batch 49 = 0.6803092956542969\n",
      "Loss for batch 50 = 0.5574235916137695\n",
      "Loss for batch 51 = 0.46899569034576416\n",
      "Loss for batch 52 = 0.5907711386680603\n",
      "Loss for batch 53 = 0.5696018934249878\n",
      "Loss for batch 54 = 0.5874345302581787\n",
      "Loss for batch 55 = 0.6100524067878723\n",
      "Loss for batch 56 = 0.48012226819992065\n",
      "Loss for batch 57 = 0.4532243013381958\n",
      "Loss for batch 58 = 0.47359922528266907\n",
      "Loss for batch 59 = 0.716378927230835\n",
      "Loss for batch 60 = 0.2450043112039566\n",
      "Loss for batch 61 = 0.674246072769165\n",
      "Loss for batch 62 = 0.6523080468177795\n",
      "Loss for batch 63 = 0.30939310789108276\n",
      "Loss for batch 64 = 0.48381587862968445\n",
      "Loss for batch 65 = 0.5199324488639832\n",
      "Loss for batch 66 = 0.8025988340377808\n",
      "Loss for batch 67 = 0.4012123942375183\n",
      "Loss for batch 68 = 0.6019327044487\n",
      "Loss for batch 69 = 0.3397402763366699\n",
      "Loss for batch 70 = 0.5179556608200073\n",
      "Loss for batch 71 = 0.47397419810295105\n",
      "Loss for batch 72 = 0.43524542450904846\n",
      "Loss for batch 73 = 0.49810558557510376\n",
      "Loss for batch 74 = 0.6971083879470825\n",
      "Loss for batch 75 = 0.7496331930160522\n",
      "Loss for batch 76 = 0.6965591311454773\n",
      "Loss for batch 77 = 0.7908372282981873\n",
      "Loss for batch 78 = 0.5691171884536743\n",
      "Loss for batch 79 = 0.4639664888381958\n",
      "Loss for batch 80 = 0.4984346926212311\n",
      "Loss for batch 81 = 0.5112184882164001\n",
      "Loss for batch 82 = 0.4216975271701813\n",
      "Loss for batch 83 = 0.5161617398262024\n",
      "Loss for batch 84 = 0.6146687865257263\n",
      "Loss for batch 85 = 0.8331754803657532\n",
      "Loss for batch 86 = 0.5838204622268677\n",
      "Loss for batch 87 = 0.507014274597168\n",
      "Loss for batch 88 = 0.481915146112442\n",
      "Loss for batch 89 = 0.6578456163406372\n",
      "Loss for batch 90 = 0.46768537163734436\n",
      "Loss for batch 91 = 0.4643622040748596\n",
      "Loss for batch 92 = 0.6270866394042969\n",
      "Loss for batch 93 = 0.6154995560646057\n",
      "Loss for batch 94 = 0.6519600749015808\n",
      "Loss for batch 95 = 0.8206388354301453\n",
      "Loss for batch 96 = 0.5203467607498169\n",
      "Loss for batch 97 = 0.4993894696235657\n",
      "Loss for batch 98 = 0.5878920555114746\n",
      "Loss for batch 99 = 0.7358107566833496\n",
      "Loss for batch 100 = 0.5146948099136353\n",
      "Loss for batch 101 = 0.5763444900512695\n",
      "Loss for batch 102 = 0.44444459676742554\n",
      "Loss for batch 103 = 0.7343205809593201\n",
      "Loss for batch 104 = 0.37720197439193726\n",
      "Loss for batch 105 = 0.6930579543113708\n",
      "Loss for batch 106 = 0.4931612014770508\n",
      "\n",
      "Training Loss for epoch 26 = 59.35261917114258\n",
      "\n",
      "Current Validation Loss = 13.523947715759277\n",
      "Best Validation Loss = 12.45102310180664\n",
      "Epochs without Improvement = 9\n",
      "Train Accuracy: 77.26%\n",
      "Validation Accuracy: 63.86%\n",
      "\n",
      "Epoch 27\n",
      "----------\n",
      "Loss for batch 0 = 0.5849695801734924\n",
      "Loss for batch 1 = 0.2872045934200287\n",
      "Loss for batch 2 = 0.4132827818393707\n",
      "Loss for batch 3 = 0.41677212715148926\n",
      "Loss for batch 4 = 0.37615057826042175\n",
      "Loss for batch 5 = 0.46109771728515625\n",
      "Loss for batch 6 = 0.6216245293617249\n",
      "Loss for batch 7 = 0.7259640693664551\n",
      "Loss for batch 8 = 0.4882182478904724\n",
      "Loss for batch 9 = 0.4257783889770508\n",
      "Loss for batch 10 = 0.3630128502845764\n",
      "Loss for batch 11 = 0.7464635372161865\n",
      "Loss for batch 12 = 0.6507980227470398\n",
      "Loss for batch 13 = 0.5411686897277832\n",
      "Loss for batch 14 = 0.41222479939460754\n",
      "Loss for batch 15 = 0.7082145810127258\n",
      "Loss for batch 16 = 0.6279503703117371\n",
      "Loss for batch 17 = 0.4689755439758301\n",
      "Loss for batch 18 = 0.47374802827835083\n",
      "Loss for batch 19 = 0.3624034523963928\n",
      "Loss for batch 20 = 0.44207847118377686\n",
      "Loss for batch 21 = 0.3789915144443512\n",
      "Loss for batch 22 = 0.9701053500175476\n",
      "Loss for batch 23 = 0.38418012857437134\n",
      "Loss for batch 24 = 0.4611135423183441\n",
      "Loss for batch 25 = 0.41140449047088623\n",
      "Loss for batch 26 = 0.4676647186279297\n",
      "Loss for batch 27 = 0.3841416835784912\n",
      "Loss for batch 28 = 0.49358636140823364\n",
      "Loss for batch 29 = 0.6804513931274414\n",
      "Loss for batch 30 = 0.4863916039466858\n",
      "Loss for batch 31 = 0.8082258701324463\n",
      "Loss for batch 32 = 0.34059634804725647\n",
      "Loss for batch 33 = 0.5044814348220825\n",
      "Loss for batch 34 = 0.5462396144866943\n",
      "Loss for batch 35 = 0.5707425475120544\n",
      "Loss for batch 36 = 0.41963598132133484\n",
      "Loss for batch 37 = 0.4883060157299042\n",
      "Loss for batch 38 = 0.4649067223072052\n",
      "Loss for batch 39 = 0.30100733041763306\n",
      "Loss for batch 40 = 0.38645440340042114\n",
      "Loss for batch 41 = 0.35670003294944763\n",
      "Loss for batch 42 = 0.46478068828582764\n",
      "Loss for batch 43 = 0.4516657590866089\n",
      "Loss for batch 44 = 0.42562776803970337\n",
      "Loss for batch 45 = 0.4810566008090973\n",
      "Loss for batch 46 = 0.7311344742774963\n",
      "Loss for batch 47 = 0.29597070813179016\n",
      "Loss for batch 48 = 0.22703847289085388\n",
      "Loss for batch 49 = 0.6548212766647339\n",
      "Loss for batch 50 = 0.4437475800514221\n",
      "Loss for batch 51 = 0.38928017020225525\n",
      "Loss for batch 52 = 0.521471381187439\n",
      "Loss for batch 53 = 0.5100183486938477\n",
      "Loss for batch 54 = 0.5573087930679321\n",
      "Loss for batch 55 = 0.5065664052963257\n",
      "Loss for batch 56 = 0.38542288541793823\n",
      "Loss for batch 57 = 0.365707129240036\n",
      "Loss for batch 58 = 0.36652106046676636\n",
      "Loss for batch 59 = 0.6566219329833984\n",
      "Loss for batch 60 = 0.21516533195972443\n",
      "Loss for batch 61 = 0.608167290687561\n",
      "Loss for batch 62 = 0.5672172904014587\n",
      "Loss for batch 63 = 0.2404850870370865\n",
      "Loss for batch 64 = 0.48346230387687683\n",
      "Loss for batch 65 = 0.48543301224708557\n",
      "Loss for batch 66 = 0.8032792806625366\n",
      "Loss for batch 67 = 0.27214348316192627\n",
      "Loss for batch 68 = 0.41490745544433594\n",
      "Loss for batch 69 = 0.3005909025669098\n",
      "Loss for batch 70 = 0.479549765586853\n",
      "Loss for batch 71 = 0.45169246196746826\n",
      "Loss for batch 72 = 0.33754411339759827\n",
      "Loss for batch 73 = 0.41130122542381287\n",
      "Loss for batch 74 = 0.5711450576782227\n",
      "Loss for batch 75 = 0.6750723719596863\n",
      "Loss for batch 76 = 0.5937254428863525\n",
      "Loss for batch 77 = 0.7053744792938232\n",
      "Loss for batch 78 = 0.5192258954048157\n",
      "Loss for batch 79 = 0.42889681458473206\n",
      "Loss for batch 80 = 0.4472142159938812\n",
      "Loss for batch 81 = 0.33181023597717285\n",
      "Loss for batch 82 = 0.39974695444107056\n",
      "Loss for batch 83 = 0.4275414049625397\n",
      "Loss for batch 84 = 0.5312235951423645\n",
      "Loss for batch 85 = 0.7616210579872131\n",
      "Loss for batch 86 = 0.5370305776596069\n",
      "Loss for batch 87 = 0.4807664453983307\n",
      "Loss for batch 88 = 0.40920090675354004\n",
      "Loss for batch 89 = 0.4907959997653961\n",
      "Loss for batch 90 = 0.3437643349170685\n",
      "Loss for batch 91 = 0.39723506569862366\n",
      "Loss for batch 92 = 0.5428152084350586\n",
      "Loss for batch 93 = 0.4682976305484772\n",
      "Loss for batch 94 = 0.5184289216995239\n",
      "Loss for batch 95 = 0.7314027547836304\n",
      "Loss for batch 96 = 0.37152299284935\n",
      "Loss for batch 97 = 0.40562060475349426\n",
      "Loss for batch 98 = 0.5243895649909973\n",
      "Loss for batch 99 = 0.6804511547088623\n",
      "Loss for batch 100 = 0.4953548312187195\n",
      "Loss for batch 101 = 0.5457269549369812\n",
      "Loss for batch 102 = 0.3971228003501892\n",
      "Loss for batch 103 = 0.6711527109146118\n",
      "Loss for batch 104 = 0.23299214243888855\n",
      "Loss for batch 105 = 0.5992834568023682\n",
      "Loss for batch 106 = 0.33532431721687317\n",
      "\n",
      "Training Loss for epoch 27 = 51.980411529541016\n",
      "\n",
      "Current Validation Loss = 14.090445518493652\n",
      "Best Validation Loss = 12.45102310180664\n",
      "Epochs without Improvement = 10\n",
      "Train Accuracy: 81.60%\n",
      "Validation Accuracy: 65.09%\n",
      "\n",
      "Epoch 28\n",
      "----------\n",
      "Loss for batch 0 = 0.4115186333656311\n",
      "Loss for batch 1 = 0.21857398748397827\n",
      "Loss for batch 2 = 0.3160061538219452\n",
      "Loss for batch 3 = 0.3728533089160919\n",
      "Loss for batch 4 = 0.31474173069000244\n",
      "Loss for batch 5 = 0.4350370168685913\n",
      "Loss for batch 6 = 0.5229071378707886\n",
      "Loss for batch 7 = 0.5960380434989929\n",
      "Loss for batch 8 = 0.5018764138221741\n",
      "Loss for batch 9 = 0.3296029269695282\n",
      "Loss for batch 10 = 0.28793174028396606\n",
      "Loss for batch 11 = 0.6363633871078491\n",
      "Loss for batch 12 = 0.6035913825035095\n",
      "Loss for batch 13 = 0.4353103041648865\n",
      "Loss for batch 14 = 0.33886072039604187\n",
      "Loss for batch 15 = 0.6615185141563416\n",
      "Loss for batch 16 = 0.5725451111793518\n",
      "Loss for batch 17 = 0.44983816146850586\n",
      "Loss for batch 18 = 0.4294596314430237\n",
      "Loss for batch 19 = 0.32709208130836487\n",
      "Loss for batch 20 = 0.35331016778945923\n",
      "Loss for batch 21 = 0.3155391216278076\n",
      "Loss for batch 22 = 0.9284583330154419\n",
      "Loss for batch 23 = 0.26179251074790955\n",
      "Loss for batch 24 = 0.43425342440605164\n",
      "Loss for batch 25 = 0.30972200632095337\n",
      "Loss for batch 26 = 0.4054577052593231\n",
      "Loss for batch 27 = 0.4046173691749573\n",
      "Loss for batch 28 = 0.25729575753211975\n",
      "Loss for batch 29 = 0.6449217200279236\n",
      "Loss for batch 30 = 0.4703601896762848\n",
      "Loss for batch 31 = 0.7654252052307129\n",
      "Loss for batch 32 = 0.3164674937725067\n",
      "Loss for batch 33 = 0.4616384506225586\n",
      "Loss for batch 34 = 0.4964544177055359\n",
      "Loss for batch 35 = 0.6292031407356262\n",
      "Loss for batch 36 = 0.36036059260368347\n",
      "Loss for batch 37 = 0.4426434636116028\n",
      "Loss for batch 38 = 0.4278044104576111\n",
      "Loss for batch 39 = 0.22800645232200623\n",
      "Loss for batch 40 = 0.3486228883266449\n",
      "Loss for batch 41 = 0.3124394416809082\n",
      "Loss for batch 42 = 0.3207376003265381\n",
      "Loss for batch 43 = 0.3963967263698578\n",
      "Loss for batch 44 = 0.43244749307632446\n",
      "Loss for batch 45 = 0.41993895173072815\n",
      "Loss for batch 46 = 0.7265039682388306\n",
      "Loss for batch 47 = 0.2678203582763672\n",
      "Loss for batch 48 = 0.2217368483543396\n",
      "Loss for batch 49 = 0.5505191087722778\n",
      "Loss for batch 50 = 0.4489533007144928\n",
      "Loss for batch 51 = 0.3721788227558136\n",
      "Loss for batch 52 = 0.46476954221725464\n",
      "Loss for batch 53 = 0.4718016982078552\n",
      "Loss for batch 54 = 0.5157634019851685\n",
      "Loss for batch 55 = 0.4248277246952057\n",
      "Loss for batch 56 = 0.34452301263809204\n",
      "Loss for batch 57 = 0.4223966598510742\n",
      "Loss for batch 58 = 0.3239304721355438\n",
      "Loss for batch 59 = 0.6024142503738403\n",
      "Loss for batch 60 = 0.19922392070293427\n",
      "Loss for batch 61 = 0.6583112478256226\n",
      "Loss for batch 62 = 0.5197314023971558\n",
      "Loss for batch 63 = 0.20296409726142883\n",
      "Loss for batch 64 = 0.4626699388027191\n",
      "Loss for batch 65 = 0.4311162233352661\n",
      "Loss for batch 66 = 0.6749725937843323\n",
      "Loss for batch 67 = 0.26940688490867615\n",
      "Loss for batch 68 = 0.37338724732398987\n",
      "Loss for batch 69 = 0.2638397812843323\n",
      "Loss for batch 70 = 0.4529903829097748\n",
      "Loss for batch 71 = 0.4073147475719452\n",
      "Loss for batch 72 = 0.28828325867652893\n",
      "Loss for batch 73 = 0.37490424513816833\n",
      "Loss for batch 74 = 0.5717913508415222\n",
      "Loss for batch 75 = 0.5840093493461609\n",
      "Loss for batch 76 = 0.5660311579704285\n",
      "Loss for batch 77 = 0.7106723785400391\n",
      "Loss for batch 78 = 0.4504101872444153\n",
      "Loss for batch 79 = 0.4237375855445862\n",
      "Loss for batch 80 = 0.4351096749305725\n",
      "Loss for batch 81 = 0.2724939286708832\n",
      "Loss for batch 82 = 0.3932839035987854\n",
      "Loss for batch 83 = 0.4423699676990509\n",
      "Loss for batch 84 = 0.5210863351821899\n",
      "Loss for batch 85 = 0.7005502581596375\n",
      "Loss for batch 86 = 0.5367224812507629\n",
      "Loss for batch 87 = 0.43153131008148193\n",
      "Loss for batch 88 = 0.35714492201805115\n",
      "Loss for batch 89 = 0.42568331956863403\n",
      "Loss for batch 90 = 0.3282315731048584\n",
      "Loss for batch 91 = 0.3955945074558258\n",
      "Loss for batch 92 = 0.49585047364234924\n",
      "Loss for batch 93 = 0.3947056531906128\n",
      "Loss for batch 94 = 0.48491719365119934\n",
      "Loss for batch 95 = 0.6978357434272766\n",
      "Loss for batch 96 = 0.37076812982559204\n",
      "Loss for batch 97 = 0.3865072727203369\n",
      "Loss for batch 98 = 0.5513343811035156\n",
      "Loss for batch 99 = 0.5925058722496033\n",
      "Loss for batch 100 = 0.44745466113090515\n",
      "Loss for batch 101 = 0.5402570962905884\n",
      "Loss for batch 102 = 0.36922675371170044\n",
      "Loss for batch 103 = 0.6198672652244568\n",
      "Loss for batch 104 = 0.17489729821681976\n",
      "Loss for batch 105 = 0.5976848006248474\n",
      "Loss for batch 106 = 0.25861290097236633\n",
      "\n",
      "Training Loss for epoch 28 = 47.172119140625\n",
      "\n",
      "Current Validation Loss = 14.667952537536621\n",
      "Best Validation Loss = 12.45102310180664\n",
      "Epochs without Improvement = 11\n",
      "Train Accuracy: 83.27%\n",
      "Validation Accuracy: 63.04%\n",
      "\n",
      "Epoch 29\n",
      "----------\n",
      "Loss for batch 0 = 0.42619436979293823\n",
      "Loss for batch 1 = 0.19268302619457245\n",
      "Loss for batch 2 = 0.22806110978126526\n",
      "Loss for batch 3 = 0.2962878346443176\n",
      "Loss for batch 4 = 0.31420889496803284\n",
      "Loss for batch 5 = 0.4094003140926361\n",
      "Loss for batch 6 = 0.38543587923049927\n",
      "Loss for batch 7 = 0.5573661923408508\n",
      "Loss for batch 8 = 0.4248361885547638\n",
      "Loss for batch 9 = 0.3292134702205658\n",
      "Loss for batch 10 = 0.2695848345756531\n",
      "Loss for batch 11 = 0.6954222321510315\n",
      "Loss for batch 12 = 0.5545752644538879\n",
      "Loss for batch 13 = 0.38948509097099304\n",
      "Loss for batch 14 = 0.40173202753067017\n",
      "Loss for batch 15 = 0.6308898329734802\n",
      "Loss for batch 16 = 0.578235387802124\n",
      "Loss for batch 17 = 0.43559885025024414\n",
      "Loss for batch 18 = 0.4807433784008026\n",
      "Loss for batch 19 = 0.35143741965293884\n",
      "Loss for batch 20 = 0.2973341643810272\n",
      "Loss for batch 21 = 0.340436726808548\n",
      "Loss for batch 22 = 0.8229351043701172\n",
      "Loss for batch 23 = 0.21344098448753357\n",
      "Loss for batch 24 = 0.3877691626548767\n",
      "Loss for batch 25 = 0.26936376094818115\n",
      "Loss for batch 26 = 0.37192124128341675\n",
      "Loss for batch 27 = 0.3434654474258423\n",
      "Loss for batch 28 = 0.25304967164993286\n",
      "Loss for batch 29 = 0.5459076166152954\n",
      "Loss for batch 30 = 0.34909918904304504\n",
      "Loss for batch 31 = 0.7991620898246765\n",
      "Loss for batch 32 = 0.2933657765388489\n",
      "Loss for batch 33 = 0.4327184855937958\n",
      "Loss for batch 34 = 0.4607408940792084\n",
      "Loss for batch 35 = 0.5004046559333801\n",
      "Loss for batch 36 = 0.3979214131832123\n",
      "Loss for batch 37 = 0.4814324378967285\n",
      "Loss for batch 38 = 0.4019940197467804\n",
      "Loss for batch 39 = 0.2674075663089752\n",
      "Loss for batch 40 = 0.3124634623527527\n",
      "Loss for batch 41 = 0.39067384600639343\n",
      "Loss for batch 42 = 0.32836392521858215\n",
      "Loss for batch 43 = 0.35939353704452515\n",
      "Loss for batch 44 = 0.3999027907848358\n",
      "Loss for batch 45 = 0.45297887921333313\n",
      "Loss for batch 46 = 0.659436821937561\n",
      "Loss for batch 47 = 0.22600547969341278\n",
      "Loss for batch 48 = 0.18673072755336761\n",
      "Loss for batch 49 = 0.5070345401763916\n",
      "Loss for batch 50 = 0.5067885518074036\n",
      "Loss for batch 51 = 0.3343951106071472\n",
      "Loss for batch 52 = 0.4035217761993408\n",
      "Loss for batch 53 = 0.5482468008995056\n",
      "Loss for batch 54 = 0.4721139073371887\n",
      "Loss for batch 55 = 0.42645588517189026\n",
      "Loss for batch 56 = 0.311160147190094\n",
      "Loss for batch 57 = 0.3201579749584198\n",
      "Loss for batch 58 = 0.30325332283973694\n",
      "Loss for batch 59 = 0.6390010714530945\n",
      "Loss for batch 60 = 0.2057107537984848\n",
      "Loss for batch 61 = 0.5988854765892029\n",
      "Loss for batch 62 = 0.48645198345184326\n",
      "Loss for batch 63 = 0.18082556128501892\n",
      "Loss for batch 64 = 0.48002880811691284\n",
      "Loss for batch 65 = 0.4318814277648926\n",
      "Loss for batch 66 = 0.7040870785713196\n",
      "Loss for batch 67 = 0.279432088136673\n",
      "Loss for batch 68 = 0.3284250497817993\n",
      "Loss for batch 69 = 0.2530753016471863\n",
      "Loss for batch 70 = 0.4753277897834778\n",
      "Loss for batch 71 = 0.43363338708877563\n",
      "Loss for batch 72 = 0.19538676738739014\n",
      "Loss for batch 73 = 0.4272787570953369\n",
      "Loss for batch 74 = 0.4732232689857483\n",
      "Loss for batch 75 = 0.5958273410797119\n",
      "Loss for batch 76 = 0.5318499803543091\n",
      "Loss for batch 77 = 0.7114765644073486\n",
      "Loss for batch 78 = 0.4545060098171234\n",
      "Loss for batch 79 = 0.3877442181110382\n",
      "Loss for batch 80 = 0.34191200137138367\n",
      "Loss for batch 81 = 0.30986908078193665\n",
      "Loss for batch 82 = 0.35982710123062134\n",
      "Loss for batch 83 = 0.35382646322250366\n",
      "Loss for batch 84 = 0.41256383061408997\n",
      "Loss for batch 85 = 0.6409176588058472\n",
      "Loss for batch 86 = 0.5254436731338501\n",
      "Loss for batch 87 = 0.4220096170902252\n",
      "Loss for batch 88 = 0.3359200358390808\n",
      "Loss for batch 89 = 0.4057874381542206\n",
      "Loss for batch 90 = 0.2800448536872864\n",
      "Loss for batch 91 = 0.289069265127182\n",
      "Loss for batch 92 = 0.455221027135849\n",
      "Loss for batch 93 = 0.36393582820892334\n",
      "Loss for batch 94 = 0.4626553952693939\n",
      "Loss for batch 95 = 0.7587517499923706\n",
      "Loss for batch 96 = 0.3495486378669739\n",
      "Loss for batch 97 = 0.315121591091156\n",
      "Loss for batch 98 = 0.5632998943328857\n",
      "Loss for batch 99 = 0.5498294830322266\n",
      "Loss for batch 100 = 0.4290437698364258\n",
      "Loss for batch 101 = 0.5387563109397888\n",
      "Loss for batch 102 = 0.34143343567848206\n",
      "Loss for batch 103 = 0.5775215029716492\n",
      "Loss for batch 104 = 0.2001134753227234\n",
      "Loss for batch 105 = 0.5967644453048706\n",
      "Loss for batch 106 = 0.2066064178943634\n",
      "\n",
      "Training Loss for epoch 29 = 44.69019317626953\n",
      "\n",
      "Current Validation Loss = 14.593121528625488\n",
      "Best Validation Loss = 12.45102310180664\n",
      "Epochs without Improvement = 12\n",
      "Train Accuracy: 84.77%\n",
      "Validation Accuracy: 64.07%\n",
      "\n",
      "Epoch 30\n",
      "----------\n",
      "Loss for batch 0 = 0.3741120994091034\n",
      "Loss for batch 1 = 0.15601590275764465\n",
      "Loss for batch 2 = 0.22680842876434326\n",
      "Loss for batch 3 = 0.3030875027179718\n",
      "Loss for batch 4 = 0.30652400851249695\n",
      "Loss for batch 5 = 0.4520142674446106\n",
      "Loss for batch 6 = 0.26492634415626526\n",
      "Loss for batch 7 = 0.530794084072113\n",
      "Loss for batch 8 = 0.37133094668388367\n",
      "Loss for batch 9 = 0.32692211866378784\n",
      "Loss for batch 10 = 0.2613314986228943\n",
      "Loss for batch 11 = 0.626939594745636\n",
      "Loss for batch 12 = 0.4915179908275604\n",
      "Loss for batch 13 = 0.4524172842502594\n",
      "Loss for batch 14 = 0.3507339656352997\n",
      "Loss for batch 15 = 0.6244751811027527\n",
      "Loss for batch 16 = 0.5503717064857483\n",
      "Loss for batch 17 = 0.41418176889419556\n",
      "Loss for batch 18 = 0.42485564947128296\n",
      "Loss for batch 19 = 0.34852054715156555\n",
      "Loss for batch 20 = 0.30195286870002747\n",
      "Loss for batch 21 = 0.29209303855895996\n",
      "Loss for batch 22 = 0.8419796824455261\n",
      "Loss for batch 23 = 0.21441267430782318\n",
      "Loss for batch 24 = 0.3355465531349182\n",
      "Loss for batch 25 = 0.2700568437576294\n",
      "Loss for batch 26 = 0.32992124557495117\n",
      "Loss for batch 27 = 0.3170057535171509\n",
      "Loss for batch 28 = 0.23527568578720093\n",
      "Loss for batch 29 = 0.4680774509906769\n",
      "Loss for batch 30 = 0.3075319528579712\n",
      "Loss for batch 31 = 0.7757763266563416\n",
      "Loss for batch 32 = 0.28698745369911194\n",
      "Loss for batch 33 = 0.3869182765483856\n",
      "Loss for batch 34 = 0.46515941619873047\n",
      "Loss for batch 35 = 0.6554014682769775\n",
      "Loss for batch 36 = 0.3274243175983429\n",
      "Loss for batch 37 = 0.41167739033699036\n",
      "Loss for batch 38 = 0.4429148733615875\n",
      "Loss for batch 39 = 0.1865341067314148\n",
      "Loss for batch 40 = 0.3503073751926422\n",
      "Loss for batch 41 = 0.27196547389030457\n",
      "Loss for batch 42 = 0.33542463183403015\n",
      "Loss for batch 43 = 0.3116317689418793\n",
      "Loss for batch 44 = 0.4014095366001129\n",
      "Loss for batch 45 = 0.37277454137802124\n",
      "Loss for batch 46 = 0.6541972160339355\n",
      "Loss for batch 47 = 0.25156542658805847\n",
      "Loss for batch 48 = 0.16915620863437653\n",
      "Loss for batch 49 = 0.4760563373565674\n",
      "Loss for batch 50 = 0.42710816860198975\n",
      "Loss for batch 51 = 0.319030225276947\n",
      "Loss for batch 52 = 0.5496050715446472\n",
      "Loss for batch 53 = 0.38437047600746155\n",
      "Loss for batch 54 = 0.4713376462459564\n",
      "Loss for batch 55 = 0.37176039814949036\n",
      "Loss for batch 56 = 0.2788148522377014\n",
      "Loss for batch 57 = 0.267876535654068\n",
      "Loss for batch 58 = 0.30874019861221313\n",
      "Loss for batch 59 = 0.6039571762084961\n",
      "Loss for batch 60 = 0.254741370677948\n",
      "Loss for batch 61 = 0.610299289226532\n",
      "Loss for batch 62 = 0.44033703207969666\n",
      "Loss for batch 63 = 0.15786300599575043\n",
      "Loss for batch 64 = 0.4651659429073334\n",
      "Loss for batch 65 = 0.36497265100479126\n",
      "Loss for batch 66 = 0.6369533538818359\n",
      "Loss for batch 67 = 0.30015116930007935\n",
      "Loss for batch 68 = 0.343515008687973\n",
      "Loss for batch 69 = 0.24134737253189087\n",
      "Loss for batch 70 = 0.40201520919799805\n",
      "Loss for batch 71 = 0.3954698145389557\n",
      "Loss for batch 72 = 0.2050764113664627\n",
      "Loss for batch 73 = 0.35458219051361084\n",
      "Loss for batch 74 = 0.46788567304611206\n",
      "Loss for batch 75 = 0.5282031893730164\n",
      "Loss for batch 76 = 0.5343819856643677\n",
      "Loss for batch 77 = 0.5877134203910828\n",
      "Loss for batch 78 = 0.44636431336402893\n",
      "Loss for batch 79 = 0.3856503963470459\n",
      "Loss for batch 80 = 0.29698681831359863\n",
      "Loss for batch 81 = 0.25352898240089417\n",
      "Loss for batch 82 = 0.34501391649246216\n",
      "Loss for batch 83 = 0.3551284074783325\n",
      "Loss for batch 84 = 0.3974183201789856\n",
      "Loss for batch 85 = 0.603765070438385\n",
      "Loss for batch 86 = 0.4970959424972534\n",
      "Loss for batch 87 = 0.3863397538661957\n",
      "Loss for batch 88 = 0.32760947942733765\n",
      "Loss for batch 89 = 0.3553362488746643\n",
      "Loss for batch 90 = 0.2465069741010666\n",
      "Loss for batch 91 = 0.357845276594162\n",
      "Loss for batch 92 = 0.4374080300331116\n",
      "Loss for batch 93 = 0.34461838006973267\n",
      "Loss for batch 94 = 0.4097258448600769\n",
      "Loss for batch 95 = 0.6036123633384705\n",
      "Loss for batch 96 = 0.32045596837997437\n",
      "Loss for batch 97 = 0.393941193819046\n",
      "Loss for batch 98 = 0.5879161953926086\n",
      "Loss for batch 99 = 0.5607726573944092\n",
      "Loss for batch 100 = 0.38917186856269836\n",
      "Loss for batch 101 = 0.46704161167144775\n",
      "Loss for batch 102 = 0.35921457409858704\n",
      "Loss for batch 103 = 0.48438379168510437\n",
      "Loss for batch 104 = 0.1750805526971817\n",
      "Loss for batch 105 = 0.5675172209739685\n",
      "Loss for batch 106 = 0.27432548999786377\n",
      "\n",
      "Training Loss for epoch 30 = 42.208072662353516\n",
      "\n",
      "Current Validation Loss = 15.41381549835205\n",
      "Best Validation Loss = 12.45102310180664\n",
      "Epochs without Improvement = 13\n",
      "Train Accuracy: 85.39%\n",
      "Validation Accuracy: 62.01%\n",
      "\n",
      "Epoch 31\n",
      "----------\n",
      "Loss for batch 0 = 0.5209330320358276\n",
      "Loss for batch 1 = 0.1612871289253235\n",
      "Loss for batch 2 = 0.2751675546169281\n",
      "Loss for batch 3 = 0.27657827734947205\n",
      "Loss for batch 4 = 0.2954493761062622\n",
      "Loss for batch 5 = 0.4756861925125122\n",
      "Loss for batch 6 = 0.19558948278427124\n",
      "Loss for batch 7 = 0.5256817936897278\n",
      "Loss for batch 8 = 0.3245542645454407\n",
      "Loss for batch 9 = 0.2981565296649933\n",
      "Loss for batch 10 = 0.2637070417404175\n",
      "Loss for batch 11 = 0.5996105670928955\n",
      "Loss for batch 12 = 0.4522291421890259\n",
      "Loss for batch 13 = 0.4540097415447235\n",
      "Loss for batch 14 = 0.30989792943000793\n",
      "Loss for batch 15 = 0.6493933796882629\n",
      "Loss for batch 16 = 0.502199113368988\n",
      "Loss for batch 17 = 0.38704368472099304\n",
      "Loss for batch 18 = 0.3490945100784302\n",
      "Loss for batch 19 = 0.2695150673389435\n",
      "Loss for batch 20 = 0.276123583316803\n",
      "Loss for batch 21 = 0.2471935898065567\n",
      "Loss for batch 22 = 0.7527458071708679\n",
      "Loss for batch 23 = 0.1754327118396759\n",
      "Loss for batch 24 = 0.30219876766204834\n",
      "Loss for batch 25 = 0.2544151842594147\n",
      "Loss for batch 26 = 0.3345043659210205\n",
      "Loss for batch 27 = 0.30774444341659546\n",
      "Loss for batch 28 = 0.20378124713897705\n",
      "Loss for batch 29 = 0.5197132229804993\n",
      "Loss for batch 30 = 0.3366466164588928\n",
      "Loss for batch 31 = 0.6748241782188416\n",
      "Loss for batch 32 = 0.21000748872756958\n",
      "Loss for batch 33 = 0.3109148144721985\n",
      "Loss for batch 34 = 0.4537784159183502\n",
      "Loss for batch 35 = 0.4734516143798828\n",
      "Loss for batch 36 = 0.31490615010261536\n",
      "Loss for batch 37 = 0.5052072405815125\n",
      "Loss for batch 38 = 0.4175902307033539\n",
      "Loss for batch 39 = 0.15137876570224762\n",
      "Loss for batch 40 = 0.33512216806411743\n",
      "Loss for batch 41 = 0.24437181651592255\n",
      "Loss for batch 42 = 0.2160203605890274\n",
      "Loss for batch 43 = 0.2956448197364807\n",
      "Loss for batch 44 = 0.36152026057243347\n",
      "Loss for batch 45 = 0.42204612493515015\n",
      "Loss for batch 46 = 0.6091815233230591\n",
      "Loss for batch 47 = 0.18193377554416656\n",
      "Loss for batch 48 = 0.2276318073272705\n",
      "Loss for batch 49 = 0.4729936122894287\n",
      "Loss for batch 50 = 0.35799747705459595\n",
      "Loss for batch 51 = 0.273758202791214\n",
      "Loss for batch 52 = 0.46466702222824097\n",
      "Loss for batch 53 = 0.3788808286190033\n",
      "Loss for batch 54 = 0.4145212769508362\n",
      "Loss for batch 55 = 0.37265917658805847\n",
      "Loss for batch 56 = 0.2229023277759552\n",
      "Loss for batch 57 = 0.26024043560028076\n",
      "Loss for batch 58 = 0.23969358205795288\n",
      "Loss for batch 59 = 0.43911877274513245\n",
      "Loss for batch 60 = 0.18355774879455566\n",
      "Loss for batch 61 = 0.5355361700057983\n",
      "Loss for batch 62 = 0.46438068151474\n",
      "Loss for batch 63 = 0.12170126289129257\n",
      "Loss for batch 64 = 0.49282902479171753\n",
      "Loss for batch 65 = 0.40221425890922546\n",
      "Loss for batch 66 = 0.6195732355117798\n",
      "Loss for batch 67 = 0.32218867540359497\n",
      "Loss for batch 68 = 0.28341731429100037\n",
      "Loss for batch 69 = 0.17489036917686462\n",
      "Loss for batch 70 = 0.3748414218425751\n",
      "Loss for batch 71 = 0.2994729280471802\n",
      "Loss for batch 72 = 0.1502285599708557\n",
      "Loss for batch 73 = 0.3589686453342438\n",
      "Loss for batch 74 = 0.43662336468696594\n",
      "Loss for batch 75 = 0.47526097297668457\n",
      "Loss for batch 76 = 0.43417224287986755\n",
      "Loss for batch 77 = 0.57687908411026\n",
      "Loss for batch 78 = 0.427002489566803\n",
      "Loss for batch 79 = 0.36292195320129395\n",
      "Loss for batch 80 = 0.2644941210746765\n",
      "Loss for batch 81 = 0.2744125425815582\n",
      "Loss for batch 82 = 0.33120831847190857\n",
      "Loss for batch 83 = 0.29468804597854614\n",
      "Loss for batch 84 = 0.37442252039909363\n",
      "Loss for batch 85 = 0.5699549317359924\n",
      "Loss for batch 86 = 0.5133882164955139\n",
      "Loss for batch 87 = 0.33803632855415344\n",
      "Loss for batch 88 = 0.30746734142303467\n",
      "Loss for batch 89 = 0.2907749116420746\n",
      "Loss for batch 90 = 0.20836810767650604\n",
      "Loss for batch 91 = 0.29076647758483887\n",
      "Loss for batch 92 = 0.43727222084999084\n",
      "Loss for batch 93 = 0.29944220185279846\n",
      "Loss for batch 94 = 0.3745233714580536\n",
      "Loss for batch 95 = 0.644228458404541\n",
      "Loss for batch 96 = 0.3146265745162964\n",
      "Loss for batch 97 = 0.2985016107559204\n",
      "Loss for batch 98 = 0.4927991032600403\n",
      "Loss for batch 99 = 0.48896870017051697\n",
      "Loss for batch 100 = 0.34785547852516174\n",
      "Loss for batch 101 = 0.33767223358154297\n",
      "Loss for batch 102 = 0.3076018989086151\n",
      "Loss for batch 103 = 0.48712262511253357\n",
      "Loss for batch 104 = 0.17234522104263306\n",
      "Loss for batch 105 = 0.5437204837799072\n",
      "Loss for batch 106 = 0.1610264778137207\n",
      "\n",
      "Training Loss for epoch 31 = 38.86159133911133\n",
      "\n",
      "Current Validation Loss = 16.398601531982422\n",
      "Best Validation Loss = 12.45102310180664\n",
      "Epochs without Improvement = 14\n",
      "Train Accuracy: 87.18%\n",
      "Validation Accuracy: 62.42%\n",
      "\n",
      "Epoch 32\n",
      "----------\n",
      "Loss for batch 0 = 0.3938669264316559\n",
      "Loss for batch 1 = 0.1301639974117279\n",
      "Loss for batch 2 = 0.22305597364902496\n",
      "Loss for batch 3 = 0.14990635216236115\n",
      "Loss for batch 4 = 0.17134220898151398\n",
      "Loss for batch 5 = 0.4080072045326233\n",
      "Loss for batch 6 = 0.15495797991752625\n",
      "Loss for batch 7 = 0.5221174955368042\n",
      "Loss for batch 8 = 0.4402701258659363\n",
      "Loss for batch 9 = 0.2941320836544037\n",
      "Loss for batch 10 = 0.2699962258338928\n",
      "Loss for batch 11 = 0.4985893964767456\n",
      "Loss for batch 12 = 0.5022991895675659\n",
      "Loss for batch 13 = 0.41155561804771423\n",
      "Loss for batch 14 = 0.1987009048461914\n",
      "Loss for batch 15 = 0.8479722738265991\n",
      "Loss for batch 16 = 0.5631167888641357\n",
      "Loss for batch 17 = 0.3729480504989624\n",
      "Loss for batch 18 = 0.4170498549938202\n",
      "Loss for batch 19 = 0.2870206832885742\n",
      "Loss for batch 20 = 0.30053505301475525\n",
      "Loss for batch 21 = 0.2677503824234009\n",
      "Loss for batch 22 = 0.5624961853027344\n",
      "Loss for batch 23 = 0.26987937092781067\n",
      "Loss for batch 24 = 0.2987019121646881\n",
      "Loss for batch 25 = 0.22320589423179626\n",
      "Loss for batch 26 = 0.2748095393180847\n",
      "Loss for batch 27 = 0.3053327798843384\n",
      "Loss for batch 28 = 0.17727667093276978\n",
      "Loss for batch 29 = 0.4831581711769104\n",
      "Loss for batch 30 = 0.31492143869400024\n",
      "Loss for batch 31 = 0.640968918800354\n",
      "Loss for batch 32 = 0.26499417424201965\n",
      "Loss for batch 33 = 0.2747100293636322\n",
      "Loss for batch 34 = 0.3996983468532562\n",
      "Loss for batch 35 = 0.4487048387527466\n",
      "Loss for batch 36 = 0.2957572340965271\n",
      "Loss for batch 37 = 0.396598756313324\n",
      "Loss for batch 38 = 0.4262025058269501\n",
      "Loss for batch 39 = 0.15102198719978333\n",
      "Loss for batch 40 = 0.2968643605709076\n",
      "Loss for batch 41 = 0.22549667954444885\n",
      "Loss for batch 42 = 0.19706258177757263\n",
      "Loss for batch 43 = 0.2389334887266159\n",
      "Loss for batch 44 = 0.4013541042804718\n",
      "Loss for batch 45 = 0.3919793665409088\n",
      "Loss for batch 46 = 0.5673690438270569\n",
      "Loss for batch 47 = 0.20373407006263733\n",
      "Loss for batch 48 = 0.1468147486448288\n",
      "Loss for batch 49 = 0.4828644394874573\n",
      "Loss for batch 50 = 0.3377496302127838\n",
      "Loss for batch 51 = 0.2659299969673157\n",
      "Loss for batch 52 = 0.4414937198162079\n",
      "Loss for batch 53 = 0.2292131930589676\n",
      "Loss for batch 54 = 0.41681766510009766\n",
      "Loss for batch 55 = 0.23510442674160004\n",
      "Loss for batch 56 = 0.2046985775232315\n",
      "Loss for batch 57 = 0.27151086926460266\n",
      "Loss for batch 58 = 0.16471156477928162\n",
      "Loss for batch 59 = 0.4508886933326721\n",
      "Loss for batch 60 = 0.18343009054660797\n",
      "Loss for batch 61 = 0.4875015616416931\n",
      "Loss for batch 62 = 0.39805224537849426\n",
      "Loss for batch 63 = 0.10098239779472351\n",
      "Loss for batch 64 = 0.46354085206985474\n",
      "Loss for batch 65 = 0.35415777564048767\n",
      "Loss for batch 66 = 0.5976688861846924\n",
      "Loss for batch 67 = 0.23760417103767395\n",
      "Loss for batch 68 = 0.3108714818954468\n",
      "Loss for batch 69 = 0.1589762419462204\n",
      "Loss for batch 70 = 0.36024078726768494\n",
      "Loss for batch 71 = 0.314502090215683\n",
      "Loss for batch 72 = 0.14652717113494873\n",
      "Loss for batch 73 = 0.3591037094593048\n",
      "Loss for batch 74 = 0.3599921464920044\n",
      "Loss for batch 75 = 0.5328254103660583\n",
      "Loss for batch 76 = 0.4308336079120636\n",
      "Loss for batch 77 = 0.4512644410133362\n",
      "Loss for batch 78 = 0.4159930646419525\n",
      "Loss for batch 79 = 0.29418399930000305\n",
      "Loss for batch 80 = 0.26140880584716797\n",
      "Loss for batch 81 = 0.16850939393043518\n",
      "Loss for batch 82 = 0.3682103455066681\n",
      "Loss for batch 83 = 0.3774715065956116\n",
      "Loss for batch 84 = 0.4371504783630371\n",
      "Loss for batch 85 = 0.5898081064224243\n",
      "Loss for batch 86 = 0.49506834149360657\n",
      "Loss for batch 87 = 0.3032761812210083\n",
      "Loss for batch 88 = 0.2584517300128937\n",
      "Loss for batch 89 = 0.32874852418899536\n",
      "Loss for batch 90 = 0.21119678020477295\n",
      "Loss for batch 91 = 0.313968688249588\n",
      "Loss for batch 92 = 0.4881727993488312\n",
      "Loss for batch 93 = 0.2962256968021393\n",
      "Loss for batch 94 = 0.3382512331008911\n",
      "Loss for batch 95 = 0.6030250191688538\n",
      "Loss for batch 96 = 0.34947705268859863\n",
      "Loss for batch 97 = 0.2788912057876587\n",
      "Loss for batch 98 = 0.5502901077270508\n",
      "Loss for batch 99 = 0.49113014340400696\n",
      "Loss for batch 100 = 0.30866822600364685\n",
      "Loss for batch 101 = 0.4885115623474121\n",
      "Loss for batch 102 = 0.3676214814186096\n",
      "Loss for batch 103 = 0.510684072971344\n",
      "Loss for batch 104 = 0.12036865204572678\n",
      "Loss for batch 105 = 0.515112578868866\n",
      "Loss for batch 106 = 0.15997184813022614\n",
      "\n",
      "Training Loss for epoch 32 = 37.14231491088867\n",
      "\n",
      "Current Validation Loss = 15.879958152770996\n",
      "Best Validation Loss = 12.45102310180664\n",
      "Epochs without Improvement = 15\n",
      "Train Accuracy: 87.79%\n",
      "Validation Accuracy: 63.04%\n",
      "\n",
      "Epoch 33\n",
      "----------\n",
      "Loss for batch 0 = 0.30355319380760193\n",
      "Loss for batch 1 = 0.10690497606992722\n",
      "Loss for batch 2 = 0.21225376427173615\n",
      "Loss for batch 3 = 0.22992247343063354\n",
      "Loss for batch 4 = 0.19064238667488098\n",
      "Loss for batch 5 = 0.32689157128334045\n",
      "Loss for batch 6 = 0.2339748740196228\n",
      "Loss for batch 7 = 0.4032839834690094\n",
      "Loss for batch 8 = 0.29214340448379517\n",
      "Loss for batch 9 = 0.2982715666294098\n",
      "Loss for batch 10 = 0.348294734954834\n",
      "Loss for batch 11 = 0.45935508608818054\n",
      "Loss for batch 12 = 0.5494836568832397\n",
      "Loss for batch 13 = 0.39344656467437744\n",
      "Loss for batch 14 = 0.22656074166297913\n",
      "Loss for batch 15 = 0.690483033657074\n",
      "Loss for batch 16 = 0.571094810962677\n",
      "Loss for batch 17 = 0.3488028645515442\n",
      "Loss for batch 18 = 0.2984068989753723\n",
      "Loss for batch 19 = 0.22502444684505463\n",
      "Loss for batch 20 = 0.20411282777786255\n",
      "Loss for batch 21 = 0.1835169941186905\n",
      "Loss for batch 22 = 0.5969028472900391\n",
      "Loss for batch 23 = 0.12891297042369843\n",
      "Loss for batch 24 = 0.2133164256811142\n",
      "Loss for batch 25 = 0.1902449131011963\n",
      "Loss for batch 26 = 0.2825792729854584\n",
      "Loss for batch 27 = 0.27896955609321594\n",
      "Loss for batch 28 = 0.2625555396080017\n",
      "Loss for batch 29 = 0.36272764205932617\n",
      "Loss for batch 30 = 0.2978532016277313\n",
      "Loss for batch 31 = 0.7235048413276672\n",
      "Loss for batch 32 = 0.15518932044506073\n",
      "Loss for batch 33 = 0.26733630895614624\n",
      "Loss for batch 34 = 0.38803979754447937\n",
      "Loss for batch 35 = 0.4090386927127838\n",
      "Loss for batch 36 = 0.22781984508037567\n",
      "Loss for batch 37 = 0.4397525489330292\n",
      "Loss for batch 38 = 0.4265201985836029\n",
      "Loss for batch 39 = 0.07372084259986877\n",
      "Loss for batch 40 = 0.2588614821434021\n",
      "Loss for batch 41 = 0.19872668385505676\n",
      "Loss for batch 42 = 0.18532684445381165\n",
      "Loss for batch 43 = 0.20109699666500092\n",
      "Loss for batch 44 = 0.32893919944763184\n",
      "Loss for batch 45 = 0.45762598514556885\n",
      "Loss for batch 46 = 0.5800181031227112\n",
      "Loss for batch 47 = 0.1910708248615265\n",
      "Loss for batch 48 = 0.11921507865190506\n",
      "Loss for batch 49 = 0.42428651452064514\n",
      "Loss for batch 50 = 0.3566107153892517\n",
      "Loss for batch 51 = 0.24556098878383636\n",
      "Loss for batch 52 = 0.3760560154914856\n",
      "Loss for batch 53 = 0.3643602132797241\n",
      "Loss for batch 54 = 0.4949485659599304\n",
      "Loss for batch 55 = 0.25436028838157654\n",
      "Loss for batch 56 = 0.22404083609580994\n",
      "Loss for batch 57 = 0.25141388177871704\n",
      "Loss for batch 58 = 0.18309558928012848\n",
      "Loss for batch 59 = 0.3541747033596039\n",
      "Loss for batch 60 = 0.15686562657356262\n",
      "Loss for batch 61 = 0.4495575726032257\n",
      "Loss for batch 62 = 0.3928396701812744\n",
      "Loss for batch 63 = 0.09813211113214493\n",
      "Loss for batch 64 = 0.45182719826698303\n",
      "Loss for batch 65 = 0.5045879483222961\n",
      "Loss for batch 66 = 0.7144818305969238\n",
      "Loss for batch 67 = 0.20563580095767975\n",
      "Loss for batch 68 = 0.3212476968765259\n",
      "Loss for batch 69 = 0.1789419949054718\n",
      "Loss for batch 70 = 0.36249375343322754\n",
      "Loss for batch 71 = 0.29069089889526367\n",
      "Loss for batch 72 = 0.14703962206840515\n",
      "Loss for batch 73 = 0.3332637846469879\n",
      "Loss for batch 74 = 0.42771613597869873\n",
      "Loss for batch 75 = 0.4076528251171112\n",
      "Loss for batch 76 = 0.39194563031196594\n",
      "Loss for batch 77 = 0.6017020344734192\n",
      "Loss for batch 78 = 0.4266069233417511\n",
      "Loss for batch 79 = 0.20479057729244232\n",
      "Loss for batch 80 = 0.26825979351997375\n",
      "Loss for batch 81 = 0.15992039442062378\n",
      "Loss for batch 82 = 0.33171460032463074\n",
      "Loss for batch 83 = 0.3011215627193451\n",
      "Loss for batch 84 = 0.2753002941608429\n",
      "Loss for batch 85 = 0.6103476881980896\n",
      "Loss for batch 86 = 0.44308558106422424\n",
      "Loss for batch 87 = 0.28034475445747375\n",
      "Loss for batch 88 = 0.24939540028572083\n",
      "Loss for batch 89 = 0.24235202372074127\n",
      "Loss for batch 90 = 0.2154398113489151\n",
      "Loss for batch 91 = 0.32682713866233826\n",
      "Loss for batch 92 = 0.38228046894073486\n",
      "Loss for batch 93 = 0.26616889238357544\n",
      "Loss for batch 94 = 0.3718845844268799\n",
      "Loss for batch 95 = 0.5417270660400391\n",
      "Loss for batch 96 = 0.3904682993888855\n",
      "Loss for batch 97 = 0.2193663865327835\n",
      "Loss for batch 98 = 0.4729245901107788\n",
      "Loss for batch 99 = 0.5656206607818604\n",
      "Loss for batch 100 = 0.3830776810646057\n",
      "Loss for batch 101 = 0.2788996398448944\n",
      "Loss for batch 102 = 0.25105607509613037\n",
      "Loss for batch 103 = 0.4317854046821594\n",
      "Loss for batch 104 = 0.23653675615787506\n",
      "Loss for batch 105 = 0.5477228760719299\n",
      "Loss for batch 106 = 0.12346918880939484\n",
      "\n",
      "Training Loss for epoch 33 = 35.10631561279297\n",
      "\n",
      "Current Validation Loss = 17.063644409179688\n",
      "Best Validation Loss = 12.45102310180664\n",
      "Epochs without Improvement = 16\n",
      "Train Accuracy: 88.35%\n",
      "Validation Accuracy: 64.48%\n",
      "\n",
      "Epoch 34\n",
      "----------\n",
      "Loss for batch 0 = 0.2946694791316986\n",
      "Loss for batch 1 = 0.07531611621379852\n",
      "Loss for batch 2 = 0.2241862565279007\n",
      "Loss for batch 3 = 0.2126288115978241\n",
      "Loss for batch 4 = 0.14256766438484192\n",
      "Loss for batch 5 = 0.36981552839279175\n",
      "Loss for batch 6 = 0.12112953513860703\n",
      "Loss for batch 7 = 0.42788180708885193\n",
      "Loss for batch 8 = 0.27606311440467834\n",
      "Loss for batch 9 = 0.22762568295001984\n",
      "Loss for batch 10 = 0.09256774187088013\n",
      "Loss for batch 11 = 0.49258747696876526\n",
      "Loss for batch 12 = 0.39569205045700073\n",
      "Loss for batch 13 = 0.4429982602596283\n",
      "Loss for batch 14 = 0.2080436646938324\n",
      "Loss for batch 15 = 0.6025307774543762\n",
      "Loss for batch 16 = 0.41772711277008057\n",
      "Loss for batch 17 = 0.3915878236293793\n",
      "Loss for batch 18 = 0.2669905126094818\n",
      "Loss for batch 19 = 0.1656336933374405\n",
      "Loss for batch 20 = 0.15960875153541565\n",
      "Loss for batch 21 = 0.1716437041759491\n",
      "Loss for batch 22 = 0.7076849341392517\n",
      "Loss for batch 23 = 0.1058368906378746\n",
      "Loss for batch 24 = 0.17213791608810425\n",
      "Loss for batch 25 = 0.15798816084861755\n",
      "Loss for batch 26 = 0.2428859919309616\n",
      "Loss for batch 27 = 0.2803415358066559\n",
      "Loss for batch 28 = 0.1260315477848053\n",
      "Loss for batch 29 = 0.5885246992111206\n",
      "Loss for batch 30 = 0.2829182744026184\n",
      "Loss for batch 31 = 0.572496771812439\n",
      "Loss for batch 32 = 0.11926671862602234\n",
      "Loss for batch 33 = 0.27672791481018066\n",
      "Loss for batch 34 = 0.4219132363796234\n",
      "Loss for batch 35 = 0.43049943447113037\n",
      "Loss for batch 36 = 0.4213721454143524\n",
      "Loss for batch 37 = 0.31785330176353455\n",
      "Loss for batch 38 = 0.3453540802001953\n",
      "Loss for batch 39 = 0.09292127192020416\n",
      "Loss for batch 40 = 0.29867076873779297\n",
      "Loss for batch 41 = 0.18328164517879486\n",
      "Loss for batch 42 = 0.1750258356332779\n",
      "Loss for batch 43 = 0.2182316929101944\n",
      "Loss for batch 44 = 0.49435916543006897\n",
      "Loss for batch 45 = 0.4007434844970703\n",
      "Loss for batch 46 = 0.854552686214447\n",
      "Loss for batch 47 = 0.19402991235256195\n",
      "Loss for batch 48 = 0.24151082336902618\n",
      "Loss for batch 49 = 0.34930405020713806\n",
      "Loss for batch 50 = 0.46043965220451355\n",
      "Loss for batch 51 = 0.285542756319046\n",
      "Loss for batch 52 = 0.444315105676651\n",
      "Loss for batch 53 = 0.38107869029045105\n",
      "Loss for batch 54 = 0.4563167691230774\n",
      "Loss for batch 55 = 0.29818981885910034\n",
      "Loss for batch 56 = 0.23342354595661163\n",
      "Loss for batch 57 = 0.17891934514045715\n",
      "Loss for batch 58 = 0.22684167325496674\n",
      "Loss for batch 59 = 0.37398818135261536\n",
      "Loss for batch 60 = 0.1308913677930832\n",
      "Loss for batch 61 = 0.4432932138442993\n",
      "Loss for batch 62 = 0.3997448980808258\n",
      "Loss for batch 63 = 0.10902204364538193\n",
      "Loss for batch 64 = 0.4511389434337616\n",
      "Loss for batch 65 = 0.4483879506587982\n",
      "Loss for batch 66 = 0.5324245095252991\n",
      "Loss for batch 67 = 0.21112412214279175\n",
      "Loss for batch 68 = 0.27063894271850586\n",
      "Loss for batch 69 = 0.2053186595439911\n",
      "Loss for batch 70 = 0.318217933177948\n",
      "Loss for batch 71 = 0.2749117910861969\n",
      "Loss for batch 72 = 0.20355963706970215\n",
      "Loss for batch 73 = 0.3124315142631531\n",
      "Loss for batch 74 = 0.3410818874835968\n",
      "Loss for batch 75 = 0.40722692012786865\n",
      "Loss for batch 76 = 0.3597571849822998\n",
      "Loss for batch 77 = 0.49403059482574463\n",
      "Loss for batch 78 = 0.5141584873199463\n",
      "Loss for batch 79 = 0.21012496948242188\n",
      "Loss for batch 80 = 0.23550257086753845\n",
      "Loss for batch 81 = 0.2771325409412384\n",
      "Loss for batch 82 = 0.3305147588253021\n",
      "Loss for batch 83 = 0.28923362493515015\n",
      "Loss for batch 84 = 0.2958611845970154\n",
      "Loss for batch 85 = 0.56440269947052\n",
      "Loss for batch 86 = 0.48751333355903625\n",
      "Loss for batch 87 = 0.24252758920192719\n",
      "Loss for batch 88 = 0.21216994524002075\n",
      "Loss for batch 89 = 0.23337732255458832\n",
      "Loss for batch 90 = 0.33406639099121094\n",
      "Loss for batch 91 = 0.24659612774848938\n",
      "Loss for batch 92 = 0.3640248775482178\n",
      "Loss for batch 93 = 0.3275625705718994\n",
      "Loss for batch 94 = 0.31807971000671387\n",
      "Loss for batch 95 = 0.4676872193813324\n",
      "Loss for batch 96 = 0.25755733251571655\n",
      "Loss for batch 97 = 0.17774054408073425\n",
      "Loss for batch 98 = 0.45870575308799744\n",
      "Loss for batch 99 = 0.46018165349960327\n",
      "Loss for batch 100 = 0.33819398283958435\n",
      "Loss for batch 101 = 0.21940837800502777\n",
      "Loss for batch 102 = 0.18622539937496185\n",
      "Loss for batch 103 = 0.36119329929351807\n",
      "Loss for batch 104 = 0.08074069768190384\n",
      "Loss for batch 105 = 0.5188844203948975\n",
      "Loss for batch 106 = 0.07684159278869629\n",
      "\n",
      "Training Loss for epoch 34 = 33.590423583984375\n",
      "\n",
      "Current Validation Loss = 18.123828887939453\n",
      "Best Validation Loss = 12.45102310180664\n",
      "Epochs without Improvement = 17\n",
      "Train Accuracy: 90.08%\n",
      "Validation Accuracy: 64.27%\n",
      "\n",
      "Epoch 35\n",
      "----------\n",
      "Loss for batch 0 = 0.30642735958099365\n",
      "Loss for batch 1 = 0.06454144418239594\n",
      "Loss for batch 2 = 0.25649112462997437\n",
      "Loss for batch 3 = 0.14680221676826477\n",
      "Loss for batch 4 = 0.16082997620105743\n",
      "Loss for batch 5 = 0.3569725453853607\n",
      "Loss for batch 6 = 0.12801940739154816\n",
      "Loss for batch 7 = 0.3061310946941376\n",
      "Loss for batch 8 = 0.33630555868148804\n",
      "Loss for batch 9 = 0.1900171935558319\n",
      "Loss for batch 10 = 0.11239273846149445\n",
      "Loss for batch 11 = 0.43389642238616943\n",
      "Loss for batch 12 = 0.28758952021598816\n",
      "Loss for batch 13 = 0.3056302070617676\n",
      "Loss for batch 14 = 0.32686281204223633\n",
      "Loss for batch 15 = 0.6041256785392761\n",
      "Loss for batch 16 = 0.4349837005138397\n",
      "Loss for batch 17 = 0.3347526788711548\n",
      "Loss for batch 18 = 0.24624623358249664\n",
      "Loss for batch 19 = 0.14353272318840027\n",
      "Loss for batch 20 = 0.14131729304790497\n",
      "Loss for batch 21 = 0.16734597086906433\n",
      "Loss for batch 22 = 0.48329252004623413\n",
      "Loss for batch 23 = 0.08076368272304535\n",
      "Loss for batch 24 = 0.15206065773963928\n",
      "Loss for batch 25 = 0.13954411447048187\n",
      "Loss for batch 26 = 0.24343262612819672\n",
      "Loss for batch 27 = 0.24521932005882263\n",
      "Loss for batch 28 = 0.11472474038600922\n",
      "Loss for batch 29 = 0.2468818724155426\n",
      "Loss for batch 30 = 0.22866015136241913\n",
      "Loss for batch 31 = 0.5693939924240112\n",
      "Loss for batch 32 = 0.08938099443912506\n",
      "Loss for batch 33 = 0.2729390859603882\n",
      "Loss for batch 34 = 0.47345083951950073\n",
      "Loss for batch 35 = 0.31487059593200684\n",
      "Loss for batch 36 = 0.1393132507801056\n",
      "Loss for batch 37 = 0.32544398307800293\n",
      "Loss for batch 38 = 0.33256447315216064\n",
      "Loss for batch 39 = 0.048168331384658813\n",
      "Loss for batch 40 = 0.307830274105072\n",
      "Loss for batch 41 = 0.20198188722133636\n",
      "Loss for batch 42 = 0.1912047415971756\n",
      "Loss for batch 43 = 0.16326741874217987\n",
      "Loss for batch 44 = 0.40174081921577454\n",
      "Loss for batch 45 = 0.34425631165504456\n",
      "Loss for batch 46 = 0.6374369859695435\n",
      "Loss for batch 47 = 0.06677085906267166\n",
      "Loss for batch 48 = 0.15775610506534576\n",
      "Loss for batch 49 = 0.367946058511734\n",
      "Loss for batch 50 = 0.41128382086753845\n",
      "Loss for batch 51 = 0.2714015245437622\n",
      "Loss for batch 52 = 0.40109983086586\n",
      "Loss for batch 53 = 0.21723413467407227\n",
      "Loss for batch 54 = 0.37560975551605225\n",
      "Loss for batch 55 = 0.23108318448066711\n",
      "Loss for batch 56 = 0.1341007649898529\n",
      "Loss for batch 57 = 0.21453288197517395\n",
      "Loss for batch 58 = 0.1703626960515976\n",
      "Loss for batch 59 = 0.4385647177696228\n",
      "Loss for batch 60 = 0.15888218581676483\n",
      "Loss for batch 61 = 0.4440392553806305\n",
      "Loss for batch 62 = 0.3558695614337921\n",
      "Loss for batch 63 = 0.1324520707130432\n",
      "Loss for batch 64 = 0.3985782861709595\n",
      "Loss for batch 65 = 0.45686089992523193\n",
      "Loss for batch 66 = 0.5311795473098755\n",
      "Loss for batch 67 = 0.23346981406211853\n",
      "Loss for batch 68 = 0.34965917468070984\n",
      "Loss for batch 69 = 0.2127530425786972\n",
      "Loss for batch 70 = 0.30766376852989197\n",
      "Loss for batch 71 = 0.22639012336730957\n",
      "Loss for batch 72 = 0.1137743890285492\n",
      "Loss for batch 73 = 0.3034871518611908\n",
      "Loss for batch 74 = 0.3158375918865204\n",
      "Loss for batch 75 = 0.4950309991836548\n",
      "Loss for batch 76 = 0.35679545998573303\n",
      "Loss for batch 77 = 0.5179478526115417\n",
      "Loss for batch 78 = 0.432429701089859\n",
      "Loss for batch 79 = 0.21496613323688507\n",
      "Loss for batch 80 = 0.2069474458694458\n",
      "Loss for batch 81 = 0.167141854763031\n",
      "Loss for batch 82 = 0.25632786750793457\n",
      "Loss for batch 83 = 0.29282835125923157\n",
      "Loss for batch 84 = 0.2484162300825119\n",
      "Loss for batch 85 = 0.5250371098518372\n",
      "Loss for batch 86 = 0.43583378195762634\n",
      "Loss for batch 87 = 0.17384114861488342\n",
      "Loss for batch 88 = 0.20600388944149017\n",
      "Loss for batch 89 = 0.2558212876319885\n",
      "Loss for batch 90 = 0.12271199375391006\n",
      "Loss for batch 91 = 0.2529919147491455\n",
      "Loss for batch 92 = 0.34330299496650696\n",
      "Loss for batch 93 = 0.3005112111568451\n",
      "Loss for batch 94 = 0.29326432943344116\n",
      "Loss for batch 95 = 0.5055773854255676\n",
      "Loss for batch 96 = 0.3317294716835022\n",
      "Loss for batch 97 = 0.22419650852680206\n",
      "Loss for batch 98 = 0.4245681166648865\n",
      "Loss for batch 99 = 0.40584298968315125\n",
      "Loss for batch 100 = 0.30824315547943115\n",
      "Loss for batch 101 = 0.21089263260364532\n",
      "Loss for batch 102 = 0.1519436091184616\n",
      "Loss for batch 103 = 0.2974153161048889\n",
      "Loss for batch 104 = 0.07156765460968018\n",
      "Loss for batch 105 = 0.46672961115837097\n",
      "Loss for batch 106 = 0.09330537915229797\n",
      "\n",
      "Training Loss for epoch 35 = 30.12784194946289\n",
      "\n",
      "Current Validation Loss = 18.431495666503906\n",
      "Best Validation Loss = 12.45102310180664\n",
      "Epochs without Improvement = 18\n",
      "Train Accuracy: 90.93%\n",
      "Validation Accuracy: 63.66%\n",
      "\n",
      "Epoch 36\n",
      "----------\n",
      "Loss for batch 0 = 0.25458475947380066\n",
      "Loss for batch 1 = 0.06020418554544449\n",
      "Loss for batch 2 = 0.1668604016304016\n",
      "Loss for batch 3 = 0.13334877789020538\n",
      "Loss for batch 4 = 0.08529087156057358\n",
      "Loss for batch 5 = 0.2667229473590851\n",
      "Loss for batch 6 = 0.14842896163463593\n",
      "Loss for batch 7 = 0.4969157874584198\n",
      "Loss for batch 8 = 0.22155189514160156\n",
      "Loss for batch 9 = 0.16934418678283691\n",
      "Loss for batch 10 = 0.1043969988822937\n",
      "Loss for batch 11 = 0.4434262812137604\n",
      "Loss for batch 12 = 0.3032565414905548\n",
      "Loss for batch 13 = 0.3470737338066101\n",
      "Loss for batch 14 = 0.061959948390722275\n",
      "Loss for batch 15 = 0.6420501470565796\n",
      "Loss for batch 16 = 0.4371697008609772\n",
      "Loss for batch 17 = 0.260903000831604\n",
      "Loss for batch 18 = 0.18429243564605713\n",
      "Loss for batch 19 = 0.18235823512077332\n",
      "Loss for batch 20 = 0.08778905868530273\n",
      "Loss for batch 21 = 0.13608486950397491\n",
      "Loss for batch 22 = 0.4616682231426239\n",
      "Loss for batch 23 = 0.14177268743515015\n",
      "Loss for batch 24 = 0.13781467080116272\n",
      "Loss for batch 25 = 0.09698299318552017\n",
      "Loss for batch 26 = 0.18059653043746948\n",
      "Loss for batch 27 = 0.29129558801651\n",
      "Loss for batch 28 = 0.11722585558891296\n",
      "Loss for batch 29 = 0.23686689138412476\n",
      "Loss for batch 30 = 0.274264395236969\n",
      "Loss for batch 31 = 0.4349139630794525\n",
      "Loss for batch 32 = 0.11786387115716934\n",
      "Loss for batch 33 = 0.1641620397567749\n",
      "Loss for batch 34 = 0.325608491897583\n",
      "Loss for batch 35 = 0.43325167894363403\n",
      "Loss for batch 36 = 0.20059078931808472\n",
      "Loss for batch 37 = 0.3415232002735138\n",
      "Loss for batch 38 = 0.3034791052341461\n",
      "Loss for batch 39 = 0.05524005368351936\n",
      "Loss for batch 40 = 0.19847942888736725\n",
      "Loss for batch 41 = 0.18948763608932495\n",
      "Loss for batch 42 = 0.20171934366226196\n",
      "Loss for batch 43 = 0.16279704868793488\n",
      "Loss for batch 44 = 0.32956328988075256\n",
      "Loss for batch 45 = 0.297239750623703\n",
      "Loss for batch 46 = 0.5855763554573059\n",
      "Loss for batch 47 = 0.0495075061917305\n",
      "Loss for batch 48 = 0.09693677723407745\n",
      "Loss for batch 49 = 0.46337029337882996\n",
      "Loss for batch 50 = 0.3107951283454895\n",
      "Loss for batch 51 = 0.21645015478134155\n",
      "Loss for batch 52 = 0.46092721819877625\n",
      "Loss for batch 53 = 0.1697564721107483\n",
      "Loss for batch 54 = 0.3579222559928894\n",
      "Loss for batch 55 = 0.21497872471809387\n",
      "Loss for batch 56 = 0.13766510784626007\n",
      "Loss for batch 57 = 0.1730959266424179\n",
      "Loss for batch 58 = 0.10789522528648376\n",
      "Loss for batch 59 = 0.4461532235145569\n",
      "Loss for batch 60 = 0.13427366316318512\n",
      "Loss for batch 61 = 0.34608447551727295\n",
      "Loss for batch 62 = 0.35784900188446045\n",
      "Loss for batch 63 = 0.04905439540743828\n",
      "Loss for batch 64 = 0.3333113491535187\n",
      "Loss for batch 65 = 0.3685702383518219\n",
      "Loss for batch 66 = 0.5728338956832886\n",
      "Loss for batch 67 = 0.16657817363739014\n",
      "Loss for batch 68 = 0.38313791155815125\n",
      "Loss for batch 69 = 0.12714780867099762\n",
      "Loss for batch 70 = 0.3059264123439789\n",
      "Loss for batch 71 = 0.16388459503650665\n",
      "Loss for batch 72 = 0.2861504554748535\n",
      "Loss for batch 73 = 0.19363361597061157\n",
      "Loss for batch 74 = 0.3406296670436859\n",
      "Loss for batch 75 = 0.3725825250148773\n",
      "Loss for batch 76 = 0.40661701560020447\n",
      "Loss for batch 77 = 0.38682234287261963\n",
      "Loss for batch 78 = 0.4132401645183563\n",
      "Loss for batch 79 = 0.14414562284946442\n",
      "Loss for batch 80 = 0.20263798534870148\n",
      "Loss for batch 81 = 0.13215136528015137\n",
      "Loss for batch 82 = 0.2783292531967163\n",
      "Loss for batch 83 = 0.17676590383052826\n",
      "Loss for batch 84 = 0.2052023559808731\n",
      "Loss for batch 85 = 0.5198971033096313\n",
      "Loss for batch 86 = 0.3996964693069458\n",
      "Loss for batch 87 = 0.18443089723587036\n",
      "Loss for batch 88 = 0.1925821453332901\n",
      "Loss for batch 89 = 0.17392192780971527\n",
      "Loss for batch 90 = 0.11847719550132751\n",
      "Loss for batch 91 = 0.21545790135860443\n",
      "Loss for batch 92 = 0.3491043150424957\n",
      "Loss for batch 93 = 0.2152542918920517\n",
      "Loss for batch 94 = 0.26468148827552795\n",
      "Loss for batch 95 = 0.4238157868385315\n",
      "Loss for batch 96 = 0.20592212677001953\n",
      "Loss for batch 97 = 0.14527994394302368\n",
      "Loss for batch 98 = 0.33864670991897583\n",
      "Loss for batch 99 = 0.3450271785259247\n",
      "Loss for batch 100 = 0.25618526339530945\n",
      "Loss for batch 101 = 0.14115895330905914\n",
      "Loss for batch 102 = 0.1518186777830124\n",
      "Loss for batch 103 = 0.26645171642303467\n",
      "Loss for batch 104 = 0.06277735531330109\n",
      "Loss for batch 105 = 0.43042391538619995\n",
      "Loss for batch 106 = 0.059607915580272675\n",
      "\n",
      "Training Loss for epoch 36 = 26.98763084411621\n",
      "\n",
      "Current Validation Loss = 19.844573974609375\n",
      "Best Validation Loss = 12.45102310180664\n",
      "Epochs without Improvement = 19\n",
      "Train Accuracy: 92.75%\n",
      "Validation Accuracy: 62.63%\n",
      "\n",
      "Epoch 37\n",
      "----------\n",
      "Loss for batch 0 = 0.27635452151298523\n",
      "Loss for batch 1 = 0.059402696788311005\n",
      "Loss for batch 2 = 0.15358823537826538\n",
      "Loss for batch 3 = 0.12415122985839844\n",
      "Loss for batch 4 = 0.08001060038805008\n",
      "Loss for batch 5 = 0.2582348585128784\n",
      "Loss for batch 6 = 0.11110946536064148\n",
      "Loss for batch 7 = 0.3692720830440521\n",
      "Loss for batch 8 = 0.21500065922737122\n",
      "Loss for batch 9 = 0.168339341878891\n",
      "Loss for batch 10 = 0.10463408380746841\n",
      "Loss for batch 11 = 0.2997541129589081\n",
      "Loss for batch 12 = 0.24067826569080353\n",
      "Loss for batch 13 = 0.32032251358032227\n",
      "Loss for batch 14 = 0.053708266466856\n",
      "Loss for batch 15 = 0.5386896729469299\n",
      "Loss for batch 16 = 0.27362996339797974\n",
      "Loss for batch 17 = 0.2219664305448532\n",
      "Loss for batch 18 = 0.17075498402118683\n",
      "Loss for batch 19 = 0.09858982264995575\n",
      "Loss for batch 20 = 0.09885048866271973\n",
      "Loss for batch 21 = 0.13382773101329803\n",
      "Loss for batch 22 = 0.4373317062854767\n",
      "Loss for batch 23 = 0.12736886739730835\n",
      "Loss for batch 24 = 0.11123445630073547\n",
      "Loss for batch 25 = 0.08383531123399734\n",
      "Loss for batch 26 = 0.17231234908103943\n",
      "Loss for batch 27 = 0.21940724551677704\n",
      "Loss for batch 28 = 0.09981121122837067\n",
      "Loss for batch 29 = 0.1887199580669403\n",
      "Loss for batch 30 = 0.24372467398643494\n",
      "Loss for batch 31 = 0.3961182236671448\n",
      "Loss for batch 32 = 0.08358801156282425\n",
      "Loss for batch 33 = 0.14814557135105133\n",
      "Loss for batch 34 = 0.3023657500743866\n",
      "Loss for batch 35 = 0.3215494155883789\n",
      "Loss for batch 36 = 0.14568404853343964\n",
      "Loss for batch 37 = 0.2831018567085266\n",
      "Loss for batch 38 = 0.3055213987827301\n",
      "Loss for batch 39 = 0.060471925884485245\n",
      "Loss for batch 40 = 0.20231428742408752\n",
      "Loss for batch 41 = 0.16533507406711578\n",
      "Loss for batch 42 = 0.1280459463596344\n",
      "Loss for batch 43 = 0.1644410789012909\n",
      "Loss for batch 44 = 0.3322768807411194\n",
      "Loss for batch 45 = 0.20867598056793213\n",
      "Loss for batch 46 = 0.471456915140152\n",
      "Loss for batch 47 = 0.036465760320425034\n",
      "Loss for batch 48 = 0.07493856549263\n",
      "Loss for batch 49 = 0.36418724060058594\n",
      "Loss for batch 50 = 0.27308252453804016\n",
      "Loss for batch 51 = 0.20592430233955383\n",
      "Loss for batch 52 = 0.3772982060909271\n",
      "Loss for batch 53 = 0.11659234017133713\n",
      "Loss for batch 54 = 0.3537236750125885\n",
      "Loss for batch 55 = 0.20023342967033386\n",
      "Loss for batch 56 = 0.103831447660923\n",
      "Loss for batch 57 = 0.22527378797531128\n",
      "Loss for batch 58 = 0.11578624695539474\n",
      "Loss for batch 59 = 0.39305198192596436\n",
      "Loss for batch 60 = 0.1128687709569931\n",
      "Loss for batch 61 = 0.34234440326690674\n",
      "Loss for batch 62 = 0.3456149101257324\n",
      "Loss for batch 63 = 0.039395757019519806\n",
      "Loss for batch 64 = 0.297775000333786\n",
      "Loss for batch 65 = 0.3810294568538666\n",
      "Loss for batch 66 = 0.5055394172668457\n",
      "Loss for batch 67 = 0.16638697683811188\n",
      "Loss for batch 68 = 0.3692033886909485\n",
      "Loss for batch 69 = 0.14512178301811218\n",
      "Loss for batch 70 = 0.25473570823669434\n",
      "Loss for batch 71 = 0.1586223691701889\n",
      "Loss for batch 72 = 0.19652146100997925\n",
      "Loss for batch 73 = 0.17950749397277832\n",
      "Loss for batch 74 = 0.24036188423633575\n",
      "Loss for batch 75 = 0.3144785761833191\n",
      "Loss for batch 76 = 0.23395885527133942\n",
      "Loss for batch 77 = 0.370978981256485\n",
      "Loss for batch 78 = 0.39226484298706055\n",
      "Loss for batch 79 = 0.1006484255194664\n",
      "Loss for batch 80 = 0.20731934905052185\n",
      "Loss for batch 81 = 0.14482882618904114\n",
      "Loss for batch 82 = 0.26267004013061523\n",
      "Loss for batch 83 = 0.18401333689689636\n",
      "Loss for batch 84 = 0.18334440886974335\n",
      "Loss for batch 85 = 0.44729045033454895\n",
      "Loss for batch 86 = 0.3771604001522064\n",
      "Loss for batch 87 = 0.1579490751028061\n",
      "Loss for batch 88 = 0.18014636635780334\n",
      "Loss for batch 89 = 0.1281820386648178\n",
      "Loss for batch 90 = 0.07748834043741226\n",
      "Loss for batch 91 = 0.19399641454219818\n",
      "Loss for batch 92 = 0.32891184091567993\n",
      "Loss for batch 93 = 0.2102387398481369\n",
      "Loss for batch 94 = 0.2389863133430481\n",
      "Loss for batch 95 = 0.3304077982902527\n",
      "Loss for batch 96 = 0.17591536045074463\n",
      "Loss for batch 97 = 0.12337616831064224\n",
      "Loss for batch 98 = 0.3086956739425659\n",
      "Loss for batch 99 = 0.2790883779525757\n",
      "Loss for batch 100 = 0.21973522007465363\n",
      "Loss for batch 101 = 0.08709588646888733\n",
      "Loss for batch 102 = 0.11075232923030853\n",
      "Loss for batch 103 = 0.22929078340530396\n",
      "Loss for batch 104 = 0.0554913729429245\n",
      "Loss for batch 105 = 0.41013017296791077\n",
      "Loss for batch 106 = 0.045592814683914185\n",
      "\n",
      "Training Loss for epoch 37 = 23.459558486938477\n",
      "\n",
      "Current Validation Loss = 20.517431259155273\n",
      "Best Validation Loss = 12.45102310180664\n",
      "Epochs without Improvement = 20\n",
      "Train Accuracy: 94.28%\n",
      "Validation Accuracy: 62.42%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TrainLoop(modelv4, optimizer_4, loss_fn_4, train_loader, val_loader, scheduler_4, 100, 20, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 66.43%\n"
     ]
    }
   ],
   "source": [
    "# test_set = CustomTextClassificationDataset(test_x, test_y, word_to_index)\n",
    "# test_loader = DataLoader(test_set, 32)\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "with torch.inference_mode():\n",
    "    for batch in test_loader:\n",
    "        caption_tokens = batch['text_indices'].to('cuda')\n",
    "        labels = batch['label'].to('cuda', dtype=torch.long)\n",
    "        preds = torch.argmax(modelv4(caption_tokens), dim=1)\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        pred_labels.extend(preds.cpu().numpy())\n",
    "\n",
    "true_labels = np.array(true_labels)\n",
    "pred_labels = np.array(pred_labels)\n",
    "\n",
    "modelv4_accuracy = accuracy_score(true_labels, pred_labels)\n",
    "\n",
    "print(f'Accuracy: {modelv4_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelv5 = LSTMmodelv2(4, 25, 512, bidirectionality=True)\n",
    "loss_fn_5 = torch.nn.NLLLoss()\n",
    "optimizer_5 = torch.optim.NAdam(modelv5.parameters(), 0.001)\n",
    "scheduler_5 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_5, 'min', 0.4, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(layer):\n",
    "    if isinstance(layer, torch.nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(layer.weight)  # Xavier (Glorot) initialization\n",
    "        torch.nn.init.constant_(layer.bias, 0)  # Initialize biases to zero\n",
    "    elif isinstance(layer, torch.nn.LSTM):\n",
    "        # Initialize LSTM weights and biases\n",
    "        for name, param in layer.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                torch.nn.init.orthogonal_(param)  # Orthogonal initialization for LSTM weights\n",
    "            elif 'bias' in name:\n",
    "                torch.nn.init.constant_(param, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMmodelv2(\n",
       "  (embedding): Embedding(1193514, 25)\n",
       "  (lstm): LSTM(25, 512, num_layers=4, batch_first=True, bidirectional=True)\n",
       "  (linear): Linear(in_features=1024, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelv5.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 82.00 MiB. GPU 0 has a total capacty of 4.00 GiB of which 0 bytes is free. Of the allocated memory 3.26 GiB is allocated by PyTorch, and 129.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32mc:\\College\\Projects\\Multimodal Sentiment Analysis using Text and Images\\Notebooks\\nn_2.ipynb Cell 53\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/College/Projects/Multimodal%20Sentiment%20Analysis%20using%20Text%20and%20Images/Notebooks/nn_2.ipynb#Y152sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m TrainLoop(modelv5, optimizer_5, loss_fn_5, train_loader, val_loader, scheduler_5, \u001b[39m100\u001b[39;49m, \u001b[39m20\u001b[39;49m, device\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32mc:\\College\\Projects\\Multimodal Sentiment Analysis using Text and Images\\Notebooks\\nn_2.ipynb Cell 53\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/College/Projects/Multimodal%20Sentiment%20Analysis%20using%20Text%20and%20Images/Notebooks/nn_2.ipynb#Y152sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mTrainLoop\u001b[39m(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/College/Projects/Multimodal%20Sentiment%20Analysis%20using%20Text%20and%20Images/Notebooks/nn_2.ipynb#Y152sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     model,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/College/Projects/Multimodal%20Sentiment%20Analysis%20using%20Text%20and%20Images/Notebooks/nn_2.ipynb#Y152sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     optimizer:torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mOptimizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/College/Projects/Multimodal%20Sentiment%20Analysis%20using%20Text%20and%20Images/Notebooks/nn_2.ipynb#Y152sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     device:\u001b[39mstr\u001b[39m\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/College/Projects/Multimodal%20Sentiment%20Analysis%20using%20Text%20and%20Images/Notebooks/nn_2.ipynb#Y152sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m ):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/College/Projects/Multimodal%20Sentiment%20Analysis%20using%20Text%20and%20Images/Notebooks/nn_2.ipynb#Y152sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     model\u001b[39m.\u001b[39;49mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/College/Projects/Multimodal%20Sentiment%20Analysis%20using%20Text%20and%20Images/Notebooks/nn_2.ipynb#Y152sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     best_val_loss \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(\u001b[39m'\u001b[39m\u001b[39minf\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/College/Projects/Multimodal%20Sentiment%20Analysis%20using%20Text%20and%20Images/Notebooks/nn_2.ipynb#Y152sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     epochs_without_improvement \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1156\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m   1158\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m-> 1160\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[0;32m    809\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 810\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    812\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    813\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    814\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    815\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    820\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    821\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:218\u001b[0m, in \u001b[0;36mRNNBase._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    213\u001b[0m ret \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m_apply(fn, recurse)\n\u001b[0;32m    215\u001b[0m \u001b[39m# Resets _flat_weights\u001b[39;00m\n\u001b[0;32m    216\u001b[0m \u001b[39m# Note: be v. careful before removing this, as 3rd party device types\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \u001b[39m# likely rely on this behavior to properly .to() modules like LSTM.\u001b[39;00m\n\u001b[1;32m--> 218\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_init_flat_weights()\n\u001b[0;32m    220\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:155\u001b[0m, in \u001b[0;36mRNNBase._init_flat_weights\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights \u001b[39m=\u001b[39m [\u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, wn) \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, wn) \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    152\u001b[0m                       \u001b[39mfor\u001b[39;00m wn \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights_names]\n\u001b[0;32m    153\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weight_refs \u001b[39m=\u001b[39m [weakref\u001b[39m.\u001b[39mref(w) \u001b[39mif\u001b[39;00m w \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m                           \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights]\n\u001b[1;32m--> 155\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mflatten_parameters()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:206\u001b[0m, in \u001b[0;36mRNNBase.flatten_parameters\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproj_size \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    205\u001b[0m     num_weights \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 206\u001b[0m torch\u001b[39m.\u001b[39;49m_cudnn_rnn_flatten_weight(\n\u001b[0;32m    207\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, num_weights,\n\u001b[0;32m    208\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_size, rnn\u001b[39m.\u001b[39;49mget_cudnn_mode(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmode),\n\u001b[0;32m    209\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhidden_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mproj_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[0;32m    210\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first, \u001b[39mbool\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional))\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 82.00 MiB. GPU 0 has a total capacty of 4.00 GiB of which 0 bytes is free. Of the allocated memory 3.26 GiB is allocated by PyTorch, and 129.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "TrainLoop(modelv5, optimizer_5, loss_fn_5, train_loader, val_loader, scheduler_5, 100, 20, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "glove_path = \".vector_cache/glove.twitter.27B.50d.txt\"\n",
    "embeddings_index = {}\n",
    "with open(glove_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "word_to_index = {word: i for i, word in enumerate(embeddings_index.keys())}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data = pd.read_csv(\"../Data/Text/Engineered.csv\")\n",
    "data['Caption'] = data['Caption'].apply(clean_caption)\n",
    "data['Caption'] = data['Caption'].apply(word_tokenize)\n",
    "data['Caption'] = data['Caption'].apply(lambda x: pad_sequence(x, max_seq_length=30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(np.array(data['Caption']), y, test_size=0.2, shuffle=True, stratify=y, random_state=1)\n",
    "train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size=0.125, shuffle=True, stratify=train_y, random_state=1)\n",
    "train_set = CustomTextClassificationDataset(train_x, train_y, word_to_index)\n",
    "val_set = CustomTextClassificationDataset(val_x, val_y, word_to_index)\n",
    "train_loader = DataLoader(train_set, 32)\n",
    "val_loader = DataLoader(val_set, 32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
